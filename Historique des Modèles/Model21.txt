def Resnet_block(x, filters, size, num_comb,add=False):

    # Shortcut : si le nombre de filtres change, on l’adapte
    if x.shape[-1] != filters:
        shortcut = layers.Conv2D(filters, (1,1), padding='same', use_bias=False)(x)
        shortcut = layers.BatchNormalization()(shortcut)
    else:
        shortcut = x

    if (add==True):
      for i in range(0, num_comb):
          size = size + 2*i
          x = layers.Conv2D(filters, (size, size), padding='same', use_bias=False)(x)
          x = layers.BatchNormalization()(x)
          x = layers.Activation('relu')(x)
    else :
        for i in range(0, num_comb):
          x = layers.Conv2D(filters, (size, size), padding='same', use_bias=False)(x)
          x = layers.BatchNormalization()(x)
          x = layers.Activation('relu')(x)


    # Ajout du shortcut
    x = layers.BatchNormalization()(x)
    x = layers.add([x, shortcut])
    x = layers.Activation('relu')(x)

    return x

def model0(input_shape=(32,32,3), num_classes=10):
    inputs = layers.Input(shape=input_shape)


    FILTER = 20
    DROP = 0.25

    x = Resnet_block(inputs, filters=FILTER, size=3, num_comb=1)
    x = Resnet_block(x, filters=FILTER, size=3, num_comb=1)
    x = layers.Conv2D(FILTER, (3, 3), padding='same', use_bias=False)(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.MaxPooling2D((2,2))(x)
    x = layers.Dropout(DROP)(x)

    x = Resnet_block(x, filters=FILTER, size=3, num_comb=1)
    x = Resnet_block(x, filters=FILTER, size=3, num_comb=1)
    x = layers.Conv2D(FILTER, (3, 3), padding='same', use_bias=False)(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.MaxPooling2D((2,2))(x)
    x = layers.Dropout(DROP)(x)



    # Classification finale
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dense(128  , activation='relu')(x)
    x = layers.Dropout(DROP*(1.3))(x)
    outputs = layers.Dense(10, activation='softmax')(x)

    model = models.Model(inputs, outputs, name="Simple_ResNet")
    return model

#Paramètre entrainement:
#def step_decay(epoch):
#   initial_lr = 0.02
#   drop = 0.96
#   epochs_drop = 1
#   lr = initial_lr * (drop ** (epoch // epochs_drop))
#   return lr
#   Epoch 100 ; batch 64