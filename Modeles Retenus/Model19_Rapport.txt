

Analyzing model 
C:/Users/localgroup/STM32Cube/Repository/Packs/STMicroelectronics/X-CUBE-AI/10.2.0/Utilities/windows/stedgeai.exe analyze --target stm32l4 --name cifar10 -m C:/Users/localgroup/Downloads/RN_7951_233K.h5 --compression high --verbosity 1 --workspace C:/Users/LOCALG~1/AppData/Local/Temp/mxAI_workspace28769613133004082734791410699902 --output C:/Users/localgroup/.stm32cubemx/cifar10_output 
ST Edge AI Core v2.2.0-20266 2adc00962 
Creating c (debug) info json file C:\Users\localgroup\.stm32cubemx\cifar10_output\cifar10_c_info.json 
  
 Exec/report summary (analyze) 
 ------------------------------------------------------------------------------------------------------------- 
 model file         :   C:\Users\localgroup\Downloads\RN_7951_233K.h5                                          
 type               :   keras                                                                                  
 c_name             :   cifar10                                                                                
 compression        :   high                                                                                   
 options            :   allocate-inputs, allocate-outputs                                                      
 optimization       :   balanced                                                                               
 target/series      :   stm32l4                                                                                
 workspace dir      :   C:\Users\LOCALG~1\AppData\Local\Temp\mxAI_workspace28769613133004082734791410699902    
 output dir         :   C:\Users\localgroup\.stm32cubemx\cifar10_output                                        
 model_fmt          :   float                                                                                  
 model_name         :   RN_7951_233K                                                                           
 model_hash         :   0x898e4d6d9346c16933e12af4173e0b32                                                     
 params #           :   59,026 items (230.57 KiB)                                                              
 ------------------------------------------------------------------------------------------------------------- 
 input 1/1          :   'input_layer_3', f32(1x32x32x3), 12.00 KBytes, activations                             
 output 1/1         :   'dense_7', f32(1x10), 40 Bytes, activations                                            
 macc               :   8,427,720                                                                              
 weights (ro)       :   201,896 B (197.16 KiB) (1 segment) / -34,208(-14.5%) vs float model                    
 activations (rw)   :   70,400 B (68.75 KiB) (1 segment) *                                                     
 ram (total)        :   70,400 B (68.75 KiB) = 70,400 + 0 + 0                                                  
 ------------------------------------------------------------------------------------------------------------- 
 (*) 'input'/'output' buffers are allocated in the activations buffer 
Computing AI RT data/code size (target=stm32l4).. 
 Model name - RN_7951_233K 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 m_id   layer (original)                                          oshape                 param/size           macc                     connected to 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 0      input_layer_3 (InputLayer)                                [b:1,h:32,w:32,c:3] 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 1      conv2d_49 (Conv2D)                                        [b:1,h:32,w:32,c:8]    216/864           221,184                    input_layer_3 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 2      batch_normalization_61 (BatchNormalization)               [b:1,h:32,w:32,c:8]    16/64              16,384                        conv2d_49 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 3      activation_58 (Activation)                                [b:1,h:32,w:32,c:8]                        8,192           batch_normalization_61 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 4      conv2d_50 (Conv2D)                                        [b:1,h:32,w:32,c:8]    576/2,304         589,824                    activation_58 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 5      batch_normalization_62 (BatchNormalization)               [b:1,h:32,w:32,c:8]    16/64              16,384                        conv2d_50 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 6      activation_59 (Activation)                                [b:1,h:32,w:32,c:8]                        8,192           batch_normalization_62 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 7      conv2d_51 (Conv2D)                                        [b:1,h:32,w:32,c:8]    576/2,304         589,824                    activation_59 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 8      batch_normalization_63 (BatchNormalization)               [b:1,h:32,w:32,c:8]    16/64              16,384                        conv2d_51 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 9      activation_60 (Activation)                                [b:1,h:32,w:32,c:8]                        8,192           batch_normalization_63 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 10     conv2d_52 (Conv2D)                                        [b:1,h:32,w:32,c:8]    576/2,304         589,824                    activation_60 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 11     batch_normalization_64 (BatchNormalization)               [b:1,h:32,w:32,c:8]    16/64              16,384                        conv2d_52 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 12     activation_61 (Activation)                                [b:1,h:32,w:32,c:8]                        8,192           batch_normalization_64 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 13     batch_normalization_65 (BatchNormalization)               [b:1,h:32,w:32,c:8]    16/64              16,384                    activation_61 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 14     add_12 (Add)                                              [b:1,h:32,w:32,c:8]                        8,192           batch_normalization_65 
                                                                                                                                      activation_58 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 15     activation_62 (Activation)                                [b:1,h:32,w:32,c:8]                        8,192                           add_12 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 16     conv2d_53 (Conv2D)                                        [b:1,h:32,w:32,c:8]    576/2,304         589,824                    activation_62 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 17     batch_normalization_66 (BatchNormalization)               [b:1,h:32,w:32,c:8]    16/64              16,384                        conv2d_53 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 18     activation_63 (Activation)                                [b:1,h:32,w:32,c:8]                        8,192           batch_normalization_66 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 19     conv2d_54 (Conv2D)                                        [b:1,h:32,w:32,c:8]    576/2,304         589,824                    activation_63 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 20     batch_normalization_67 (BatchNormalization)               [b:1,h:32,w:32,c:8]    16/64              16,384                        conv2d_54 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 21     activation_64 (Activation)                                [b:1,h:32,w:32,c:8]                        8,192           batch_normalization_67 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 22     conv2d_55 (Conv2D)                                        [b:1,h:32,w:32,c:8]    576/2,304         589,824                    activation_64 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 23     batch_normalization_68 (BatchNormalization)               [b:1,h:32,w:32,c:8]    16/64              16,384                        conv2d_55 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 24     activation_65 (Activation)                                [b:1,h:32,w:32,c:8]                        8,192           batch_normalization_68 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 25     batch_normalization_69 (BatchNormalization)               [b:1,h:32,w:32,c:8]    16/64              16,384                    activation_65 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 26     add_13 (Add)                                              [b:1,h:32,w:32,c:8]                        8,192           batch_normalization_69 
                                                                                                                                      activation_62 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 27     activation_66 (Activation)                                [b:1,h:32,w:32,c:8]                        8,192                           add_13 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 28     max_pooling2d_12 (MaxPooling2D)                           [b:1,h:16,w:16,c:8]                        8,192                    activation_66 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 30     conv2d_56 (Conv2D)                                        [b:1,h:16,w:16,c:16]   1,152/4,608       294,912                 max_pooling2d_12 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 31     batch_normalization_70 (BatchNormalization)               [b:1,h:16,w:16,c:16]   32/128              8,192                        conv2d_56 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 32     activation_67 (Activation)                                [b:1,h:16,w:16,c:16]                       4,096           batch_normalization_70 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 33     conv2d_57 (Conv2D)                                        [b:1,h:16,w:16,c:16]   2,304/9,216       589,824                    activation_67 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 34     batch_normalization_71 (BatchNormalization)               [b:1,h:16,w:16,c:16]   32/128              8,192                        conv2d_57 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 35     activation_68 (Activation)                                [b:1,h:16,w:16,c:16]                       4,096           batch_normalization_71 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 36     conv2d_58 (Conv2D)                                        [b:1,h:16,w:16,c:16]   2,304/9,216       589,824                    activation_68 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 37     batch_normalization_72 (BatchNormalization)               [b:1,h:16,w:16,c:16]   32/128              8,192                        conv2d_58 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 38     activation_69 (Activation)                                [b:1,h:16,w:16,c:16]                       4,096           batch_normalization_72 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 39     conv2d_59 (Conv2D)                                        [b:1,h:16,w:16,c:16]   128/512            32,768                 max_pooling2d_12 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 40     batch_normalization_74 (BatchNormalization)               [b:1,h:16,w:16,c:16]   32/128              8,192                    activation_69 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 41     batch_normalization_73 (BatchNormalization)               [b:1,h:16,w:16,c:16]   32/128              8,192                        conv2d_59 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 42     add_14 (Add)                                              [b:1,h:16,w:16,c:16]                       4,096           batch_normalization_74 
                                                                                                                             batch_normalization_73 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 43     activation_70 (Activation)                                [b:1,h:16,w:16,c:16]                       4,096                           add_14 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 44     conv2d_60 (Conv2D)                                        [b:1,h:16,w:16,c:16]   2,304/9,216       589,824                    activation_70 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 45     batch_normalization_75 (BatchNormalization)               [b:1,h:16,w:16,c:16]   32/128              8,192                        conv2d_60 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 46     activation_71 (Activation)                                [b:1,h:16,w:16,c:16]                       4,096           batch_normalization_75 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 47     conv2d_61 (Conv2D)                                        [b:1,h:16,w:16,c:16]   2,304/9,216       589,824                    activation_71 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 48     batch_normalization_76 (BatchNormalization)               [b:1,h:16,w:16,c:16]   32/128              8,192                        conv2d_61 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 49     activation_72 (Activation)                                [b:1,h:16,w:16,c:16]                       4,096           batch_normalization_76 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 50     conv2d_62 (Conv2D)                                        [b:1,h:16,w:16,c:16]   2,304/9,216       589,824                    activation_72 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 51     batch_normalization_77 (BatchNormalization)               [b:1,h:16,w:16,c:16]   32/128              8,192                        conv2d_62 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 52     activation_73 (Activation)                                [b:1,h:16,w:16,c:16]                       4,096           batch_normalization_77 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 53     batch_normalization_78 (BatchNormalization)               [b:1,h:16,w:16,c:16]   32/128              8,192                    activation_73 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 54     add_15 (Add)                                              [b:1,h:16,w:16,c:16]                       4,096           batch_normalization_78 
                                                                                                                                      activation_70 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 55     activation_74 (Activation)                                [b:1,h:16,w:16,c:16]                       4,096                           add_15 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 56     max_pooling2d_13 (MaxPooling2D)                           [b:1,h:8,w:8,c:16]                         4,096                    activation_74 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 58     conv2d_63 (Conv2D)                                        [b:1,h:8,w:8,c:32]     4,608/18,432      294,912                 max_pooling2d_13 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 59     batch_normalization_79 (BatchNormalization)               [b:1,h:8,w:8,c:32]     64/256              4,096                        conv2d_63 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 60     activation_75 (Activation)                                [b:1,h:8,w:8,c:32]                         2,048           batch_normalization_79 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 61     conv2d_64 (Conv2D)                                        [b:1,h:8,w:8,c:32]     9,216/36,864      589,824                    activation_75 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 62     batch_normalization_80 (BatchNormalization)               [b:1,h:8,w:8,c:32]     64/256              4,096                        conv2d_64 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 63     activation_76 (Activation)                                [b:1,h:8,w:8,c:32]                         2,048           batch_normalization_80 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 64     max_pooling2d_14 (MaxPooling2D)                           [b:1,h:4,w:4,c:32]                         2,048                    activation_76 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 66     conv2d_65 (Conv2D)                                        [b:1,h:4,w:4,c:64]     18,432/73,728     294,912                 max_pooling2d_14 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 67     batch_normalization_81 (BatchNormalization)               [b:1,h:4,w:4,c:64]     128/512             2,048                        conv2d_65 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 68     activation_77 (Activation)                                [b:1,h:4,w:4,c:64]                         1,024           batch_normalization_81 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 69     max_pooling2d_15 (MaxPooling2D)                           [b:1,h:2,w:2,c:64]                         1,024                    activation_77 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 71     global_average_poo..g2d_3_pool (GlobalAveragePooling2D)   [b:1,h:1,w:1,c:64]                           256                 max_pooling2d_15 
        global_average_pooling2d_3 (GlobalAveragePooling2D)       [b:1,c:64]                                         global_average_poo..g2d_3_pool 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 72     dense_6_dense (Dense)                                     [b:1,c:128]            8,320/33,280        8,320       global_average_pooling2d_3 
        dense_6 (Dense)                                           [b:1,c:128]                                  128                    dense_6_dense 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 74     dense_7_dense (Dense)                                     [b:1,c:10]             1,290/5,160         1,290                          dense_6 
        dense_7 (Dense)                                           [b:1,c:10]                                   150                    dense_7_dense 
 ------ --------------------------------------------------------- ---------------------- --------------- --------- -------------------------------- 
 model: macc=8,609,696 weights=236,104 activations=-- io=-- 
 Number of operations per c-layer 
 ------- ------ --------------------------------------- ----------- -------------- 
 c_id    m_id   name (type)                                     #op           type 
 ------- ------ --------------------------------------- ----------- -------------- 
 0       2      conv2d_49 (Conv2D)                          221,192   smul_f32_f32 
 1       3      activation_58 (Nonlinearity)                  8,192     op_f32_f32 
 2       5      conv2d_50 (Conv2D)                          589,832   smul_f32_f32 
 3       6      activation_59 (Nonlinearity)                  8,192     op_f32_f32 
 4       8      conv2d_51 (Conv2D)                          589,832   smul_f32_f32 
 5       9      activation_60 (Nonlinearity)                  8,192     op_f32_f32 
 6       11     conv2d_52 (Conv2D)                          589,832   smul_f32_f32 
 7       12     activation_61 (Nonlinearity)                  8,192     op_f32_f32 
 8       13     batch_normalization_65 (ScaleBias)           16,384   smul_f32_f32 
 9       14     add_12 (Eltwise/add)                          8,192     op_f32_f32 
 10      15     activation_62 (Nonlinearity)                  8,192     op_f32_f32 
 11      17     conv2d_53 (Conv2D)                          589,832   smul_f32_f32 
 12      18     activation_63 (Nonlinearity)                  8,192     op_f32_f32 
 13      20     conv2d_54 (Conv2D)                          589,832   smul_f32_f32 
 14      21     activation_64 (Nonlinearity)                  8,192     op_f32_f32 
 15      23     conv2d_55 (Conv2D)                          589,832   smul_f32_f32 
 16      24     activation_65 (Nonlinearity)                  8,192     op_f32_f32 
 17      25     batch_normalization_69 (ScaleBias)           16,384   smul_f32_f32 
 18      26     add_13 (Eltwise/add)                          8,192     op_f32_f32 
 19      27     activation_66 (Nonlinearity)                  8,192     op_f32_f32 
 20      28     max_pooling2d_12 (Pool)                       8,192   smul_f32_f32 
 21      41     conv2d_59 (Conv2D)                           32,784   smul_f32_f32 
 22      31     conv2d_56 (Conv2D)                          294,928   smul_f32_f32 
 23      32     activation_67 (Nonlinearity)                  4,096     op_f32_f32 
 24      34     conv2d_57 (Conv2D)                          589,840   smul_f32_f32 
 25      35     activation_68 (Nonlinearity)                  4,096     op_f32_f32 
 26      37     conv2d_58 (Conv2D)                          589,840   smul_f32_f32 
 27      38     activation_69 (Nonlinearity)                  4,096     op_f32_f32 
 28      40     batch_normalization_74 (ScaleBias)            8,192   smul_f32_f32 
 29      42     add_14 (Eltwise/add)                          4,096     op_f32_f32 
 30      43     activation_70 (Nonlinearity)                  4,096     op_f32_f32 
 31      45     conv2d_60 (Conv2D)                          589,840   smul_f32_f32 
 32      46     activation_71 (Nonlinearity)                  4,096     op_f32_f32 
 33      48     conv2d_61 (Conv2D)                          589,840   smul_f32_f32 
 34      49     activation_72 (Nonlinearity)                  4,096     op_f32_f32 
 35      51     conv2d_62 (Conv2D)                          589,840   smul_f32_f32 
 36      52     activation_73 (Nonlinearity)                  4,096     op_f32_f32 
 37      53     batch_normalization_78 (ScaleBias)            8,192   smul_f32_f32 
 38      54     add_15 (Eltwise/add)                          4,096     op_f32_f32 
 39      55     activation_74 (Nonlinearity)                  4,096     op_f32_f32 
 40      56     max_pooling2d_13 (Pool)                       4,096   smul_f32_f32 
 41      59     conv2d_63 (Conv2D)                          294,944   smul_f32_f32 
 42      60     activation_75 (Nonlinearity)                  2,048     op_f32_f32 
 43      64     conv2d_64 (Conv2D)                          593,952   smul_f32_f32 
 44      69     conv2d_65 (Conv2D)                          297,024   smul_f32_f32 
 45      71     global_average_poo..g2d_3_pool (Pool)           256   smul_f32_f32 
 46      72     dense_6_dense (Dense)                         8,320    smul_f32_f4 
 47      72     dense_6 (Nonlinearity)                          128     op_f32_f32 
 48      74     dense_7_dense (Dense)                         1,290    smul_f32_f4 
 49      74     dense_7 (Nonlinearity)                          150     op_f32_f32 
 ------- ------ --------------------------------------- ----------- -------------- 
 total                                                    8,427,720 
 Number of operation types 
 ---------------- ----------- ----------- 
 operation type             #           % 
 ---------------- ----------- ----------- 
 smul_f32_f32       8,284,712       98.3% 
 op_f32_f32           133,398        1.6% 
 smul_f32_f4            9,610        0.1% 
 Complexity report (model) 
 ------ --------------------------------- ------------------------- ------------------------- ---------- 
 m_id   name                              c_macc                    c_rom                     c_id 
 ------ --------------------------------- ------------------------- ------------------------- ---------- 
 2      batch_normalization_61            ||||||             2.6%   |                  0.4%   [0] 
 3      activation_58                     |                  0.1%   |                  0.0%   [1] 
 5      batch_normalization_62            |||||||||||||||    7.0%   |                  1.2%   [2] 
 6      activation_59                     |                  0.1%   |                  0.0%   [3] 
 8      batch_normalization_63            |||||||||||||||    7.0%   |                  1.2%   [4] 
 9      activation_60                     |                  0.1%   |                  0.0%   [5] 
 11     batch_normalization_64            |||||||||||||||    7.0%   |                  1.2%   [6] 
 12     activation_61                     |                  0.1%   |                  0.0%   [7] 
 13     batch_normalization_65            |                  0.2%   |                  0.0%   [8] 
 14     add_12                            |                  0.1%   |                  0.0%   [9] 
 15     activation_62                     |                  0.1%   |                  0.0%   [10] 
 17     batch_normalization_66            |||||||||||||||    7.0%   |                  1.2%   [11] 
 18     activation_63                     |                  0.1%   |                  0.0%   [12] 
 20     batch_normalization_67            |||||||||||||||    7.0%   |                  1.2%   [13] 
 21     activation_64                     |                  0.1%   |                  0.0%   [14] 
 23     batch_normalization_68            |||||||||||||||    7.0%   |                  1.2%   [15] 
 24     activation_65                     |                  0.1%   |                  0.0%   [16] 
 25     batch_normalization_69            |                  0.2%   |                  0.0%   [17] 
 26     add_13                            |                  0.1%   |                  0.0%   [18] 
 27     activation_66                     |                  0.1%   |                  0.0%   [19] 
 28     max_pooling2d_12                  |                  0.1%   |                  0.0%   [20] 
 31     batch_normalization_70            ||||||||           3.5%   |                  2.3%   [22] 
 32     activation_67                     |                  0.0%   |                  0.0%   [23] 
 34     batch_normalization_71            |||||||||||||||    7.0%   ||                 4.6%   [24] 
 35     activation_68                     |                  0.0%   |                  0.0%   [25] 
 37     batch_normalization_72            |||||||||||||||    7.0%   ||                 4.6%   [26] 
 38     activation_69                     |                  0.0%   |                  0.0%   [27] 
 40     batch_normalization_74            |                  0.1%   |                  0.1%   [28] 
 41     batch_normalization_73            |                  0.4%   |                  0.3%   [21] 
 42     add_14                            |                  0.0%   |                  0.0%   [29] 
 43     activation_70                     |                  0.0%   |                  0.0%   [30] 
 45     batch_normalization_75            |||||||||||||||    7.0%   ||                 4.6%   [31] 
 46     activation_71                     |                  0.0%   |                  0.0%   [32] 
 48     batch_normalization_76            |||||||||||||||    7.0%   ||                 4.6%   [33] 
 49     activation_72                     |                  0.0%   |                  0.0%   [34] 
 51     batch_normalization_77            |||||||||||||||    7.0%   ||                 4.6%   [35] 
 52     activation_73                     |                  0.0%   |                  0.0%   [36] 
 53     batch_normalization_78            |                  0.1%   |                  0.1%   [37] 
 54     add_15                            |                  0.0%   |                  0.0%   [38] 
 55     activation_74                     |                  0.0%   |                  0.0%   [39] 
 56     max_pooling2d_13                  |                  0.0%   |                  0.0%   [40] 
 59     batch_normalization_79            ||||||||           3.5%   ||||               9.2%   [41] 
 60     activation_75                     |                  0.0%   |                  0.0%   [42] 
 64     max_pooling2d_14                  ||||||||||||||||   7.0%   ||||||||          18.3%   [43] 
 69     max_pooling2d_15                  ||||||||           3.5%   ||||||||||||||||  36.6%   [44] 
 71     global_average_pooling2d_3_pool   |                  0.0%   |                  0.0%   [45] 
 72     dense_6_dense                     |                  0.1%   |                  2.3%   [46, 47] 
 74     dense_7_dense                     |                  0.0%   |                  0.4%   [48, 49] 
 ------ --------------------------------- ------------------------- ------------------------- ---------- 
 macc=8,427,720 weights=201,896 act=70,400 ram_io=0 
 Requested memory size by section - "stm32l4" target 
 ------------------------------ -------- --------- -------- -------- 
 module                             text    rodata     data      bss 
 ------------------------------ -------- --------- -------- -------- 
 NetworkRuntime1020_CM4_GCC.a     15,356         0        0        0 
 cifar10.o                         2,268       400   14,832      448 
 cifar10_data.o                       48        16       88        0 
 lib (toolchain)*                    614        24        0        0 
 ------------------------------ -------- --------- -------- -------- 
 RT total**                       18,286       440   14,920      448 
 ------------------------------ -------- --------- -------- -------- 
 weights                               0   201,896        0        0 
 activations                           0         0        0   70,400 
 io                                    0         0        0        0 
 ------------------------------ -------- --------- -------- -------- 
 TOTAL                            18,286   202,336   14,920   70,848 
 ------------------------------ -------- --------- -------- -------- 
 *  toolchain objects (libm/libgcc*) 
 ** RT AI runtime objects (kernels+infrastructure) 
  Summary - "stm32l4" target 
  --------------------------------------------------- 
               FLASH (ro)      %*   RAM (rw)       % 
  --------------------------------------------------- 
  RT total         33,646   14.3%     15,368   17.9% 
  --------------------------------------------------- 
  TOTAL           235,542             85,768 
  --------------------------------------------------- 
  *  rt/total 
Creating txt report file C:\Users\localgroup\.stm32cubemx\cifar10_output\cifar10_analyze_report.txt 
elapsed time (analyze): 18.157s 
Model file:      RN_7951_233K.h5 
Total Flash:     235542 B (230.02 KiB) 
    Weights:     201896 B (197.16 KiB) 
    Library:     33646 B (32.86 KiB) 
Total Ram:       85768 B (83.76 KiB) 
    Activations: 70400 B (68.75 KiB) 
    Library:     15368 B (15.01 KiB) 
    Input:       12288 B (12.00 KiB included in Activations) 
    Output:      40 B (included in Activations) 
Done 
Analyze complete on AI model