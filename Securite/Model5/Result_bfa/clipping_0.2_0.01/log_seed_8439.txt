save path : ./save/tinyvgg_quan/clipping_0.2_0.01
{'data_path': './dataset', 'arch': 'tinyvgg_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': False, 'clipping_coeff': 0.2, 'learning_rate': 0.01, 'manualSeed': 8439, 'save_path': './save/tinyvgg_quan/clipping_0.2_0.01', 'enable_bfa': False, 'resume': '', 'quan_bitwidth': None, 'reset_weight': False, 'evaluate': False, 'n_iter': 30, 'model_only': False, 'random_bfa': False, 'use_cuda': True}
Random Seed: 8439
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'tinyvgg_quan'
=> network :
 TinyVGG(
  (features): Sequential(
    (0): quan_Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Dropout2d(p=0.3, inplace=False)
    (6): quan_Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace=True)
    (8): quan_Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace=True)
    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (11): Dropout2d(p=0.3, inplace=False)
    (12): quan_Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): quan_Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Dropout2d(p=0.3, inplace=False)
  )
  (classifier): Sequential(
    (0): quan_Linear(in_features=2048, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): quan_Linear(in_features=128, out_features=10, bias=True)
  )
)
=> do not use any checkpoint for tinyvgg_quan model

==>>[2025-10-24 10:41:17] [Epoch=000/040] [Need: 00:00:00] [LR=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/500]   Time 22.823 (22.823)   Data 22.365 (22.365)   Loss 2.3097 (2.3097)   Prec@1 11.000 (11.000)   Prec@5 44.000 (44.000)   [2025-10-24 10:41:40]
  Epoch: [000][100/500]   Time 0.018 (0.245)   Data 0.000 (0.222)   Loss 2.3001 (2.3030)   Prec@1 12.000 (10.356)   Prec@5 56.000 (50.059)   [2025-10-24 10:41:42]
  Epoch: [000][200/500]   Time 0.020 (0.137)   Data 0.000 (0.111)   Loss 2.3029 (2.3028)   Prec@1 9.000 (10.104)   Prec@5 53.000 (50.090)   [2025-10-24 10:41:45]
  Epoch: [000][300/500]   Time 0.019 (0.097)   Data 0.000 (0.075)   Loss 2.3032 (2.3027)   Prec@1 9.000 (10.183)   Prec@5 55.000 (50.405)   [2025-10-24 10:41:47]
  Epoch: [000][400/500]   Time 0.019 (0.080)   Data 0.000 (0.056)   Loss 2.3013 (2.3023)   Prec@1 6.000 (10.369)   Prec@5 52.000 (50.870)   [2025-10-24 10:41:50]
  **Train** Prec@1 10.858 Prec@5 51.848 Error@1 89.142
  **Test** Prec@1 18.290 Prec@5 68.520 Error@1 81.710
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:42:16] [Epoch=001/040] [Need: 00:37:52] [LR=0.0100] [Best : Accuracy=18.29, Error=81.71]
  Epoch: [001][000/500]   Time 22.466 (22.466)   Data 22.402 (22.402)   Loss 2.2681 (2.2681)   Prec@1 14.000 (14.000)   Prec@5 60.000 (60.000)   [2025-10-24 10:42:38]
  Epoch: [001][100/500]   Time 0.019 (0.243)   Data 0.001 (0.222)   Loss 2.1158 (2.1881)   Prec@1 21.000 (18.208)   Prec@5 75.000 (68.158)   [2025-10-24 10:42:40]
  Epoch: [001][200/500]   Time 0.019 (0.136)   Data 0.000 (0.112)   Loss 2.0658 (2.1329)   Prec@1 25.000 (20.159)   Prec@5 75.000 (72.159)   [2025-10-24 10:42:43]
  Epoch: [001][300/500]   Time 0.019 (0.101)   Data 0.000 (0.075)   Loss 2.0145 (2.0884)   Prec@1 30.000 (21.595)   Prec@5 80.000 (74.651)   [2025-10-24 10:42:46]
  Epoch: [001][400/500]   Time 0.020 (0.083)   Data 0.000 (0.056)   Loss 1.9133 (2.0503)   Prec@1 28.000 (22.661)   Prec@5 83.000 (76.466)   [2025-10-24 10:42:49]
  **Train** Prec@1 23.760 Prec@5 77.866 Error@1 76.240
  **Test** Prec@1 33.270 Prec@5 87.270 Error@1 66.730
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:43:14] [Epoch=002/040] [Need: 00:36:55] [LR=0.0100] [Best : Accuracy=33.27, Error=66.73]
  Epoch: [002][000/500]   Time 22.594 (22.594)   Data 22.529 (22.529)   Loss 1.8576 (1.8576)   Prec@1 24.000 (24.000)   Prec@5 86.000 (86.000)   [2025-10-24 10:43:37]
  Epoch: [002][100/500]   Time 0.017 (0.243)   Data 0.000 (0.223)   Loss 1.9166 (1.8439)   Prec@1 32.000 (29.307)   Prec@5 77.000 (84.861)   [2025-10-24 10:43:39]
  Epoch: [002][200/500]   Time 0.018 (0.136)   Data 0.000 (0.112)   Loss 1.7527 (1.8309)   Prec@1 37.000 (30.453)   Prec@5 88.000 (85.154)   [2025-10-24 10:43:41]
  Epoch: [002][300/500]   Time 0.019 (0.097)   Data 0.001 (0.075)   Loss 1.8638 (1.8160)   Prec@1 28.000 (31.216)   Prec@5 86.000 (85.618)   [2025-10-24 10:43:43]
  Epoch: [002][400/500]   Time 0.017 (0.080)   Data 0.000 (0.056)   Loss 1.8477 (1.8006)   Prec@1 31.000 (31.988)   Prec@5 82.000 (85.993)   [2025-10-24 10:43:46]
  **Train** Prec@1 32.592 Prec@5 86.362 Error@1 67.408
  **Test** Prec@1 41.290 Prec@5 90.950 Error@1 58.710
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:44:14] [Epoch=003/040] [Need: 00:36:15] [LR=0.0100] [Best : Accuracy=41.29, Error=58.71]
  Epoch: [003][000/500]   Time 23.344 (23.344)   Data 23.278 (23.278)   Loss 1.7159 (1.7159)   Prec@1 35.000 (35.000)   Prec@5 91.000 (91.000)   [2025-10-24 10:44:37]
  Epoch: [003][100/500]   Time 0.017 (0.251)   Data 0.000 (0.231)   Loss 1.7326 (1.7006)   Prec@1 34.000 (36.584)   Prec@5 85.000 (88.198)   [2025-10-24 10:44:39]
  Epoch: [003][200/500]   Time 0.018 (0.135)   Data 0.000 (0.116)   Loss 1.5463 (1.6837)   Prec@1 39.000 (36.881)   Prec@5 94.000 (88.483)   [2025-10-24 10:44:41]
  Epoch: [003][300/500]   Time 0.021 (0.097)   Data 0.000 (0.078)   Loss 1.7172 (1.6764)   Prec@1 32.000 (36.907)   Prec@5 87.000 (88.645)   [2025-10-24 10:44:43]
  Epoch: [003][400/500]   Time 0.020 (0.077)   Data 0.000 (0.058)   Loss 1.5109 (1.6640)   Prec@1 41.000 (37.516)   Prec@5 87.000 (88.890)   [2025-10-24 10:44:45]
  **Train** Prec@1 37.872 Prec@5 89.096 Error@1 62.128
  **Test** Prec@1 44.850 Prec@5 92.710 Error@1 55.150
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:45:11] [Epoch=004/040] [Need: 00:34:58] [LR=0.0100] [Best : Accuracy=44.85, Error=55.15]
  Epoch: [004][000/500]   Time 22.728 (22.728)   Data 22.663 (22.663)   Loss 1.6483 (1.6483)   Prec@1 42.000 (42.000)   Prec@5 90.000 (90.000)   [2025-10-24 10:45:34]
  Epoch: [004][100/500]   Time 0.019 (0.246)   Data 0.000 (0.225)   Loss 1.7776 (1.6102)   Prec@1 28.000 (40.188)   Prec@5 86.000 (89.950)   [2025-10-24 10:45:36]
  Epoch: [004][200/500]   Time 0.018 (0.133)   Data 0.000 (0.113)   Loss 1.5963 (1.5973)   Prec@1 43.000 (40.786)   Prec@5 92.000 (90.174)   [2025-10-24 10:45:38]
  Epoch: [004][300/500]   Time 0.018 (0.095)   Data 0.000 (0.076)   Loss 1.4912 (1.5899)   Prec@1 40.000 (41.063)   Prec@5 96.000 (90.382)   [2025-10-24 10:45:40]
  Epoch: [004][400/500]   Time 0.018 (0.076)   Data 0.000 (0.057)   Loss 1.7018 (1.5845)   Prec@1 32.000 (41.299)   Prec@5 87.000 (90.429)   [2025-10-24 10:45:41]
  **Train** Prec@1 41.556 Prec@5 90.544 Error@1 58.444
  **Test** Prec@1 48.730 Prec@5 94.070 Error@1 51.270
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:46:06] [Epoch=005/040] [Need: 00:33:41] [LR=0.0100] [Best : Accuracy=48.73, Error=51.27]
  Epoch: [005][000/500]   Time 21.862 (21.862)   Data 21.798 (21.798)   Loss 1.6137 (1.6137)   Prec@1 44.000 (44.000)   Prec@5 90.000 (90.000)   [2025-10-24 10:46:28]
  Epoch: [005][100/500]   Time 0.068 (0.241)   Data 0.000 (0.216)   Loss 1.5012 (1.5261)   Prec@1 48.000 (43.842)   Prec@5 91.000 (90.990)   [2025-10-24 10:46:31]
  Epoch: [005][200/500]   Time 0.019 (0.133)   Data 0.000 (0.109)   Loss 1.4822 (1.5142)   Prec@1 43.000 (44.284)   Prec@5 91.000 (91.154)   [2025-10-24 10:46:33]
  Epoch: [005][300/500]   Time 0.019 (0.095)   Data 0.000 (0.073)   Loss 1.4481 (1.5046)   Prec@1 43.000 (44.711)   Prec@5 91.000 (91.405)   [2025-10-24 10:46:35]
  Epoch: [005][400/500]   Time 0.019 (0.076)   Data 0.000 (0.055)   Loss 1.5306 (1.4972)   Prec@1 44.000 (44.918)   Prec@5 90.000 (91.554)   [2025-10-24 10:46:37]
  **Train** Prec@1 45.136 Prec@5 91.680 Error@1 54.864
  **Test** Prec@1 52.750 Prec@5 94.170 Error@1 47.250
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:47:02] [Epoch=006/040] [Need: 00:32:31] [LR=0.0100] [Best : Accuracy=52.75, Error=47.25]
  Epoch: [006][000/500]   Time 22.025 (22.025)   Data 21.961 (21.961)   Loss 1.4025 (1.4025)   Prec@1 48.000 (48.000)   Prec@5 92.000 (92.000)   [2025-10-24 10:47:24]
  Epoch: [006][100/500]   Time 0.019 (0.238)   Data 0.000 (0.218)   Loss 1.4672 (1.4447)   Prec@1 42.000 (46.911)   Prec@5 91.000 (91.861)   [2025-10-24 10:47:26]
  Epoch: [006][200/500]   Time 0.017 (0.133)   Data 0.000 (0.109)   Loss 1.3626 (1.4446)   Prec@1 57.000 (47.114)   Prec@5 94.000 (92.070)   [2025-10-24 10:47:29]
  Epoch: [006][300/500]   Time 0.021 (0.101)   Data 0.001 (0.073)   Loss 1.5466 (1.4337)   Prec@1 42.000 (47.525)   Prec@5 91.000 (92.262)   [2025-10-24 10:47:32]
  Epoch: [006][400/500]   Time 0.020 (0.080)   Data 0.000 (0.055)   Loss 1.3754 (1.4246)   Prec@1 53.000 (47.855)   Prec@5 93.000 (92.494)   [2025-10-24 10:47:34]
  **Train** Prec@1 48.188 Prec@5 92.542 Error@1 51.812
  **Test** Prec@1 56.300 Prec@5 95.020 Error@1 43.700
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:48:01] [Epoch=007/040] [Need: 00:31:39] [LR=0.0100] [Best : Accuracy=56.30, Error=43.70]
  Epoch: [007][000/500]   Time 22.141 (22.141)   Data 22.075 (22.075)   Loss 1.2458 (1.2458)   Prec@1 59.000 (59.000)   Prec@5 99.000 (99.000)   [2025-10-24 10:48:23]
  Epoch: [007][100/500]   Time 0.019 (0.239)   Data 0.002 (0.219)   Loss 1.3112 (1.3748)   Prec@1 54.000 (49.386)   Prec@5 91.000 (93.396)   [2025-10-24 10:48:25]
  Epoch: [007][200/500]   Time 0.021 (0.130)   Data 0.000 (0.110)   Loss 1.5273 (1.3872)   Prec@1 44.000 (49.383)   Prec@5 90.000 (93.065)   [2025-10-24 10:48:27]
  Epoch: [007][300/500]   Time 0.018 (0.096)   Data 0.000 (0.074)   Loss 1.2765 (1.3776)   Prec@1 51.000 (49.794)   Prec@5 98.000 (93.066)   [2025-10-24 10:48:29]
  Epoch: [007][400/500]   Time 0.017 (0.077)   Data 0.000 (0.055)   Loss 1.4081 (1.3666)   Prec@1 54.000 (50.254)   Prec@5 90.000 (93.195)   [2025-10-24 10:48:31]
  **Train** Prec@1 50.682 Prec@5 93.258 Error@1 49.318
  **Test** Prec@1 59.370 Prec@5 95.880 Error@1 40.630
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:48:57] [Epoch=008/040] [Need: 00:30:37] [LR=0.0100] [Best : Accuracy=59.37, Error=40.63]
  Epoch: [008][000/500]   Time 22.358 (22.358)   Data 22.293 (22.293)   Loss 1.2717 (1.2717)   Prec@1 54.000 (54.000)   Prec@5 98.000 (98.000)   [2025-10-24 10:49:19]
  Epoch: [008][100/500]   Time 0.017 (0.249)   Data 0.000 (0.221)   Loss 1.4497 (1.3066)   Prec@1 42.000 (52.594)   Prec@5 95.000 (93.762)   [2025-10-24 10:49:22]
  Epoch: [008][200/500]   Time 0.020 (0.139)   Data 0.001 (0.111)   Loss 1.2085 (1.3112)   Prec@1 55.000 (52.701)   Prec@5 99.000 (93.816)   [2025-10-24 10:49:25]
  Epoch: [008][300/500]   Time 0.018 (0.102)   Data 0.000 (0.074)   Loss 1.3109 (1.3064)   Prec@1 50.000 (52.937)   Prec@5 94.000 (93.880)   [2025-10-24 10:49:28]
  Epoch: [008][400/500]   Time 0.067 (0.085)   Data 0.000 (0.056)   Loss 1.2477 (1.3020)   Prec@1 60.000 (53.007)   Prec@5 91.000 (93.888)   [2025-10-24 10:49:31]
  **Train** Prec@1 53.212 Prec@5 93.896 Error@1 46.788
  **Test** Prec@1 61.350 Prec@5 96.240 Error@1 38.650
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:49:56] [Epoch=009/040] [Need: 00:29:47] [LR=0.0100] [Best : Accuracy=61.35, Error=38.65]
  Epoch: [009][000/500]   Time 21.839 (21.839)   Data 21.774 (21.774)   Loss 1.3845 (1.3845)   Prec@1 52.000 (52.000)   Prec@5 94.000 (94.000)   [2025-10-24 10:50:18]
  Epoch: [009][100/500]   Time 0.018 (0.237)   Data 0.000 (0.216)   Loss 1.4723 (1.2703)   Prec@1 51.000 (54.198)   Prec@5 90.000 (94.079)   [2025-10-24 10:50:20]
  Epoch: [009][200/500]   Time 0.020 (0.135)   Data 0.000 (0.109)   Loss 1.3289 (1.2604)   Prec@1 53.000 (54.537)   Prec@5 97.000 (94.358)   [2025-10-24 10:50:24]
  Epoch: [009][300/500]   Time 0.018 (0.096)   Data 0.000 (0.073)   Loss 1.3059 (1.2575)   Prec@1 58.000 (54.555)   Prec@5 94.000 (94.379)   [2025-10-24 10:50:25]
  Epoch: [009][400/500]   Time 0.022 (0.081)   Data 0.001 (0.054)   Loss 1.1826 (1.2541)   Prec@1 54.000 (54.815)   Prec@5 96.000 (94.411)   [2025-10-24 10:50:29]
  **Train** Prec@1 54.984 Prec@5 94.418 Error@1 45.016
  **Test** Prec@1 62.420 Prec@5 96.620 Error@1 37.580
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:50:54] [Epoch=010/040] [Need: 00:28:48] [LR=0.0100] [Best : Accuracy=62.42, Error=37.58]
  Epoch: [010][000/500]   Time 21.808 (21.808)   Data 21.740 (21.740)   Loss 1.0330 (1.0330)   Prec@1 62.000 (62.000)   Prec@5 96.000 (96.000)   [2025-10-24 10:51:16]
  Epoch: [010][100/500]   Time 0.022 (0.236)   Data 0.000 (0.215)   Loss 1.0330 (1.2093)   Prec@1 64.000 (55.832)   Prec@5 96.000 (94.733)   [2025-10-24 10:51:18]
  Epoch: [010][200/500]   Time 0.018 (0.132)   Data 0.000 (0.108)   Loss 1.1944 (1.2069)   Prec@1 60.000 (56.075)   Prec@5 93.000 (94.801)   [2025-10-24 10:51:20]
  Epoch: [010][300/500]   Time 0.018 (0.094)   Data 0.000 (0.072)   Loss 1.3498 (1.2089)   Prec@1 50.000 (56.299)   Prec@5 97.000 (94.774)   [2025-10-24 10:51:22]
  Epoch: [010][400/500]   Time 0.019 (0.076)   Data 0.000 (0.054)   Loss 1.1503 (1.2124)   Prec@1 55.000 (56.239)   Prec@5 96.000 (94.681)   [2025-10-24 10:51:24]
  **Train** Prec@1 56.578 Prec@5 94.706 Error@1 43.422
  **Test** Prec@1 65.080 Prec@5 96.470 Error@1 34.920
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:51:51] [Epoch=011/040] [Need: 00:27:49] [LR=0.0100] [Best : Accuracy=65.08, Error=34.92]
  Epoch: [011][000/500]   Time 21.820 (21.820)   Data 21.754 (21.754)   Loss 1.2037 (1.2037)   Prec@1 53.000 (53.000)   Prec@5 99.000 (99.000)   [2025-10-24 10:52:13]
  Epoch: [011][100/500]   Time 0.065 (0.239)   Data 0.000 (0.216)   Loss 1.3219 (1.1870)   Prec@1 51.000 (57.317)   Prec@5 92.000 (95.089)   [2025-10-24 10:52:15]
  Epoch: [011][200/500]   Time 0.019 (0.134)   Data 0.000 (0.108)   Loss 1.2430 (1.1820)   Prec@1 64.000 (57.990)   Prec@5 89.000 (94.891)   [2025-10-24 10:52:18]
  Epoch: [011][300/500]   Time 0.020 (0.099)   Data 0.000 (0.072)   Loss 1.1100 (1.1809)   Prec@1 67.000 (58.123)   Prec@5 96.000 (95.007)   [2025-10-24 10:52:21]
  Epoch: [011][400/500]   Time 0.065 (0.080)   Data 0.000 (0.054)   Loss 1.1049 (1.1763)   Prec@1 64.000 (58.277)   Prec@5 97.000 (95.122)   [2025-10-24 10:52:23]
  **Train** Prec@1 58.306 Prec@5 95.258 Error@1 41.694
  **Test** Prec@1 65.420 Prec@5 97.070 Error@1 34.580
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:52:49] [Epoch=012/040] [Need: 00:26:52] [LR=0.0100] [Best : Accuracy=65.42, Error=34.58]
  Epoch: [012][000/500]   Time 22.209 (22.209)   Data 22.145 (22.145)   Loss 1.0591 (1.0591)   Prec@1 61.000 (61.000)   Prec@5 96.000 (96.000)   [2025-10-24 10:53:11]
  Epoch: [012][100/500]   Time 0.019 (0.241)   Data 0.000 (0.219)   Loss 0.9541 (1.1386)   Prec@1 63.000 (59.634)   Prec@5 97.000 (95.139)   [2025-10-24 10:53:13]
  Epoch: [012][200/500]   Time 0.020 (0.131)   Data 0.000 (0.110)   Loss 1.2358 (1.1482)   Prec@1 59.000 (59.323)   Prec@5 95.000 (95.129)   [2025-10-24 10:53:15]
  Epoch: [012][300/500]   Time 0.020 (0.096)   Data 0.000 (0.074)   Loss 1.0094 (1.1453)   Prec@1 66.000 (59.292)   Prec@5 97.000 (95.186)   [2025-10-24 10:53:18]
  Epoch: [012][400/500]   Time 0.017 (0.081)   Data 0.000 (0.055)   Loss 1.2185 (1.1383)   Prec@1 58.000 (59.491)   Prec@5 94.000 (95.309)   [2025-10-24 10:53:21]
  **Train** Prec@1 59.392 Prec@5 95.336 Error@1 40.608
  **Test** Prec@1 68.800 Prec@5 97.610 Error@1 31.200
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:53:48] [Epoch=013/040] [Need: 00:25:58] [LR=0.0100] [Best : Accuracy=68.80, Error=31.20]
  Epoch: [013][000/500]   Time 22.159 (22.159)   Data 22.093 (22.093)   Loss 0.9132 (0.9132)   Prec@1 69.000 (69.000)   Prec@5 98.000 (98.000)   [2025-10-24 10:54:10]
  Epoch: [013][100/500]   Time 0.020 (0.250)   Data 0.000 (0.219)   Loss 1.0583 (1.1290)   Prec@1 63.000 (60.248)   Prec@5 95.000 (95.366)   [2025-10-24 10:54:13]
  Epoch: [013][200/500]   Time 0.067 (0.137)   Data 0.000 (0.110)   Loss 1.3693 (1.1147)   Prec@1 51.000 (60.771)   Prec@5 94.000 (95.517)   [2025-10-24 10:54:16]
  Epoch: [013][300/500]   Time 0.020 (0.101)   Data 0.001 (0.074)   Loss 1.2319 (1.1089)   Prec@1 54.000 (60.924)   Prec@5 95.000 (95.698)   [2025-10-24 10:54:18]
  Epoch: [013][400/500]   Time 0.067 (0.083)   Data 0.000 (0.055)   Loss 1.0888 (1.1069)   Prec@1 63.000 (61.010)   Prec@5 96.000 (95.718)   [2025-10-24 10:54:21]
  **Train** Prec@1 61.000 Prec@5 95.692 Error@1 39.000
  **Test** Prec@1 69.600 Prec@5 97.490 Error@1 30.400
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:54:49] [Epoch=014/040] [Need: 00:25:06] [LR=0.0100] [Best : Accuracy=69.60, Error=30.40]
  Epoch: [014][000/500]   Time 21.994 (21.994)   Data 21.930 (21.930)   Loss 1.0376 (1.0376)   Prec@1 68.000 (68.000)   Prec@5 96.000 (96.000)   [2025-10-24 10:55:11]
  Epoch: [014][100/500]   Time 0.019 (0.237)   Data 0.000 (0.217)   Loss 1.2022 (1.0843)   Prec@1 52.000 (61.287)   Prec@5 97.000 (95.574)   [2025-10-24 10:55:13]
  Epoch: [014][200/500]   Time 0.018 (0.134)   Data 0.000 (0.109)   Loss 1.0694 (1.0823)   Prec@1 65.000 (61.726)   Prec@5 100.000 (95.751)   [2025-10-24 10:55:15]
  Epoch: [014][300/500]   Time 0.022 (0.096)   Data 0.000 (0.073)   Loss 1.1658 (1.0780)   Prec@1 56.000 (61.817)   Prec@5 98.000 (95.811)   [2025-10-24 10:55:17]
  Epoch: [014][400/500]   Time 0.019 (0.079)   Data 0.000 (0.055)   Loss 1.0706 (1.0819)   Prec@1 58.000 (61.781)   Prec@5 98.000 (95.838)   [2025-10-24 10:55:20]
  **Train** Prec@1 61.992 Prec@5 95.862 Error@1 38.008
  **Test** Prec@1 70.100 Prec@5 97.890 Error@1 29.900
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:55:46] [Epoch=015/040] [Need: 00:24:07] [LR=0.0100] [Best : Accuracy=70.10, Error=29.90]
  Epoch: [015][000/500]   Time 22.327 (22.327)   Data 22.263 (22.263)   Loss 1.2402 (1.2402)   Prec@1 57.000 (57.000)   Prec@5 92.000 (92.000)   [2025-10-24 10:56:09]
  Epoch: [015][100/500]   Time 0.022 (0.252)   Data 0.000 (0.221)   Loss 1.1811 (1.0650)   Prec@1 55.000 (62.149)   Prec@5 99.000 (95.762)   [2025-10-24 10:56:12]
  Epoch: [015][200/500]   Time 0.020 (0.142)   Data 0.001 (0.111)   Loss 1.0514 (1.0637)   Prec@1 67.000 (62.438)   Prec@5 95.000 (95.692)   [2025-10-24 10:56:15]
  Epoch: [015][300/500]   Time 0.066 (0.105)   Data 0.000 (0.074)   Loss 1.0860 (1.0625)   Prec@1 58.000 (62.591)   Prec@5 94.000 (95.714)   [2025-10-24 10:56:18]
  Epoch: [015][400/500]   Time 0.019 (0.088)   Data 0.000 (0.056)   Loss 0.9483 (1.0594)   Prec@1 69.000 (62.766)   Prec@5 97.000 (95.828)   [2025-10-24 10:56:22]
  **Train** Prec@1 63.084 Prec@5 95.888 Error@1 36.916
  **Test** Prec@1 71.630 Prec@5 97.760 Error@1 28.370
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:57:03] [Epoch=016/040] [Need: 00:23:37] [LR=0.0100] [Best : Accuracy=71.63, Error=28.37]
  Epoch: [016][000/500]   Time 35.492 (35.492)   Data 35.423 (35.423)   Loss 0.9052 (0.9052)   Prec@1 68.000 (68.000)   Prec@5 95.000 (95.000)   [2025-10-24 10:57:38]
  Epoch: [016][100/500]   Time 0.026 (0.374)   Data 0.000 (0.351)   Loss 0.8850 (1.0353)   Prec@1 73.000 (63.545)   Prec@5 97.000 (96.119)   [2025-10-24 10:57:40]
  Epoch: [016][200/500]   Time 0.023 (0.198)   Data 0.002 (0.177)   Loss 0.8375 (1.0282)   Prec@1 68.000 (63.831)   Prec@5 97.000 (96.303)   [2025-10-24 10:57:43]
  Epoch: [016][300/500]   Time 0.020 (0.140)   Data 0.000 (0.118)   Loss 0.9269 (1.0200)   Prec@1 64.000 (64.037)   Prec@5 96.000 (96.289)   [2025-10-24 10:57:45]
  Epoch: [016][400/500]   Time 0.021 (0.110)   Data 0.000 (0.089)   Loss 1.1399 (1.0242)   Prec@1 58.000 (63.990)   Prec@5 94.000 (96.227)   [2025-10-24 10:57:47]
  **Train** Prec@1 64.100 Prec@5 96.304 Error@1 35.900
  **Test** Prec@1 71.380 Prec@5 97.840 Error@1 28.620

==>>[2025-10-24 10:58:25] [Epoch=017/040] [Need: 00:23:10] [LR=0.0100] [Best : Accuracy=71.63, Error=28.37]
  Epoch: [017][000/500]   Time 34.923 (34.923)   Data 34.853 (34.853)   Loss 0.9156 (0.9156)   Prec@1 70.000 (70.000)   Prec@5 98.000 (98.000)   [2025-10-24 10:59:00]
  Epoch: [017][100/500]   Time 0.023 (0.367)   Data 0.000 (0.345)   Loss 1.0698 (0.9971)   Prec@1 58.000 (64.782)   Prec@5 96.000 (96.723)   [2025-10-24 10:59:02]
  Epoch: [017][200/500]   Time 0.021 (0.195)   Data 0.000 (0.174)   Loss 1.0402 (1.0039)   Prec@1 57.000 (64.711)   Prec@5 99.000 (96.632)   [2025-10-24 10:59:04]
  Epoch: [017][300/500]   Time 0.019 (0.137)   Data 0.000 (0.116)   Loss 0.9502 (1.0080)   Prec@1 66.000 (64.651)   Prec@5 98.000 (96.528)   [2025-10-24 10:59:07]
  Epoch: [017][400/500]   Time 0.022 (0.108)   Data 0.000 (0.087)   Loss 1.0296 (1.0034)   Prec@1 66.000 (64.781)   Prec@5 97.000 (96.574)   [2025-10-24 10:59:09]
  **Train** Prec@1 64.760 Prec@5 96.542 Error@1 35.240
  **Test** Prec@1 72.990 Prec@5 98.160 Error@1 27.010
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:59:46] [Epoch=018/040] [Need: 00:22:35] [LR=0.0100] [Best : Accuracy=72.99, Error=27.01]
  Epoch: [018][000/500]   Time 35.310 (35.310)   Data 35.243 (35.243)   Loss 0.9804 (0.9804)   Prec@1 70.000 (70.000)   Prec@5 97.000 (97.000)   [2025-10-24 11:00:22]
  Epoch: [018][100/500]   Time 0.022 (0.372)   Data 0.001 (0.349)   Loss 1.0453 (0.9883)   Prec@1 61.000 (65.564)   Prec@5 98.000 (96.564)   [2025-10-24 11:00:24]
  Epoch: [018][200/500]   Time 0.020 (0.197)   Data 0.000 (0.176)   Loss 1.0943 (0.9868)   Prec@1 63.000 (65.507)   Prec@5 93.000 (96.672)   [2025-10-24 11:00:26]
  Epoch: [018][300/500]   Time 0.021 (0.138)   Data 0.000 (0.117)   Loss 0.8210 (0.9918)   Prec@1 66.000 (65.233)   Prec@5 99.000 (96.628)   [2025-10-24 11:00:28]
  Epoch: [018][400/500]   Time 0.020 (0.109)   Data 0.000 (0.088)   Loss 0.8791 (0.9890)   Prec@1 67.000 (65.369)   Prec@5 99.000 (96.651)   [2025-10-24 11:00:30]
  **Train** Prec@1 65.468 Prec@5 96.672 Error@1 34.532
  **Test** Prec@1 73.890 Prec@5 98.140 Error@1 26.110
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 11:01:22] [Epoch=019/040] [Need: 00:22:10] [LR=0.0100] [Best : Accuracy=73.89, Error=26.11]
  Epoch: [019][000/500]   Time 56.794 (56.794)   Data 56.489 (56.489)   Loss 1.0209 (1.0209)   Prec@1 64.000 (64.000)   Prec@5 96.000 (96.000)   [2025-10-24 11:02:19]
  Epoch: [019][100/500]   Time 0.029 (0.594)   Data 0.001 (0.560)   Loss 1.0731 (0.9570)   Prec@1 61.000 (66.386)   Prec@5 96.000 (96.832)   [2025-10-24 11:02:22]
  Epoch: [019][200/500]   Time 0.025 (0.313)   Data 0.000 (0.282)   Loss 1.0648 (0.9492)   Prec@1 61.000 (66.796)   Prec@5 98.000 (96.841)   [2025-10-24 11:02:25]
  Epoch: [019][300/500]   Time 0.024 (0.219)   Data 0.001 (0.188)   Loss 0.8421 (0.9588)   Prec@1 74.000 (66.445)   Prec@5 99.000 (96.797)   [2025-10-24 11:02:28]
  Epoch: [019][400/500]   Time 0.032 (0.171)   Data 0.000 (0.142)   Loss 0.9223 (0.9540)   Prec@1 68.000 (66.633)   Prec@5 100.000 (96.865)   [2025-10-24 11:02:31]
  **Train** Prec@1 66.664 Prec@5 96.812 Error@1 33.336
  **Test** Prec@1 73.790 Prec@5 98.300 Error@1 26.210

==>>[2025-10-24 11:03:30] [Epoch=020/040] [Need: 00:22:12] [LR=0.0100] [Best : Accuracy=73.89, Error=26.11]
  Epoch: [020][000/500]   Time 57.197 (57.197)   Data 56.865 (56.865)   Loss 0.9252 (0.9252)   Prec@1 71.000 (71.000)   Prec@5 97.000 (97.000)   [2025-10-24 11:04:27]
  Epoch: [020][100/500]   Time 0.033 (0.598)   Data 0.000 (0.564)   Loss 0.8425 (0.9506)   Prec@1 68.000 (67.139)   Prec@5 98.000 (96.812)   [2025-10-24 11:04:31]
  Epoch: [020][200/500]   Time 0.040 (0.315)   Data 0.001 (0.284)   Loss 0.8729 (0.9547)   Prec@1 65.000 (66.831)   Prec@5 97.000 (96.940)   [2025-10-24 11:04:34]
  Epoch: [020][300/500]   Time 0.028 (0.220)   Data 0.001 (0.190)   Loss 0.9066 (0.9487)   Prec@1 69.000 (67.080)   Prec@5 97.000 (96.864)   [2025-10-24 11:04:36]
  Epoch: [020][400/500]   Time 0.037 (0.173)   Data 0.000 (0.142)   Loss 0.8648 (0.9465)   Prec@1 71.000 (67.160)   Prec@5 96.000 (96.800)   [2025-10-24 11:04:39]
  **Train** Prec@1 67.214 Prec@5 96.862 Error@1 32.786
  **Test** Prec@1 74.660 Prec@5 98.310 Error@1 25.340
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 11:05:39] [Epoch=021/040] [Need: 00:22:01] [LR=0.0100] [Best : Accuracy=74.66, Error=25.34]
  Epoch: [021][000/500]   Time 55.711 (55.711)   Data 55.348 (55.348)   Loss 0.8068 (0.8068)   Prec@1 73.000 (73.000)   Prec@5 98.000 (98.000)   [2025-10-24 11:06:35]
  Epoch: [021][100/500]   Time 0.036 (0.582)   Data 0.002 (0.549)   Loss 0.8396 (0.9399)   Prec@1 68.000 (67.723)   Prec@5 98.000 (96.881)   [2025-10-24 11:06:38]
  Epoch: [021][200/500]   Time 0.033 (0.307)   Data 0.002 (0.276)   Loss 0.8023 (0.9362)   Prec@1 76.000 (67.786)   Prec@5 100.000 (96.925)   [2025-10-24 11:06:41]
  Epoch: [021][300/500]   Time 0.025 (0.215)   Data 0.000 (0.185)   Loss 0.8858 (0.9349)   Prec@1 69.000 (67.817)   Prec@5 97.000 (96.897)   [2025-10-24 11:06:44]
  Epoch: [021][400/500]   Time 0.033 (0.169)   Data 0.000 (0.139)   Loss 0.8939 (0.9344)   Prec@1 70.000 (67.825)   Prec@5 95.000 (96.975)   [2025-10-24 11:06:47]
  **Train** Prec@1 68.006 Prec@5 96.970 Error@1 31.994
  **Test** Prec@1 76.060 Prec@5 98.500 Error@1 23.940
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 11:07:45] [Epoch=022/040] [Need: 00:21:38] [LR=0.0100] [Best : Accuracy=76.06, Error=23.94]
  Epoch: [022][000/500]   Time 56.176 (56.176)   Data 55.739 (55.739)   Loss 0.9437 (0.9437)   Prec@1 64.000 (64.000)   Prec@5 97.000 (97.000)   [2025-10-24 11:08:41]
  Epoch: [022][100/500]   Time 0.032 (0.586)   Data 0.000 (0.552)   Loss 0.8676 (0.9204)   Prec@1 70.000 (67.990)   Prec@5 97.000 (96.881)   [2025-10-24 11:08:44]
  Epoch: [022][200/500]   Time 0.030 (0.310)   Data 0.000 (0.278)   Loss 0.7760 (0.9134)   Prec@1 75.000 (68.169)   Prec@5 97.000 (96.995)   [2025-10-24 11:08:48]
  Epoch: [022][300/500]   Time 0.045 (0.217)   Data 0.001 (0.186)   Loss 0.8295 (0.9129)   Prec@1 72.000 (68.309)   Prec@5 100.000 (97.040)   [2025-10-24 11:08:50]
  Epoch: [022][400/500]   Time 0.035 (0.170)   Data 0.000 (0.140)   Loss 0.8866 (0.9140)   Prec@1 72.000 (68.317)   Prec@5 96.000 (96.985)   [2025-10-24 11:08:53]
  **Train** Prec@1 68.330 Prec@5 97.048 Error@1 31.670
  **Test** Prec@1 76.120 Prec@5 98.550 Error@1 23.880
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 11:09:55] [Epoch=023/040] [Need: 00:21:08] [LR=0.0100] [Best : Accuracy=76.12, Error=23.88]
  Epoch: [023][000/500]   Time 55.544 (55.544)   Data 55.224 (55.224)   Loss 1.0677 (1.0677)   Prec@1 64.000 (64.000)   Prec@5 98.000 (98.000)   [2025-10-24 11:10:50]
  Epoch: [023][100/500]   Time 0.032 (0.580)   Data 0.002 (0.547)   Loss 1.0140 (0.9020)   Prec@1 68.000 (68.733)   Prec@5 98.000 (97.327)   [2025-10-24 11:10:53]
  Epoch: [023][200/500]   Time 0.030 (0.306)   Data 0.001 (0.275)   Loss 0.9156 (0.8903)   Prec@1 68.000 (68.955)   Prec@5 96.000 (97.234)   [2025-10-24 11:10:56]
  Epoch: [023][300/500]   Time 0.031 (0.214)   Data 0.000 (0.184)   Loss 0.9239 (0.8916)   Prec@1 69.000 (68.990)   Prec@5 99.000 (97.309)   [2025-10-24 11:10:59]
  Epoch: [023][400/500]   Time 0.028 (0.168)   Data 0.001 (0.138)   Loss 0.7360 (0.8913)   Prec@1 76.000 (69.095)   Prec@5 99.000 (97.264)   [2025-10-24 11:11:02]
  **Train** Prec@1 69.124 Prec@5 97.282 Error@1 30.876
  **Test** Prec@1 76.310 Prec@5 98.450 Error@1 23.690
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 11:12:01] [Epoch=024/040] [Need: 00:20:28] [LR=0.0100] [Best : Accuracy=76.31, Error=23.69]
  Epoch: [024][000/500]   Time 39.580 (39.580)   Data 39.276 (39.276)   Loss 0.8602 (0.8602)   Prec@1 69.000 (69.000)   Prec@5 96.000 (96.000)   [2025-10-24 11:12:41]
  Epoch: [024][100/500]   Time 0.024 (0.414)   Data 0.000 (0.389)   Loss 0.8976 (0.8941)   Prec@1 70.000 (69.337)   Prec@5 96.000 (97.030)   [2025-10-24 11:12:43]
  Epoch: [024][200/500]   Time 0.023 (0.219)   Data 0.001 (0.196)   Loss 0.9330 (0.8878)   Prec@1 64.000 (69.333)   Prec@5 98.000 (97.149)   [2025-10-24 11:12:45]
  Epoch: [024][300/500]   Time 0.025 (0.154)   Data 0.001 (0.131)   Loss 0.9572 (0.8833)   Prec@1 68.000 (69.472)   Prec@5 96.000 (97.209)   [2025-10-24 11:12:48]
  Epoch: [024][400/500]   Time 0.020 (0.121)   Data 0.000 (0.098)   Loss 0.8566 (0.8812)   Prec@1 75.000 (69.449)   Prec@5 98.000 (97.267)   [2025-10-24 11:12:50]
  **Train** Prec@1 69.432 Prec@5 97.252 Error@1 30.568
  **Test** Prec@1 77.150 Prec@5 98.650 Error@1 22.850
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 11:13:28] [Epoch=025/040] [Need: 00:19:18] [LR=0.0010] [Best : Accuracy=77.15, Error=22.85]
  Epoch: [025][000/500]   Time 35.319 (35.319)   Data 35.246 (35.246)   Loss 0.9379 (0.9379)   Prec@1 70.000 (70.000)   Prec@5 98.000 (98.000)   [2025-10-24 11:14:03]
  Epoch: [025][100/500]   Time 0.020 (0.372)   Data 0.001 (0.349)   Loss 0.8171 (0.8311)   Prec@1 70.000 (71.426)   Prec@5 97.000 (97.485)   [2025-10-24 11:14:06]
  Epoch: [025][200/500]   Time 0.086 (0.197)   Data 0.000 (0.176)   Loss 0.6815 (0.8306)   Prec@1 79.000 (71.498)   Prec@5 98.000 (97.587)   [2025-10-24 11:14:08]
  Epoch: [025][300/500]   Time 0.022 (0.138)   Data 0.000 (0.117)   Loss 0.8108 (0.8232)   Prec@1 74.000 (71.698)   Prec@5 96.000 (97.691)   [2025-10-24 11:14:10]
  Epoch: [025][400/500]   Time 0.024 (0.109)   Data 0.000 (0.088)   Loss 1.0020 (0.8163)   Prec@1 62.000 (71.910)   Prec@5 97.000 (97.731)   [2025-10-24 11:14:12]
  **Train** Prec@1 72.174 Prec@5 97.786 Error@1 27.826
  **Test** Prec@1 78.720 Prec@5 98.860 Error@1 21.280
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 11:16:13] [Epoch=026/040] [Need: 00:18:48] [LR=0.0010] [Best : Accuracy=78.72, Error=21.28]
  Epoch: [026][000/500]   Time 37.433 (37.433)   Data 37.147 (37.147)   Loss 0.7957 (0.7957)   Prec@1 68.000 (68.000)   Prec@5 99.000 (99.000)   [2025-10-24 11:16:51]
  Epoch: [026][100/500]   Time 0.019 (0.392)   Data 0.001 (0.368)   Loss 0.6279 (0.8037)   Prec@1 80.000 (72.644)   Prec@5 100.000 (97.911)   [2025-10-24 11:16:53]
  Epoch: [026][200/500]   Time 0.019 (0.207)   Data 0.000 (0.185)   Loss 0.6487 (0.8042)   Prec@1 81.000 (72.602)   Prec@5 98.000 (97.891)   [2025-10-24 11:16:55]
  Epoch: [026][300/500]   Time 0.023 (0.145)   Data 0.000 (0.124)   Loss 0.6902 (0.7996)   Prec@1 78.000 (72.781)   Prec@5 99.000 (97.804)   [2025-10-24 11:16:57]
  Epoch: [026][400/500]   Time 0.020 (0.114)   Data 0.000 (0.093)   Loss 0.8992 (0.8017)   Prec@1 68.000 (72.688)   Prec@5 100.000 (97.758)   [2025-10-24 11:16:59]
  **Train** Prec@1 72.808 Prec@5 97.806 Error@1 27.192
  **Test** Prec@1 78.850 Prec@5 98.880 Error@1 21.150
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 11:17:36] [Epoch=027/040] [Need: 00:17:28] [LR=0.0010] [Best : Accuracy=78.85, Error=21.15]
  Epoch: [027][000/500]   Time 58.015 (58.015)   Data 57.489 (57.489)   Loss 0.8334 (0.8334)   Prec@1 74.000 (74.000)   Prec@5 100.000 (100.000)   [2025-10-24 11:18:34]
  Epoch: [027][100/500]   Time 0.057 (0.707)   Data 0.000 (0.573)   Loss 0.6538 (0.7820)   Prec@1 72.000 (72.931)   Prec@5 98.000 (97.743)   [2025-10-24 11:18:47]
  Epoch: [027][200/500]   Time 0.124 (0.419)   Data 0.001 (0.290)   Loss 0.8704 (0.7867)   Prec@1 68.000 (72.662)   Prec@5 97.000 (97.771)   [2025-10-24 11:19:00]
  Epoch: [027][300/500]   Time 0.504 (0.318)   Data 0.000 (0.195)   Loss 0.6622 (0.7826)   Prec@1 79.000 (72.963)   Prec@5 100.000 (97.804)   [2025-10-24 11:19:12]
  Epoch: [027][400/500]   Time 0.153 (0.276)   Data 0.001 (0.148)   Loss 1.1147 (0.7835)   Prec@1 56.000 (72.833)   Prec@5 95.000 (97.810)   [2025-10-24 11:19:26]
  **Train** Prec@1 72.938 Prec@5 97.826 Error@1 27.062
  **Test** Prec@1 78.730 Prec@5 98.760 Error@1 21.270

==>>[2025-10-24 11:20:58] [Epoch=028/040] [Need: 00:16:59] [LR=0.0010] [Best : Accuracy=78.85, Error=21.15]
  Epoch: [028][000/500]   Time 78.181 (78.181)   Data 77.497 (77.497)   Loss 0.9079 (0.9079)   Prec@1 68.000 (68.000)   Prec@5 95.000 (95.000)   [2025-10-24 11:22:16]
  Epoch: [028][100/500]   Time 0.101 (0.928)   Data 0.036 (0.771)   Loss 0.7662 (0.7867)   Prec@1 73.000 (72.455)   Prec@5 97.000 (97.970)   [2025-10-24 11:22:31]
  Epoch: [028][200/500]   Time 0.145 (0.525)   Data 0.000 (0.389)   Loss 0.8259 (0.7795)   Prec@1 69.000 (73.055)   Prec@5 97.000 (98.005)   [2025-10-24 11:22:43]
  Epoch: [028][300/500]   Time 0.125 (0.397)   Data 0.000 (0.261)   Loss 0.7321 (0.7774)   Prec@1 71.000 (73.040)   Prec@5 97.000 (97.944)   [2025-10-24 11:22:57]
  Epoch: [028][400/500]   Time 0.223 (0.337)   Data 0.000 (0.197)   Loss 0.7816 (0.7771)   Prec@1 72.000 (73.097)   Prec@5 99.000 (97.903)   [2025-10-24 11:23:13]
  **Train** Prec@1 73.340 Prec@5 97.924 Error@1 26.660
  **Test** Prec@1 79.150 Prec@5 98.750 Error@1 20.850
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 11:24:08] [Epoch=029/040] [Need: 00:16:14] [LR=0.0010] [Best : Accuracy=79.15, Error=20.85]
  Epoch: [029][000/500]   Time 27.845 (27.845)   Data 27.767 (27.767)   Loss 0.6606 (0.6606)   Prec@1 74.000 (74.000)   Prec@5 98.000 (98.000)   [2025-10-24 11:24:36]
  Epoch: [029][100/500]   Time 0.026 (0.303)   Data 0.000 (0.275)   Loss 0.6855 (0.7648)   Prec@1 76.000 (73.356)   Prec@5 99.000 (98.020)   [2025-10-24 11:24:38]
  Epoch: [029][200/500]   Time 0.025 (0.166)   Data 0.000 (0.139)   Loss 0.6007 (0.7663)   Prec@1 79.000 (73.428)   Prec@5 99.000 (98.005)   [2025-10-24 11:24:41]
  Epoch: [029][300/500]   Time 0.024 (0.120)   Data 0.000 (0.093)   Loss 0.7424 (0.7659)   Prec@1 75.000 (73.615)   Prec@5 99.000 (97.993)   [2025-10-24 11:24:44]
  Epoch: [029][400/500]   Time 0.025 (0.097)   Data 0.000 (0.070)   Loss 0.7124 (0.7657)   Prec@1 76.000 (73.618)   Prec@5 98.000 (97.953)   [2025-10-24 11:24:47]
  **Train** Prec@1 73.584 Prec@5 97.904 Error@1 26.416
  **Test** Prec@1 79.020 Prec@5 98.790 Error@1 20.980

==>>[2025-10-24 11:25:16] [Epoch=030/040] [Need: 00:14:39] [LR=0.0010] [Best : Accuracy=79.15, Error=20.85]
  Epoch: [030][000/500]   Time 27.573 (27.573)   Data 27.492 (27.492)   Loss 0.7932 (0.7932)   Prec@1 76.000 (76.000)   Prec@5 96.000 (96.000)   [2025-10-24 11:25:43]
  Epoch: [030][100/500]   Time 0.034 (0.306)   Data 0.000 (0.273)   Loss 0.7598 (0.7716)   Prec@1 72.000 (73.228)   Prec@5 96.000 (97.871)   [2025-10-24 11:25:47]
  Epoch: [030][200/500]   Time 0.029 (0.167)   Data 0.000 (0.137)   Loss 0.8638 (0.7705)   Prec@1 75.000 (73.458)   Prec@5 98.000 (97.801)   [2025-10-24 11:25:49]
  Epoch: [030][300/500]   Time 0.050 (0.121)   Data 0.000 (0.092)   Loss 0.9648 (0.7737)   Prec@1 69.000 (73.309)   Prec@5 98.000 (97.787)   [2025-10-24 11:25:52]
  Epoch: [030][400/500]   Time 0.029 (0.098)   Data 0.000 (0.069)   Loss 0.9827 (0.7730)   Prec@1 59.000 (73.379)   Prec@5 98.000 (97.858)   [2025-10-24 11:25:55]
  **Train** Prec@1 73.370 Prec@5 97.874 Error@1 26.630
  **Test** Prec@1 79.700 Prec@5 98.800 Error@1 20.300
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 11:26:26] [Epoch=031/040] [Need: 00:13:06] [LR=0.0010] [Best : Accuracy=79.70, Error=20.30]
  Epoch: [031][000/500]   Time 29.035 (29.035)   Data 28.936 (28.936)   Loss 0.9282 (0.9282)   Prec@1 69.000 (69.000)   Prec@5 97.000 (97.000)   [2025-10-24 11:26:55]
  Epoch: [031][100/500]   Time 0.033 (0.323)   Data 0.000 (0.287)   Loss 0.7660 (0.7653)   Prec@1 77.000 (73.158)   Prec@5 97.000 (98.178)   [2025-10-24 11:26:58]
  Epoch: [031][200/500]   Time 0.030 (0.180)   Data 0.000 (0.144)   Loss 0.9358 (0.7673)   Prec@1 65.000 (73.398)   Prec@5 96.000 (98.080)   [2025-10-24 11:27:02]
  Epoch: [031][300/500]   Time 0.028 (0.131)   Data 0.002 (0.097)   Loss 0.8289 (0.7688)   Prec@1 65.000 (73.422)   Prec@5 98.000 (97.967)   [2025-10-24 11:27:05]
  Epoch: [031][400/500]   Time 0.024 (0.105)   Data 0.001 (0.073)   Loss 0.6154 (0.7648)   Prec@1 80.000 (73.551)   Prec@5 100.000 (98.025)   [2025-10-24 11:27:08]
  **Train** Prec@1 73.650 Prec@5 98.014 Error@1 26.350
  **Test** Prec@1 79.370 Prec@5 98.790 Error@1 20.630

==>>[2025-10-24 11:27:37] [Epoch=032/040] [Need: 00:11:34] [LR=0.0010] [Best : Accuracy=79.70, Error=20.30]
  Epoch: [032][000/500]   Time 24.194 (24.194)   Data 24.123 (24.123)   Loss 0.7170 (0.7170)   Prec@1 75.000 (75.000)   Prec@5 97.000 (97.000)   [2025-10-24 11:28:01]
  Epoch: [032][100/500]   Time 0.014 (0.258)   Data 0.000 (0.239)   Loss 0.6715 (0.7545)   Prec@1 76.000 (74.000)   Prec@5 98.000 (98.010)   [2025-10-24 11:28:03]
  Epoch: [032][200/500]   Time 0.015 (0.138)   Data 0.000 (0.120)   Loss 0.7432 (0.7603)   Prec@1 82.000 (73.836)   Prec@5 98.000 (97.935)   [2025-10-24 11:28:05]
  Epoch: [032][300/500]   Time 0.018 (0.098)   Data 0.000 (0.080)   Loss 0.8899 (0.7608)   Prec@1 75.000 (73.887)   Prec@5 97.000 (97.934)   [2025-10-24 11:28:06]
  Epoch: [032][400/500]   Time 0.020 (0.078)   Data 0.001 (0.060)   Loss 0.8568 (0.7640)   Prec@1 67.000 (73.766)   Prec@5 99.000 (97.905)   [2025-10-24 11:28:08]
  **Train** Prec@1 73.728 Prec@5 97.958 Error@1 26.272
  **Test** Prec@1 79.060 Prec@5 98.850 Error@1 20.940

==>>[2025-10-24 11:28:31] [Epoch=033/040] [Need: 00:10:01] [LR=0.0010] [Best : Accuracy=79.70, Error=20.30]
  Epoch: [033][000/500]   Time 19.307 (19.307)   Data 19.241 (19.241)   Loss 0.8412 (0.8412)   Prec@1 70.000 (70.000)   Prec@5 97.000 (97.000)   [2025-10-24 11:28:51]
  Epoch: [033][100/500]   Time 0.014 (0.208)   Data 0.000 (0.191)   Loss 0.7527 (0.7625)   Prec@1 72.000 (73.911)   Prec@5 99.000 (97.911)   [2025-10-24 11:28:52]
  Epoch: [033][200/500]   Time 0.014 (0.112)   Data 0.000 (0.096)   Loss 0.5717 (0.7533)   Prec@1 81.000 (74.050)   Prec@5 98.000 (97.985)   [2025-10-24 11:28:54]
  Epoch: [033][300/500]   Time 0.017 (0.080)   Data 0.000 (0.064)   Loss 0.7231 (0.7506)   Prec@1 76.000 (74.103)   Prec@5 99.000 (97.983)   [2025-10-24 11:28:55]
  Epoch: [033][400/500]   Time 0.015 (0.064)   Data 0.000 (0.048)   Loss 0.8497 (0.7511)   Prec@1 71.000 (74.050)   Prec@5 97.000 (98.012)   [2025-10-24 11:28:57]
  **Train** Prec@1 73.966 Prec@5 98.008 Error@1 26.034
  **Test** Prec@1 79.900 Prec@5 98.860 Error@1 20.100
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 11:29:19] [Epoch=034/040] [Need: 00:08:28] [LR=0.0010] [Best : Accuracy=79.90, Error=20.10]
  Epoch: [034][000/500]   Time 18.970 (18.970)   Data 18.904 (18.904)   Loss 0.6489 (0.6489)   Prec@1 77.000 (77.000)   Prec@5 97.000 (97.000)   [2025-10-24 11:29:38]
  Epoch: [034][100/500]   Time 0.017 (0.206)   Data 0.000 (0.187)   Loss 0.5264 (0.7397)   Prec@1 83.000 (74.495)   Prec@5 99.000 (98.149)   [2025-10-24 11:29:40]
  Epoch: [034][200/500]   Time 0.018 (0.112)   Data 0.000 (0.094)   Loss 0.7470 (0.7441)   Prec@1 70.000 (74.522)   Prec@5 100.000 (98.124)   [2025-10-24 11:29:41]
  Epoch: [034][300/500]   Time 0.016 (0.080)   Data 0.000 (0.063)   Loss 0.8075 (0.7449)   Prec@1 72.000 (74.522)   Prec@5 100.000 (98.090)   [2025-10-24 11:29:43]
  Epoch: [034][400/500]   Time 0.016 (0.064)   Data 0.000 (0.047)   Loss 0.6350 (0.7456)   Prec@1 74.000 (74.489)   Prec@5 98.000 (98.080)   [2025-10-24 11:29:45]
  **Train** Prec@1 74.450 Prec@5 98.016 Error@1 25.550
  **Test** Prec@1 79.700 Prec@5 98.800 Error@1 20.300

==>>[2025-10-24 11:30:08] [Epoch=035/040] [Need: 00:06:58] [LR=0.0010] [Best : Accuracy=79.90, Error=20.10]
  Epoch: [035][000/500]   Time 20.302 (20.302)   Data 20.238 (20.238)   Loss 0.6304 (0.6304)   Prec@1 81.000 (81.000)   Prec@5 98.000 (98.000)   [2025-10-24 11:30:28]
  Epoch: [035][100/500]   Time 0.015 (0.219)   Data 0.000 (0.201)   Loss 0.8230 (0.7498)   Prec@1 68.000 (74.069)   Prec@5 99.000 (97.950)   [2025-10-24 11:30:30]
  Epoch: [035][200/500]   Time 0.016 (0.118)   Data 0.002 (0.101)   Loss 0.7973 (0.7521)   Prec@1 70.000 (74.144)   Prec@5 98.000 (97.995)   [2025-10-24 11:30:31]
  Epoch: [035][300/500]   Time 0.015 (0.084)   Data 0.000 (0.067)   Loss 0.6797 (0.7479)   Prec@1 79.000 (74.355)   Prec@5 97.000 (98.050)   [2025-10-24 11:30:33]
  Epoch: [035][400/500]   Time 0.014 (0.067)   Data 0.000 (0.051)   Loss 0.7618 (0.7479)   Prec@1 76.000 (74.382)   Prec@5 99.000 (98.075)   [2025-10-24 11:30:35]
  **Train** Prec@1 74.354 Prec@5 98.096 Error@1 25.646
  **Test** Prec@1 79.400 Prec@5 98.900 Error@1 20.600

==>>[2025-10-24 11:30:58] [Epoch=036/040] [Need: 00:05:31] [LR=0.0010] [Best : Accuracy=79.90, Error=20.10]
  Epoch: [036][000/500]   Time 19.878 (19.878)   Data 19.813 (19.813)   Loss 0.7168 (0.7168)   Prec@1 70.000 (70.000)   Prec@5 98.000 (98.000)   [2025-10-24 11:31:18]
  Epoch: [036][100/500]   Time 0.015 (0.214)   Data 0.000 (0.196)   Loss 0.7472 (0.7550)   Prec@1 71.000 (74.020)   Prec@5 99.000 (97.990)   [2025-10-24 11:31:19]
  Epoch: [036][200/500]   Time 0.016 (0.116)   Data 0.000 (0.099)   Loss 0.6941 (0.7511)   Prec@1 73.000 (74.065)   Prec@5 98.000 (98.050)   [2025-10-24 11:31:21]
  Epoch: [036][300/500]   Time 0.017 (0.083)   Data 0.000 (0.066)   Loss 0.7851 (0.7466)   Prec@1 75.000 (74.332)   Prec@5 98.000 (98.000)   [2025-10-24 11:31:23]
  Epoch: [036][400/500]   Time 0.015 (0.066)   Data 0.000 (0.050)   Loss 0.7348 (0.7467)   Prec@1 72.000 (74.182)   Prec@5 97.000 (98.010)   [2025-10-24 11:31:24]
  **Train** Prec@1 74.224 Prec@5 98.014 Error@1 25.776
  **Test** Prec@1 80.020 Prec@5 98.890 Error@1 19.980
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 11:31:47] [Epoch=037/040] [Need: 00:04:05] [LR=0.0010] [Best : Accuracy=80.02, Error=19.98]
  Epoch: [037][000/500]   Time 19.247 (19.247)   Data 19.181 (19.181)   Loss 0.5772 (0.5772)   Prec@1 79.000 (79.000)   Prec@5 100.000 (100.000)   [2025-10-24 11:32:07]
  Epoch: [037][100/500]   Time 0.016 (0.209)   Data 0.000 (0.190)   Loss 0.7004 (0.7386)   Prec@1 74.000 (74.733)   Prec@5 100.000 (98.069)   [2025-10-24 11:32:08]
  Epoch: [037][200/500]   Time 0.017 (0.113)   Data 0.000 (0.096)   Loss 0.6492 (0.7365)   Prec@1 77.000 (74.925)   Prec@5 98.000 (98.030)   [2025-10-24 11:32:10]
  Epoch: [037][300/500]   Time 0.016 (0.081)   Data 0.000 (0.064)   Loss 0.8577 (0.7426)   Prec@1 75.000 (74.671)   Prec@5 98.000 (98.023)   [2025-10-24 11:32:12]
  Epoch: [037][400/500]   Time 0.015 (0.065)   Data 0.000 (0.048)   Loss 0.5811 (0.7416)   Prec@1 80.000 (74.673)   Prec@5 99.000 (98.115)   [2025-10-24 11:32:13]
  **Train** Prec@1 74.760 Prec@5 98.128 Error@1 25.240
  **Test** Prec@1 80.160 Prec@5 98.920 Error@1 19.840
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 11:32:36] [Epoch=038/040] [Need: 00:02:42] [LR=0.0010] [Best : Accuracy=80.16, Error=19.84]
  Epoch: [038][000/500]   Time 19.810 (19.810)   Data 19.743 (19.743)   Loss 1.0510 (1.0510)   Prec@1 65.000 (65.000)   Prec@5 96.000 (96.000)   [2025-10-24 11:32:56]
  Epoch: [038][100/500]   Time 0.016 (0.213)   Data 0.000 (0.196)   Loss 0.7721 (0.7673)   Prec@1 75.000 (74.198)   Prec@5 98.000 (97.941)   [2025-10-24 11:32:58]
  Epoch: [038][200/500]   Time 0.015 (0.115)   Data 0.000 (0.098)   Loss 0.6539 (0.7471)   Prec@1 78.000 (74.557)   Prec@5 99.000 (98.055)   [2025-10-24 11:32:59]
  Epoch: [038][300/500]   Time 0.017 (0.082)   Data 0.000 (0.066)   Loss 0.6626 (0.7451)   Prec@1 77.000 (74.671)   Prec@5 98.000 (98.090)   [2025-10-24 11:33:01]
  Epoch: [038][400/500]   Time 0.016 (0.066)   Data 0.000 (0.049)   Loss 0.7850 (0.7412)   Prec@1 72.000 (74.711)   Prec@5 97.000 (98.137)   [2025-10-24 11:33:03]
  **Train** Prec@1 74.602 Prec@5 98.144 Error@1 25.398
  **Test** Prec@1 80.220 Prec@5 98.920 Error@1 19.780
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 11:33:25] [Epoch=039/040] [Need: 00:01:20] [LR=0.0010] [Best : Accuracy=80.22, Error=19.78]
  Epoch: [039][000/500]   Time 20.291 (20.291)   Data 20.226 (20.226)   Loss 0.8995 (0.8995)   Prec@1 71.000 (71.000)   Prec@5 97.000 (97.000)   [2025-10-24 11:33:45]
  Epoch: [039][100/500]   Time 0.016 (0.221)   Data 0.000 (0.200)   Loss 0.7524 (0.7575)   Prec@1 71.000 (74.119)   Prec@5 97.000 (98.099)   [2025-10-24 11:33:47]
  Epoch: [039][200/500]   Time 0.021 (0.119)   Data 0.000 (0.101)   Loss 0.8455 (0.7443)   Prec@1 74.000 (74.627)   Prec@5 97.000 (98.104)   [2025-10-24 11:33:49]
  Epoch: [039][300/500]   Time 0.013 (0.086)   Data 0.000 (0.067)   Loss 0.6824 (0.7363)   Prec@1 75.000 (74.817)   Prec@5 99.000 (98.123)   [2025-10-24 11:33:51]
  Epoch: [039][400/500]   Time 0.018 (0.069)   Data 0.000 (0.051)   Loss 0.6521 (0.7382)   Prec@1 76.000 (74.828)   Prec@5 98.000 (98.105)   [2025-10-24 11:33:52]
  **Train** Prec@1 74.770 Prec@5 98.124 Error@1 25.230
  **Test** Prec@1 80.490 Prec@5 98.940 Error@1 19.510
=> Obtain best accuracy, and update the best model
