save path : ./save/tinyvgg_quan/clipping_0.2_0.01
{'data_path': './dataset', 'arch': 'tinyvgg_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': False, 'clipping_coeff': 0.2, 'learning_rate': 0.01, 'manualSeed': 4499, 'save_path': './save/tinyvgg_quan/clipping_0.2_0.01', 'enable_bfa': False, 'resume': '', 'quan_bitwidth': None, 'reset_weight': False, 'evaluate': False, 'n_iter': 30, 'model_only': False, 'random_bfa': False, 'use_cuda': True}
Random Seed: 4499
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'tinyvgg_quan'
=> network :
 TinyVGG(
  (features): Sequential(
    (0): quan_Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Dropout2d(p=0.3, inplace=False)
    (6): quan_Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace=True)
    (8): quan_Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace=True)
    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (11): Dropout2d(p=0.3, inplace=False)
    (12): quan_Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): quan_Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Dropout2d(p=0.3, inplace=False)
  )
  (classifier): Sequential(
    (0): quan_Linear(in_features=2048, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): quan_Linear(in_features=128, out_features=10, bias=True)
  )
)
=> do not use any checkpoint for tinyvgg_quan model

==>>[2025-10-24 10:00:34] [Epoch=000/040] [Need: 00:00:00] [LR=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/500]   Time 18.576 (18.576)   Data 18.152 (18.152)   Loss 2.3068 (2.3068)   Prec@1 9.000 (9.000)   Prec@5 44.000 (44.000)   [2025-10-24 10:00:52]
  Epoch: [000][100/500]   Time 0.015 (0.201)   Data 0.000 (0.180)   Loss 2.3060 (2.3032)   Prec@1 10.000 (10.168)   Prec@5 45.000 (49.861)   [2025-10-24 10:00:54]
  Epoch: [000][200/500]   Time 0.017 (0.109)   Data 0.000 (0.090)   Loss 2.3027 (2.3029)   Prec@1 8.000 (10.219)   Prec@5 48.000 (50.234)   [2025-10-24 10:00:56]
  Epoch: [000][300/500]   Time 0.015 (0.078)   Data 0.000 (0.060)   Loss 2.2989 (2.3020)   Prec@1 12.000 (10.807)   Prec@5 56.000 (51.548)   [2025-10-24 10:00:57]
  Epoch: [000][400/500]   Time 0.016 (0.062)   Data 0.000 (0.045)   Loss 2.0968 (2.2821)   Prec@1 15.000 (12.362)   Prec@5 77.000 (54.618)   [2025-10-24 10:00:59]
  **Train** Prec@1 14.224 Prec@5 58.760 Error@1 85.776
  **Test** Prec@1 26.070 Prec@5 77.770 Error@1 73.930
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:01:23] [Epoch=001/040] [Need: 00:32:15] [LR=0.0100] [Best : Accuracy=26.07, Error=73.93]
  Epoch: [001][000/500]   Time 25.758 (25.758)   Data 25.693 (25.693)   Loss 1.9147 (1.9147)   Prec@1 29.000 (29.000)   Prec@5 80.000 (80.000)   [2025-10-24 10:01:49]
  Epoch: [001][100/500]   Time 0.018 (0.275)   Data 0.000 (0.255)   Loss 1.9162 (2.0470)   Prec@1 22.000 (23.703)   Prec@5 78.000 (78.030)   [2025-10-24 10:01:51]
  Epoch: [001][200/500]   Time 0.018 (0.148)   Data 0.002 (0.128)   Loss 1.9833 (2.0199)   Prec@1 27.000 (24.622)   Prec@5 83.000 (78.801)   [2025-10-24 10:01:53]
  Epoch: [001][300/500]   Time 0.019 (0.105)   Data 0.000 (0.086)   Loss 1.9526 (1.9957)   Prec@1 24.000 (24.987)   Prec@5 79.000 (79.691)   [2025-10-24 10:01:55]
  Epoch: [001][400/500]   Time 0.017 (0.084)   Data 0.000 (0.064)   Loss 1.9934 (1.9735)   Prec@1 26.000 (25.850)   Prec@5 77.000 (80.459)   [2025-10-24 10:01:57]
  **Train** Prec@1 26.422 Prec@5 81.144 Error@1 73.578
  **Test** Prec@1 34.980 Prec@5 87.750 Error@1 65.020
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:05:51] [Epoch=002/040] [Need: 01:40:17] [LR=0.0100] [Best : Accuracy=34.98, Error=65.02]
  Epoch: [002][000/500]   Time 24.989 (24.989)   Data 24.921 (24.921)   Loss 1.9538 (1.9538)   Prec@1 21.000 (21.000)   Prec@5 75.000 (75.000)   [2025-10-24 10:06:16]
  Epoch: [002][100/500]   Time 0.019 (0.267)   Data 0.000 (0.247)   Loss 1.8385 (1.8420)   Prec@1 25.000 (30.079)   Prec@5 84.000 (85.307)   [2025-10-24 10:06:18]
  Epoch: [002][200/500]   Time 0.017 (0.144)   Data 0.000 (0.124)   Loss 1.8719 (1.8302)   Prec@1 29.000 (30.537)   Prec@5 82.000 (85.388)   [2025-10-24 10:06:20]
  Epoch: [002][300/500]   Time 0.018 (0.102)   Data 0.000 (0.083)   Loss 1.8411 (1.8161)   Prec@1 34.000 (31.512)   Prec@5 82.000 (85.774)   [2025-10-24 10:06:21]
  Epoch: [002][400/500]   Time 0.020 (0.081)   Data 0.001 (0.063)   Loss 1.7027 (1.7983)   Prec@1 35.000 (32.322)   Prec@5 86.000 (86.135)   [2025-10-24 10:06:23]
  **Train** Prec@1 32.806 Prec@5 86.418 Error@1 67.194
  **Test** Prec@1 38.900 Prec@5 90.190 Error@1 61.100
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:06:50] [Epoch=003/040] [Need: 01:17:17] [LR=0.0100] [Best : Accuracy=38.90, Error=61.10]
  Epoch: [003][000/500]   Time 22.070 (22.070)   Data 22.004 (22.004)   Loss 1.7141 (1.7141)   Prec@1 36.000 (36.000)   Prec@5 84.000 (84.000)   [2025-10-24 10:07:12]
  Epoch: [003][100/500]   Time 0.020 (0.239)   Data 0.000 (0.218)   Loss 1.6856 (1.7343)   Prec@1 36.000 (34.485)   Prec@5 85.000 (87.861)   [2025-10-24 10:07:14]
  Epoch: [003][200/500]   Time 0.019 (0.129)   Data 0.000 (0.110)   Loss 1.6362 (1.7118)   Prec@1 38.000 (35.801)   Prec@5 93.000 (88.179)   [2025-10-24 10:07:16]
  Epoch: [003][300/500]   Time 0.019 (0.093)   Data 0.000 (0.074)   Loss 1.7062 (1.7013)   Prec@1 38.000 (36.276)   Prec@5 90.000 (88.365)   [2025-10-24 10:07:18]
  Epoch: [003][400/500]   Time 0.018 (0.074)   Data 0.000 (0.055)   Loss 1.5286 (1.6907)   Prec@1 39.000 (36.756)   Prec@5 93.000 (88.549)   [2025-10-24 10:07:20]
  **Train** Prec@1 37.136 Prec@5 88.694 Error@1 62.864
  **Test** Prec@1 42.670 Prec@5 92.210 Error@1 57.330
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:07:45] [Epoch=004/040] [Need: 01:04:35] [LR=0.0100] [Best : Accuracy=42.67, Error=57.33]
  Epoch: [004][000/500]   Time 21.826 (21.826)   Data 21.760 (21.760)   Loss 1.5989 (1.5989)   Prec@1 47.000 (47.000)   Prec@5 94.000 (94.000)   [2025-10-24 10:08:06]
  Epoch: [004][100/500]   Time 0.017 (0.236)   Data 0.000 (0.216)   Loss 1.6586 (1.6275)   Prec@1 35.000 (39.723)   Prec@5 92.000 (89.624)   [2025-10-24 10:08:08]
  Epoch: [004][200/500]   Time 0.019 (0.128)   Data 0.000 (0.108)   Loss 1.5197 (1.6284)   Prec@1 39.000 (39.672)   Prec@5 95.000 (89.821)   [2025-10-24 10:08:10]
  Epoch: [004][300/500]   Time 0.019 (0.092)   Data 0.000 (0.072)   Loss 1.4308 (1.6174)   Prec@1 46.000 (40.056)   Prec@5 96.000 (90.063)   [2025-10-24 10:08:12]
  Epoch: [004][400/500]   Time 0.018 (0.074)   Data 0.000 (0.055)   Loss 1.5381 (1.6107)   Prec@1 39.000 (40.406)   Prec@5 89.000 (90.142)   [2025-10-24 10:08:14]
  **Train** Prec@1 40.798 Prec@5 90.152 Error@1 59.202
  **Test** Prec@1 47.470 Prec@5 93.670 Error@1 52.530
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:08:39] [Epoch=005/040] [Need: 00:56:37] [LR=0.0100] [Best : Accuracy=47.47, Error=52.53]
  Epoch: [005][000/500]   Time 22.624 (22.624)   Data 22.555 (22.555)   Loss 1.4265 (1.4265)   Prec@1 49.000 (49.000)   Prec@5 91.000 (91.000)   [2025-10-24 10:09:02]
  Epoch: [005][100/500]   Time 0.018 (0.244)   Data 0.000 (0.223)   Loss 1.5854 (1.5492)   Prec@1 38.000 (42.327)   Prec@5 95.000 (90.772)   [2025-10-24 10:09:04]
  Epoch: [005][200/500]   Time 0.016 (0.132)   Data 0.000 (0.112)   Loss 1.5584 (1.5495)   Prec@1 41.000 (42.572)   Prec@5 88.000 (90.677)   [2025-10-24 10:09:06]
  Epoch: [005][300/500]   Time 0.020 (0.094)   Data 0.000 (0.075)   Loss 1.5066 (1.5447)   Prec@1 48.000 (42.973)   Prec@5 93.000 (90.741)   [2025-10-24 10:09:08]
  Epoch: [005][400/500]   Time 0.021 (0.075)   Data 0.000 (0.057)   Loss 1.4043 (1.5365)   Prec@1 50.000 (43.287)   Prec@5 96.000 (90.930)   [2025-10-24 10:09:09]
  **Train** Prec@1 43.636 Prec@5 91.024 Error@1 56.364
  **Test** Prec@1 51.010 Prec@5 94.320 Error@1 48.990
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:09:34] [Epoch=006/040] [Need: 00:51:03] [LR=0.0100] [Best : Accuracy=51.01, Error=48.99]
  Epoch: [006][000/500]   Time 22.532 (22.532)   Data 22.463 (22.463)   Loss 1.3613 (1.3613)   Prec@1 49.000 (49.000)   Prec@5 91.000 (91.000)   [2025-10-24 10:09:57]
  Epoch: [006][100/500]   Time 0.020 (0.243)   Data 0.000 (0.223)   Loss 1.5097 (1.4902)   Prec@1 45.000 (44.891)   Prec@5 93.000 (91.782)   [2025-10-24 10:09:59]
  Epoch: [006][200/500]   Time 0.081 (0.132)   Data 0.000 (0.112)   Loss 1.6464 (1.4828)   Prec@1 42.000 (45.582)   Prec@5 90.000 (91.960)   [2025-10-24 10:10:01]
  Epoch: [006][300/500]   Time 0.020 (0.094)   Data 0.000 (0.075)   Loss 1.3463 (1.4740)   Prec@1 51.000 (45.973)   Prec@5 94.000 (92.093)   [2025-10-24 10:10:03]
  Epoch: [006][400/500]   Time 0.018 (0.076)   Data 0.000 (0.056)   Loss 1.4175 (1.4695)   Prec@1 40.000 (46.137)   Prec@5 96.000 (92.100)   [2025-10-24 10:10:05]
  **Train** Prec@1 46.516 Prec@5 92.144 Error@1 53.484
  **Test** Prec@1 54.290 Prec@5 94.800 Error@1 45.710
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:10:30] [Epoch=007/040] [Need: 00:46:51] [LR=0.0100] [Best : Accuracy=54.29, Error=45.71]
  Epoch: [007][000/500]   Time 22.800 (22.800)   Data 22.732 (22.732)   Loss 1.6800 (1.6800)   Prec@1 34.000 (34.000)   Prec@5 91.000 (91.000)   [2025-10-24 10:10:53]
  Epoch: [007][100/500]   Time 0.019 (0.247)   Data 0.000 (0.225)   Loss 1.2758 (1.4279)   Prec@1 58.000 (47.505)   Prec@5 91.000 (92.545)   [2025-10-24 10:10:55]
  Epoch: [007][200/500]   Time 0.020 (0.133)   Data 0.000 (0.113)   Loss 1.4869 (1.4133)   Prec@1 49.000 (48.284)   Prec@5 94.000 (92.582)   [2025-10-24 10:10:57]
  Epoch: [007][300/500]   Time 0.017 (0.095)   Data 0.000 (0.076)   Loss 1.5268 (1.4113)   Prec@1 48.000 (48.452)   Prec@5 88.000 (92.611)   [2025-10-24 10:10:59]
  Epoch: [007][400/500]   Time 0.019 (0.076)   Data 0.000 (0.057)   Loss 1.1859 (1.4051)   Prec@1 59.000 (48.556)   Prec@5 95.000 (92.711)   [2025-10-24 10:11:01]
  **Train** Prec@1 48.680 Prec@5 92.826 Error@1 51.320
  **Test** Prec@1 55.930 Prec@5 95.060 Error@1 44.070
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:11:26] [Epoch=008/040] [Need: 00:43:28] [LR=0.0100] [Best : Accuracy=55.93, Error=44.07]
  Epoch: [008][000/500]   Time 22.016 (22.016)   Data 21.948 (21.948)   Loss 1.3512 (1.3512)   Prec@1 53.000 (53.000)   Prec@5 92.000 (92.000)   [2025-10-24 10:11:48]
  Epoch: [008][100/500]   Time 0.021 (0.239)   Data 0.000 (0.218)   Loss 1.2498 (1.3480)   Prec@1 52.000 (50.545)   Prec@5 98.000 (93.644)   [2025-10-24 10:11:50]
  Epoch: [008][200/500]   Time 0.017 (0.129)   Data 0.000 (0.109)   Loss 1.4872 (1.3462)   Prec@1 41.000 (50.493)   Prec@5 93.000 (93.662)   [2025-10-24 10:11:52]
  Epoch: [008][300/500]   Time 0.020 (0.093)   Data 0.001 (0.073)   Loss 1.4537 (1.3496)   Prec@1 48.000 (50.711)   Prec@5 95.000 (93.535)   [2025-10-24 10:11:54]
  Epoch: [008][400/500]   Time 0.016 (0.074)   Data 0.000 (0.055)   Loss 1.4479 (1.3497)   Prec@1 51.000 (50.903)   Prec@5 94.000 (93.561)   [2025-10-24 10:11:56]
  **Train** Prec@1 51.012 Prec@5 93.658 Error@1 48.988
  **Test** Prec@1 59.160 Prec@5 96.290 Error@1 40.840
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:12:22] [Epoch=009/040] [Need: 00:40:38] [LR=0.0100] [Best : Accuracy=59.16, Error=40.84]
  Epoch: [009][000/500]   Time 22.181 (22.181)   Data 22.116 (22.116)   Loss 1.2457 (1.2457)   Prec@1 53.000 (53.000)   Prec@5 93.000 (93.000)   [2025-10-24 10:12:44]
  Epoch: [009][100/500]   Time 0.019 (0.240)   Data 0.000 (0.219)   Loss 1.1208 (1.3208)   Prec@1 64.000 (52.079)   Prec@5 96.000 (93.752)   [2025-10-24 10:12:46]
  Epoch: [009][200/500]   Time 0.019 (0.130)   Data 0.000 (0.110)   Loss 1.2428 (1.3119)   Prec@1 50.000 (52.383)   Prec@5 99.000 (93.761)   [2025-10-24 10:12:48]
  Epoch: [009][300/500]   Time 0.021 (0.093)   Data 0.001 (0.074)   Loss 1.1765 (1.3063)   Prec@1 59.000 (52.608)   Prec@5 95.000 (93.831)   [2025-10-24 10:12:50]
  Epoch: [009][400/500]   Time 0.021 (0.074)   Data 0.000 (0.055)   Loss 1.3327 (1.2979)   Prec@1 52.000 (52.863)   Prec@5 93.000 (94.002)   [2025-10-24 10:12:52]
  **Train** Prec@1 53.042 Prec@5 94.038 Error@1 46.958
  **Test** Prec@1 61.230 Prec@5 96.460 Error@1 38.770
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:13:17] [Epoch=010/040] [Need: 00:38:09] [LR=0.0100] [Best : Accuracy=61.23, Error=38.77]
  Epoch: [010][000/500]   Time 21.880 (21.880)   Data 21.814 (21.814)   Loss 1.4179 (1.4179)   Prec@1 46.000 (46.000)   Prec@5 93.000 (93.000)   [2025-10-24 10:13:39]
  Epoch: [010][100/500]   Time 0.019 (0.237)   Data 0.002 (0.216)   Loss 1.2576 (1.2570)   Prec@1 54.000 (54.455)   Prec@5 93.000 (94.059)   [2025-10-24 10:13:41]
  Epoch: [010][200/500]   Time 0.019 (0.128)   Data 0.000 (0.109)   Loss 1.2559 (1.2572)   Prec@1 49.000 (54.587)   Prec@5 97.000 (94.269)   [2025-10-24 10:13:43]
  Epoch: [010][300/500]   Time 0.019 (0.092)   Data 0.001 (0.073)   Loss 1.3258 (1.2540)   Prec@1 51.000 (54.791)   Prec@5 94.000 (94.183)   [2025-10-24 10:13:45]
  Epoch: [010][400/500]   Time 0.018 (0.074)   Data 0.000 (0.055)   Loss 1.1878 (1.2506)   Prec@1 58.000 (54.980)   Prec@5 95.000 (94.085)   [2025-10-24 10:13:47]
  **Train** Prec@1 55.134 Prec@5 94.244 Error@1 44.866
  **Test** Prec@1 63.470 Prec@5 96.870 Error@1 36.530
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:14:12] [Epoch=011/040] [Need: 00:35:57] [LR=0.0100] [Best : Accuracy=63.47, Error=36.53]
  Epoch: [011][000/500]   Time 22.654 (22.654)   Data 22.590 (22.590)   Loss 1.1527 (1.1527)   Prec@1 59.000 (59.000)   Prec@5 96.000 (96.000)   [2025-10-24 10:14:35]
  Epoch: [011][100/500]   Time 0.021 (0.245)   Data 0.000 (0.224)   Loss 1.2363 (1.2104)   Prec@1 52.000 (56.020)   Prec@5 96.000 (94.881)   [2025-10-24 10:14:37]
  Epoch: [011][200/500]   Time 0.020 (0.132)   Data 0.000 (0.113)   Loss 1.1801 (1.2130)   Prec@1 59.000 (56.249)   Prec@5 94.000 (94.905)   [2025-10-24 10:14:39]
  Epoch: [011][300/500]   Time 0.021 (0.095)   Data 0.002 (0.075)   Loss 1.1495 (1.2103)   Prec@1 61.000 (56.588)   Prec@5 96.000 (94.927)   [2025-10-24 10:14:41]
  Epoch: [011][400/500]   Time 0.018 (0.076)   Data 0.000 (0.057)   Loss 1.2022 (1.2121)   Prec@1 62.000 (56.499)   Prec@5 91.000 (94.855)   [2025-10-24 10:14:43]
  **Train** Prec@1 56.688 Prec@5 94.850 Error@1 43.312
  **Test** Prec@1 65.530 Prec@5 97.250 Error@1 34.470
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:15:08] [Epoch=012/040] [Need: 00:33:59] [LR=0.0100] [Best : Accuracy=65.53, Error=34.47]
  Epoch: [012][000/500]   Time 21.998 (21.998)   Data 21.932 (21.932)   Loss 1.2516 (1.2516)   Prec@1 59.000 (59.000)   Prec@5 96.000 (96.000)   [2025-10-24 10:15:30]
  Epoch: [012][100/500]   Time 0.018 (0.237)   Data 0.000 (0.217)   Loss 1.2560 (1.1717)   Prec@1 51.000 (57.782)   Prec@5 94.000 (95.168)   [2025-10-24 10:15:32]
  Epoch: [012][200/500]   Time 0.021 (0.128)   Data 0.000 (0.109)   Loss 1.3108 (1.1670)   Prec@1 57.000 (58.378)   Prec@5 92.000 (95.114)   [2025-10-24 10:15:34]
  Epoch: [012][300/500]   Time 0.017 (0.092)   Data 0.000 (0.073)   Loss 1.2500 (1.1713)   Prec@1 60.000 (58.186)   Prec@5 93.000 (95.103)   [2025-10-24 10:15:36]
  Epoch: [012][400/500]   Time 0.019 (0.074)   Data 0.000 (0.055)   Loss 1.1167 (1.1670)   Prec@1 64.000 (58.404)   Prec@5 96.000 (95.157)   [2025-10-24 10:15:38]
  **Train** Prec@1 58.588 Prec@5 95.286 Error@1 41.412
  **Test** Prec@1 66.690 Prec@5 97.270 Error@1 33.310
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:16:03] [Epoch=013/040] [Need: 00:32:09] [LR=0.0100] [Best : Accuracy=66.69, Error=33.31]
  Epoch: [013][000/500]   Time 22.287 (22.287)   Data 22.215 (22.215)   Loss 1.0902 (1.0902)   Prec@1 58.000 (58.000)   Prec@5 98.000 (98.000)   [2025-10-24 10:16:25]
  Epoch: [013][100/500]   Time 0.021 (0.241)   Data 0.000 (0.220)   Loss 1.2607 (1.1243)   Prec@1 56.000 (59.564)   Prec@5 96.000 (95.693)   [2025-10-24 10:16:27]
  Epoch: [013][200/500]   Time 0.017 (0.130)   Data 0.000 (0.111)   Loss 0.9887 (1.1349)   Prec@1 66.000 (59.562)   Prec@5 94.000 (95.498)   [2025-10-24 10:16:29]
  Epoch: [013][300/500]   Time 0.018 (0.093)   Data 0.000 (0.074)   Loss 1.0891 (1.1278)   Prec@1 55.000 (59.731)   Prec@5 99.000 (95.608)   [2025-10-24 10:16:31]
  Epoch: [013][400/500]   Time 0.021 (0.075)   Data 0.000 (0.056)   Loss 1.1395 (1.1293)   Prec@1 61.000 (59.808)   Prec@5 94.000 (95.549)   [2025-10-24 10:16:33]
  **Train** Prec@1 59.882 Prec@5 95.592 Error@1 40.118
  **Test** Prec@1 68.080 Prec@5 97.470 Error@1 31.920
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:17:00] [Epoch=014/040] [Need: 00:30:30] [LR=0.0100] [Best : Accuracy=68.08, Error=31.92]
  Epoch: [014][000/500]   Time 24.319 (24.319)   Data 24.246 (24.246)   Loss 1.2450 (1.2450)   Prec@1 53.000 (53.000)   Prec@5 97.000 (97.000)   [2025-10-24 10:17:24]
  Epoch: [014][100/500]   Time 0.017 (0.261)   Data 0.000 (0.240)   Loss 1.0983 (1.1081)   Prec@1 62.000 (60.762)   Prec@5 91.000 (95.624)   [2025-10-24 10:17:26]
  Epoch: [014][200/500]   Time 0.017 (0.141)   Data 0.000 (0.121)   Loss 1.0482 (1.1020)   Prec@1 61.000 (61.020)   Prec@5 97.000 (95.721)   [2025-10-24 10:17:28]
  Epoch: [014][300/500]   Time 0.019 (0.100)   Data 0.000 (0.081)   Loss 1.0481 (1.0994)   Prec@1 64.000 (61.113)   Prec@5 96.000 (95.674)   [2025-10-24 10:17:30]
  Epoch: [014][400/500]   Time 0.018 (0.080)   Data 0.000 (0.061)   Loss 1.2659 (1.0973)   Prec@1 65.000 (61.242)   Prec@5 92.000 (95.696)   [2025-10-24 10:17:32]
  **Train** Prec@1 61.346 Prec@5 95.680 Error@1 38.654
  **Test** Prec@1 69.660 Prec@5 97.710 Error@1 30.340
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:17:57] [Epoch=015/040] [Need: 00:28:58] [LR=0.0100] [Best : Accuracy=69.66, Error=30.34]
  Epoch: [015][000/500]   Time 22.339 (22.339)   Data 22.274 (22.274)   Loss 1.1895 (1.1895)   Prec@1 63.000 (63.000)   Prec@5 96.000 (96.000)   [2025-10-24 10:18:19]
  Epoch: [015][100/500]   Time 0.019 (0.241)   Data 0.000 (0.221)   Loss 1.1141 (1.0642)   Prec@1 61.000 (62.436)   Prec@5 94.000 (96.228)   [2025-10-24 10:18:21]
  Epoch: [015][200/500]   Time 0.017 (0.130)   Data 0.000 (0.111)   Loss 0.9731 (1.0621)   Prec@1 71.000 (62.537)   Prec@5 95.000 (96.159)   [2025-10-24 10:18:23]
  Epoch: [015][300/500]   Time 0.018 (0.093)   Data 0.000 (0.074)   Loss 1.1562 (1.0698)   Prec@1 59.000 (62.312)   Prec@5 95.000 (95.960)   [2025-10-24 10:18:25]
  Epoch: [015][400/500]   Time 0.021 (0.075)   Data 0.001 (0.056)   Loss 0.9938 (1.0676)   Prec@1 64.000 (62.419)   Prec@5 97.000 (95.935)   [2025-10-24 10:18:27]
  **Train** Prec@1 62.524 Prec@5 95.898 Error@1 37.476
  **Test** Prec@1 71.170 Prec@5 97.840 Error@1 28.830
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:18:52] [Epoch=016/040] [Need: 00:27:27] [LR=0.0100] [Best : Accuracy=71.17, Error=28.83]
  Epoch: [016][000/500]   Time 22.025 (22.025)   Data 21.960 (21.960)   Loss 1.1699 (1.1699)   Prec@1 64.000 (64.000)   Prec@5 96.000 (96.000)   [2025-10-24 10:19:14]
  Epoch: [016][100/500]   Time 0.018 (0.237)   Data 0.001 (0.218)   Loss 1.1151 (1.0608)   Prec@1 57.000 (62.594)   Prec@5 98.000 (96.238)   [2025-10-24 10:19:16]
  Epoch: [016][200/500]   Time 0.020 (0.129)   Data 0.000 (0.110)   Loss 1.1408 (1.0465)   Prec@1 62.000 (63.159)   Prec@5 97.000 (96.348)   [2025-10-24 10:19:18]
  Epoch: [016][300/500]   Time 0.017 (0.092)   Data 0.000 (0.073)   Loss 1.2278 (1.0494)   Prec@1 59.000 (62.960)   Prec@5 93.000 (96.229)   [2025-10-24 10:19:20]
  Epoch: [016][400/500]   Time 0.023 (0.074)   Data 0.000 (0.055)   Loss 0.9367 (1.0441)   Prec@1 71.000 (63.212)   Prec@5 98.000 (96.200)   [2025-10-24 10:19:22]
  **Train** Prec@1 63.414 Prec@5 96.282 Error@1 36.586
  **Test** Prec@1 71.870 Prec@5 97.930 Error@1 28.130
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:19:47] [Epoch=017/040] [Need: 00:25:59] [LR=0.0100] [Best : Accuracy=71.87, Error=28.13]
  Epoch: [017][000/500]   Time 21.650 (21.650)   Data 21.585 (21.585)   Loss 1.0330 (1.0330)   Prec@1 67.000 (67.000)   Prec@5 97.000 (97.000)   [2025-10-24 10:20:08]
  Epoch: [017][100/500]   Time 0.020 (0.234)   Data 0.000 (0.214)   Loss 1.0759 (1.0261)   Prec@1 66.000 (63.941)   Prec@5 94.000 (96.218)   [2025-10-24 10:20:10]
  Epoch: [017][200/500]   Time 0.019 (0.127)   Data 0.000 (0.108)   Loss 0.7653 (1.0180)   Prec@1 75.000 (63.995)   Prec@5 99.000 (96.333)   [2025-10-24 10:20:12]
  Epoch: [017][300/500]   Time 0.018 (0.091)   Data 0.000 (0.072)   Loss 1.1111 (1.0091)   Prec@1 61.000 (64.365)   Prec@5 94.000 (96.276)   [2025-10-24 10:20:14]
  Epoch: [017][400/500]   Time 0.018 (0.073)   Data 0.000 (0.054)   Loss 0.9704 (1.0114)   Prec@1 67.000 (64.456)   Prec@5 98.000 (96.289)   [2025-10-24 10:20:16]
  **Train** Prec@1 64.400 Prec@5 96.320 Error@1 35.600
  **Test** Prec@1 72.910 Prec@5 98.060 Error@1 27.090
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:20:41] [Epoch=018/040] [Need: 00:24:35] [LR=0.0100] [Best : Accuracy=72.91, Error=27.09]
  Epoch: [018][000/500]   Time 22.139 (22.139)   Data 22.074 (22.074)   Loss 0.9311 (0.9311)   Prec@1 66.000 (66.000)   Prec@5 95.000 (95.000)   [2025-10-24 10:21:03]
  Epoch: [018][100/500]   Time 0.019 (0.239)   Data 0.000 (0.219)   Loss 1.1865 (0.9950)   Prec@1 53.000 (65.426)   Prec@5 95.000 (96.822)   [2025-10-24 10:21:05]
  Epoch: [018][200/500]   Time 0.019 (0.130)   Data 0.000 (0.110)   Loss 0.9082 (0.9934)   Prec@1 65.000 (65.512)   Prec@5 97.000 (96.592)   [2025-10-24 10:21:07]
  Epoch: [018][300/500]   Time 0.019 (0.093)   Data 0.000 (0.074)   Loss 1.1323 (0.9920)   Prec@1 57.000 (65.674)   Prec@5 92.000 (96.591)   [2025-10-24 10:21:09]
  Epoch: [018][400/500]   Time 0.019 (0.074)   Data 0.000 (0.056)   Loss 0.7578 (0.9950)   Prec@1 73.000 (65.454)   Prec@5 98.000 (96.559)   [2025-10-24 10:21:11]
  **Train** Prec@1 65.444 Prec@5 96.604 Error@1 34.556
  **Test** Prec@1 73.540 Prec@5 98.160 Error@1 26.460
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:21:37] [Epoch=019/040] [Need: 00:23:15] [LR=0.0100] [Best : Accuracy=73.54, Error=26.46]
  Epoch: [019][000/500]   Time 22.814 (22.814)   Data 22.742 (22.742)   Loss 0.9600 (0.9600)   Prec@1 68.000 (68.000)   Prec@5 98.000 (98.000)   [2025-10-24 10:21:59]
  Epoch: [019][100/500]   Time 0.019 (0.246)   Data 0.000 (0.225)   Loss 1.1342 (0.9760)   Prec@1 65.000 (66.109)   Prec@5 96.000 (96.723)   [2025-10-24 10:22:01]
  Epoch: [019][200/500]   Time 0.020 (0.133)   Data 0.000 (0.113)   Loss 1.1591 (0.9721)   Prec@1 58.000 (66.030)   Prec@5 95.000 (96.726)   [2025-10-24 10:22:03]
  Epoch: [019][300/500]   Time 0.017 (0.095)   Data 0.000 (0.076)   Loss 0.9500 (0.9656)   Prec@1 68.000 (66.186)   Prec@5 98.000 (96.867)   [2025-10-24 10:22:05]
  Epoch: [019][400/500]   Time 0.019 (0.076)   Data 0.000 (0.057)   Loss 0.9966 (0.9722)   Prec@1 71.000 (66.010)   Prec@5 94.000 (96.781)   [2025-10-24 10:22:07]
  **Train** Prec@1 66.244 Prec@5 96.788 Error@1 33.756
  **Test** Prec@1 75.040 Prec@5 98.400 Error@1 24.960
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:22:33] [Epoch=020/040] [Need: 00:21:58] [LR=0.0100] [Best : Accuracy=75.04, Error=24.96]
  Epoch: [020][000/500]   Time 22.770 (22.770)   Data 22.701 (22.701)   Loss 1.0351 (1.0351)   Prec@1 66.000 (66.000)   Prec@5 97.000 (97.000)   [2025-10-24 10:22:56]
  Epoch: [020][100/500]   Time 0.018 (0.245)   Data 0.001 (0.225)   Loss 0.9449 (0.9438)   Prec@1 69.000 (67.238)   Prec@5 94.000 (96.970)   [2025-10-24 10:22:58]
  Epoch: [020][200/500]   Time 0.029 (0.133)   Data 0.000 (0.113)   Loss 0.9965 (0.9482)   Prec@1 65.000 (67.498)   Prec@5 94.000 (96.756)   [2025-10-24 10:23:00]
  Epoch: [020][300/500]   Time 0.019 (0.095)   Data 0.000 (0.076)   Loss 0.8718 (0.9438)   Prec@1 68.000 (67.545)   Prec@5 98.000 (96.728)   [2025-10-24 10:23:02]
  Epoch: [020][400/500]   Time 0.019 (0.076)   Data 0.001 (0.057)   Loss 0.9428 (0.9437)   Prec@1 68.000 (67.389)   Prec@5 97.000 (96.758)   [2025-10-24 10:23:03]
  **Train** Prec@1 67.372 Prec@5 96.828 Error@1 32.628
  **Test** Prec@1 75.220 Prec@5 98.380 Error@1 24.780
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:23:29] [Epoch=021/040] [Need: 00:20:44] [LR=0.0100] [Best : Accuracy=75.22, Error=24.78]
  Epoch: [021][000/500]   Time 22.257 (22.257)   Data 22.193 (22.193)   Loss 1.0816 (1.0816)   Prec@1 62.000 (62.000)   Prec@5 95.000 (95.000)   [2025-10-24 10:23:51]
  Epoch: [021][100/500]   Time 0.019 (0.241)   Data 0.000 (0.220)   Loss 0.8723 (0.9321)   Prec@1 73.000 (67.752)   Prec@5 96.000 (96.881)   [2025-10-24 10:23:53]
  Epoch: [021][200/500]   Time 0.018 (0.131)   Data 0.000 (0.111)   Loss 0.9063 (0.9228)   Prec@1 71.000 (67.876)   Prec@5 99.000 (97.085)   [2025-10-24 10:23:55]
  Epoch: [021][300/500]   Time 0.017 (0.093)   Data 0.001 (0.074)   Loss 1.0428 (0.9229)   Prec@1 63.000 (68.013)   Prec@5 95.000 (97.017)   [2025-10-24 10:23:57]
  Epoch: [021][400/500]   Time 0.018 (0.075)   Data 0.000 (0.056)   Loss 1.0706 (0.9221)   Prec@1 64.000 (68.142)   Prec@5 96.000 (97.000)   [2025-10-24 10:23:59]
  **Train** Prec@1 68.236 Prec@5 97.004 Error@1 31.764
  **Test** Prec@1 75.540 Prec@5 98.640 Error@1 24.460
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:24:25] [Epoch=022/040] [Need: 00:19:30] [LR=0.0100] [Best : Accuracy=75.54, Error=24.46]
  Epoch: [022][000/500]   Time 22.800 (22.800)   Data 22.735 (22.735)   Loss 0.8336 (0.8336)   Prec@1 71.000 (71.000)   Prec@5 100.000 (100.000)   [2025-10-24 10:24:48]
  Epoch: [022][100/500]   Time 0.018 (0.246)   Data 0.000 (0.225)   Loss 0.7444 (0.9159)   Prec@1 72.000 (68.713)   Prec@5 96.000 (96.436)   [2025-10-24 10:24:50]
  Epoch: [022][200/500]   Time 0.017 (0.133)   Data 0.000 (0.113)   Loss 0.8530 (0.9134)   Prec@1 71.000 (68.627)   Prec@5 99.000 (96.751)   [2025-10-24 10:24:52]
  Epoch: [022][300/500]   Time 0.019 (0.095)   Data 0.002 (0.076)   Loss 1.0342 (0.9122)   Prec@1 64.000 (68.425)   Prec@5 95.000 (96.880)   [2025-10-24 10:24:54]
  Epoch: [022][400/500]   Time 0.020 (0.076)   Data 0.000 (0.057)   Loss 0.9313 (0.9074)   Prec@1 67.000 (68.661)   Prec@5 98.000 (96.928)   [2025-10-24 10:24:55]
  **Train** Prec@1 68.744 Prec@5 96.958 Error@1 31.256
  **Test** Prec@1 76.530 Prec@5 98.370 Error@1 23.470
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:25:21] [Epoch=023/040] [Need: 00:18:18] [LR=0.0100] [Best : Accuracy=76.53, Error=23.47]
  Epoch: [023][000/500]   Time 22.356 (22.356)   Data 22.290 (22.290)   Loss 0.8605 (0.8605)   Prec@1 67.000 (67.000)   Prec@5 97.000 (97.000)   [2025-10-24 10:25:43]
  Epoch: [023][100/500]   Time 0.019 (0.242)   Data 0.000 (0.221)   Loss 0.8310 (0.9017)   Prec@1 68.000 (68.168)   Prec@5 97.000 (97.040)   [2025-10-24 10:25:45]
  Epoch: [023][200/500]   Time 0.023 (0.131)   Data 0.001 (0.111)   Loss 0.9507 (0.9044)   Prec@1 70.000 (68.234)   Prec@5 97.000 (97.104)   [2025-10-24 10:25:47]
  Epoch: [023][300/500]   Time 0.018 (0.093)   Data 0.000 (0.074)   Loss 0.7928 (0.9048)   Prec@1 73.000 (68.532)   Prec@5 97.000 (97.070)   [2025-10-24 10:25:49]
  Epoch: [023][400/500]   Time 0.018 (0.075)   Data 0.000 (0.056)   Loss 0.9474 (0.8998)   Prec@1 66.000 (68.835)   Prec@5 94.000 (97.092)   [2025-10-24 10:25:51]
  **Train** Prec@1 69.066 Prec@5 97.170 Error@1 30.934
  **Test** Prec@1 76.830 Prec@5 98.720 Error@1 23.170
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:26:16] [Epoch=024/040] [Need: 00:17:08] [LR=0.0100] [Best : Accuracy=76.83, Error=23.17]
  Epoch: [024][000/500]   Time 22.198 (22.198)   Data 22.129 (22.129)   Loss 0.8078 (0.8078)   Prec@1 69.000 (69.000)   Prec@5 96.000 (96.000)   [2025-10-24 10:26:38]
  Epoch: [024][100/500]   Time 0.021 (0.241)   Data 0.000 (0.219)   Loss 0.9728 (0.8776)   Prec@1 68.000 (69.703)   Prec@5 96.000 (97.050)   [2025-10-24 10:26:40]
  Epoch: [024][200/500]   Time 0.018 (0.130)   Data 0.000 (0.110)   Loss 0.9677 (0.8875)   Prec@1 65.000 (69.363)   Prec@5 97.000 (97.090)   [2025-10-24 10:26:42]
  Epoch: [024][300/500]   Time 0.017 (0.093)   Data 0.000 (0.074)   Loss 0.9004 (0.8825)   Prec@1 67.000 (69.784)   Prec@5 95.000 (97.073)   [2025-10-24 10:26:44]
  Epoch: [024][400/500]   Time 0.017 (0.075)   Data 0.000 (0.056)   Loss 0.8719 (0.8782)   Prec@1 74.000 (69.825)   Prec@5 97.000 (97.202)   [2025-10-24 10:26:46]
  **Train** Prec@1 69.826 Prec@5 97.212 Error@1 30.174
  **Test** Prec@1 77.820 Prec@5 98.760 Error@1 22.180
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:27:11] [Epoch=025/040] [Need: 00:15:58] [LR=0.0010] [Best : Accuracy=77.82, Error=22.18]
  Epoch: [025][000/500]   Time 22.603 (22.603)   Data 22.534 (22.534)   Loss 0.8606 (0.8606)   Prec@1 69.000 (69.000)   Prec@5 99.000 (99.000)   [2025-10-24 10:27:34]
  Epoch: [025][100/500]   Time 0.019 (0.243)   Data 0.001 (0.223)   Loss 0.9334 (0.8458)   Prec@1 67.000 (70.634)   Prec@5 96.000 (97.653)   [2025-10-24 10:27:36]
  Epoch: [025][200/500]   Time 0.017 (0.131)   Data 0.000 (0.112)   Loss 0.7146 (0.8303)   Prec@1 76.000 (71.353)   Prec@5 97.000 (97.706)   [2025-10-24 10:27:38]
  Epoch: [025][300/500]   Time 0.019 (0.094)   Data 0.000 (0.075)   Loss 1.0678 (0.8197)   Prec@1 67.000 (71.678)   Prec@5 94.000 (97.728)   [2025-10-24 10:27:40]
  Epoch: [025][400/500]   Time 0.018 (0.075)   Data 0.000 (0.056)   Loss 0.7533 (0.8176)   Prec@1 79.000 (71.843)   Prec@5 100.000 (97.708)   [2025-10-24 10:27:41]
  **Train** Prec@1 71.978 Prec@5 97.692 Error@1 28.022
  **Test** Prec@1 79.080 Prec@5 98.890 Error@1 20.920
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:28:07] [Epoch=026/040] [Need: 00:14:50] [LR=0.0010] [Best : Accuracy=79.08, Error=20.92]
  Epoch: [026][000/500]   Time 22.194 (22.194)   Data 22.128 (22.128)   Loss 0.9342 (0.9342)   Prec@1 69.000 (69.000)   Prec@5 97.000 (97.000)   [2025-10-24 10:28:29]
  Epoch: [026][100/500]   Time 0.021 (0.239)   Data 0.001 (0.219)   Loss 0.7273 (0.8030)   Prec@1 69.000 (71.436)   Prec@5 99.000 (97.871)   [2025-10-24 10:28:31]
  Epoch: [026][200/500]   Time 0.020 (0.130)   Data 0.000 (0.110)   Loss 0.8866 (0.8003)   Prec@1 68.000 (72.030)   Prec@5 97.000 (97.821)   [2025-10-24 10:28:33]
  Epoch: [026][300/500]   Time 0.018 (0.093)   Data 0.000 (0.074)   Loss 0.8067 (0.7979)   Prec@1 75.000 (72.246)   Prec@5 99.000 (97.827)   [2025-10-24 10:28:35]
  Epoch: [026][400/500]   Time 0.018 (0.074)   Data 0.000 (0.055)   Loss 0.7505 (0.8001)   Prec@1 74.000 (72.416)   Prec@5 100.000 (97.771)   [2025-10-24 10:28:37]
  **Train** Prec@1 72.704 Prec@5 97.786 Error@1 27.296
  **Test** Prec@1 79.580 Prec@5 98.880 Error@1 20.420
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:29:02] [Epoch=027/040] [Need: 00:13:42] [LR=0.0010] [Best : Accuracy=79.58, Error=20.42]
  Epoch: [027][000/500]   Time 22.359 (22.359)   Data 22.292 (22.292)   Loss 0.8476 (0.8476)   Prec@1 68.000 (68.000)   Prec@5 96.000 (96.000)   [2025-10-24 10:29:24]
  Epoch: [027][100/500]   Time 0.021 (0.241)   Data 0.001 (0.221)   Loss 0.8522 (0.7818)   Prec@1 72.000 (73.079)   Prec@5 100.000 (97.822)   [2025-10-24 10:29:26]
  Epoch: [027][200/500]   Time 0.017 (0.130)   Data 0.000 (0.111)   Loss 0.9157 (0.7950)   Prec@1 71.000 (72.781)   Prec@5 97.000 (97.731)   [2025-10-24 10:29:28]
  Epoch: [027][300/500]   Time 0.017 (0.093)   Data 0.000 (0.074)   Loss 0.6838 (0.7946)   Prec@1 74.000 (72.698)   Prec@5 99.000 (97.734)   [2025-10-24 10:29:30]
  Epoch: [027][400/500]   Time 0.020 (0.075)   Data 0.001 (0.056)   Loss 0.9469 (0.7915)   Prec@1 72.000 (72.930)   Prec@5 95.000 (97.761)   [2025-10-24 10:29:32]
  **Train** Prec@1 73.032 Prec@5 97.798 Error@1 26.968
  **Test** Prec@1 79.350 Prec@5 98.860 Error@1 20.650

==>>[2025-10-24 10:29:57] [Epoch=028/040] [Need: 00:12:35] [LR=0.0010] [Best : Accuracy=79.58, Error=20.42]
  Epoch: [028][000/500]   Time 22.010 (22.010)   Data 21.944 (21.944)   Loss 0.7170 (0.7170)   Prec@1 80.000 (80.000)   Prec@5 96.000 (96.000)   [2025-10-24 10:30:19]
  Epoch: [028][100/500]   Time 0.038 (0.238)   Data 0.000 (0.217)   Loss 0.7858 (0.7802)   Prec@1 75.000 (73.238)   Prec@5 97.000 (97.911)   [2025-10-24 10:30:21]
  Epoch: [028][200/500]   Time 0.019 (0.130)   Data 0.000 (0.110)   Loss 0.7369 (0.7827)   Prec@1 78.000 (73.174)   Prec@5 99.000 (97.801)   [2025-10-24 10:30:23]
  Epoch: [028][300/500]   Time 0.020 (0.093)   Data 0.001 (0.074)   Loss 0.6223 (0.7764)   Prec@1 75.000 (73.385)   Prec@5 99.000 (97.860)   [2025-10-24 10:30:25]
  Epoch: [028][400/500]   Time 0.019 (0.074)   Data 0.001 (0.055)   Loss 0.7783 (0.7771)   Prec@1 73.000 (73.389)   Prec@5 99.000 (97.850)   [2025-10-24 10:30:26]
  **Train** Prec@1 73.370 Prec@5 97.828 Error@1 26.630
  **Test** Prec@1 79.920 Prec@5 98.910 Error@1 20.080
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:30:52] [Epoch=029/040] [Need: 00:11:29] [LR=0.0010] [Best : Accuracy=79.92, Error=20.08]
  Epoch: [029][000/500]   Time 22.120 (22.120)   Data 22.052 (22.052)   Loss 0.8219 (0.8219)   Prec@1 75.000 (75.000)   Prec@5 97.000 (97.000)   [2025-10-24 10:31:14]
  Epoch: [029][100/500]   Time 0.020 (0.239)   Data 0.000 (0.219)   Loss 0.8660 (0.7582)   Prec@1 72.000 (74.723)   Prec@5 96.000 (97.624)   [2025-10-24 10:31:16]
  Epoch: [029][200/500]   Time 0.018 (0.129)   Data 0.001 (0.110)   Loss 0.9309 (0.7606)   Prec@1 71.000 (74.095)   Prec@5 97.000 (97.891)   [2025-10-24 10:31:18]
  Epoch: [029][300/500]   Time 0.018 (0.093)   Data 0.000 (0.074)   Loss 0.8277 (0.7632)   Prec@1 69.000 (73.860)   Prec@5 99.000 (97.957)   [2025-10-24 10:31:20]
  Epoch: [029][400/500]   Time 0.019 (0.074)   Data 0.001 (0.055)   Loss 0.7140 (0.7646)   Prec@1 73.000 (73.873)   Prec@5 99.000 (97.968)   [2025-10-24 10:31:21]
  **Train** Prec@1 73.758 Prec@5 97.968 Error@1 26.242
  **Test** Prec@1 80.200 Prec@5 98.910 Error@1 19.800
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:31:47] [Epoch=030/040] [Need: 00:10:24] [LR=0.0010] [Best : Accuracy=80.20, Error=19.80]
  Epoch: [030][000/500]   Time 21.880 (21.880)   Data 21.813 (21.813)   Loss 0.8961 (0.8961)   Prec@1 68.000 (68.000)   Prec@5 97.000 (97.000)   [2025-10-24 10:32:09]
  Epoch: [030][100/500]   Time 0.020 (0.238)   Data 0.000 (0.216)   Loss 0.7905 (0.7560)   Prec@1 70.000 (73.149)   Prec@5 99.000 (97.950)   [2025-10-24 10:32:11]
  Epoch: [030][200/500]   Time 0.018 (0.129)   Data 0.000 (0.109)   Loss 0.6406 (0.7613)   Prec@1 79.000 (73.423)   Prec@5 97.000 (97.990)   [2025-10-24 10:32:13]
  Epoch: [030][300/500]   Time 0.019 (0.092)   Data 0.000 (0.073)   Loss 0.6149 (0.7646)   Prec@1 79.000 (73.312)   Prec@5 99.000 (97.987)   [2025-10-24 10:32:15]
  Epoch: [030][400/500]   Time 0.020 (0.074)   Data 0.000 (0.055)   Loss 0.6083 (0.7643)   Prec@1 78.000 (73.342)   Prec@5 98.000 (97.985)   [2025-10-24 10:32:17]
  **Train** Prec@1 73.432 Prec@5 97.960 Error@1 26.568
  **Test** Prec@1 80.010 Prec@5 98.840 Error@1 19.990

==>>[2025-10-24 10:32:41] [Epoch=031/040] [Need: 00:09:19] [LR=0.0010] [Best : Accuracy=80.20, Error=19.80]
  Epoch: [031][000/500]   Time 22.919 (22.919)   Data 22.851 (22.851)   Loss 0.8460 (0.8460)   Prec@1 73.000 (73.000)   Prec@5 95.000 (95.000)   [2025-10-24 10:33:04]
  Epoch: [031][100/500]   Time 0.016 (0.247)   Data 0.000 (0.226)   Loss 0.7205 (0.7750)   Prec@1 74.000 (72.891)   Prec@5 98.000 (98.238)   [2025-10-24 10:33:06]
  Epoch: [031][200/500]   Time 0.019 (0.134)   Data 0.000 (0.114)   Loss 0.7314 (0.7661)   Prec@1 78.000 (73.512)   Prec@5 98.000 (98.139)   [2025-10-24 10:33:08]
  Epoch: [031][300/500]   Time 0.019 (0.095)   Data 0.000 (0.076)   Loss 0.8608 (0.7577)   Prec@1 74.000 (73.860)   Prec@5 97.000 (98.056)   [2025-10-24 10:33:10]
  Epoch: [031][400/500]   Time 0.018 (0.076)   Data 0.000 (0.057)   Loss 0.6631 (0.7569)   Prec@1 78.000 (73.746)   Prec@5 100.000 (98.015)   [2025-10-24 10:33:12]
  **Train** Prec@1 73.768 Prec@5 98.010 Error@1 26.232
  **Test** Prec@1 80.230 Prec@5 98.910 Error@1 19.770
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:33:37] [Epoch=032/040] [Need: 00:08:15] [LR=0.0010] [Best : Accuracy=80.23, Error=19.77]
  Epoch: [032][000/500]   Time 23.199 (23.199)   Data 23.131 (23.131)   Loss 0.8402 (0.8402)   Prec@1 76.000 (76.000)   Prec@5 95.000 (95.000)   [2025-10-24 10:34:01]
  Epoch: [032][100/500]   Time 0.018 (0.249)   Data 0.000 (0.229)   Loss 0.8476 (0.7555)   Prec@1 65.000 (73.644)   Prec@5 99.000 (98.030)   [2025-10-24 10:34:02]
  Epoch: [032][200/500]   Time 0.018 (0.135)   Data 0.000 (0.115)   Loss 0.8284 (0.7621)   Prec@1 73.000 (73.791)   Prec@5 97.000 (97.965)   [2025-10-24 10:34:04]
  Epoch: [032][300/500]   Time 0.018 (0.096)   Data 0.000 (0.077)   Loss 0.7310 (0.7563)   Prec@1 69.000 (74.110)   Prec@5 99.000 (98.047)   [2025-10-24 10:34:06]
  Epoch: [032][400/500]   Time 0.019 (0.077)   Data 0.000 (0.058)   Loss 0.6521 (0.7593)   Prec@1 81.000 (74.100)   Prec@5 98.000 (97.973)   [2025-10-24 10:34:08]
  **Train** Prec@1 74.126 Prec@5 97.978 Error@1 25.874
  **Test** Prec@1 79.940 Prec@5 98.940 Error@1 20.060

==>>[2025-10-24 10:34:33] [Epoch=033/040] [Need: 00:07:12] [LR=0.0010] [Best : Accuracy=80.23, Error=19.77]
  Epoch: [033][000/500]   Time 23.157 (23.157)   Data 23.092 (23.092)   Loss 0.7432 (0.7432)   Prec@1 77.000 (77.000)   Prec@5 97.000 (97.000)   [2025-10-24 10:34:56]
  Epoch: [033][100/500]   Time 0.018 (0.250)   Data 0.000 (0.229)   Loss 0.9350 (0.7533)   Prec@1 64.000 (74.218)   Prec@5 98.000 (98.050)   [2025-10-24 10:34:58]
  Epoch: [033][200/500]   Time 0.017 (0.135)   Data 0.000 (0.115)   Loss 0.7294 (0.7517)   Prec@1 71.000 (74.005)   Prec@5 100.000 (97.950)   [2025-10-24 10:35:00]
  Epoch: [033][300/500]   Time 0.018 (0.096)   Data 0.001 (0.077)   Loss 0.8076 (0.7497)   Prec@1 70.000 (74.173)   Prec@5 97.000 (97.934)   [2025-10-24 10:35:02]
  Epoch: [033][400/500]   Time 0.019 (0.077)   Data 0.000 (0.058)   Loss 0.7082 (0.7480)   Prec@1 78.000 (74.309)   Prec@5 99.000 (97.970)   [2025-10-24 10:35:04]
  **Train** Prec@1 74.232 Prec@5 97.986 Error@1 25.768
  **Test** Prec@1 80.390 Prec@5 98.970 Error@1 19.610
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:35:30] [Epoch=034/040] [Need: 00:06:09] [LR=0.0010] [Best : Accuracy=80.39, Error=19.61]
  Epoch: [034][000/500]   Time 22.515 (22.515)   Data 22.449 (22.449)   Loss 0.7629 (0.7629)   Prec@1 72.000 (72.000)   Prec@5 98.000 (98.000)   [2025-10-24 10:35:52]
  Epoch: [034][100/500]   Time 0.022 (0.243)   Data 0.000 (0.222)   Loss 0.8613 (0.7530)   Prec@1 68.000 (74.010)   Prec@5 95.000 (97.921)   [2025-10-24 10:35:54]
  Epoch: [034][200/500]   Time 0.018 (0.131)   Data 0.000 (0.112)   Loss 0.7221 (0.7638)   Prec@1 76.000 (73.572)   Prec@5 95.000 (97.950)   [2025-10-24 10:35:56]
  Epoch: [034][300/500]   Time 0.019 (0.094)   Data 0.000 (0.075)   Loss 0.7368 (0.7604)   Prec@1 72.000 (73.801)   Prec@5 97.000 (97.884)   [2025-10-24 10:35:58]
  Epoch: [034][400/500]   Time 0.023 (0.075)   Data 0.000 (0.056)   Loss 0.7122 (0.7568)   Prec@1 76.000 (74.040)   Prec@5 97.000 (97.948)   [2025-10-24 10:36:00]
  **Train** Prec@1 74.178 Prec@5 97.974 Error@1 25.822
  **Test** Prec@1 80.120 Prec@5 98.930 Error@1 19.880

==>>[2025-10-24 10:36:24] [Epoch=035/040] [Need: 00:05:07] [LR=0.0010] [Best : Accuracy=80.39, Error=19.61]
  Epoch: [035][000/500]   Time 21.815 (21.815)   Data 21.749 (21.749)   Loss 0.8977 (0.8977)   Prec@1 66.000 (66.000)   Prec@5 98.000 (98.000)   [2025-10-24 10:36:46]
  Epoch: [035][100/500]   Time 0.017 (0.236)   Data 0.000 (0.215)   Loss 0.6222 (0.7514)   Prec@1 79.000 (73.950)   Prec@5 99.000 (97.802)   [2025-10-24 10:36:48]
  Epoch: [035][200/500]   Time 0.019 (0.128)   Data 0.000 (0.108)   Loss 0.7983 (0.7389)   Prec@1 76.000 (74.338)   Prec@5 98.000 (97.876)   [2025-10-24 10:36:50]
  Epoch: [035][300/500]   Time 0.024 (0.091)   Data 0.000 (0.072)   Loss 0.7187 (0.7419)   Prec@1 76.000 (74.389)   Prec@5 99.000 (98.003)   [2025-10-24 10:36:52]
  Epoch: [035][400/500]   Time 0.019 (0.073)   Data 0.001 (0.054)   Loss 0.8797 (0.7459)   Prec@1 67.000 (74.352)   Prec@5 100.000 (98.035)   [2025-10-24 10:36:54]
  **Train** Prec@1 74.194 Prec@5 98.000 Error@1 25.806
  **Test** Prec@1 80.200 Prec@5 98.900 Error@1 19.800

==>>[2025-10-24 10:37:19] [Epoch=036/040] [Need: 00:04:04] [LR=0.0010] [Best : Accuracy=80.39, Error=19.61]
  Epoch: [036][000/500]   Time 22.816 (22.816)   Data 22.749 (22.749)   Loss 0.6333 (0.6333)   Prec@1 76.000 (76.000)   Prec@5 100.000 (100.000)   [2025-10-24 10:37:41]
  Epoch: [036][100/500]   Time 0.019 (0.246)   Data 0.000 (0.225)   Loss 0.7074 (0.7435)   Prec@1 76.000 (74.614)   Prec@5 97.000 (98.059)   [2025-10-24 10:37:43]
  Epoch: [036][200/500]   Time 0.019 (0.133)   Data 0.000 (0.113)   Loss 0.8491 (0.7475)   Prec@1 66.000 (74.159)   Prec@5 97.000 (98.010)   [2025-10-24 10:37:45]
  Epoch: [036][300/500]   Time 0.018 (0.095)   Data 0.000 (0.076)   Loss 0.5390 (0.7466)   Prec@1 84.000 (74.256)   Prec@5 99.000 (98.013)   [2025-10-24 10:37:47]
  Epoch: [036][400/500]   Time 0.018 (0.076)   Data 0.000 (0.057)   Loss 0.8546 (0.7468)   Prec@1 72.000 (74.342)   Prec@5 98.000 (97.985)   [2025-10-24 10:37:49]
  **Train** Prec@1 74.468 Prec@5 98.026 Error@1 25.532
  **Test** Prec@1 80.200 Prec@5 98.950 Error@1 19.800

==>>[2025-10-24 10:38:14] [Epoch=037/040] [Need: 00:03:03] [LR=0.0010] [Best : Accuracy=80.39, Error=19.61]
  Epoch: [037][000/500]   Time 22.336 (22.336)   Data 22.269 (22.269)   Loss 0.7347 (0.7347)   Prec@1 74.000 (74.000)   Prec@5 99.000 (99.000)   [2025-10-24 10:38:37]
  Epoch: [037][100/500]   Time 0.021 (0.241)   Data 0.000 (0.221)   Loss 0.8294 (0.7282)   Prec@1 69.000 (75.069)   Prec@5 97.000 (98.158)   [2025-10-24 10:38:39]
  Epoch: [037][200/500]   Time 0.018 (0.131)   Data 0.000 (0.111)   Loss 0.5713 (0.7339)   Prec@1 86.000 (74.771)   Prec@5 98.000 (98.025)   [2025-10-24 10:38:40]
  Epoch: [037][300/500]   Time 0.019 (0.093)   Data 0.001 (0.074)   Loss 0.6766 (0.7354)   Prec@1 78.000 (74.678)   Prec@5 96.000 (98.043)   [2025-10-24 10:38:42]
  Epoch: [037][400/500]   Time 0.020 (0.075)   Data 0.001 (0.056)   Loss 0.8527 (0.7397)   Prec@1 71.000 (74.673)   Prec@5 97.000 (98.050)   [2025-10-24 10:38:44]
  **Train** Prec@1 74.622 Prec@5 98.024 Error@1 25.378
  **Test** Prec@1 80.790 Prec@5 98.990 Error@1 19.210
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 10:39:09] [Epoch=038/040] [Need: 00:02:01] [LR=0.0010] [Best : Accuracy=80.79, Error=19.21]
  Epoch: [038][000/500]   Time 22.329 (22.329)   Data 22.257 (22.257)   Loss 0.8468 (0.8468)   Prec@1 76.000 (76.000)   Prec@5 97.000 (97.000)   [2025-10-24 10:39:32]
  Epoch: [038][100/500]   Time 0.018 (0.241)   Data 0.000 (0.221)   Loss 0.6930 (0.7568)   Prec@1 77.000 (74.337)   Prec@5 98.000 (97.871)   [2025-10-24 10:39:34]
  Epoch: [038][200/500]   Time 0.017 (0.130)   Data 0.000 (0.111)   Loss 0.8526 (0.7417)   Prec@1 72.000 (74.821)   Prec@5 100.000 (98.065)   [2025-10-24 10:39:35]
  Epoch: [038][300/500]   Time 0.022 (0.093)   Data 0.000 (0.074)   Loss 0.7232 (0.7373)   Prec@1 75.000 (74.953)   Prec@5 97.000 (98.093)   [2025-10-24 10:39:37]
  Epoch: [038][400/500]   Time 0.017 (0.075)   Data 0.000 (0.056)   Loss 0.9467 (0.7405)   Prec@1 68.000 (74.855)   Prec@5 96.000 (98.070)   [2025-10-24 10:39:39]
  **Train** Prec@1 74.722 Prec@5 98.058 Error@1 25.278
  **Test** Prec@1 80.630 Prec@5 98.990 Error@1 19.370

==>>[2025-10-24 10:40:06] [Epoch=039/040] [Need: 00:01:00] [LR=0.0010] [Best : Accuracy=80.79, Error=19.21]
  Epoch: [039][000/500]   Time 23.112 (23.112)   Data 23.037 (23.037)   Loss 0.9168 (0.9168)   Prec@1 67.000 (67.000)   Prec@5 96.000 (96.000)   [2025-10-24 10:40:29]
  Epoch: [039][100/500]   Time 0.018 (0.249)   Data 0.000 (0.228)   Loss 0.6932 (0.7424)   Prec@1 76.000 (74.495)   Prec@5 97.000 (97.970)   [2025-10-24 10:40:31]
  Epoch: [039][200/500]   Time 0.021 (0.135)   Data 0.001 (0.115)   Loss 0.6324 (0.7442)   Prec@1 77.000 (74.587)   Prec@5 99.000 (98.055)   [2025-10-24 10:40:33]
  Epoch: [039][300/500]   Time 0.022 (0.096)   Data 0.000 (0.077)   Loss 0.6344 (0.7423)   Prec@1 79.000 (74.548)   Prec@5 100.000 (98.076)   [2025-10-24 10:40:35]
  Epoch: [039][400/500]   Time 0.018 (0.077)   Data 0.000 (0.058)   Loss 0.8167 (0.7399)   Prec@1 73.000 (74.663)   Prec@5 98.000 (98.067)   [2025-10-24 10:40:37]
  **Train** Prec@1 74.728 Prec@5 98.016 Error@1 25.272
  **Test** Prec@1 80.580 Prec@5 98.940 Error@1 19.420
