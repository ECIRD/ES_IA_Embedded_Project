save path : ./save/tinyvgg_quan/randbet_0.2_0.01_10_-1
{'data_path': './dataset', 'arch': 'tinyvgg_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': True, 'clipping_coeff': 0.2, 'learning_rate': 0.01, 'manualSeed': 280, 'save_path': './save/tinyvgg_quan/randbet_0.2_0.01_10_-1', 'enable_bfa': False, 'resume': '', 'quan_bitwidth': None, 'reset_weight': False, 'evaluate': False, 'n_iter': 30, 'model_only': False, 'random_bfa': False, 'use_cuda': True}
Random Seed: 280
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'tinyvgg_quan'
=> network :
 TinyVGG(
  (features): Sequential(
    (0): quan_Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Dropout2d(p=0.3, inplace=False)
    (6): quan_Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace=True)
    (8): quan_Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace=True)
    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (11): Dropout2d(p=0.3, inplace=False)
    (12): quan_Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): quan_Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Dropout2d(p=0.3, inplace=False)
  )
  (classifier): Sequential(
    (0): quan_Linear(in_features=2048, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): quan_Linear(in_features=128, out_features=10, bias=True)
  )
)
=> do not use any checkpoint for tinyvgg_quan model

==>>[2025-10-24 16:59:12] [Epoch=000/040] [Need: 00:00:00] [LR=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/500]   Time 18.794 (18.794)   Data 17.497 (17.497)   Loss 2.3055 (2.3055)   Prec@1 3.000 (3.000)   Prec@5 42.000 (42.000)   [2025-10-24 16:59:31]
  Epoch: [000][100/500]   Time 0.346 (0.556)   Data 0.001 (0.175)   Loss 2.3046 (2.3029)   Prec@1 8.000 (10.277)   Prec@5 49.000 (49.812)   [2025-10-24 17:00:09]
  Epoch: [000][200/500]   Time 0.382 (0.465)   Data 0.000 (0.089)   Loss 2.3012 (2.3027)   Prec@1 9.000 (10.318)   Prec@5 52.000 (50.313)   [2025-10-24 17:00:46]
  Epoch: [000][300/500]   Time 0.442 (0.435)   Data 0.001 (0.060)   Loss 2.2850 (2.3012)   Prec@1 11.000 (10.548)   Prec@5 59.000 (51.724)   [2025-10-24 17:01:23]
  Epoch: [000][400/500]   Time 0.313 (0.420)   Data 0.000 (0.045)   Loss 2.0882 (2.2756)   Prec@1 19.000 (12.623)   Prec@5 77.000 (56.095)   [2025-10-24 17:02:01]
  **Train** Prec@1 14.578 Prec@5 60.028 Error@1 85.422
  **Test** Prec@1 27.410 Prec@5 79.930 Error@1 72.590
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 17:02:57] [Epoch=001/040] [Need: 02:26:13] [LR=0.0100] [Best : Accuracy=27.41, Error=72.59]
  Epoch: [001][000/500]   Time 18.017 (18.017)   Data 17.480 (17.480)   Loss 2.0277 (2.0277)   Prec@1 23.000 (23.000)   Prec@5 77.000 (77.000)   [2025-10-24 17:03:16]
  Epoch: [001][100/500]   Time 0.413 (0.556)   Data 0.001 (0.175)   Loss 2.0317 (2.0273)   Prec@1 22.000 (24.307)   Prec@5 77.000 (78.495)   [2025-10-24 17:03:54]
  Epoch: [001][200/500]   Time 0.388 (0.463)   Data 0.002 (0.088)   Loss 1.8785 (2.0010)   Prec@1 28.000 (24.940)   Prec@5 83.000 (79.637)   [2025-10-24 17:04:31]
  Epoch: [001][300/500]   Time 0.393 (0.435)   Data 0.002 (0.059)   Loss 1.9398 (1.9817)   Prec@1 26.000 (25.734)   Prec@5 76.000 (80.352)   [2025-10-24 17:05:08]
  Epoch: [001][400/500]   Time 0.334 (0.418)   Data 0.001 (0.045)   Loss 1.9533 (1.9651)   Prec@1 27.000 (26.192)   Prec@5 78.000 (81.025)   [2025-10-24 17:05:45]
  **Train** Prec@1 26.780 Prec@5 81.650 Error@1 73.220
  **Test** Prec@1 33.980 Prec@5 86.840 Error@1 66.020
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 17:06:41] [Epoch=002/040] [Need: 02:22:00] [LR=0.0100] [Best : Accuracy=33.98, Error=66.02]
  Epoch: [002][000/500]   Time 18.255 (18.255)   Data 17.721 (17.721)   Loss 1.7889 (1.7889)   Prec@1 31.000 (31.000)   Prec@5 90.000 (90.000)   [2025-10-24 17:06:59]
  Epoch: [002][100/500]   Time 0.404 (0.548)   Data 0.000 (0.177)   Loss 1.8454 (1.8578)   Prec@1 29.000 (30.050)   Prec@5 86.000 (85.436)   [2025-10-24 17:07:36]
  Epoch: [002][200/500]   Time 0.391 (0.460)   Data 0.000 (0.090)   Loss 1.9343 (1.8420)   Prec@1 34.000 (30.502)   Prec@5 88.000 (85.498)   [2025-10-24 17:08:13]
  Epoch: [002][300/500]   Time 0.430 (0.430)   Data 0.002 (0.060)   Loss 1.8449 (1.8307)   Prec@1 35.000 (30.907)   Prec@5 85.000 (85.618)   [2025-10-24 17:08:50]
  Epoch: [002][400/500]   Time 0.388 (0.415)   Data 0.002 (0.045)   Loss 1.6956 (1.8122)   Prec@1 29.000 (31.526)   Prec@5 91.000 (86.102)   [2025-10-24 17:09:28]
  **Train** Prec@1 32.102 Prec@5 86.426 Error@1 67.898
  **Test** Prec@1 37.350 Prec@5 89.840 Error@1 62.650
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 17:10:23] [Epoch=003/040] [Need: 02:17:48] [LR=0.0100] [Best : Accuracy=37.35, Error=62.65]
  Epoch: [003][000/500]   Time 18.404 (18.404)   Data 17.868 (17.868)   Loss 1.7108 (1.7108)   Prec@1 35.000 (35.000)   Prec@5 90.000 (90.000)   [2025-10-24 17:10:41]
  Epoch: [003][100/500]   Time 0.394 (0.547)   Data 0.002 (0.178)   Loss 1.8598 (1.7306)   Prec@1 31.000 (35.158)   Prec@5 83.000 (88.000)   [2025-10-24 17:11:18]
  Epoch: [003][200/500]   Time 0.415 (0.460)   Data 0.002 (0.090)   Loss 1.7107 (1.7157)   Prec@1 36.000 (35.706)   Prec@5 91.000 (88.294)   [2025-10-24 17:11:55]
  Epoch: [003][300/500]   Time 0.316 (0.431)   Data 0.000 (0.061)   Loss 1.4160 (1.7033)   Prec@1 43.000 (36.156)   Prec@5 95.000 (88.525)   [2025-10-24 17:12:33]
  Epoch: [003][400/500]   Time 0.370 (0.418)   Data 0.001 (0.046)   Loss 1.6439 (1.6899)   Prec@1 33.000 (36.845)   Prec@5 91.000 (88.718)   [2025-10-24 17:13:10]
  **Train** Prec@1 37.452 Prec@5 88.928 Error@1 62.548
  **Test** Prec@1 45.420 Prec@5 92.790 Error@1 54.580
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 17:14:06] [Epoch=004/040] [Need: 02:14:03] [LR=0.0100] [Best : Accuracy=45.42, Error=54.58]
  Epoch: [004][000/500]   Time 18.046 (18.046)   Data 17.512 (17.512)   Loss 1.7254 (1.7254)   Prec@1 41.000 (41.000)   Prec@5 84.000 (84.000)   [2025-10-24 17:14:24]
  Epoch: [004][100/500]   Time 0.346 (0.552)   Data 0.001 (0.175)   Loss 1.6935 (1.6239)   Prec@1 36.000 (39.713)   Prec@5 88.000 (89.792)   [2025-10-24 17:15:02]
  Epoch: [004][200/500]   Time 0.329 (0.470)   Data 0.000 (0.089)   Loss 1.7420 (1.6162)   Prec@1 31.000 (39.746)   Prec@5 84.000 (89.781)   [2025-10-24 17:15:41]
  Epoch: [004][300/500]   Time 0.381 (0.440)   Data 0.002 (0.060)   Loss 1.7510 (1.6070)   Prec@1 39.000 (40.123)   Prec@5 89.000 (90.060)   [2025-10-24 17:16:19]
  Epoch: [004][400/500]   Time 0.434 (0.423)   Data 0.002 (0.045)   Loss 1.5673 (1.5975)   Prec@1 45.000 (40.506)   Prec@5 89.000 (90.197)   [2025-10-24 17:16:56]
  **Train** Prec@1 40.964 Prec@5 90.280 Error@1 59.036
  **Test** Prec@1 50.220 Prec@5 93.830 Error@1 49.780
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 17:17:52] [Epoch=005/040] [Need: 02:10:36] [LR=0.0100] [Best : Accuracy=50.22, Error=49.78]
  Epoch: [005][000/500]   Time 18.129 (18.129)   Data 17.606 (17.606)   Loss 1.4985 (1.4985)   Prec@1 48.000 (48.000)   Prec@5 91.000 (91.000)   [2025-10-24 17:18:10]
  Epoch: [005][100/500]   Time 0.393 (0.549)   Data 0.002 (0.176)   Loss 1.6539 (1.5368)   Prec@1 40.000 (43.079)   Prec@5 91.000 (91.436)   [2025-10-24 17:18:48]
  Epoch: [005][200/500]   Time 0.327 (0.463)   Data 0.001 (0.089)   Loss 1.5528 (1.5342)   Prec@1 45.000 (43.502)   Prec@5 93.000 (91.224)   [2025-10-24 17:19:25]
  Epoch: [005][300/500]   Time 0.402 (0.431)   Data 0.002 (0.060)   Loss 1.4885 (1.5317)   Prec@1 49.000 (43.575)   Prec@5 95.000 (91.140)   [2025-10-24 17:20:02]
  Epoch: [005][400/500]   Time 0.328 (0.417)   Data 0.000 (0.045)   Loss 1.5455 (1.5261)   Prec@1 39.000 (43.833)   Prec@5 93.000 (91.207)   [2025-10-24 17:20:39]
  **Train** Prec@1 44.122 Prec@5 91.216 Error@1 55.878
  **Test** Prec@1 52.560 Prec@5 94.370 Error@1 47.440
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 17:21:35] [Epoch=006/040] [Need: 02:06:48] [LR=0.0100] [Best : Accuracy=52.56, Error=47.44]
  Epoch: [006][000/500]   Time 18.169 (18.169)   Data 17.645 (17.645)   Loss 1.3925 (1.3925)   Prec@1 46.000 (46.000)   Prec@5 93.000 (93.000)   [2025-10-24 17:21:53]
  Epoch: [006][100/500]   Time 0.325 (0.549)   Data 0.000 (0.176)   Loss 1.4060 (1.4747)   Prec@1 52.000 (46.604)   Prec@5 92.000 (92.030)   [2025-10-24 17:22:31]
  Epoch: [006][200/500]   Time 0.384 (0.460)   Data 0.000 (0.089)   Loss 1.4957 (1.4691)   Prec@1 45.000 (46.488)   Prec@5 92.000 (91.940)   [2025-10-24 17:23:08]
  Epoch: [006][300/500]   Time 0.393 (0.430)   Data 0.002 (0.060)   Loss 1.2592 (1.4618)   Prec@1 58.000 (46.857)   Prec@5 96.000 (91.977)   [2025-10-24 17:23:45]
  Epoch: [006][400/500]   Time 0.352 (0.417)   Data 0.001 (0.045)   Loss 1.2630 (1.4598)   Prec@1 58.000 (47.005)   Prec@5 97.000 (92.012)   [2025-10-24 17:24:23]
  **Train** Prec@1 47.242 Prec@5 92.046 Error@1 52.758
  **Test** Prec@1 55.920 Prec@5 94.800 Error@1 44.080
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 17:25:19] [Epoch=007/040] [Need: 02:03:03] [LR=0.0100] [Best : Accuracy=55.92, Error=44.08]
  Epoch: [007][000/500]   Time 18.340 (18.340)   Data 17.765 (17.765)   Loss 1.4638 (1.4638)   Prec@1 41.000 (41.000)   Prec@5 89.000 (89.000)   [2025-10-24 17:25:37]
  Epoch: [007][100/500]   Time 0.327 (0.551)   Data 0.001 (0.177)   Loss 1.3410 (1.4093)   Prec@1 52.000 (47.891)   Prec@5 92.000 (92.634)   [2025-10-24 17:26:14]
  Epoch: [007][200/500]   Time 0.415 (0.462)   Data 0.001 (0.090)   Loss 1.4417 (1.4047)   Prec@1 42.000 (48.632)   Prec@5 87.000 (92.617)   [2025-10-24 17:26:52]
  Epoch: [007][300/500]   Time 0.393 (0.431)   Data 0.002 (0.060)   Loss 1.4673 (1.4023)   Prec@1 48.000 (48.874)   Prec@5 93.000 (92.532)   [2025-10-24 17:27:29]
  Epoch: [007][400/500]   Time 0.395 (0.415)   Data 0.002 (0.046)   Loss 1.6310 (1.3972)   Prec@1 44.000 (49.244)   Prec@5 88.000 (92.671)   [2025-10-24 17:28:05]
  **Train** Prec@1 49.614 Prec@5 92.762 Error@1 50.386
  **Test** Prec@1 57.320 Prec@5 95.350 Error@1 42.680
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 17:29:02] [Epoch=008/040] [Need: 01:59:17] [LR=0.0100] [Best : Accuracy=57.32, Error=42.68]
  Epoch: [008][000/500]   Time 18.208 (18.208)   Data 17.677 (17.677)   Loss 1.3341 (1.3341)   Prec@1 49.000 (49.000)   Prec@5 94.000 (94.000)   [2025-10-24 17:29:20]
  Epoch: [008][100/500]   Time 0.336 (0.556)   Data 0.001 (0.176)   Loss 1.4042 (1.3571)   Prec@1 50.000 (51.446)   Prec@5 93.000 (93.218)   [2025-10-24 17:29:58]
  Epoch: [008][200/500]   Time 0.335 (0.461)   Data 0.001 (0.089)   Loss 1.2857 (1.3368)   Prec@1 52.000 (52.179)   Prec@5 96.000 (93.597)   [2025-10-24 17:30:35]
  Epoch: [008][300/500]   Time 0.331 (0.431)   Data 0.001 (0.060)   Loss 1.4692 (1.3332)   Prec@1 53.000 (52.299)   Prec@5 92.000 (93.568)   [2025-10-24 17:31:12]
  Epoch: [008][400/500]   Time 0.396 (0.416)   Data 0.002 (0.045)   Loss 1.1248 (1.3300)   Prec@1 58.000 (52.322)   Prec@5 95.000 (93.506)   [2025-10-24 17:31:49]
  **Train** Prec@1 52.490 Prec@5 93.520 Error@1 47.510
  **Test** Prec@1 61.120 Prec@5 96.090 Error@1 38.880
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 17:32:45] [Epoch=009/040] [Need: 01:55:30] [LR=0.0100] [Best : Accuracy=61.12, Error=38.88]
  Epoch: [009][000/500]   Time 18.142 (18.142)   Data 17.582 (17.582)   Loss 1.4301 (1.4301)   Prec@1 47.000 (47.000)   Prec@5 92.000 (92.000)   [2025-10-24 17:33:03]
  Epoch: [009][100/500]   Time 0.361 (0.545)   Data 0.001 (0.175)   Loss 1.2784 (1.3126)   Prec@1 56.000 (53.178)   Prec@5 94.000 (93.614)   [2025-10-24 17:33:40]
  Epoch: [009][200/500]   Time 0.396 (0.461)   Data 0.002 (0.089)   Loss 1.2416 (1.2968)   Prec@1 57.000 (53.512)   Prec@5 96.000 (93.990)   [2025-10-24 17:34:17]
  Epoch: [009][300/500]   Time 0.326 (0.431)   Data 0.000 (0.060)   Loss 1.1414 (1.2832)   Prec@1 57.000 (54.096)   Prec@5 95.000 (94.133)   [2025-10-24 17:34:54]
  Epoch: [009][400/500]   Time 0.326 (0.417)   Data 0.001 (0.045)   Loss 1.2713 (1.2783)   Prec@1 54.000 (54.304)   Prec@5 98.000 (94.127)   [2025-10-24 17:35:32]
  **Train** Prec@1 54.600 Prec@5 94.182 Error@1 45.400
  **Test** Prec@1 63.550 Prec@5 96.610 Error@1 36.450
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 17:36:27] [Epoch=010/040] [Need: 01:51:43] [LR=0.0100] [Best : Accuracy=63.55, Error=36.45]
  Epoch: [010][000/500]   Time 18.126 (18.126)   Data 17.555 (17.555)   Loss 1.2374 (1.2374)   Prec@1 58.000 (58.000)   Prec@5 92.000 (92.000)   [2025-10-24 17:36:45]
  Epoch: [010][100/500]   Time 0.410 (0.548)   Data 0.002 (0.175)   Loss 1.2031 (1.2250)   Prec@1 51.000 (55.960)   Prec@5 95.000 (94.485)   [2025-10-24 17:37:22]
  Epoch: [010][200/500]   Time 0.395 (0.459)   Data 0.002 (0.089)   Loss 1.3123 (1.2316)   Prec@1 54.000 (55.826)   Prec@5 96.000 (94.493)   [2025-10-24 17:37:59]
  Epoch: [010][300/500]   Time 0.390 (0.426)   Data 0.002 (0.060)   Loss 1.2180 (1.2255)   Prec@1 53.000 (56.053)   Prec@5 93.000 (94.575)   [2025-10-24 17:38:35]
  Epoch: [010][400/500]   Time 0.398 (0.414)   Data 0.002 (0.045)   Loss 1.2167 (1.2224)   Prec@1 53.000 (56.252)   Prec@5 93.000 (94.676)   [2025-10-24 17:39:13]
  **Train** Prec@1 56.506 Prec@5 94.688 Error@1 43.494
  **Test** Prec@1 65.300 Prec@5 96.970 Error@1 34.700
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 17:40:09] [Epoch=011/040] [Need: 01:47:57] [LR=0.0100] [Best : Accuracy=65.30, Error=34.70]
  Epoch: [011][000/500]   Time 18.700 (18.700)   Data 18.167 (18.167)   Loss 1.2999 (1.2999)   Prec@1 52.000 (52.000)   Prec@5 97.000 (97.000)   [2025-10-24 17:40:28]
  Epoch: [011][100/500]   Time 0.339 (0.552)   Data 0.001 (0.181)   Loss 1.1517 (1.1883)   Prec@1 61.000 (57.168)   Prec@5 97.000 (95.386)   [2025-10-24 17:41:05]
  Epoch: [011][200/500]   Time 0.480 (0.462)   Data 0.002 (0.092)   Loss 1.0976 (1.1863)   Prec@1 63.000 (57.517)   Prec@5 97.000 (95.119)   [2025-10-24 17:41:42]
  Epoch: [011][300/500]   Time 0.327 (0.429)   Data 0.000 (0.062)   Loss 1.1457 (1.1783)   Prec@1 60.000 (57.983)   Prec@5 95.000 (95.203)   [2025-10-24 17:42:19]
  Epoch: [011][400/500]   Time 0.328 (0.417)   Data 0.000 (0.047)   Loss 1.1083 (1.1778)   Prec@1 56.000 (57.803)   Prec@5 95.000 (95.212)   [2025-10-24 17:42:56]
  **Train** Prec@1 58.092 Prec@5 95.232 Error@1 41.908
  **Test** Prec@1 67.210 Prec@5 97.480 Error@1 32.790
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 17:43:52] [Epoch=012/040] [Need: 01:44:11] [LR=0.0100] [Best : Accuracy=67.21, Error=32.79]
  Epoch: [012][000/500]   Time 18.156 (18.156)   Data 17.623 (17.623)   Loss 1.1448 (1.1448)   Prec@1 61.000 (61.000)   Prec@5 95.000 (95.000)   [2025-10-24 17:44:10]
  Epoch: [012][100/500]   Time 0.410 (0.548)   Data 0.002 (0.176)   Loss 1.0933 (1.1588)   Prec@1 61.000 (59.099)   Prec@5 97.000 (95.426)   [2025-10-24 17:44:47]
  Epoch: [012][200/500]   Time 0.315 (0.461)   Data 0.001 (0.089)   Loss 1.3810 (1.1468)   Prec@1 49.000 (59.418)   Prec@5 95.000 (95.338)   [2025-10-24 17:45:25]
  Epoch: [012][300/500]   Time 0.396 (0.431)   Data 0.003 (0.060)   Loss 1.0470 (1.1467)   Prec@1 73.000 (59.415)   Prec@5 96.000 (95.419)   [2025-10-24 17:46:02]
  Epoch: [012][400/500]   Time 0.415 (0.416)   Data 0.002 (0.045)   Loss 1.0897 (1.1404)   Prec@1 66.000 (59.741)   Prec@5 93.000 (95.489)   [2025-10-24 17:46:39]
  **Train** Prec@1 59.846 Prec@5 95.464 Error@1 40.154
  **Test** Prec@1 68.690 Prec@5 97.580 Error@1 31.310
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 17:47:34] [Epoch=013/040] [Need: 01:40:26] [LR=0.0100] [Best : Accuracy=68.69, Error=31.31]
  Epoch: [013][000/500]   Time 18.385 (18.385)   Data 17.848 (17.848)   Loss 0.9959 (0.9959)   Prec@1 62.000 (62.000)   Prec@5 97.000 (97.000)   [2025-10-24 17:47:52]
  Epoch: [013][100/500]   Time 0.398 (0.546)   Data 0.002 (0.178)   Loss 1.1562 (1.0925)   Prec@1 67.000 (61.495)   Prec@5 94.000 (95.743)   [2025-10-24 17:48:29]
  Epoch: [013][200/500]   Time 0.332 (0.459)   Data 0.001 (0.090)   Loss 1.0293 (1.1053)   Prec@1 62.000 (61.313)   Prec@5 99.000 (95.701)   [2025-10-24 17:49:06]
  Epoch: [013][300/500]   Time 0.408 (0.431)   Data 0.002 (0.061)   Loss 1.0346 (1.1028)   Prec@1 63.000 (61.239)   Prec@5 97.000 (95.671)   [2025-10-24 17:49:44]
  Epoch: [013][400/500]   Time 0.394 (0.418)   Data 0.002 (0.046)   Loss 1.0203 (1.0998)   Prec@1 66.000 (61.307)   Prec@5 96.000 (95.716)   [2025-10-24 17:50:22]
  **Train** Prec@1 61.530 Prec@5 95.730 Error@1 38.470
  **Test** Prec@1 69.090 Prec@5 97.350 Error@1 30.910
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 17:51:17] [Epoch=014/040] [Need: 01:36:43] [LR=0.0100] [Best : Accuracy=69.09, Error=30.91]
  Epoch: [014][000/500]   Time 18.113 (18.113)   Data 17.560 (17.560)   Loss 1.0060 (1.0060)   Prec@1 66.000 (66.000)   Prec@5 99.000 (99.000)   [2025-10-24 17:51:35]
  Epoch: [014][100/500]   Time 0.416 (0.548)   Data 0.003 (0.175)   Loss 1.1136 (1.0649)   Prec@1 65.000 (62.386)   Prec@5 95.000 (96.218)   [2025-10-24 17:52:13]
  Epoch: [014][200/500]   Time 0.332 (0.462)   Data 0.000 (0.089)   Loss 1.1474 (1.0744)   Prec@1 63.000 (62.015)   Prec@5 95.000 (95.900)   [2025-10-24 17:52:50]
  Epoch: [014][300/500]   Time 0.343 (0.431)   Data 0.002 (0.060)   Loss 0.9468 (1.0736)   Prec@1 64.000 (62.030)   Prec@5 99.000 (95.977)   [2025-10-24 17:53:27]
  Epoch: [014][400/500]   Time 0.342 (0.413)   Data 0.001 (0.045)   Loss 0.9406 (1.0643)   Prec@1 67.000 (62.551)   Prec@5 96.000 (96.040)   [2025-10-24 17:54:03]
  **Train** Prec@1 62.734 Prec@5 96.036 Error@1 37.266
  **Test** Prec@1 70.100 Prec@5 97.740 Error@1 29.900
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 17:54:59] [Epoch=015/040] [Need: 01:32:57] [LR=0.0100] [Best : Accuracy=70.10, Error=29.90]
  Epoch: [015][000/500]   Time 18.437 (18.437)   Data 17.903 (17.903)   Loss 1.0583 (1.0583)   Prec@1 64.000 (64.000)   Prec@5 97.000 (97.000)   [2025-10-24 17:55:17]
  Epoch: [015][100/500]   Time 0.343 (0.550)   Data 0.002 (0.179)   Loss 1.0473 (1.0338)   Prec@1 61.000 (63.545)   Prec@5 95.000 (96.337)   [2025-10-24 17:55:55]
  Epoch: [015][200/500]   Time 0.401 (0.462)   Data 0.002 (0.090)   Loss 1.0043 (1.0361)   Prec@1 71.000 (63.542)   Prec@5 93.000 (96.453)   [2025-10-24 17:56:32]
  Epoch: [015][300/500]   Time 0.326 (0.431)   Data 0.000 (0.061)   Loss 0.9213 (1.0364)   Prec@1 65.000 (63.608)   Prec@5 98.000 (96.439)   [2025-10-24 17:57:09]
  Epoch: [015][400/500]   Time 0.325 (0.416)   Data 0.001 (0.046)   Loss 1.0716 (1.0363)   Prec@1 58.000 (63.681)   Prec@5 98.000 (96.364)   [2025-10-24 17:57:46]
  **Train** Prec@1 63.692 Prec@5 96.346 Error@1 36.308
  **Test** Prec@1 72.030 Prec@5 97.930 Error@1 27.970
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 17:58:41] [Epoch=016/040] [Need: 01:29:13] [LR=0.0100] [Best : Accuracy=72.03, Error=27.97]
  Epoch: [016][000/500]   Time 17.845 (17.845)   Data 17.300 (17.300)   Loss 1.0186 (1.0186)   Prec@1 64.000 (64.000)   Prec@5 95.000 (95.000)   [2025-10-24 17:58:59]
  Epoch: [016][100/500]   Time 0.347 (0.547)   Data 0.002 (0.173)   Loss 0.9376 (0.9950)   Prec@1 69.000 (65.109)   Prec@5 99.000 (96.693)   [2025-10-24 17:59:37]
  Epoch: [016][200/500]   Time 0.371 (0.463)   Data 0.000 (0.087)   Loss 1.1828 (1.0083)   Prec@1 58.000 (64.498)   Prec@5 92.000 (96.547)   [2025-10-24 18:00:15]
  Epoch: [016][300/500]   Time 0.424 (0.434)   Data 0.004 (0.059)   Loss 0.8326 (1.0099)   Prec@1 67.000 (64.369)   Prec@5 98.000 (96.595)   [2025-10-24 18:00:52]
  Epoch: [016][400/500]   Time 0.355 (0.420)   Data 0.001 (0.045)   Loss 0.8685 (1.0052)   Prec@1 67.000 (64.681)   Prec@5 98.000 (96.574)   [2025-10-24 18:01:30]
  **Train** Prec@1 64.718 Prec@5 96.600 Error@1 35.282
  **Test** Prec@1 72.430 Prec@5 98.020 Error@1 27.570
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 18:02:26] [Epoch=017/040] [Need: 01:25:31] [LR=0.0100] [Best : Accuracy=72.43, Error=27.57]
  Epoch: [017][000/500]   Time 18.195 (18.195)   Data 17.658 (17.658)   Loss 1.0202 (1.0202)   Prec@1 63.000 (63.000)   Prec@5 97.000 (97.000)   [2025-10-24 18:02:44]
  Epoch: [017][100/500]   Time 0.393 (0.548)   Data 0.002 (0.176)   Loss 1.0346 (0.9738)   Prec@1 65.000 (65.752)   Prec@5 95.000 (96.723)   [2025-10-24 18:03:21]
  Epoch: [017][200/500]   Time 0.426 (0.460)   Data 0.004 (0.089)   Loss 0.8677 (0.9803)   Prec@1 74.000 (65.886)   Prec@5 96.000 (96.517)   [2025-10-24 18:03:58]
  Epoch: [017][300/500]   Time 0.362 (0.433)   Data 0.002 (0.060)   Loss 0.8905 (0.9860)   Prec@1 67.000 (65.890)   Prec@5 97.000 (96.485)   [2025-10-24 18:04:36]
  Epoch: [017][400/500]   Time 0.396 (0.418)   Data 0.002 (0.045)   Loss 0.9835 (0.9839)   Prec@1 66.000 (65.885)   Prec@5 99.000 (96.549)   [2025-10-24 18:05:13]
  **Train** Prec@1 65.938 Prec@5 96.600 Error@1 34.062
  **Test** Prec@1 73.650 Prec@5 98.110 Error@1 26.350
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 18:06:09] [Epoch=018/040] [Need: 01:21:49] [LR=0.0100] [Best : Accuracy=73.65, Error=26.35]
  Epoch: [018][000/500]   Time 18.153 (18.153)   Data 17.599 (17.599)   Loss 0.9499 (0.9499)   Prec@1 65.000 (65.000)   Prec@5 96.000 (96.000)   [2025-10-24 18:06:27]
  Epoch: [018][100/500]   Time 0.348 (0.544)   Data 0.002 (0.176)   Loss 1.0486 (0.9622)   Prec@1 66.000 (66.723)   Prec@5 92.000 (96.683)   [2025-10-24 18:07:04]
  Epoch: [018][200/500]   Time 0.399 (0.458)   Data 0.002 (0.089)   Loss 0.9557 (0.9595)   Prec@1 68.000 (66.861)   Prec@5 99.000 (96.697)   [2025-10-24 18:07:41]
  Epoch: [018][300/500]   Time 0.362 (0.428)   Data 0.001 (0.060)   Loss 0.9815 (0.9608)   Prec@1 66.000 (66.807)   Prec@5 97.000 (96.704)   [2025-10-24 18:08:18]
  Epoch: [018][400/500]   Time 0.350 (0.413)   Data 0.002 (0.045)   Loss 1.2312 (0.9606)   Prec@1 58.000 (66.840)   Prec@5 94.000 (96.751)   [2025-10-24 18:08:55]
  **Train** Prec@1 66.762 Prec@5 96.694 Error@1 33.238
  **Test** Prec@1 74.120 Prec@5 98.160 Error@1 25.880
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 18:09:51] [Epoch=019/040] [Need: 01:18:04] [LR=0.0100] [Best : Accuracy=74.12, Error=25.88]
  Epoch: [019][000/500]   Time 18.061 (18.061)   Data 17.523 (17.523)   Loss 1.0957 (1.0957)   Prec@1 71.000 (71.000)   Prec@5 94.000 (94.000)   [2025-10-24 18:10:09]
  Epoch: [019][100/500]   Time 0.338 (0.545)   Data 0.000 (0.175)   Loss 0.9020 (0.9592)   Prec@1 71.000 (66.832)   Prec@5 94.000 (96.515)   [2025-10-24 18:10:46]
  Epoch: [019][200/500]   Time 0.401 (0.460)   Data 0.001 (0.088)   Loss 1.0840 (0.9488)   Prec@1 66.000 (67.124)   Prec@5 92.000 (96.692)   [2025-10-24 18:11:23]
  Epoch: [019][300/500]   Time 0.408 (0.432)   Data 0.002 (0.060)   Loss 0.8991 (0.9414)   Prec@1 69.000 (67.402)   Prec@5 98.000 (96.754)   [2025-10-24 18:12:01]
  Epoch: [019][400/500]   Time 0.417 (0.418)   Data 0.001 (0.045)   Loss 0.8923 (0.9428)   Prec@1 69.000 (67.132)   Prec@5 97.000 (96.810)   [2025-10-24 18:12:39]
  **Train** Prec@1 67.228 Prec@5 96.842 Error@1 32.772
  **Test** Prec@1 75.160 Prec@5 98.250 Error@1 24.840
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 18:13:35] [Epoch=020/040] [Need: 01:14:22] [LR=0.0100] [Best : Accuracy=75.16, Error=24.84]
  Epoch: [020][000/500]   Time 18.165 (18.165)   Data 17.620 (17.620)   Loss 1.1176 (1.1176)   Prec@1 63.000 (63.000)   Prec@5 96.000 (96.000)   [2025-10-24 18:13:53]
  Epoch: [020][100/500]   Time 0.343 (0.541)   Data 0.002 (0.176)   Loss 0.8770 (0.9257)   Prec@1 70.000 (67.911)   Prec@5 96.000 (96.950)   [2025-10-24 18:14:30]
  Epoch: [020][200/500]   Time 0.397 (0.460)   Data 0.002 (0.089)   Loss 0.7712 (0.9280)   Prec@1 65.000 (67.756)   Prec@5 97.000 (96.990)   [2025-10-24 18:15:08]
  Epoch: [020][300/500]   Time 0.395 (0.432)   Data 0.002 (0.060)   Loss 0.8095 (0.9305)   Prec@1 74.000 (67.817)   Prec@5 97.000 (96.980)   [2025-10-24 18:15:45]
  Epoch: [020][400/500]   Time 0.398 (0.417)   Data 0.002 (0.045)   Loss 1.0215 (0.9294)   Prec@1 68.000 (67.900)   Prec@5 94.000 (96.933)   [2025-10-24 18:16:23]
  **Train** Prec@1 68.180 Prec@5 96.964 Error@1 31.820
  **Test** Prec@1 76.210 Prec@5 98.260 Error@1 23.790
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 18:17:18] [Epoch=021/040] [Need: 01:10:39] [LR=0.0100] [Best : Accuracy=76.21, Error=23.79]
  Epoch: [021][000/500]   Time 18.300 (18.300)   Data 17.767 (17.767)   Loss 0.8299 (0.8299)   Prec@1 70.000 (70.000)   Prec@5 98.000 (98.000)   [2025-10-24 18:17:36]
  Epoch: [021][100/500]   Time 0.402 (0.557)   Data 0.002 (0.177)   Loss 0.8632 (0.9171)   Prec@1 72.000 (68.515)   Prec@5 99.000 (96.891)   [2025-10-24 18:18:14]
  Epoch: [021][200/500]   Time 0.331 (0.462)   Data 0.001 (0.090)   Loss 0.8898 (0.9104)   Prec@1 68.000 (68.642)   Prec@5 99.000 (96.905)   [2025-10-24 18:18:51]
  Epoch: [021][300/500]   Time 0.391 (0.431)   Data 0.002 (0.060)   Loss 1.0551 (0.9112)   Prec@1 61.000 (68.615)   Prec@5 94.000 (96.870)   [2025-10-24 18:19:28]
  Epoch: [021][400/500]   Time 0.395 (0.416)   Data 0.002 (0.046)   Loss 0.9828 (0.9092)   Prec@1 67.000 (68.611)   Prec@5 97.000 (96.950)   [2025-10-24 18:20:05]
  **Train** Prec@1 68.592 Prec@5 97.014 Error@1 31.408
  **Test** Prec@1 76.270 Prec@5 98.290 Error@1 23.730
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 18:21:01] [Epoch=022/040] [Need: 01:06:55] [LR=0.0100] [Best : Accuracy=76.27, Error=23.73]
  Epoch: [022][000/500]   Time 18.211 (18.211)   Data 17.666 (17.666)   Loss 0.8483 (0.8483)   Prec@1 67.000 (67.000)   Prec@5 99.000 (99.000)   [2025-10-24 18:21:19]
  Epoch: [022][100/500]   Time 0.391 (0.551)   Data 0.002 (0.176)   Loss 0.9959 (0.8877)   Prec@1 70.000 (69.178)   Prec@5 96.000 (97.188)   [2025-10-24 18:21:57]
  Epoch: [022][200/500]   Time 0.343 (0.458)   Data 0.001 (0.089)   Loss 1.1681 (0.8892)   Prec@1 64.000 (69.239)   Prec@5 93.000 (97.144)   [2025-10-24 18:22:33]
  Epoch: [022][300/500]   Time 0.317 (0.431)   Data 0.001 (0.060)   Loss 0.8665 (0.8877)   Prec@1 72.000 (69.316)   Prec@5 97.000 (97.219)   [2025-10-24 18:23:11]
  Epoch: [022][400/500]   Time 0.414 (0.416)   Data 0.002 (0.045)   Loss 0.8372 (0.8875)   Prec@1 73.000 (69.349)   Prec@5 100.000 (97.135)   [2025-10-24 18:23:48]
  **Train** Prec@1 69.514 Prec@5 97.172 Error@1 30.486
  **Test** Prec@1 76.670 Prec@5 98.360 Error@1 23.330
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 18:24:44] [Epoch=023/040] [Need: 01:03:12] [LR=0.0100] [Best : Accuracy=76.67, Error=23.33]
  Epoch: [023][000/500]   Time 18.861 (18.861)   Data 18.302 (18.302)   Loss 0.9040 (0.9040)   Prec@1 67.000 (67.000)   Prec@5 97.000 (97.000)   [2025-10-24 18:25:03]
  Epoch: [023][100/500]   Time 0.389 (0.556)   Data 0.002 (0.183)   Loss 0.9469 (0.8873)   Prec@1 68.000 (69.406)   Prec@5 96.000 (97.208)   [2025-10-24 18:25:40]
  Epoch: [023][200/500]   Time 0.398 (0.467)   Data 0.002 (0.092)   Loss 1.0253 (0.8852)   Prec@1 63.000 (69.592)   Prec@5 98.000 (97.129)   [2025-10-24 18:26:18]
  Epoch: [023][300/500]   Time 0.410 (0.431)   Data 0.001 (0.062)   Loss 0.7925 (0.8737)   Prec@1 72.000 (70.030)   Prec@5 97.000 (97.252)   [2025-10-24 18:26:54]
  Epoch: [023][400/500]   Time 0.412 (0.416)   Data 0.002 (0.047)   Loss 0.8811 (0.8767)   Prec@1 68.000 (69.736)   Prec@5 99.000 (97.292)   [2025-10-24 18:27:31]
  **Train** Prec@1 69.880 Prec@5 97.266 Error@1 30.120
  **Test** Prec@1 77.100 Prec@5 98.450 Error@1 22.900
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 18:28:27] [Epoch=024/040] [Need: 00:59:29] [LR=0.0100] [Best : Accuracy=77.10, Error=22.90]
  Epoch: [024][000/500]   Time 18.069 (18.069)   Data 17.523 (17.523)   Loss 0.7642 (0.7642)   Prec@1 74.000 (74.000)   Prec@5 99.000 (99.000)   [2025-10-24 18:28:45]
  Epoch: [024][100/500]   Time 0.372 (0.547)   Data 0.001 (0.175)   Loss 0.9214 (0.8529)   Prec@1 67.000 (70.673)   Prec@5 97.000 (97.495)   [2025-10-24 18:29:22]
  Epoch: [024][200/500]   Time 0.385 (0.461)   Data 0.002 (0.088)   Loss 0.8373 (0.8720)   Prec@1 76.000 (70.035)   Prec@5 96.000 (97.259)   [2025-10-24 18:29:59]
  Epoch: [024][300/500]   Time 0.397 (0.432)   Data 0.003 (0.060)   Loss 0.8408 (0.8694)   Prec@1 76.000 (70.146)   Prec@5 97.000 (97.153)   [2025-10-24 18:30:37]
  Epoch: [024][400/500]   Time 0.392 (0.417)   Data 0.002 (0.045)   Loss 0.9194 (0.8684)   Prec@1 64.000 (70.140)   Prec@5 98.000 (97.234)   [2025-10-24 18:31:14]
  **Train** Prec@1 70.276 Prec@5 97.336 Error@1 29.724
  **Test** Prec@1 77.640 Prec@5 98.520 Error@1 22.360
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 18:32:10] [Epoch=025/040] [Need: 00:55:46] [LR=0.0010] [Best : Accuracy=77.64, Error=22.36]
  Epoch: [025][000/500]   Time 17.973 (17.973)   Data 17.430 (17.430)   Loss 1.0216 (1.0216)   Prec@1 68.000 (68.000)   Prec@5 96.000 (96.000)   [2025-10-24 18:32:28]
  Epoch: [025][100/500]   Time 0.403 (0.556)   Data 0.002 (0.174)   Loss 0.8130 (0.8222)   Prec@1 68.000 (71.525)   Prec@5 97.000 (97.673)   [2025-10-24 18:33:06]
  Epoch: [025][200/500]   Time 0.399 (0.464)   Data 0.000 (0.088)   Loss 0.7132 (0.8105)   Prec@1 79.000 (72.189)   Prec@5 99.000 (97.657)   [2025-10-24 18:33:43]
  Epoch: [025][300/500]   Time 0.340 (0.434)   Data 0.001 (0.059)   Loss 0.8928 (0.8010)   Prec@1 68.000 (72.425)   Prec@5 97.000 (97.728)   [2025-10-24 18:34:20]
  Epoch: [025][400/500]   Time 0.394 (0.418)   Data 0.001 (0.045)   Loss 1.0092 (0.7955)   Prec@1 64.000 (72.579)   Prec@5 97.000 (97.833)   [2025-10-24 18:34:57]
  **Train** Prec@1 72.796 Prec@5 97.876 Error@1 27.204
  **Test** Prec@1 79.450 Prec@5 98.730 Error@1 20.550
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 18:35:54] [Epoch=026/040] [Need: 00:52:03] [LR=0.0010] [Best : Accuracy=79.45, Error=20.55]
  Epoch: [026][000/500]   Time 18.401 (18.401)   Data 17.872 (17.872)   Loss 0.8396 (0.8396)   Prec@1 77.000 (77.000)   Prec@5 99.000 (99.000)   [2025-10-24 18:36:12]
  Epoch: [026][100/500]   Time 0.329 (0.547)   Data 0.000 (0.178)   Loss 0.9044 (0.7815)   Prec@1 65.000 (73.079)   Prec@5 96.000 (97.782)   [2025-10-24 18:36:49]
  Epoch: [026][200/500]   Time 0.340 (0.462)   Data 0.001 (0.090)   Loss 0.4915 (0.7830)   Prec@1 84.000 (72.950)   Prec@5 99.000 (97.731)   [2025-10-24 18:37:27]
  Epoch: [026][300/500]   Time 0.324 (0.429)   Data 0.001 (0.061)   Loss 0.7232 (0.7789)   Prec@1 75.000 (73.246)   Prec@5 99.000 (97.744)   [2025-10-24 18:38:03]
  Epoch: [026][400/500]   Time 0.330 (0.414)   Data 0.000 (0.046)   Loss 0.8392 (0.7800)   Prec@1 71.000 (73.177)   Prec@5 99.000 (97.703)   [2025-10-24 18:38:40]
  **Train** Prec@1 73.184 Prec@5 97.738 Error@1 26.816
  **Test** Prec@1 79.480 Prec@5 98.870 Error@1 20.520
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 18:39:36] [Epoch=027/040] [Need: 00:48:20] [LR=0.0010] [Best : Accuracy=79.48, Error=20.52]
  Epoch: [027][000/500]   Time 18.915 (18.915)   Data 18.384 (18.384)   Loss 0.7602 (0.7602)   Prec@1 73.000 (73.000)   Prec@5 97.000 (97.000)   [2025-10-24 18:39:55]
  Epoch: [027][100/500]   Time 0.396 (0.554)   Data 0.002 (0.183)   Loss 0.7019 (0.7640)   Prec@1 77.000 (73.545)   Prec@5 95.000 (97.871)   [2025-10-24 18:40:32]
  Epoch: [027][200/500]   Time 0.324 (0.466)   Data 0.000 (0.093)   Loss 0.8247 (0.7658)   Prec@1 72.000 (73.846)   Prec@5 97.000 (98.000)   [2025-10-24 18:41:10]
  Epoch: [027][300/500]   Time 0.358 (0.435)   Data 0.001 (0.062)   Loss 0.7361 (0.7708)   Prec@1 78.000 (73.615)   Prec@5 96.000 (97.944)   [2025-10-24 18:41:47]
  Epoch: [027][400/500]   Time 0.407 (0.420)   Data 0.002 (0.047)   Loss 0.7843 (0.7671)   Prec@1 73.000 (73.743)   Prec@5 99.000 (97.945)   [2025-10-24 18:42:25]
  **Train** Prec@1 73.816 Prec@5 98.004 Error@1 26.184
  **Test** Prec@1 79.930 Prec@5 98.780 Error@1 20.070
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 18:43:20] [Epoch=028/040] [Need: 00:44:37] [LR=0.0010] [Best : Accuracy=79.93, Error=20.07]
  Epoch: [028][000/500]   Time 18.270 (18.270)   Data 17.717 (17.717)   Loss 0.8071 (0.8071)   Prec@1 67.000 (67.000)   Prec@5 99.000 (99.000)   [2025-10-24 18:43:38]
  Epoch: [028][100/500]   Time 0.354 (0.548)   Data 0.001 (0.177)   Loss 0.7210 (0.7460)   Prec@1 76.000 (74.178)   Prec@5 98.000 (97.911)   [2025-10-24 18:44:15]
  Epoch: [028][200/500]   Time 0.408 (0.464)   Data 0.002 (0.090)   Loss 0.9980 (0.7523)   Prec@1 63.000 (74.159)   Prec@5 96.000 (97.950)   [2025-10-24 18:44:54]
  Epoch: [028][300/500]   Time 0.349 (0.434)   Data 0.001 (0.060)   Loss 0.8230 (0.7548)   Prec@1 73.000 (74.000)   Prec@5 95.000 (98.007)   [2025-10-24 18:45:31]
  Epoch: [028][400/500]   Time 0.411 (0.418)   Data 0.002 (0.046)   Loss 0.6945 (0.7541)   Prec@1 78.000 (74.072)   Prec@5 98.000 (98.045)   [2025-10-24 18:46:08]
  **Train** Prec@1 74.186 Prec@5 98.034 Error@1 25.814
  **Test** Prec@1 79.560 Prec@5 98.780 Error@1 20.440

==>>[2025-10-24 18:47:04] [Epoch=029/040] [Need: 00:40:54] [LR=0.0010] [Best : Accuracy=79.93, Error=20.07]
  Epoch: [029][000/500]   Time 18.095 (18.095)   Data 17.533 (17.533)   Loss 0.6487 (0.6487)   Prec@1 76.000 (76.000)   Prec@5 98.000 (98.000)   [2025-10-24 18:47:22]
  Epoch: [029][100/500]   Time 0.325 (0.545)   Data 0.000 (0.175)   Loss 0.6593 (0.7478)   Prec@1 79.000 (74.317)   Prec@5 98.000 (98.149)   [2025-10-24 18:47:59]
  Epoch: [029][200/500]   Time 0.407 (0.461)   Data 0.002 (0.089)   Loss 0.6855 (0.7506)   Prec@1 73.000 (74.229)   Prec@5 100.000 (98.075)   [2025-10-24 18:48:37]
  Epoch: [029][300/500]   Time 0.398 (0.432)   Data 0.002 (0.060)   Loss 0.5719 (0.7502)   Prec@1 79.000 (74.272)   Prec@5 98.000 (97.987)   [2025-10-24 18:49:14]
  Epoch: [029][400/500]   Time 0.349 (0.419)   Data 0.002 (0.045)   Loss 0.6019 (0.7482)   Prec@1 78.000 (74.304)   Prec@5 98.000 (98.010)   [2025-10-24 18:49:52]
  **Train** Prec@1 74.206 Prec@5 97.996 Error@1 25.794
  **Test** Prec@1 80.250 Prec@5 98.890 Error@1 19.750
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 18:50:48] [Epoch=030/040] [Need: 00:37:11] [LR=0.0010] [Best : Accuracy=80.25, Error=19.75]
  Epoch: [030][000/500]   Time 18.041 (18.041)   Data 17.477 (17.477)   Loss 0.7350 (0.7350)   Prec@1 74.000 (74.000)   Prec@5 97.000 (97.000)   [2025-10-24 18:51:06]
  Epoch: [030][100/500]   Time 0.399 (0.559)   Data 0.002 (0.175)   Loss 0.6614 (0.7441)   Prec@1 76.000 (74.238)   Prec@5 99.000 (98.050)   [2025-10-24 18:51:45]
  Epoch: [030][200/500]   Time 0.326 (0.467)   Data 0.001 (0.088)   Loss 0.7271 (0.7465)   Prec@1 76.000 (74.264)   Prec@5 98.000 (98.124)   [2025-10-24 18:52:22]
  Epoch: [030][300/500]   Time 0.415 (0.436)   Data 0.002 (0.059)   Loss 0.6786 (0.7451)   Prec@1 75.000 (74.246)   Prec@5 100.000 (98.083)   [2025-10-24 18:53:00]
  Epoch: [030][400/500]   Time 0.355 (0.421)   Data 0.002 (0.045)   Loss 0.8018 (0.7476)   Prec@1 72.000 (74.269)   Prec@5 99.000 (98.010)   [2025-10-24 18:53:37]
  **Train** Prec@1 74.398 Prec@5 98.062 Error@1 25.602
  **Test** Prec@1 80.150 Prec@5 98.760 Error@1 19.850

==>>[2025-10-24 18:54:33] [Epoch=031/040] [Need: 00:33:29] [LR=0.0010] [Best : Accuracy=80.25, Error=19.75]
  Epoch: [031][000/500]   Time 19.093 (19.093)   Data 18.555 (18.555)   Loss 0.7079 (0.7079)   Prec@1 75.000 (75.000)   Prec@5 99.000 (99.000)   [2025-10-24 18:54:52]
  Epoch: [031][100/500]   Time 0.328 (0.555)   Data 0.000 (0.185)   Loss 0.4657 (0.7422)   Prec@1 85.000 (74.752)   Prec@5 99.000 (97.980)   [2025-10-24 18:55:29]
  Epoch: [031][200/500]   Time 0.340 (0.460)   Data 0.001 (0.094)   Loss 0.8059 (0.7404)   Prec@1 65.000 (74.652)   Prec@5 100.000 (98.015)   [2025-10-24 18:56:05]
  Epoch: [031][300/500]   Time 0.334 (0.431)   Data 0.001 (0.063)   Loss 0.6423 (0.7411)   Prec@1 81.000 (74.654)   Prec@5 98.000 (97.963)   [2025-10-24 18:56:43]
  Epoch: [031][400/500]   Time 0.390 (0.418)   Data 0.002 (0.048)   Loss 0.7566 (0.7413)   Prec@1 75.000 (74.613)   Prec@5 98.000 (98.012)   [2025-10-24 18:57:21]
  **Train** Prec@1 74.604 Prec@5 98.052 Error@1 25.396
  **Test** Prec@1 80.400 Prec@5 98.810 Error@1 19.600
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 18:58:17] [Epoch=032/040] [Need: 00:29:46] [LR=0.0010] [Best : Accuracy=80.40, Error=19.60]
  Epoch: [032][000/500]   Time 18.084 (18.084)   Data 17.537 (17.537)   Loss 0.7172 (0.7172)   Prec@1 74.000 (74.000)   Prec@5 98.000 (98.000)   [2025-10-24 18:58:35]
  Epoch: [032][100/500]   Time 0.389 (0.551)   Data 0.002 (0.175)   Loss 0.7981 (0.7297)   Prec@1 70.000 (75.416)   Prec@5 99.000 (97.950)   [2025-10-24 18:59:12]
  Epoch: [032][200/500]   Time 0.398 (0.462)   Data 0.002 (0.089)   Loss 0.5880 (0.7327)   Prec@1 82.000 (75.025)   Prec@5 100.000 (98.119)   [2025-10-24 18:59:50]
  Epoch: [032][300/500]   Time 0.342 (0.431)   Data 0.000 (0.060)   Loss 0.9229 (0.7359)   Prec@1 69.000 (74.841)   Prec@5 98.000 (98.090)   [2025-10-24 19:00:26]
  Epoch: [032][400/500]   Time 0.327 (0.418)   Data 0.000 (0.045)   Loss 0.6590 (0.7367)   Prec@1 83.000 (74.875)   Prec@5 98.000 (98.050)   [2025-10-24 19:01:04]
  **Train** Prec@1 74.930 Prec@5 98.054 Error@1 25.070
  **Test** Prec@1 80.410 Prec@5 98.820 Error@1 19.590
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 19:02:00] [Epoch=033/040] [Need: 00:26:02] [LR=0.0010] [Best : Accuracy=80.41, Error=19.59]
  Epoch: [033][000/500]   Time 18.059 (18.059)   Data 17.495 (17.495)   Loss 0.7628 (0.7628)   Prec@1 73.000 (73.000)   Prec@5 97.000 (97.000)   [2025-10-24 19:02:18]
  Epoch: [033][100/500]   Time 0.418 (0.542)   Data 0.002 (0.174)   Loss 0.9263 (0.7436)   Prec@1 66.000 (74.416)   Prec@5 97.000 (97.990)   [2025-10-24 19:02:55]
  Epoch: [033][200/500]   Time 0.322 (0.457)   Data 0.001 (0.088)   Loss 0.6971 (0.7474)   Prec@1 77.000 (74.294)   Prec@5 97.000 (97.896)   [2025-10-24 19:03:32]
  Epoch: [033][300/500]   Time 0.362 (0.432)   Data 0.001 (0.060)   Loss 0.6737 (0.7419)   Prec@1 78.000 (74.439)   Prec@5 99.000 (97.950)   [2025-10-24 19:04:10]
  Epoch: [033][400/500]   Time 0.426 (0.418)   Data 0.002 (0.045)   Loss 0.8416 (0.7361)   Prec@1 72.000 (74.678)   Prec@5 95.000 (97.993)   [2025-10-24 19:04:48]
  **Train** Prec@1 74.666 Prec@5 98.058 Error@1 25.334
  **Test** Prec@1 80.530 Prec@5 98.780 Error@1 19.470
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 19:05:45] [Epoch=034/040] [Need: 00:22:19] [LR=0.0010] [Best : Accuracy=80.53, Error=19.47]
  Epoch: [034][000/500]   Time 18.364 (18.364)   Data 17.829 (17.829)   Loss 0.6633 (0.6633)   Prec@1 74.000 (74.000)   Prec@5 100.000 (100.000)   [2025-10-24 19:06:04]
  Epoch: [034][100/500]   Time 0.399 (0.552)   Data 0.002 (0.178)   Loss 0.9073 (0.7402)   Prec@1 64.000 (74.653)   Prec@5 99.000 (98.079)   [2025-10-24 19:06:41]
  Epoch: [034][200/500]   Time 0.361 (0.460)   Data 0.002 (0.090)   Loss 0.6794 (0.7409)   Prec@1 78.000 (74.687)   Prec@5 95.000 (98.085)   [2025-10-24 19:07:18]
  Epoch: [034][300/500]   Time 0.350 (0.429)   Data 0.002 (0.061)   Loss 0.9321 (0.7374)   Prec@1 67.000 (74.754)   Prec@5 95.000 (98.093)   [2025-10-24 19:07:55]
  Epoch: [034][400/500]   Time 0.387 (0.415)   Data 0.001 (0.046)   Loss 0.7620 (0.7359)   Prec@1 75.000 (74.823)   Prec@5 97.000 (98.072)   [2025-10-24 19:08:32]
  **Train** Prec@1 75.010 Prec@5 98.076 Error@1 24.990
  **Test** Prec@1 80.740 Prec@5 98.870 Error@1 19.260
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 19:09:27] [Epoch=035/040] [Need: 00:18:36] [LR=0.0010] [Best : Accuracy=80.74, Error=19.26]
  Epoch: [035][000/500]   Time 19.589 (19.589)   Data 19.055 (19.055)   Loss 0.6685 (0.6685)   Prec@1 76.000 (76.000)   Prec@5 100.000 (100.000)   [2025-10-24 19:09:47]
  Epoch: [035][100/500]   Time 0.360 (0.553)   Data 0.001 (0.190)   Loss 0.6208 (0.7340)   Prec@1 80.000 (74.990)   Prec@5 99.000 (98.257)   [2025-10-24 19:10:23]
  Epoch: [035][200/500]   Time 0.415 (0.465)   Data 0.002 (0.096)   Loss 0.7416 (0.7248)   Prec@1 74.000 (75.299)   Prec@5 98.000 (98.104)   [2025-10-24 19:11:01]
  Epoch: [035][300/500]   Time 0.345 (0.436)   Data 0.000 (0.065)   Loss 0.6037 (0.7292)   Prec@1 77.000 (75.080)   Prec@5 100.000 (98.053)   [2025-10-24 19:11:39]
  Epoch: [035][400/500]   Time 0.324 (0.423)   Data 0.001 (0.049)   Loss 0.7728 (0.7245)   Prec@1 68.000 (75.065)   Prec@5 100.000 (98.075)   [2025-10-24 19:12:17]
  **Train** Prec@1 75.038 Prec@5 98.044 Error@1 24.962
  **Test** Prec@1 80.810 Prec@5 98.870 Error@1 19.190
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 19:13:13] [Epoch=036/040] [Need: 00:14:53] [LR=0.0010] [Best : Accuracy=80.81, Error=19.19]
  Epoch: [036][000/500]   Time 18.059 (18.059)   Data 17.532 (17.532)   Loss 0.9008 (0.9008)   Prec@1 68.000 (68.000)   Prec@5 98.000 (98.000)   [2025-10-24 19:13:32]
  Epoch: [036][100/500]   Time 0.330 (0.536)   Data 0.000 (0.175)   Loss 0.8478 (0.7292)   Prec@1 69.000 (74.743)   Prec@5 97.000 (98.089)   [2025-10-24 19:14:08]
  Epoch: [036][200/500]   Time 0.324 (0.454)   Data 0.001 (0.088)   Loss 0.5021 (0.7224)   Prec@1 82.000 (74.910)   Prec@5 96.000 (98.169)   [2025-10-24 19:14:45]
  Epoch: [036][300/500]   Time 0.359 (0.427)   Data 0.000 (0.060)   Loss 0.8228 (0.7230)   Prec@1 67.000 (75.090)   Prec@5 98.000 (98.163)   [2025-10-24 19:15:22]
  Epoch: [036][400/500]   Time 0.325 (0.415)   Data 0.001 (0.045)   Loss 0.6426 (0.7260)   Prec@1 78.000 (75.045)   Prec@5 100.000 (98.140)   [2025-10-24 19:16:00]
  **Train** Prec@1 75.086 Prec@5 98.160 Error@1 24.914
  **Test** Prec@1 80.720 Prec@5 98.820 Error@1 19.280

==>>[2025-10-24 19:16:56] [Epoch=037/040] [Need: 00:11:10] [LR=0.0010] [Best : Accuracy=80.81, Error=19.19]
  Epoch: [037][000/500]   Time 18.644 (18.644)   Data 18.109 (18.109)   Loss 0.4907 (0.4907)   Prec@1 85.000 (85.000)   Prec@5 99.000 (99.000)   [2025-10-24 19:17:15]
  Epoch: [037][100/500]   Time 0.400 (0.551)   Data 0.002 (0.181)   Loss 0.7120 (0.7161)   Prec@1 71.000 (76.069)   Prec@5 98.000 (98.109)   [2025-10-24 19:17:52]
  Epoch: [037][200/500]   Time 0.348 (0.463)   Data 0.002 (0.091)   Loss 0.6648 (0.7292)   Prec@1 76.000 (75.607)   Prec@5 98.000 (98.010)   [2025-10-24 19:18:29]
  Epoch: [037][300/500]   Time 0.397 (0.434)   Data 0.002 (0.061)   Loss 0.6082 (0.7293)   Prec@1 83.000 (75.492)   Prec@5 97.000 (98.073)   [2025-10-24 19:19:06]
  Epoch: [037][400/500]   Time 0.368 (0.417)   Data 0.001 (0.046)   Loss 0.7280 (0.7276)   Prec@1 77.000 (75.441)   Prec@5 97.000 (98.092)   [2025-10-24 19:19:43]
  **Train** Prec@1 75.510 Prec@5 98.096 Error@1 24.490
  **Test** Prec@1 80.910 Prec@5 98.820 Error@1 19.090
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 19:20:39] [Epoch=038/040] [Need: 00:07:26] [LR=0.0010] [Best : Accuracy=80.91, Error=19.09]
  Epoch: [038][000/500]   Time 18.392 (18.392)   Data 17.857 (17.857)   Loss 0.5756 (0.5756)   Prec@1 76.000 (76.000)   Prec@5 98.000 (98.000)   [2025-10-24 19:20:57]
  Epoch: [038][100/500]   Time 0.319 (0.549)   Data 0.001 (0.178)   Loss 0.7389 (0.7292)   Prec@1 75.000 (74.931)   Prec@5 98.000 (98.020)   [2025-10-24 19:21:35]
  Epoch: [038][200/500]   Time 0.393 (0.463)   Data 0.002 (0.090)   Loss 0.8677 (0.7239)   Prec@1 73.000 (75.274)   Prec@5 99.000 (98.000)   [2025-10-24 19:22:12]
  Epoch: [038][300/500]   Time 0.412 (0.432)   Data 0.002 (0.061)   Loss 0.5873 (0.7234)   Prec@1 81.000 (75.319)   Prec@5 99.000 (98.076)   [2025-10-24 19:22:49]
  Epoch: [038][400/500]   Time 0.396 (0.419)   Data 0.001 (0.046)   Loss 0.7460 (0.7207)   Prec@1 78.000 (75.556)   Prec@5 97.000 (98.100)   [2025-10-24 19:23:27]
  **Train** Prec@1 75.338 Prec@5 98.090 Error@1 24.662
  **Test** Prec@1 80.820 Prec@5 98.720 Error@1 19.180

==>>[2025-10-24 19:24:48] [Epoch=039/040] [Need: 00:03:43] [LR=0.0010] [Best : Accuracy=80.91, Error=19.09]
  Epoch: [039][000/500]   Time 18.387 (18.387)   Data 17.857 (17.857)   Loss 0.4117 (0.4117)   Prec@1 85.000 (85.000)   Prec@5 100.000 (100.000)   [2025-10-24 19:25:06]
  Epoch: [039][100/500]   Time 0.405 (0.541)   Data 0.002 (0.178)   Loss 0.8157 (0.7121)   Prec@1 70.000 (75.822)   Prec@5 98.000 (98.297)   [2025-10-24 19:25:42]
  Epoch: [039][200/500]   Time 0.342 (0.456)   Data 0.002 (0.090)   Loss 0.6979 (0.7084)   Prec@1 71.000 (75.990)   Prec@5 98.000 (98.249)   [2025-10-24 19:26:19]
  Epoch: [039][300/500]   Time 0.396 (0.426)   Data 0.002 (0.061)   Loss 0.6609 (0.7070)   Prec@1 80.000 (76.080)   Prec@5 100.000 (98.166)   [2025-10-24 19:26:56]
  Epoch: [039][400/500]   Time 0.331 (0.412)   Data 0.000 (0.046)   Loss 0.8966 (0.7170)   Prec@1 73.000 (75.738)   Prec@5 96.000 (98.122)   [2025-10-24 19:27:33]
  **Train** Prec@1 75.728 Prec@5 98.102 Error@1 24.272
  **Test** Prec@1 81.050 Prec@5 98.830 Error@1 18.950
=> Obtain best accuracy, and update the best model
