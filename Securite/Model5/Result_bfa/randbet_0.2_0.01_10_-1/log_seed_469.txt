save path : ./save/tinyvgg_quan/randbet_0.2_0.01_10_-1
{'data_path': './dataset', 'arch': 'tinyvgg_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': True, 'clipping_coeff': 0.2, 'learning_rate': 0.01, 'manualSeed': 469, 'save_path': './save/tinyvgg_quan/randbet_0.2_0.01_10_-1', 'enable_bfa': False, 'resume': '', 'quan_bitwidth': None, 'reset_weight': False, 'evaluate': False, 'n_iter': 30, 'model_only': False, 'random_bfa': False, 'use_cuda': True}
Random Seed: 469
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'tinyvgg_quan'
=> network :
 TinyVGG(
  (features): Sequential(
    (0): quan_Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Dropout2d(p=0.3, inplace=False)
    (6): quan_Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace=True)
    (8): quan_Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace=True)
    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (11): Dropout2d(p=0.3, inplace=False)
    (12): quan_Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): quan_Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Dropout2d(p=0.3, inplace=False)
  )
  (classifier): Sequential(
    (0): quan_Linear(in_features=2048, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): quan_Linear(in_features=128, out_features=10, bias=True)
  )
)
=> do not use any checkpoint for tinyvgg_quan model

==>>[2025-10-24 11:42:30] [Epoch=000/040] [Need: 00:00:00] [LR=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/500]   Time 21.946 (21.946)   Data 21.108 (21.108)   Loss 2.3016 (2.3016)   Prec@1 8.000 (8.000)   Prec@5 47.000 (47.000)   [2025-10-24 11:42:52]
  Epoch: [000][100/500]   Time 0.417 (0.619)   Data 0.001 (0.211)   Loss 2.2979 (2.3027)   Prec@1 8.000 (10.020)   Prec@5 59.000 (50.743)   [2025-10-24 11:43:33]
  Epoch: [000][200/500]   Time 0.418 (0.523)   Data 0.003 (0.107)   Loss 2.2871 (2.3016)   Prec@1 17.000 (10.493)   Prec@5 59.000 (51.154)   [2025-10-24 11:44:16]
  Epoch: [000][300/500]   Time 0.499 (0.488)   Data 0.002 (0.072)   Loss 2.2228 (2.2869)   Prec@1 13.000 (12.027)   Prec@5 68.000 (54.844)   [2025-10-24 11:44:57]
  Epoch: [000][400/500]   Time 0.461 (0.473)   Data 0.001 (0.054)   Loss 2.1271 (2.2521)   Prec@1 20.000 (14.349)   Prec@5 76.000 (59.404)   [2025-10-24 11:45:40]
  **Train** Prec@1 15.958 Prec@5 62.764 Error@1 84.042
  **Test** Prec@1 28.820 Prec@5 80.070 Error@1 71.180
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 11:46:42] [Epoch=001/040] [Need: 02:43:41] [LR=0.0100] [Best : Accuracy=28.82, Error=71.18]
  Epoch: [001][000/500]   Time 20.650 (20.650)   Data 20.080 (20.080)   Loss 1.9979 (1.9979)   Prec@1 28.000 (28.000)   Prec@5 80.000 (80.000)   [2025-10-24 11:47:03]
  Epoch: [001][100/500]   Time 0.353 (0.621)   Data 0.000 (0.200)   Loss 1.9258 (2.0296)   Prec@1 28.000 (24.267)   Prec@5 83.000 (78.337)   [2025-10-24 11:47:45]
  Epoch: [001][200/500]   Time 0.429 (0.516)   Data 0.002 (0.101)   Loss 2.0716 (2.0009)   Prec@1 25.000 (25.065)   Prec@5 71.000 (79.468)   [2025-10-24 11:48:26]
  Epoch: [001][300/500]   Time 0.431 (0.479)   Data 0.002 (0.068)   Loss 1.8930 (1.9779)   Prec@1 27.000 (25.821)   Prec@5 86.000 (80.339)   [2025-10-24 11:49:06]
  Epoch: [001][400/500]   Time 0.359 (0.462)   Data 0.000 (0.051)   Loss 1.8269 (1.9568)   Prec@1 30.000 (26.701)   Prec@5 85.000 (81.055)   [2025-10-24 11:49:48]
  **Train** Prec@1 27.450 Prec@5 81.884 Error@1 72.550
  **Test** Prec@1 36.210 Prec@5 88.620 Error@1 63.790
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 11:50:50] [Epoch=002/040] [Need: 02:38:04] [LR=0.0100] [Best : Accuracy=36.21, Error=63.79]
  Epoch: [002][000/500]   Time 20.395 (20.395)   Data 19.804 (19.804)   Loss 1.7076 (1.7076)   Prec@1 34.000 (34.000)   Prec@5 88.000 (88.000)   [2025-10-24 11:51:10]
  Epoch: [002][100/500]   Time 0.442 (0.601)   Data 0.002 (0.197)   Loss 1.7764 (1.8233)   Prec@1 27.000 (31.693)   Prec@5 88.000 (85.307)   [2025-10-24 11:51:50]
  Epoch: [002][200/500]   Time 0.435 (0.512)   Data 0.002 (0.100)   Loss 1.7936 (1.8072)   Prec@1 26.000 (32.493)   Prec@5 84.000 (85.657)   [2025-10-24 11:52:33]
  Epoch: [002][300/500]   Time 0.371 (0.477)   Data 0.001 (0.067)   Loss 1.7407 (1.7982)   Prec@1 29.000 (32.774)   Prec@5 87.000 (86.073)   [2025-10-24 11:53:13]
  Epoch: [002][400/500]   Time 0.442 (0.460)   Data 0.002 (0.051)   Loss 1.6974 (1.7848)   Prec@1 37.000 (33.222)   Prec@5 93.000 (86.504)   [2025-10-24 11:53:54]
  **Train** Prec@1 33.698 Prec@5 86.698 Error@1 66.302
  **Test** Prec@1 40.810 Prec@5 91.050 Error@1 59.190
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 11:54:55] [Epoch=003/040] [Need: 02:32:57] [LR=0.0100] [Best : Accuracy=40.81, Error=59.19]
  Epoch: [003][000/500]   Time 19.729 (19.729)   Data 19.153 (19.153)   Loss 1.7946 (1.7946)   Prec@1 33.000 (33.000)   Prec@5 87.000 (87.000)   [2025-10-24 11:55:14]
  Epoch: [003][100/500]   Time 0.350 (0.593)   Data 0.001 (0.191)   Loss 1.6796 (1.6973)   Prec@1 38.000 (36.495)   Prec@5 91.000 (88.733)   [2025-10-24 11:55:55]
  Epoch: [003][200/500]   Time 0.435 (0.500)   Data 0.002 (0.097)   Loss 1.6432 (1.6983)   Prec@1 29.000 (36.249)   Prec@5 86.000 (88.652)   [2025-10-24 11:56:35]
  Epoch: [003][300/500]   Time 0.441 (0.471)   Data 0.002 (0.065)   Loss 1.6896 (1.6915)   Prec@1 33.000 (36.688)   Prec@5 87.000 (88.827)   [2025-10-24 11:57:16]
  Epoch: [003][400/500]   Time 0.362 (0.455)   Data 0.001 (0.049)   Loss 1.7394 (1.6779)   Prec@1 40.000 (37.162)   Prec@5 88.000 (88.940)   [2025-10-24 11:57:57]
  **Train** Prec@1 37.438 Prec@5 89.078 Error@1 62.562
  **Test** Prec@1 45.900 Prec@5 93.130 Error@1 54.100
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 11:58:58] [Epoch=004/040] [Need: 02:28:08] [LR=0.0100] [Best : Accuracy=45.90, Error=54.10]
  Epoch: [004][000/500]   Time 19.967 (19.967)   Data 19.407 (19.407)   Loss 1.6038 (1.6038)   Prec@1 42.000 (42.000)   Prec@5 93.000 (93.000)   [2025-10-24 11:59:18]
  Epoch: [004][100/500]   Time 0.427 (0.599)   Data 0.002 (0.194)   Loss 1.6211 (1.6212)   Prec@1 37.000 (39.842)   Prec@5 91.000 (89.743)   [2025-10-24 11:59:59]
  Epoch: [004][200/500]   Time 0.427 (0.503)   Data 0.002 (0.098)   Loss 1.5775 (1.6215)   Prec@1 47.000 (40.244)   Prec@5 88.000 (89.836)   [2025-10-24 12:00:39]
  Epoch: [004][300/500]   Time 0.422 (0.469)   Data 0.002 (0.066)   Loss 1.5064 (1.6102)   Prec@1 40.000 (40.522)   Prec@5 92.000 (90.030)   [2025-10-24 12:01:19]
  Epoch: [004][400/500]   Time 0.385 (0.457)   Data 0.001 (0.050)   Loss 1.4451 (1.6021)   Prec@1 50.000 (40.743)   Prec@5 88.000 (90.170)   [2025-10-24 12:02:01]
  **Train** Prec@1 40.972 Prec@5 90.346 Error@1 59.028
  **Test** Prec@1 49.370 Prec@5 93.810 Error@1 50.630
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 12:03:05] [Epoch=005/040] [Need: 02:23:58] [LR=0.0100] [Best : Accuracy=49.37, Error=50.63]
  Epoch: [005][000/500]   Time 19.737 (19.737)   Data 19.169 (19.169)   Loss 1.5858 (1.5858)   Prec@1 44.000 (44.000)   Prec@5 88.000 (88.000)   [2025-10-24 12:03:24]
  Epoch: [005][100/500]   Time 0.363 (0.598)   Data 0.001 (0.191)   Loss 1.6211 (1.5409)   Prec@1 42.000 (42.238)   Prec@5 89.000 (91.733)   [2025-10-24 12:04:05]
  Epoch: [005][200/500]   Time 0.426 (0.502)   Data 0.002 (0.097)   Loss 1.4859 (1.5460)   Prec@1 48.000 (42.642)   Prec@5 92.000 (91.254)   [2025-10-24 12:04:45]
  Epoch: [005][300/500]   Time 0.375 (0.469)   Data 0.002 (0.065)   Loss 1.7053 (1.5390)   Prec@1 42.000 (43.073)   Prec@5 84.000 (91.312)   [2025-10-24 12:05:26]
  Epoch: [005][400/500]   Time 0.467 (0.454)   Data 0.001 (0.049)   Loss 1.5153 (1.5370)   Prec@1 46.000 (43.299)   Prec@5 93.000 (91.309)   [2025-10-24 12:06:07]
  **Train** Prec@1 43.512 Prec@5 91.308 Error@1 56.488
  **Test** Prec@1 52.170 Prec@5 94.800 Error@1 47.830
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 12:07:53] [Epoch=006/040] [Need: 02:23:42] [LR=0.0100] [Best : Accuracy=52.17, Error=47.83]
  Epoch: [006][000/500]   Time 27.658 (27.658)   Data 25.200 (25.200)   Loss 1.5562 (1.5562)   Prec@1 40.000 (40.000)   Prec@5 90.000 (90.000)   [2025-10-24 12:08:20]
  Epoch: [006][100/500]   Time 2.347 (2.404)   Data 0.004 (0.253)   Loss 1.5784 (1.4798)   Prec@1 42.000 (45.446)   Prec@5 87.000 (92.149)   [2025-10-24 12:11:55]
  Epoch: [006][200/500]   Time 1.049 (2.368)   Data 0.005 (0.129)   Loss 1.4944 (1.4831)   Prec@1 43.000 (45.627)   Prec@5 95.000 (92.164)   [2025-10-24 12:15:48]
  Epoch: [006][300/500]   Time 2.837 (2.386)   Data 0.003 (0.087)   Loss 1.3888 (1.4786)   Prec@1 48.000 (45.588)   Prec@5 93.000 (92.156)   [2025-10-24 12:19:51]
  Epoch: [006][400/500]   Time 2.135 (2.386)   Data 0.003 (0.066)   Loss 1.5568 (1.4756)   Prec@1 47.000 (45.771)   Prec@5 90.000 (92.204)   [2025-10-24 12:23:49]
  **Train** Prec@1 45.990 Prec@5 92.304 Error@1 54.010
  **Test** Prec@1 53.790 Prec@5 95.000 Error@1 46.210
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 12:27:50] [Epoch=007/040] [Need: 03:33:37] [LR=0.0100] [Best : Accuracy=53.79, Error=46.21]
  Epoch: [007][000/500]   Time 28.278 (28.278)   Data 26.367 (26.367)   Loss 1.4248 (1.4248)   Prec@1 44.000 (44.000)   Prec@5 94.000 (94.000)   [2025-10-24 12:28:18]
  Epoch: [007][100/500]   Time 2.180 (2.562)   Data 0.005 (0.264)   Loss 1.5487 (1.4327)   Prec@1 46.000 (47.802)   Prec@5 94.000 (92.733)   [2025-10-24 12:32:09]
  Epoch: [007][200/500]   Time 2.314 (3.070)   Data 0.005 (0.135)   Loss 1.5008 (1.4189)   Prec@1 49.000 (48.030)   Prec@5 91.000 (92.881)   [2025-10-24 12:38:07]
  Epoch: [007][300/500]   Time 2.494 (2.806)   Data 0.004 (0.091)   Loss 1.4387 (1.4229)   Prec@1 47.000 (47.937)   Prec@5 92.000 (92.847)   [2025-10-24 12:41:54]
  Epoch: [007][400/500]   Time 2.010 (2.671)   Data 0.002 (0.069)   Loss 1.3870 (1.4153)   Prec@1 54.000 (48.279)   Prec@5 93.000 (92.998)   [2025-10-24 12:45:41]
  **Train** Prec@1 48.434 Prec@5 93.046 Error@1 51.566
  **Test** Prec@1 57.710 Prec@5 95.770 Error@1 42.290
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 12:47:50] [Epoch=008/040] [Need: 04:21:16] [LR=0.0100] [Best : Accuracy=57.71, Error=42.29]
  Epoch: [008][000/500]   Time 19.873 (19.873)   Data 19.316 (19.316)   Loss 1.4969 (1.4969)   Prec@1 41.000 (41.000)   Prec@5 95.000 (95.000)   [2025-10-24 12:48:10]
  Epoch: [008][100/500]   Time 0.429 (0.599)   Data 0.002 (0.193)   Loss 1.5695 (1.3758)   Prec@1 44.000 (50.406)   Prec@5 91.000 (93.129)   [2025-10-24 12:48:50]
  Epoch: [008][200/500]   Time 0.440 (0.503)   Data 0.002 (0.098)   Loss 1.3152 (1.3696)   Prec@1 53.000 (50.299)   Prec@5 92.000 (93.308)   [2025-10-24 12:49:31]
  Epoch: [008][300/500]   Time 0.365 (0.471)   Data 0.001 (0.066)   Loss 1.1855 (1.3579)   Prec@1 56.000 (50.767)   Prec@5 97.000 (93.515)   [2025-10-24 12:50:11]
  Epoch: [008][400/500]   Time 0.390 (0.454)   Data 0.001 (0.050)   Loss 1.4433 (1.3545)   Prec@1 49.000 (50.878)   Prec@5 93.000 (93.541)   [2025-10-24 12:50:52]
  **Train** Prec@1 51.248 Prec@5 93.582 Error@1 48.752
  **Test** Prec@1 59.430 Prec@5 96.160 Error@1 40.570
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 12:51:54] [Epoch=009/040] [Need: 03:58:59] [LR=0.0100] [Best : Accuracy=59.43, Error=40.57]
  Epoch: [009][000/500]   Time 20.858 (20.858)   Data 20.277 (20.277)   Loss 1.2435 (1.2435)   Prec@1 60.000 (60.000)   Prec@5 93.000 (93.000)   [2025-10-24 12:52:14]
  Epoch: [009][100/500]   Time 0.377 (0.600)   Data 0.000 (0.202)   Loss 1.2074 (1.3052)   Prec@1 58.000 (53.089)   Prec@5 95.000 (93.921)   [2025-10-24 12:52:54]
  Epoch: [009][200/500]   Time 0.353 (0.503)   Data 0.000 (0.102)   Loss 1.1856 (1.3113)   Prec@1 59.000 (52.706)   Prec@5 95.000 (93.925)   [2025-10-24 12:53:35]
  Epoch: [009][300/500]   Time 0.371 (0.470)   Data 0.001 (0.069)   Loss 1.3386 (1.3070)   Prec@1 52.000 (52.688)   Prec@5 93.000 (93.967)   [2025-10-24 12:54:15]
  Epoch: [009][400/500]   Time 0.438 (0.453)   Data 0.001 (0.052)   Loss 1.3689 (1.3029)   Prec@1 46.000 (52.865)   Prec@5 92.000 (94.022)   [2025-10-24 12:54:55]
  **Train** Prec@1 53.030 Prec@5 93.986 Error@1 46.970
  **Test** Prec@1 59.880 Prec@5 96.120 Error@1 40.120
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 12:55:56] [Epoch=010/040] [Need: 03:40:15] [LR=0.0100] [Best : Accuracy=59.88, Error=40.12]
  Epoch: [010][000/500]   Time 21.386 (21.386)   Data 20.818 (20.818)   Loss 1.5377 (1.5377)   Prec@1 42.000 (42.000)   Prec@5 95.000 (95.000)   [2025-10-24 12:56:17]
  Epoch: [010][100/500]   Time 0.353 (0.610)   Data 0.000 (0.208)   Loss 1.3839 (1.2917)   Prec@1 55.000 (53.168)   Prec@5 93.000 (94.040)   [2025-10-24 12:56:57]
  Epoch: [010][200/500]   Time 0.369 (0.509)   Data 0.000 (0.105)   Loss 1.2465 (1.2665)   Prec@1 56.000 (54.403)   Prec@5 96.000 (94.259)   [2025-10-24 12:57:38]
  Epoch: [010][300/500]   Time 0.420 (0.474)   Data 0.002 (0.071)   Loss 1.1783 (1.2604)   Prec@1 59.000 (54.701)   Prec@5 98.000 (94.312)   [2025-10-24 12:58:18]
  Epoch: [010][400/500]   Time 0.443 (0.455)   Data 0.002 (0.053)   Loss 1.1028 (1.2554)   Prec@1 58.000 (54.848)   Prec@5 94.000 (94.304)   [2025-10-24 12:58:58]
  **Train** Prec@1 55.040 Prec@5 94.420 Error@1 44.960
  **Test** Prec@1 64.500 Prec@5 97.040 Error@1 35.500
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 12:59:59] [Epoch=011/040] [Need: 03:24:15] [LR=0.0100] [Best : Accuracy=64.50, Error=35.50]
  Epoch: [011][000/500]   Time 20.077 (20.077)   Data 19.524 (19.524)   Loss 1.1186 (1.1186)   Prec@1 63.000 (63.000)   Prec@5 97.000 (97.000)   [2025-10-24 13:00:19]
  Epoch: [011][100/500]   Time 0.435 (0.593)   Data 0.002 (0.195)   Loss 1.2146 (1.2078)   Prec@1 65.000 (57.178)   Prec@5 97.000 (95.050)   [2025-10-24 13:00:59]
  Epoch: [011][200/500]   Time 0.449 (0.499)   Data 0.002 (0.099)   Loss 1.2121 (1.2078)   Prec@1 58.000 (57.164)   Prec@5 94.000 (94.866)   [2025-10-24 13:01:40]
  Epoch: [011][300/500]   Time 0.430 (0.470)   Data 0.002 (0.066)   Loss 1.1016 (1.2096)   Prec@1 61.000 (56.993)   Prec@5 97.000 (94.784)   [2025-10-24 13:02:21]
  Epoch: [011][400/500]   Time 0.427 (0.452)   Data 0.002 (0.050)   Loss 1.4550 (1.2053)   Prec@1 53.000 (56.998)   Prec@5 93.000 (94.885)   [2025-10-24 13:03:01]
  **Train** Prec@1 57.140 Prec@5 94.894 Error@1 42.860
  **Test** Prec@1 66.160 Prec@5 97.210 Error@1 33.840
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 13:04:01] [Epoch=012/040] [Need: 03:10:11] [LR=0.0100] [Best : Accuracy=66.16, Error=33.84]
  Epoch: [012][000/500]   Time 19.957 (19.957)   Data 19.390 (19.390)   Loss 1.3204 (1.3204)   Prec@1 49.000 (49.000)   Prec@5 95.000 (95.000)   [2025-10-24 13:04:21]
  Epoch: [012][100/500]   Time 0.430 (0.595)   Data 0.001 (0.193)   Loss 1.1182 (1.1744)   Prec@1 60.000 (58.188)   Prec@5 96.000 (94.990)   [2025-10-24 13:05:01]
  Epoch: [012][200/500]   Time 0.412 (0.501)   Data 0.001 (0.098)   Loss 1.2606 (1.1770)   Prec@1 59.000 (58.244)   Prec@5 94.000 (94.871)   [2025-10-24 13:05:42]
  Epoch: [012][300/500]   Time 2.153 (0.502)   Data 0.004 (0.066)   Loss 1.1452 (1.1659)   Prec@1 56.000 (58.475)   Prec@5 99.000 (95.143)   [2025-10-24 13:06:32]
  Epoch: [012][400/500]   Time 2.316 (0.920)   Data 0.010 (0.050)   Loss 1.3045 (1.1678)   Prec@1 51.000 (58.401)   Prec@5 92.000 (95.065)   [2025-10-24 13:10:10]
  **Train** Prec@1 58.584 Prec@5 95.120 Error@1 41.416
  **Test** Prec@1 67.350 Prec@5 97.230 Error@1 32.650
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 13:13:36] [Epoch=013/040] [Need: 03:09:09] [LR=0.0100] [Best : Accuracy=67.35, Error=32.65]
  Epoch: [013][000/500]   Time 24.761 (24.761)   Data 23.697 (23.697)   Loss 1.0668 (1.0668)   Prec@1 65.000 (65.000)   Prec@5 96.000 (96.000)   [2025-10-24 13:14:00]
  Epoch: [013][100/500]   Time 0.643 (1.891)   Data 0.003 (0.237)   Loss 1.1929 (1.1540)   Prec@1 59.000 (58.950)   Prec@5 95.000 (95.267)   [2025-10-24 13:16:47]
  Epoch: [013][200/500]   Time 1.091 (1.788)   Data 0.002 (0.121)   Loss 1.0943 (1.1401)   Prec@1 60.000 (59.343)   Prec@5 96.000 (95.388)   [2025-10-24 13:19:35]
  Epoch: [013][300/500]   Time 1.843 (1.768)   Data 0.001 (0.082)   Loss 1.1992 (1.1409)   Prec@1 63.000 (59.422)   Prec@5 94.000 (95.385)   [2025-10-24 13:22:28]
  Epoch: [013][400/500]   Time 1.879 (1.755)   Data 0.003 (0.062)   Loss 1.2169 (1.1357)   Prec@1 55.000 (59.718)   Prec@5 93.000 (95.372)   [2025-10-24 13:25:19]
  **Train** Prec@1 59.878 Prec@5 95.404 Error@1 40.122
  **Test** Prec@1 68.940 Prec@5 97.530 Error@1 31.060
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 13:28:40] [Epoch=014/040] [Need: 03:17:08] [LR=0.0100] [Best : Accuracy=68.94, Error=31.06]
  Epoch: [014][000/500]   Time 25.862 (25.862)   Data 23.860 (23.860)   Loss 1.2916 (1.2916)   Prec@1 58.000 (58.000)   Prec@5 97.000 (97.000)   [2025-10-24 13:29:06]
  Epoch: [014][100/500]   Time 1.901 (1.934)   Data 0.004 (0.239)   Loss 1.2127 (1.1134)   Prec@1 55.000 (60.554)   Prec@5 97.000 (95.495)   [2025-10-24 13:31:55]
  Epoch: [014][200/500]   Time 2.210 (1.796)   Data 0.005 (0.122)   Loss 1.2453 (1.1135)   Prec@1 61.000 (60.507)   Prec@5 92.000 (95.493)   [2025-10-24 13:34:41]
  Epoch: [014][300/500]   Time 1.762 (1.793)   Data 0.002 (0.082)   Loss 1.1722 (1.1094)   Prec@1 61.000 (60.628)   Prec@5 93.000 (95.548)   [2025-10-24 13:37:40]
  Epoch: [014][400/500]   Time 1.716 (1.737)   Data 0.002 (0.062)   Loss 1.1142 (1.1037)   Prec@1 58.000 (60.943)   Prec@5 91.000 (95.636)   [2025-10-24 13:40:17]
  **Train** Prec@1 61.192 Prec@5 95.666 Error@1 38.808
  **Test** Prec@1 70.040 Prec@5 97.730 Error@1 29.960
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 13:43:22] [Epoch=015/040] [Need: 03:21:25] [LR=0.0100] [Best : Accuracy=70.04, Error=29.96]
  Epoch: [015][000/500]   Time 25.905 (25.905)   Data 24.725 (24.725)   Loss 0.9727 (0.9727)   Prec@1 73.000 (73.000)   Prec@5 94.000 (94.000)   [2025-10-24 13:43:48]
  Epoch: [015][100/500]   Time 2.049 (1.763)   Data 0.003 (0.247)   Loss 0.9753 (1.0656)   Prec@1 65.000 (63.109)   Prec@5 96.000 (95.970)   [2025-10-24 13:46:20]
  Epoch: [015][200/500]   Time 0.863 (1.680)   Data 0.002 (0.126)   Loss 1.0992 (1.0793)   Prec@1 63.000 (62.328)   Prec@5 91.000 (95.801)   [2025-10-24 13:49:00]
  Epoch: [015][300/500]   Time 1.554 (1.683)   Data 0.005 (0.085)   Loss 1.0226 (1.0780)   Prec@1 61.000 (62.239)   Prec@5 97.000 (95.854)   [2025-10-24 13:51:49]
  Epoch: [015][400/500]   Time 1.386 (1.681)   Data 0.002 (0.064)   Loss 1.2654 (1.0767)   Prec@1 57.000 (62.229)   Prec@5 90.000 (95.865)   [2025-10-24 13:54:36]
  **Train** Prec@1 62.360 Prec@5 95.936 Error@1 37.640
  **Test** Prec@1 69.320 Prec@5 97.590 Error@1 30.680

==>>[2025-10-24 13:57:24] [Epoch=016/040] [Need: 03:22:20] [LR=0.0100] [Best : Accuracy=70.04, Error=29.96]
  Epoch: [016][000/500]   Time 26.721 (26.721)   Data 24.788 (24.788)   Loss 1.0325 (1.0325)   Prec@1 60.000 (60.000)   Prec@5 96.000 (96.000)   [2025-10-24 13:57:51]
  Epoch: [016][100/500]   Time 1.731 (2.046)   Data 0.004 (0.248)   Loss 1.0689 (1.0539)   Prec@1 63.000 (63.436)   Prec@5 100.000 (96.050)   [2025-10-24 14:00:51]
  Epoch: [016][200/500]   Time 1.791 (1.931)   Data 0.002 (0.126)   Loss 1.0275 (1.0477)   Prec@1 62.000 (63.363)   Prec@5 97.000 (96.299)   [2025-10-24 14:03:53]
  Epoch: [016][300/500]   Time 1.774 (1.884)   Data 0.002 (0.085)   Loss 0.9817 (1.0465)   Prec@1 70.000 (63.252)   Prec@5 96.000 (96.183)   [2025-10-24 14:06:51]
  Epoch: [016][400/500]   Time 1.765 (1.872)   Data 0.002 (0.065)   Loss 1.1030 (1.0456)   Prec@1 63.000 (63.317)   Prec@5 96.000 (96.137)   [2025-10-24 14:09:55]
  **Train** Prec@1 63.364 Prec@5 96.134 Error@1 36.636
  **Test** Prec@1 71.080 Prec@5 97.820 Error@1 28.920
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 14:13:24] [Epoch=017/040] [Need: 03:24:08] [LR=0.0100] [Best : Accuracy=71.08, Error=28.92]
  Epoch: [017][000/500]   Time 26.048 (26.048)   Data 24.059 (24.059)   Loss 1.0867 (1.0867)   Prec@1 61.000 (61.000)   Prec@5 96.000 (96.000)   [2025-10-24 14:13:50]
  Epoch: [017][100/500]   Time 1.736 (2.059)   Data 0.002 (0.241)   Loss 1.1582 (1.0221)   Prec@1 66.000 (63.673)   Prec@5 93.000 (96.465)   [2025-10-24 14:16:52]
  Epoch: [017][200/500]   Time 1.893 (1.853)   Data 0.002 (0.123)   Loss 0.9756 (1.0266)   Prec@1 68.000 (63.617)   Prec@5 94.000 (96.383)   [2025-10-24 14:19:37]
  Epoch: [017][300/500]   Time 1.817 (1.790)   Data 0.002 (0.083)   Loss 0.9598 (1.0299)   Prec@1 67.000 (63.678)   Prec@5 97.000 (96.369)   [2025-10-24 14:22:23]
  Epoch: [017][400/500]   Time 1.891 (1.759)   Data 0.003 (0.063)   Loss 0.9984 (1.0244)   Prec@1 64.000 (64.002)   Prec@5 97.000 (96.332)   [2025-10-24 14:25:09]
  **Train** Prec@1 64.302 Prec@5 96.366 Error@1 35.698
  **Test** Prec@1 71.440 Prec@5 97.770 Error@1 28.560
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 14:28:17] [Epoch=018/040] [Need: 03:22:36] [LR=0.0100] [Best : Accuracy=71.44, Error=28.56]
  Epoch: [018][000/500]   Time 28.063 (28.063)   Data 25.876 (25.876)   Loss 0.9847 (0.9847)   Prec@1 64.000 (64.000)   Prec@5 98.000 (98.000)   [2025-10-24 14:28:45]
  Epoch: [018][100/500]   Time 1.846 (1.957)   Data 0.003 (0.259)   Loss 1.0860 (1.0085)   Prec@1 64.000 (65.059)   Prec@5 92.000 (96.386)   [2025-10-24 14:31:35]
  Epoch: [018][200/500]   Time 1.710 (1.731)   Data 0.004 (0.132)   Loss 1.0681 (1.0023)   Prec@1 61.000 (64.811)   Prec@5 97.000 (96.488)   [2025-10-24 14:34:05]
  Epoch: [018][300/500]   Time 0.421 (1.464)   Data 0.001 (0.089)   Loss 1.0297 (0.9975)   Prec@1 65.000 (65.017)   Prec@5 97.000 (96.462)   [2025-10-24 14:35:38]
  Epoch: [018][400/500]   Time 1.943 (1.454)   Data 0.003 (0.067)   Loss 0.9462 (0.9985)   Prec@1 62.000 (65.152)   Prec@5 97.000 (96.416)   [2025-10-24 14:38:00]
  **Train** Prec@1 65.174 Prec@5 96.422 Error@1 34.826
  **Test** Prec@1 72.930 Prec@5 98.150 Error@1 27.070
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 14:41:27] [Epoch=019/040] [Need: 03:17:45] [LR=0.0100] [Best : Accuracy=72.93, Error=27.07]
  Epoch: [019][000/500]   Time 28.581 (28.581)   Data 26.397 (26.397)   Loss 0.9720 (0.9720)   Prec@1 63.000 (63.000)   Prec@5 97.000 (97.000)   [2025-10-24 14:41:55]
  Epoch: [019][100/500]   Time 1.919 (2.097)   Data 0.001 (0.265)   Loss 1.0831 (0.9880)   Prec@1 62.000 (65.861)   Prec@5 96.000 (96.446)   [2025-10-24 14:44:59]
  Epoch: [019][200/500]   Time 1.744 (1.950)   Data 0.001 (0.135)   Loss 0.8826 (0.9830)   Prec@1 72.000 (65.960)   Prec@5 98.000 (96.592)   [2025-10-24 14:47:59]
  Epoch: [019][300/500]   Time 2.424 (1.912)   Data 0.003 (0.091)   Loss 1.0834 (0.9840)   Prec@1 60.000 (65.884)   Prec@5 98.000 (96.611)   [2025-10-24 14:51:02]
  Epoch: [019][400/500]   Time 1.407 (1.897)   Data 0.001 (0.069)   Loss 0.9068 (0.9791)   Prec@1 66.000 (65.928)   Prec@5 98.000 (96.673)   [2025-10-24 14:54:08]
  **Train** Prec@1 66.004 Prec@5 96.672 Error@1 33.996
  **Test** Prec@1 73.490 Prec@5 97.970 Error@1 26.510
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 14:57:55] [Epoch=020/040] [Need: 03:15:24] [LR=0.0100] [Best : Accuracy=73.49, Error=26.51]
  Epoch: [020][000/500]   Time 27.612 (27.612)   Data 25.839 (25.839)   Loss 1.0268 (1.0268)   Prec@1 66.000 (66.000)   Prec@5 98.000 (98.000)   [2025-10-24 14:58:23]
  Epoch: [020][100/500]   Time 1.418 (2.083)   Data 0.003 (0.259)   Loss 0.9215 (0.9549)   Prec@1 67.000 (67.119)   Prec@5 98.000 (96.644)   [2025-10-24 15:01:26]
  Epoch: [020][200/500]   Time 2.034 (1.960)   Data 0.001 (0.132)   Loss 1.1683 (0.9681)   Prec@1 61.000 (66.348)   Prec@5 95.000 (96.597)   [2025-10-24 15:04:29]
  Epoch: [020][300/500]   Time 1.632 (1.894)   Data 0.003 (0.089)   Loss 0.8877 (0.9621)   Prec@1 64.000 (66.551)   Prec@5 94.000 (96.678)   [2025-10-24 15:07:25]
  Epoch: [020][400/500]   Time 0.531 (1.555)   Data 0.002 (0.067)   Loss 0.9281 (0.9592)   Prec@1 65.000 (66.656)   Prec@5 97.000 (96.636)   [2025-10-24 15:08:19]
  **Train** Prec@1 66.800 Prec@5 96.688 Error@1 33.200
  **Test** Prec@1 75.220 Prec@5 98.290 Error@1 24.780
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 15:09:24] [Epoch=021/040] [Need: 03:07:11] [LR=0.0100] [Best : Accuracy=75.22, Error=24.78]
  Epoch: [021][000/500]   Time 20.785 (20.785)   Data 20.193 (20.193)   Loss 1.0530 (1.0530)   Prec@1 66.000 (66.000)   Prec@5 96.000 (96.000)   [2025-10-24 15:09:45]
  Epoch: [021][100/500]   Time 0.489 (0.639)   Data 0.003 (0.201)   Loss 0.9373 (0.9374)   Prec@1 67.000 (67.584)   Prec@5 97.000 (96.881)   [2025-10-24 15:10:29]
  Epoch: [021][200/500]   Time 0.385 (0.524)   Data 0.001 (0.102)   Loss 0.9153 (0.9414)   Prec@1 67.000 (67.353)   Prec@5 96.000 (96.900)   [2025-10-24 15:11:09]
  Epoch: [021][300/500]   Time 0.443 (0.487)   Data 0.002 (0.069)   Loss 0.7747 (0.9443)   Prec@1 69.000 (67.256)   Prec@5 99.000 (96.953)   [2025-10-24 15:11:51]
  Epoch: [021][400/500]   Time 0.438 (0.471)   Data 0.001 (0.052)   Loss 0.8616 (0.9423)   Prec@1 70.000 (67.327)   Prec@5 96.000 (96.868)   [2025-10-24 15:12:33]
  **Train** Prec@1 67.404 Prec@5 96.894 Error@1 32.596
  **Test** Prec@1 75.110 Prec@5 98.270 Error@1 24.890

==>>[2025-10-24 15:13:37] [Epoch=022/040] [Need: 02:52:43] [LR=0.0100] [Best : Accuracy=75.22, Error=24.78]
  Epoch: [022][000/500]   Time 20.195 (20.195)   Data 19.625 (19.625)   Loss 1.0491 (1.0491)   Prec@1 64.000 (64.000)   Prec@5 95.000 (95.000)   [2025-10-24 15:13:57]
  Epoch: [022][100/500]   Time 0.365 (0.615)   Data 0.000 (0.196)   Loss 0.9123 (0.9210)   Prec@1 71.000 (68.228)   Prec@5 99.000 (96.693)   [2025-10-24 15:14:39]
  Epoch: [022][200/500]   Time 0.395 (0.518)   Data 0.002 (0.099)   Loss 0.9115 (0.9232)   Prec@1 65.000 (68.065)   Prec@5 97.000 (96.816)   [2025-10-24 15:15:21]
  Epoch: [022][300/500]   Time 0.434 (0.486)   Data 0.002 (0.067)   Loss 0.9604 (0.9275)   Prec@1 66.000 (67.811)   Prec@5 99.000 (96.797)   [2025-10-24 15:16:03]
  Epoch: [022][400/500]   Time 0.447 (0.469)   Data 0.002 (0.050)   Loss 0.8688 (0.9219)   Prec@1 69.000 (68.000)   Prec@5 98.000 (96.878)   [2025-10-24 15:16:45]
  **Train** Prec@1 68.120 Prec@5 96.874 Error@1 31.880
  **Test** Prec@1 75.260 Prec@5 98.250 Error@1 24.740
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 15:17:49] [Epoch=023/040] [Need: 02:39:08] [LR=0.0100] [Best : Accuracy=75.26, Error=24.74]
  Epoch: [023][000/500]   Time 21.511 (21.511)   Data 20.918 (20.918)   Loss 0.8677 (0.8677)   Prec@1 64.000 (64.000)   Prec@5 96.000 (96.000)   [2025-10-24 15:18:10]
  Epoch: [023][100/500]   Time 0.391 (0.631)   Data 0.000 (0.208)   Loss 1.0405 (0.9165)   Prec@1 62.000 (68.426)   Prec@5 95.000 (96.990)   [2025-10-24 15:18:52]
  Epoch: [023][200/500]   Time 0.445 (0.523)   Data 0.001 (0.105)   Loss 0.8922 (0.9103)   Prec@1 60.000 (68.721)   Prec@5 97.000 (97.005)   [2025-10-24 15:19:34]
  Epoch: [023][300/500]   Time 0.440 (0.488)   Data 0.002 (0.071)   Loss 0.7018 (0.9086)   Prec@1 76.000 (68.648)   Prec@5 98.000 (97.076)   [2025-10-24 15:20:16]
  Epoch: [023][400/500]   Time 0.432 (0.470)   Data 0.002 (0.054)   Loss 0.7635 (0.9040)   Prec@1 74.000 (68.763)   Prec@5 99.000 (97.137)   [2025-10-24 15:20:57]
  **Train** Prec@1 68.742 Prec@5 97.120 Error@1 31.258
  **Test** Prec@1 75.780 Prec@5 98.250 Error@1 24.220
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 15:22:00] [Epoch=024/040] [Need: 02:26:19] [LR=0.0100] [Best : Accuracy=75.78, Error=24.22]
  Epoch: [024][000/500]   Time 20.567 (20.567)   Data 19.996 (19.996)   Loss 1.0476 (1.0476)   Prec@1 64.000 (64.000)   Prec@5 93.000 (93.000)   [2025-10-24 15:22:20]
  Epoch: [024][100/500]   Time 0.371 (0.619)   Data 0.001 (0.199)   Loss 0.8520 (0.9149)   Prec@1 69.000 (68.653)   Prec@5 95.000 (96.931)   [2025-10-24 15:23:02]
  Epoch: [024][200/500]   Time 0.433 (0.519)   Data 0.002 (0.101)   Loss 0.7947 (0.9132)   Prec@1 74.000 (68.607)   Prec@5 98.000 (96.955)   [2025-10-24 15:23:44]
  Epoch: [024][300/500]   Time 0.364 (0.485)   Data 0.001 (0.068)   Loss 0.9874 (0.9037)   Prec@1 67.000 (68.841)   Prec@5 98.000 (96.947)   [2025-10-24 15:24:26]
  Epoch: [024][400/500]   Time 0.441 (0.468)   Data 0.001 (0.051)   Loss 0.7573 (0.8971)   Prec@1 71.000 (69.117)   Prec@5 100.000 (96.970)   [2025-10-24 15:25:07]
  **Train** Prec@1 69.176 Prec@5 97.058 Error@1 30.824
  **Test** Prec@1 75.850 Prec@5 98.380 Error@1 24.150
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 15:26:09] [Epoch=025/040] [Need: 02:14:11] [LR=0.0010] [Best : Accuracy=75.85, Error=24.15]
  Epoch: [025][000/500]   Time 21.107 (21.107)   Data 20.525 (20.525)   Loss 0.9340 (0.9340)   Prec@1 70.000 (70.000)   Prec@5 97.000 (97.000)   [2025-10-24 15:26:31]
  Epoch: [025][100/500]   Time 0.459 (0.618)   Data 0.001 (0.205)   Loss 0.7459 (0.8638)   Prec@1 75.000 (70.644)   Prec@5 98.000 (97.198)   [2025-10-24 15:27:12]
  Epoch: [025][200/500]   Time 0.426 (0.521)   Data 0.002 (0.104)   Loss 0.9344 (0.8395)   Prec@1 71.000 (71.279)   Prec@5 95.000 (97.388)   [2025-10-24 15:27:54]
  Epoch: [025][300/500]   Time 0.439 (0.484)   Data 0.002 (0.070)   Loss 0.9343 (0.8353)   Prec@1 65.000 (71.458)   Prec@5 97.000 (97.462)   [2025-10-24 15:28:35]
  Epoch: [025][400/500]   Time 0.425 (0.465)   Data 0.002 (0.053)   Loss 0.9345 (0.8311)   Prec@1 67.000 (71.469)   Prec@5 98.000 (97.464)   [2025-10-24 15:29:16]
  **Train** Prec@1 71.586 Prec@5 97.522 Error@1 28.414
  **Test** Prec@1 78.270 Prec@5 98.570 Error@1 21.730
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 15:30:18] [Epoch=026/040] [Need: 02:02:39] [LR=0.0010] [Best : Accuracy=78.27, Error=21.73]
  Epoch: [026][000/500]   Time 19.893 (19.893)   Data 19.318 (19.318)   Loss 0.9228 (0.9228)   Prec@1 69.000 (69.000)   Prec@5 96.000 (96.000)   [2025-10-24 15:30:38]
  Epoch: [026][100/500]   Time 0.445 (0.598)   Data 0.001 (0.193)   Loss 0.8428 (0.8173)   Prec@1 70.000 (71.772)   Prec@5 96.000 (97.762)   [2025-10-24 15:31:18]
  Epoch: [026][200/500]   Time 0.350 (0.504)   Data 0.000 (0.098)   Loss 0.7500 (0.8089)   Prec@1 75.000 (72.025)   Prec@5 99.000 (97.801)   [2025-10-24 15:31:59]
  Epoch: [026][300/500]   Time 0.360 (0.472)   Data 0.001 (0.066)   Loss 0.9281 (0.8085)   Prec@1 71.000 (72.123)   Prec@5 95.000 (97.744)   [2025-10-24 15:32:40]
  Epoch: [026][400/500]   Time 0.463 (0.460)   Data 0.002 (0.050)   Loss 0.8052 (0.8065)   Prec@1 77.000 (72.342)   Prec@5 96.000 (97.718)   [2025-10-24 15:33:22]
  **Train** Prec@1 72.392 Prec@5 97.720 Error@1 27.608
  **Test** Prec@1 78.740 Prec@5 98.740 Error@1 21.260
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 15:34:25] [Epoch=027/040] [Need: 01:51:39] [LR=0.0010] [Best : Accuracy=78.74, Error=21.26]
  Epoch: [027][000/500]   Time 20.244 (20.244)   Data 19.667 (19.667)   Loss 0.8342 (0.8342)   Prec@1 71.000 (71.000)   Prec@5 98.000 (98.000)   [2025-10-24 15:34:46]
  Epoch: [027][100/500]   Time 0.370 (0.615)   Data 0.001 (0.196)   Loss 0.7001 (0.8135)   Prec@1 73.000 (72.168)   Prec@5 100.000 (97.614)   [2025-10-24 15:35:27]
  Epoch: [027][200/500]   Time 0.368 (0.520)   Data 0.001 (0.099)   Loss 0.8166 (0.8033)   Prec@1 72.000 (72.338)   Prec@5 99.000 (97.761)   [2025-10-24 15:36:10]
  Epoch: [027][300/500]   Time 0.441 (0.488)   Data 0.001 (0.067)   Loss 0.7650 (0.7984)   Prec@1 75.000 (72.522)   Prec@5 97.000 (97.821)   [2025-10-24 15:36:52]
  Epoch: [027][400/500]   Time 0.450 (0.473)   Data 0.002 (0.050)   Loss 0.6555 (0.7966)   Prec@1 76.000 (72.579)   Prec@5 97.000 (97.773)   [2025-10-24 15:37:35]
  **Train** Prec@1 72.692 Prec@5 97.784 Error@1 27.308
  **Test** Prec@1 78.850 Prec@5 98.660 Error@1 21.150
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 15:38:39] [Epoch=028/040] [Need: 01:41:12] [LR=0.0010] [Best : Accuracy=78.85, Error=21.15]
  Epoch: [028][000/500]   Time 20.451 (20.451)   Data 19.868 (19.868)   Loss 0.7225 (0.7225)   Prec@1 75.000 (75.000)   Prec@5 99.000 (99.000)   [2025-10-24 15:38:59]
  Epoch: [028][100/500]   Time 0.393 (0.626)   Data 0.002 (0.198)   Loss 0.9235 (0.7985)   Prec@1 67.000 (72.941)   Prec@5 98.000 (97.455)   [2025-10-24 15:39:42]
  Epoch: [028][200/500]   Time 0.362 (0.520)   Data 0.001 (0.100)   Loss 0.8242 (0.7905)   Prec@1 71.000 (72.940)   Prec@5 97.000 (97.607)   [2025-10-24 15:40:23]
  Epoch: [028][300/500]   Time 0.434 (0.485)   Data 0.003 (0.067)   Loss 0.9253 (0.7879)   Prec@1 66.000 (72.877)   Prec@5 99.000 (97.661)   [2025-10-24 15:41:05]
  Epoch: [028][400/500]   Time 0.438 (0.469)   Data 0.002 (0.051)   Loss 0.5931 (0.7878)   Prec@1 78.000 (72.786)   Prec@5 100.000 (97.701)   [2025-10-24 15:41:47]
  **Train** Prec@1 72.748 Prec@5 97.708 Error@1 27.252
  **Test** Prec@1 79.050 Prec@5 98.660 Error@1 20.950
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 15:42:50] [Epoch=029/040] [Need: 01:31:09] [LR=0.0010] [Best : Accuracy=79.05, Error=20.95]
  Epoch: [029][000/500]   Time 20.667 (20.667)   Data 20.089 (20.089)   Loss 0.6721 (0.6721)   Prec@1 80.000 (80.000)   Prec@5 99.000 (99.000)   [2025-10-24 15:43:10]
  Epoch: [029][100/500]   Time 0.434 (0.623)   Data 0.002 (0.200)   Loss 0.7677 (0.7795)   Prec@1 72.000 (73.119)   Prec@5 98.000 (97.901)   [2025-10-24 15:43:53]
  Epoch: [029][200/500]   Time 0.449 (0.521)   Data 0.002 (0.101)   Loss 0.7763 (0.7820)   Prec@1 72.000 (73.214)   Prec@5 99.000 (97.816)   [2025-10-24 15:44:34]
  Epoch: [029][300/500]   Time 0.399 (0.487)   Data 0.001 (0.068)   Loss 0.8042 (0.7836)   Prec@1 76.000 (73.302)   Prec@5 96.000 (97.671)   [2025-10-24 15:45:16]
  Epoch: [029][400/500]   Time 0.434 (0.471)   Data 0.002 (0.052)   Loss 1.0074 (0.7812)   Prec@1 64.000 (73.182)   Prec@5 99.000 (97.746)   [2025-10-24 15:45:58]
  **Train** Prec@1 72.982 Prec@5 97.746 Error@1 27.018
  **Test** Prec@1 79.140 Prec@5 98.690 Error@1 20.860
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 15:47:01] [Epoch=030/040] [Need: 01:21:30] [LR=0.0010] [Best : Accuracy=79.14, Error=20.86]
  Epoch: [030][000/500]   Time 20.228 (20.228)   Data 19.642 (19.642)   Loss 0.7851 (0.7851)   Prec@1 73.000 (73.000)   Prec@5 99.000 (99.000)   [2025-10-24 15:47:21]
  Epoch: [030][100/500]   Time 0.384 (0.618)   Data 0.001 (0.196)   Loss 0.7505 (0.7909)   Prec@1 70.000 (73.218)   Prec@5 99.000 (97.871)   [2025-10-24 15:48:03]
  Epoch: [030][200/500]   Time 0.362 (0.520)   Data 0.001 (0.099)   Loss 0.8020 (0.7866)   Prec@1 72.000 (73.124)   Prec@5 99.000 (97.851)   [2025-10-24 15:48:45]
  Epoch: [030][300/500]   Time 0.455 (0.486)   Data 0.001 (0.067)   Loss 0.8867 (0.7789)   Prec@1 68.000 (73.385)   Prec@5 98.000 (97.947)   [2025-10-24 15:49:27]
  Epoch: [030][400/500]   Time 0.394 (0.468)   Data 0.000 (0.050)   Loss 0.8854 (0.7814)   Prec@1 67.000 (73.352)   Prec@5 97.000 (97.873)   [2025-10-24 15:50:09]
  **Train** Prec@1 73.296 Prec@5 97.840 Error@1 26.704
  **Test** Prec@1 79.150 Prec@5 98.710 Error@1 20.850
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 15:51:11] [Epoch=031/040] [Need: 01:12:11] [LR=0.0010] [Best : Accuracy=79.15, Error=20.85]
  Epoch: [031][000/500]   Time 20.648 (20.648)   Data 20.060 (20.060)   Loss 0.7141 (0.7141)   Prec@1 73.000 (73.000)   Prec@5 98.000 (98.000)   [2025-10-24 15:51:32]
  Epoch: [031][100/500]   Time 0.438 (0.617)   Data 0.002 (0.200)   Loss 0.7131 (0.7776)   Prec@1 71.000 (73.149)   Prec@5 98.000 (97.594)   [2025-10-24 15:52:13]
  Epoch: [031][200/500]   Time 0.365 (0.519)   Data 0.001 (0.101)   Loss 0.8711 (0.7791)   Prec@1 69.000 (73.159)   Prec@5 98.000 (97.761)   [2025-10-24 15:52:55]
  Epoch: [031][300/500]   Time 0.452 (0.488)   Data 0.002 (0.068)   Loss 0.8656 (0.7773)   Prec@1 74.000 (73.236)   Prec@5 97.000 (97.791)   [2025-10-24 15:53:38]
  Epoch: [031][400/500]   Time 0.384 (0.472)   Data 0.001 (0.051)   Loss 0.7206 (0.7731)   Prec@1 76.000 (73.359)   Prec@5 98.000 (97.840)   [2025-10-24 15:54:20]
  **Train** Prec@1 73.482 Prec@5 97.850 Error@1 26.518
  **Test** Prec@1 79.590 Prec@5 98.750 Error@1 20.410
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 15:55:27] [Epoch=032/040] [Need: 01:03:14] [LR=0.0010] [Best : Accuracy=79.59, Error=20.41]
  Epoch: [032][000/500]   Time 20.980 (20.980)   Data 20.393 (20.393)   Loss 0.8366 (0.8366)   Prec@1 73.000 (73.000)   Prec@5 98.000 (98.000)   [2025-10-24 15:55:48]
  Epoch: [032][100/500]   Time 0.384 (0.620)   Data 0.002 (0.203)   Loss 0.5497 (0.7631)   Prec@1 78.000 (73.931)   Prec@5 99.000 (97.871)   [2025-10-24 15:56:30]
  Epoch: [032][200/500]   Time 0.364 (0.517)   Data 0.000 (0.103)   Loss 0.8772 (0.7688)   Prec@1 74.000 (73.711)   Prec@5 99.000 (97.856)   [2025-10-24 15:57:11]
  Epoch: [032][300/500]   Time 0.394 (0.486)   Data 0.000 (0.069)   Loss 0.6845 (0.7678)   Prec@1 75.000 (73.748)   Prec@5 98.000 (97.827)   [2025-10-24 15:57:53]
  Epoch: [032][400/500]   Time 0.442 (0.470)   Data 0.001 (0.052)   Loss 0.7387 (0.7693)   Prec@1 72.000 (73.611)   Prec@5 99.000 (97.870)   [2025-10-24 15:58:36]
  **Train** Prec@1 73.580 Prec@5 97.852 Error@1 26.420
  **Test** Prec@1 79.600 Prec@5 98.720 Error@1 20.400
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 15:59:43] [Epoch=033/040] [Need: 00:54:33] [LR=0.0010] [Best : Accuracy=79.60, Error=20.40]
  Epoch: [033][000/500]   Time 28.325 (28.325)   Data 27.594 (27.594)   Loss 0.6275 (0.6275)   Prec@1 78.000 (78.000)   Prec@5 99.000 (99.000)   [2025-10-24 16:00:12]
  Epoch: [033][100/500]   Time 0.458 (0.723)   Data 0.002 (0.275)   Loss 0.7455 (0.7531)   Prec@1 80.000 (74.376)   Prec@5 98.000 (97.822)   [2025-10-24 16:00:56]
  Epoch: [033][200/500]   Time 0.369 (0.575)   Data 0.002 (0.139)   Loss 0.7873 (0.7630)   Prec@1 74.000 (74.070)   Prec@5 99.000 (98.035)   [2025-10-24 16:01:39]
  Epoch: [033][300/500]   Time 0.412 (0.521)   Data 0.002 (0.093)   Loss 0.7896 (0.7653)   Prec@1 69.000 (73.784)   Prec@5 100.000 (97.980)   [2025-10-24 16:02:20]
  Epoch: [033][400/500]   Time 0.426 (0.494)   Data 0.003 (0.070)   Loss 0.5345 (0.7677)   Prec@1 82.000 (73.616)   Prec@5 98.000 (97.960)   [2025-10-24 16:03:01]
  **Train** Prec@1 73.506 Prec@5 97.936 Error@1 26.494
  **Test** Prec@1 79.620 Prec@5 98.760 Error@1 20.380
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 16:04:05] [Epoch=034/040] [Need: 00:46:09] [LR=0.0010] [Best : Accuracy=79.62, Error=20.38]
  Epoch: [034][000/500]   Time 19.978 (19.978)   Data 19.409 (19.409)   Loss 0.8397 (0.8397)   Prec@1 68.000 (68.000)   Prec@5 98.000 (98.000)   [2025-10-24 16:04:25]
  Epoch: [034][100/500]   Time 0.359 (0.591)   Data 0.002 (0.194)   Loss 0.7361 (0.7772)   Prec@1 77.000 (73.614)   Prec@5 98.000 (97.752)   [2025-10-24 16:05:04]
  Epoch: [034][200/500]   Time 1.984 (0.984)   Data 0.002 (0.099)   Loss 0.7996 (0.7686)   Prec@1 69.000 (73.925)   Prec@5 98.000 (97.791)   [2025-10-24 16:07:22]
  Epoch: [034][300/500]   Time 1.434 (1.208)   Data 0.005 (0.067)   Loss 0.8356 (0.7645)   Prec@1 68.000 (73.950)   Prec@5 93.000 (97.870)   [2025-10-24 16:10:08]
  Epoch: [034][400/500]   Time 2.011 (1.310)   Data 0.002 (0.051)   Loss 0.5409 (0.7638)   Prec@1 80.000 (73.845)   Prec@5 99.000 (97.938)   [2025-10-24 16:12:50]
  **Train** Prec@1 73.842 Prec@5 97.938 Error@1 26.158
  **Test** Prec@1 79.650 Prec@5 98.770 Error@1 20.350
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 16:16:03] [Epoch=035/040] [Need: 00:39:04] [LR=0.0010] [Best : Accuracy=79.65, Error=20.35]
  Epoch: [035][000/500]   Time 27.098 (27.098)   Data 26.286 (26.286)   Loss 0.7425 (0.7425)   Prec@1 75.000 (75.000)   Prec@5 98.000 (98.000)   [2025-10-24 16:16:30]
  Epoch: [035][100/500]   Time 1.780 (1.937)   Data 0.005 (0.263)   Loss 0.8251 (0.7658)   Prec@1 65.000 (73.743)   Prec@5 100.000 (98.000)   [2025-10-24 16:19:18]
  Epoch: [035][200/500]   Time 1.331 (1.809)   Data 0.005 (0.134)   Loss 0.6460 (0.7600)   Prec@1 79.000 (74.144)   Prec@5 98.000 (97.915)   [2025-10-24 16:22:06]
  Epoch: [035][300/500]   Time 0.347 (1.428)   Data 0.001 (0.090)   Loss 0.7447 (0.7570)   Prec@1 72.000 (74.017)   Prec@5 97.000 (98.000)   [2025-10-24 16:23:13]
  Epoch: [035][400/500]   Time 0.368 (3.962)   Data 0.001 (0.068)   Loss 1.0284 (0.7605)   Prec@1 67.000 (73.863)   Prec@5 97.000 (97.990)   [2025-10-24 16:42:32]
  **Train** Prec@1 73.994 Prec@5 97.970 Error@1 26.006
  **Test** Prec@1 79.860 Prec@5 98.760 Error@1 20.140
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 16:43:28] [Epoch=036/040] [Need: 00:33:26] [LR=0.0010] [Best : Accuracy=79.86, Error=20.14]
  Epoch: [036][000/500]   Time 18.272 (18.272)   Data 17.700 (17.700)   Loss 0.9366 (0.9366)   Prec@1 62.000 (62.000)   Prec@5 100.000 (100.000)   [2025-10-24 16:43:46]
  Epoch: [036][100/500]   Time 0.403 (0.566)   Data 0.000 (0.177)   Loss 0.8079 (0.7563)   Prec@1 68.000 (74.129)   Prec@5 99.000 (98.079)   [2025-10-24 16:44:25]
  Epoch: [036][200/500]   Time 0.342 (0.479)   Data 0.000 (0.089)   Loss 0.7320 (0.7502)   Prec@1 78.000 (74.468)   Prec@5 98.000 (97.930)   [2025-10-24 16:45:04]
  Epoch: [036][300/500]   Time 0.370 (0.448)   Data 0.001 (0.060)   Loss 0.8440 (0.7535)   Prec@1 68.000 (74.209)   Prec@5 98.000 (97.947)   [2025-10-24 16:45:43]
  Epoch: [036][400/500]   Time 0.417 (0.435)   Data 0.002 (0.045)   Loss 0.6628 (0.7589)   Prec@1 81.000 (74.055)   Prec@5 96.000 (97.943)   [2025-10-24 16:46:23]
  **Train** Prec@1 73.908 Prec@5 97.966 Error@1 26.092
  **Test** Prec@1 79.730 Prec@5 98.740 Error@1 20.270

==>>[2025-10-24 16:47:21] [Epoch=037/040] [Need: 00:24:42] [LR=0.0010] [Best : Accuracy=79.86, Error=20.14]
  Epoch: [037][000/500]   Time 18.223 (18.223)   Data 17.655 (17.655)   Loss 0.8802 (0.8802)   Prec@1 71.000 (71.000)   Prec@5 97.000 (97.000)   [2025-10-24 16:47:39]
  Epoch: [037][100/500]   Time 0.349 (0.568)   Data 0.001 (0.176)   Loss 0.8486 (0.7504)   Prec@1 70.000 (74.079)   Prec@5 95.000 (98.050)   [2025-10-24 16:48:18]
  Epoch: [037][200/500]   Time 0.333 (0.480)   Data 0.000 (0.089)   Loss 0.7350 (0.7501)   Prec@1 76.000 (74.378)   Prec@5 98.000 (98.030)   [2025-10-24 16:48:57]
  Epoch: [037][300/500]   Time 0.417 (0.451)   Data 0.001 (0.060)   Loss 0.6109 (0.7552)   Prec@1 76.000 (74.246)   Prec@5 99.000 (97.990)   [2025-10-24 16:49:37]
  Epoch: [037][400/500]   Time 0.415 (0.438)   Data 0.002 (0.045)   Loss 0.6905 (0.7562)   Prec@1 76.000 (74.170)   Prec@5 97.000 (98.002)   [2025-10-24 16:50:16]
  **Train** Prec@1 74.192 Prec@5 97.974 Error@1 25.808
  **Test** Prec@1 80.120 Prec@5 98.690 Error@1 19.880
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 16:51:14] [Epoch=038/040] [Need: 00:16:14] [LR=0.0010] [Best : Accuracy=80.12, Error=19.88]
  Epoch: [038][000/500]   Time 18.146 (18.146)   Data 17.596 (17.596)   Loss 0.7914 (0.7914)   Prec@1 68.000 (68.000)   Prec@5 98.000 (98.000)   [2025-10-24 16:51:32]
  Epoch: [038][100/500]   Time 0.343 (0.565)   Data 0.001 (0.176)   Loss 0.7506 (0.7616)   Prec@1 71.000 (73.624)   Prec@5 100.000 (97.990)   [2025-10-24 16:52:11]
  Epoch: [038][200/500]   Time 0.428 (0.479)   Data 0.002 (0.089)   Loss 0.6631 (0.7562)   Prec@1 77.000 (74.209)   Prec@5 100.000 (97.960)   [2025-10-24 16:52:51]
  Epoch: [038][300/500]   Time 0.419 (0.450)   Data 0.002 (0.060)   Loss 0.7579 (0.7605)   Prec@1 72.000 (73.993)   Prec@5 99.000 (97.980)   [2025-10-24 16:53:30]
  Epoch: [038][400/500]   Time 0.349 (0.436)   Data 0.000 (0.045)   Loss 0.6567 (0.7554)   Prec@1 77.000 (74.175)   Prec@5 98.000 (98.007)   [2025-10-24 16:54:09]
  **Train** Prec@1 74.106 Prec@5 98.042 Error@1 25.894
  **Test** Prec@1 80.120 Prec@5 98.820 Error@1 19.880
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 16:55:07] [Epoch=039/040] [Need: 00:08:00] [LR=0.0010] [Best : Accuracy=80.12, Error=19.88]
  Epoch: [039][000/500]   Time 17.994 (17.994)   Data 17.434 (17.434)   Loss 0.8775 (0.8775)   Prec@1 66.000 (66.000)   Prec@5 97.000 (97.000)   [2025-10-24 16:55:25]
  Epoch: [039][100/500]   Time 0.417 (0.565)   Data 0.002 (0.174)   Loss 0.7495 (0.7509)   Prec@1 69.000 (74.515)   Prec@5 100.000 (98.139)   [2025-10-24 16:56:04]
  Epoch: [039][200/500]   Time 0.421 (0.479)   Data 0.002 (0.088)   Loss 0.6199 (0.7492)   Prec@1 78.000 (74.562)   Prec@5 100.000 (98.010)   [2025-10-24 16:56:43]
  Epoch: [039][300/500]   Time 0.425 (0.450)   Data 0.002 (0.059)   Loss 0.7581 (0.7515)   Prec@1 72.000 (74.382)   Prec@5 98.000 (98.043)   [2025-10-24 16:57:22]
  Epoch: [039][400/500]   Time 0.419 (0.436)   Data 0.003 (0.045)   Loss 0.7077 (0.7502)   Prec@1 75.000 (74.389)   Prec@5 97.000 (98.010)   [2025-10-24 16:58:01]
  **Train** Prec@1 74.292 Prec@5 97.976 Error@1 25.708
  **Test** Prec@1 80.080 Prec@5 98.860 Error@1 19.920
