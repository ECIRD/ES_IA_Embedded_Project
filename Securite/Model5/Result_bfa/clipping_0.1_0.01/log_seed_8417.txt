save path : ./save/tinyvgg_quan/clipping_0.1_0.01
{'data_path': './dataset', 'arch': 'tinyvgg_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': False, 'clipping_coeff': 0.1, 'learning_rate': 0.01, 'manualSeed': 8417, 'save_path': './save/tinyvgg_quan/clipping_0.1_0.01', 'enable_bfa': False, 'resume': '', 'quan_bitwidth': None, 'reset_weight': False, 'evaluate': False, 'n_iter': 30, 'model_only': False, 'random_bfa': False, 'use_cuda': True}
Random Seed: 8417
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'tinyvgg_quan'
=> network :
 TinyVGG(
  (features): Sequential(
    (0): quan_Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Dropout2d(p=0.3, inplace=False)
    (6): quan_Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace=True)
    (8): quan_Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace=True)
    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (11): Dropout2d(p=0.3, inplace=False)
    (12): quan_Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): quan_Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Dropout2d(p=0.3, inplace=False)
  )
  (classifier): Sequential(
    (0): quan_Linear(in_features=2048, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): quan_Linear(in_features=128, out_features=10, bias=True)
  )
)
=> do not use any checkpoint for tinyvgg_quan model

==>>[2025-10-24 08:37:35] [Epoch=000/040] [Need: 00:00:00] [LR=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/500]   Time 20.381 (20.381)   Data 19.957 (19.957)   Loss 2.3069 (2.3069)   Prec@1 8.000 (8.000)   Prec@5 53.000 (53.000)   [2025-10-24 08:37:56]
  Epoch: [000][100/500]   Time 0.014 (0.220)   Data 0.000 (0.198)   Loss 2.3040 (2.3037)   Prec@1 6.000 (9.782)   Prec@5 50.000 (50.287)   [2025-10-24 08:37:58]
  Epoch: [000][200/500]   Time 0.018 (0.119)   Data 0.000 (0.099)   Loss 2.3067 (2.3034)   Prec@1 7.000 (9.796)   Prec@5 41.000 (49.846)   [2025-10-24 08:37:59]
  Epoch: [000][300/500]   Time 0.016 (0.085)   Data 0.000 (0.067)   Loss 2.3089 (2.3032)   Prec@1 5.000 (9.734)   Prec@5 36.000 (49.900)   [2025-10-24 08:38:01]
  Epoch: [000][400/500]   Time 0.040 (0.070)   Data 0.001 (0.050)   Loss 2.3002 (2.3031)   Prec@1 11.000 (9.838)   Prec@5 56.000 (49.818)   [2025-10-24 08:38:03]
  **Train** Prec@1 9.888 Prec@5 49.850 Error@1 90.112
  **Test** Prec@1 10.000 Prec@5 50.030 Error@1 90.000
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:38:33] [Epoch=001/040] [Need: 00:36:56] [LR=0.0100] [Best : Accuracy=10.00, Error=90.00]
  Epoch: [001][000/500]   Time 27.261 (27.261)   Data 27.183 (27.183)   Loss 2.3004 (2.3004)   Prec@1 6.000 (6.000)   Prec@5 52.000 (52.000)   [2025-10-24 08:39:00]
  Epoch: [001][100/500]   Time 0.033 (0.299)   Data 0.000 (0.270)   Loss 2.3033 (2.3023)   Prec@1 11.000 (10.327)   Prec@5 51.000 (50.772)   [2025-10-24 08:39:03]
  Epoch: [001][200/500]   Time 0.031 (0.165)   Data 0.001 (0.136)   Loss 2.3014 (2.3023)   Prec@1 8.000 (10.388)   Prec@5 55.000 (50.975)   [2025-10-24 08:39:06]
  Epoch: [001][300/500]   Time 0.028 (0.120)   Data 0.001 (0.091)   Loss 2.3027 (2.3017)   Prec@1 11.000 (10.967)   Prec@5 47.000 (51.757)   [2025-10-24 08:39:09]
  Epoch: [001][400/500]   Time 0.029 (0.097)   Data 0.001 (0.068)   Loss 2.1926 (2.2928)   Prec@1 15.000 (11.970)   Prec@5 68.000 (54.267)   [2025-10-24 08:39:12]
  **Train** Prec@1 13.766 Prec@5 57.866 Error@1 86.234
  **Test** Prec@1 24.240 Prec@5 77.170 Error@1 75.760
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:39:40] [Epoch=002/040] [Need: 00:39:17] [LR=0.0100] [Best : Accuracy=24.24, Error=75.76]
  Epoch: [002][000/500]   Time 27.181 (27.181)   Data 27.101 (27.101)   Loss 1.9918 (1.9918)   Prec@1 29.000 (29.000)   Prec@5 74.000 (74.000)   [2025-10-24 08:40:07]
  Epoch: [002][100/500]   Time 0.031 (0.300)   Data 0.000 (0.269)   Loss 1.9959 (2.0650)   Prec@1 27.000 (23.040)   Prec@5 82.000 (76.752)   [2025-10-24 08:40:10]
  Epoch: [002][200/500]   Time 0.027 (0.165)   Data 0.000 (0.135)   Loss 1.9674 (2.0369)   Prec@1 26.000 (24.174)   Prec@5 79.000 (78.005)   [2025-10-24 08:40:13]
  Epoch: [002][300/500]   Time 0.030 (0.121)   Data 0.000 (0.090)   Loss 1.9850 (2.0131)   Prec@1 29.000 (24.884)   Prec@5 76.000 (79.017)   [2025-10-24 08:40:16]
  Epoch: [002][400/500]   Time 0.038 (0.097)   Data 0.000 (0.068)   Loss 2.0335 (1.9913)   Prec@1 26.000 (25.481)   Prec@5 81.000 (79.825)   [2025-10-24 08:40:19]
  **Train** Prec@1 26.068 Prec@5 80.712 Error@1 73.932
  **Test** Prec@1 34.240 Prec@5 87.600 Error@1 65.760
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:40:47] [Epoch=003/040] [Need: 00:39:20] [LR=0.0100] [Best : Accuracy=34.24, Error=65.76]
  Epoch: [003][000/500]   Time 57.869 (57.869)   Data 57.790 (57.790)   Loss 1.8676 (1.8676)   Prec@1 28.000 (28.000)   Prec@5 83.000 (83.000)   [2025-10-24 08:41:45]
  Epoch: [003][100/500]   Time 0.026 (0.604)   Data 0.001 (0.573)   Loss 1.7670 (1.8608)   Prec@1 32.000 (28.802)   Prec@5 87.000 (84.347)   [2025-10-24 08:41:48]
  Epoch: [003][200/500]   Time 0.027 (0.317)   Data 0.001 (0.288)   Loss 1.8751 (1.8479)   Prec@1 25.000 (29.269)   Prec@5 81.000 (84.930)   [2025-10-24 08:41:51]
  Epoch: [003][300/500]   Time 0.025 (0.221)   Data 0.000 (0.192)   Loss 1.8696 (1.8308)   Prec@1 28.000 (30.179)   Prec@5 80.000 (85.289)   [2025-10-24 08:41:54]
  Epoch: [003][400/500]   Time 0.026 (0.173)   Data 0.000 (0.145)   Loss 1.5488 (1.8163)   Prec@1 41.000 (30.803)   Prec@5 95.000 (85.698)   [2025-10-24 08:41:56]
  **Train** Prec@1 31.362 Prec@5 86.028 Error@1 68.638
  **Test** Prec@1 40.120 Prec@5 90.830 Error@1 59.880
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:42:25] [Epoch=004/040] [Need: 00:43:19] [LR=0.0100] [Best : Accuracy=40.12, Error=59.88]
  Epoch: [004][000/500]   Time 27.416 (27.416)   Data 27.335 (27.335)   Loss 1.6705 (1.6705)   Prec@1 41.000 (41.000)   Prec@5 89.000 (89.000)   [2025-10-24 08:42:52]
  Epoch: [004][100/500]   Time 0.025 (0.302)   Data 0.000 (0.271)   Loss 1.5401 (1.7383)   Prec@1 47.000 (34.495)   Prec@5 92.000 (87.822)   [2025-10-24 08:42:55]
  Epoch: [004][200/500]   Time 0.033 (0.165)   Data 0.000 (0.137)   Loss 1.7128 (1.7324)   Prec@1 39.000 (34.915)   Prec@5 86.000 (87.592)   [2025-10-24 08:42:58]
  Epoch: [004][300/500]   Time 0.031 (0.120)   Data 0.000 (0.091)   Loss 1.7036 (1.7149)   Prec@1 34.000 (35.585)   Prec@5 90.000 (87.944)   [2025-10-24 08:43:01]
  Epoch: [004][400/500]   Time 0.027 (0.097)   Data 0.000 (0.069)   Loss 1.5779 (1.7026)   Prec@1 44.000 (36.157)   Prec@5 90.000 (88.269)   [2025-10-24 08:43:04]
  **Train** Prec@1 36.416 Prec@5 88.476 Error@1 63.584
  **Test** Prec@1 43.610 Prec@5 92.560 Error@1 56.390
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:43:32] [Epoch=005/040] [Need: 00:41:31] [LR=0.0100] [Best : Accuracy=43.61, Error=56.39]
  Epoch: [005][000/500]   Time 26.552 (26.552)   Data 26.463 (26.463)   Loss 1.9057 (1.9057)   Prec@1 30.000 (30.000)   Prec@5 80.000 (80.000)   [2025-10-24 08:43:58]
  Epoch: [005][100/500]   Time 0.040 (0.293)   Data 0.000 (0.262)   Loss 1.4889 (1.6483)   Prec@1 54.000 (39.010)   Prec@5 92.000 (89.317)   [2025-10-24 08:44:01]
  Epoch: [005][200/500]   Time 0.024 (0.162)   Data 0.001 (0.132)   Loss 1.5179 (1.6318)   Prec@1 42.000 (39.294)   Prec@5 90.000 (89.741)   [2025-10-24 08:44:04]
  Epoch: [005][300/500]   Time 0.031 (0.118)   Data 0.000 (0.088)   Loss 1.7953 (1.6298)   Prec@1 29.000 (39.359)   Prec@5 85.000 (89.618)   [2025-10-24 08:44:07]
  Epoch: [005][400/500]   Time 0.030 (0.096)   Data 0.000 (0.066)   Loss 1.6166 (1.6209)   Prec@1 40.000 (39.643)   Prec@5 87.000 (89.721)   [2025-10-24 08:44:10]
  **Train** Prec@1 39.958 Prec@5 89.848 Error@1 60.042
  **Test** Prec@1 46.930 Prec@5 93.230 Error@1 53.070
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:44:38] [Epoch=006/040] [Need: 00:39:54] [LR=0.0100] [Best : Accuracy=46.93, Error=53.07]
  Epoch: [006][000/500]   Time 27.011 (27.011)   Data 26.928 (26.928)   Loss 1.5655 (1.5655)   Prec@1 45.000 (45.000)   Prec@5 91.000 (91.000)   [2025-10-24 08:45:05]
  Epoch: [006][100/500]   Time 0.032 (0.299)   Data 0.001 (0.267)   Loss 1.5460 (1.5702)   Prec@1 46.000 (42.812)   Prec@5 90.000 (90.436)   [2025-10-24 08:45:09]
  Epoch: [006][200/500]   Time 0.029 (0.166)   Data 0.000 (0.134)   Loss 1.6886 (1.5648)   Prec@1 35.000 (42.881)   Prec@5 90.000 (90.458)   [2025-10-24 08:45:12]
  Epoch: [006][300/500]   Time 0.026 (0.120)   Data 0.001 (0.090)   Loss 1.5453 (1.5577)   Prec@1 44.000 (42.694)   Prec@5 91.000 (90.648)   [2025-10-24 08:45:15]
  Epoch: [006][400/500]   Time 0.029 (0.098)   Data 0.001 (0.068)   Loss 1.5234 (1.5499)   Prec@1 43.000 (43.000)   Prec@5 91.000 (90.696)   [2025-10-24 08:45:18]
  **Train** Prec@1 43.140 Prec@5 90.846 Error@1 56.860
  **Test** Prec@1 50.810 Prec@5 94.340 Error@1 49.190
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:45:45] [Epoch=007/040] [Need: 00:38:25] [LR=0.0100] [Best : Accuracy=50.81, Error=49.19]
  Epoch: [007][000/500]   Time 26.384 (26.384)   Data 26.307 (26.307)   Loss 1.5987 (1.5987)   Prec@1 40.000 (40.000)   Prec@5 89.000 (89.000)   [2025-10-24 08:46:11]
  Epoch: [007][100/500]   Time 0.028 (0.295)   Data 0.001 (0.261)   Loss 1.4802 (1.5156)   Prec@1 47.000 (44.307)   Prec@5 92.000 (91.277)   [2025-10-24 08:46:15]
  Epoch: [007][200/500]   Time 0.037 (0.164)   Data 0.001 (0.131)   Loss 1.6502 (1.4918)   Prec@1 37.000 (45.075)   Prec@5 88.000 (91.577)   [2025-10-24 08:46:18]
  Epoch: [007][300/500]   Time 0.027 (0.120)   Data 0.001 (0.088)   Loss 1.5175 (1.4864)   Prec@1 44.000 (45.512)   Prec@5 91.000 (91.518)   [2025-10-24 08:46:21]
  Epoch: [007][400/500]   Time 0.028 (0.097)   Data 0.000 (0.066)   Loss 1.4125 (1.4791)   Prec@1 46.000 (45.803)   Prec@5 92.000 (91.608)   [2025-10-24 08:46:24]
  **Train** Prec@1 46.018 Prec@5 91.764 Error@1 53.982
  **Test** Prec@1 53.320 Prec@5 94.520 Error@1 46.680
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:46:52] [Epoch=008/040] [Need: 00:37:03] [LR=0.0100] [Best : Accuracy=53.32, Error=46.68]
  Epoch: [008][000/500]   Time 26.434 (26.434)   Data 26.356 (26.356)   Loss 1.4363 (1.4363)   Prec@1 54.000 (54.000)   Prec@5 90.000 (90.000)   [2025-10-24 08:47:18]
  Epoch: [008][100/500]   Time 0.028 (0.291)   Data 0.000 (0.261)   Loss 1.4515 (1.4316)   Prec@1 47.000 (47.446)   Prec@5 89.000 (92.505)   [2025-10-24 08:47:21]
  Epoch: [008][200/500]   Time 0.026 (0.162)   Data 0.001 (0.132)   Loss 1.3894 (1.4333)   Prec@1 47.000 (47.751)   Prec@5 96.000 (92.299)   [2025-10-24 08:47:24]
  Epoch: [008][300/500]   Time 0.034 (0.119)   Data 0.001 (0.088)   Loss 1.2947 (1.4301)   Prec@1 54.000 (47.794)   Prec@5 96.000 (92.488)   [2025-10-24 08:47:28]
  Epoch: [008][400/500]   Time 0.032 (0.098)   Data 0.000 (0.066)   Loss 1.1369 (1.4229)   Prec@1 60.000 (48.252)   Prec@5 97.000 (92.501)   [2025-10-24 08:47:31]
  **Train** Prec@1 48.426 Prec@5 92.612 Error@1 51.574
  **Test** Prec@1 55.750 Prec@5 95.050 Error@1 44.250
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:48:00] [Epoch=009/040] [Need: 00:35:48] [LR=0.0100] [Best : Accuracy=55.75, Error=44.25]
  Epoch: [009][000/500]   Time 25.486 (25.486)   Data 25.407 (25.407)   Loss 1.4431 (1.4431)   Prec@1 45.000 (45.000)   Prec@5 93.000 (93.000)   [2025-10-24 08:48:25]
  Epoch: [009][100/500]   Time 0.033 (0.284)   Data 0.002 (0.252)   Loss 1.3463 (1.3900)   Prec@1 50.000 (49.109)   Prec@5 95.000 (93.079)   [2025-10-24 08:48:28]
  Epoch: [009][200/500]   Time 0.033 (0.158)   Data 0.001 (0.127)   Loss 1.2751 (1.3918)   Prec@1 56.000 (49.224)   Prec@5 94.000 (92.811)   [2025-10-24 08:48:31]
  Epoch: [009][300/500]   Time 0.025 (0.116)   Data 0.000 (0.085)   Loss 1.2819 (1.3797)   Prec@1 54.000 (49.761)   Prec@5 93.000 (92.927)   [2025-10-24 08:48:35]
  Epoch: [009][400/500]   Time 0.040 (0.095)   Data 0.000 (0.064)   Loss 1.4470 (1.3751)   Prec@1 50.000 (50.052)   Prec@5 97.000 (93.027)   [2025-10-24 08:48:38]
  **Train** Prec@1 50.358 Prec@5 93.086 Error@1 49.642
  **Test** Prec@1 58.120 Prec@5 95.580 Error@1 41.880
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:49:06] [Epoch=010/040] [Need: 00:34:29] [LR=0.0100] [Best : Accuracy=58.12, Error=41.88]
  Epoch: [010][000/500]   Time 26.147 (26.147)   Data 26.062 (26.062)   Loss 1.3456 (1.3456)   Prec@1 43.000 (43.000)   Prec@5 92.000 (92.000)   [2025-10-24 08:49:32]
  Epoch: [010][100/500]   Time 0.025 (0.290)   Data 0.000 (0.258)   Loss 1.2718 (1.3589)   Prec@1 46.000 (50.327)   Prec@5 96.000 (93.436)   [2025-10-24 08:49:35]
  Epoch: [010][200/500]   Time 0.048 (0.160)   Data 0.005 (0.130)   Loss 1.3158 (1.3429)   Prec@1 50.000 (51.358)   Prec@5 94.000 (93.532)   [2025-10-24 08:49:38]
  Epoch: [010][300/500]   Time 0.031 (0.117)   Data 0.001 (0.087)   Loss 1.2503 (1.3347)   Prec@1 55.000 (51.761)   Prec@5 95.000 (93.708)   [2025-10-24 08:49:41]
  Epoch: [010][400/500]   Time 0.028 (0.095)   Data 0.001 (0.065)   Loss 1.3786 (1.3326)   Prec@1 49.000 (51.930)   Prec@5 94.000 (93.738)   [2025-10-24 08:49:44]
  **Train** Prec@1 52.046 Prec@5 93.720 Error@1 47.954
  **Test** Prec@1 59.260 Prec@5 95.960 Error@1 40.740
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:50:12] [Epoch=011/040] [Need: 00:33:12] [LR=0.0100] [Best : Accuracy=59.26, Error=40.74]
  Epoch: [011][000/500]   Time 26.289 (26.289)   Data 26.203 (26.203)   Loss 1.3052 (1.3052)   Prec@1 55.000 (55.000)   Prec@5 93.000 (93.000)   [2025-10-24 08:50:38]
  Epoch: [011][100/500]   Time 0.033 (0.291)   Data 0.001 (0.260)   Loss 1.4411 (1.2886)   Prec@1 61.000 (53.693)   Prec@5 90.000 (93.772)   [2025-10-24 08:50:41]
  Epoch: [011][200/500]   Time 0.030 (0.161)   Data 0.000 (0.131)   Loss 1.2718 (1.2940)   Prec@1 59.000 (53.821)   Prec@5 95.000 (93.557)   [2025-10-24 08:50:44]
  Epoch: [011][300/500]   Time 0.028 (0.117)   Data 0.000 (0.088)   Loss 1.2221 (1.2905)   Prec@1 55.000 (53.937)   Prec@5 96.000 (93.704)   [2025-10-24 08:50:47]
  Epoch: [011][400/500]   Time 0.025 (0.095)   Data 0.000 (0.066)   Loss 1.2437 (1.2896)   Prec@1 57.000 (54.052)   Prec@5 91.000 (93.751)   [2025-10-24 08:50:50]
  **Train** Prec@1 53.898 Prec@5 93.818 Error@1 46.102
  **Test** Prec@1 62.180 Prec@5 96.290 Error@1 37.820
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:51:18] [Epoch=012/040] [Need: 00:31:58] [LR=0.0100] [Best : Accuracy=62.18, Error=37.82]
  Epoch: [012][000/500]   Time 26.385 (26.385)   Data 26.305 (26.305)   Loss 1.2993 (1.2993)   Prec@1 59.000 (59.000)   Prec@5 94.000 (94.000)   [2025-10-24 08:51:44]
  Epoch: [012][100/500]   Time 0.027 (0.292)   Data 0.000 (0.261)   Loss 1.3992 (1.2571)   Prec@1 53.000 (54.851)   Prec@5 95.000 (94.228)   [2025-10-24 08:51:47]
  Epoch: [012][200/500]   Time 0.039 (0.161)   Data 0.001 (0.131)   Loss 1.3947 (1.2678)   Prec@1 51.000 (54.458)   Prec@5 92.000 (94.060)   [2025-10-24 08:51:50]
  Epoch: [012][300/500]   Time 0.032 (0.117)   Data 0.002 (0.088)   Loss 1.4131 (1.2616)   Prec@1 53.000 (54.598)   Prec@5 93.000 (94.040)   [2025-10-24 08:51:53]
  Epoch: [012][400/500]   Time 0.025 (0.095)   Data 0.000 (0.066)   Loss 1.0530 (1.2586)   Prec@1 69.000 (54.663)   Prec@5 98.000 (94.072)   [2025-10-24 08:51:56]
  **Train** Prec@1 54.906 Prec@5 94.192 Error@1 45.094
  **Test** Prec@1 62.910 Prec@5 96.630 Error@1 37.090
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:52:24] [Epoch=013/040] [Need: 00:30:44] [LR=0.0100] [Best : Accuracy=62.91, Error=37.09]
  Epoch: [013][000/500]   Time 26.703 (26.703)   Data 26.638 (26.638)   Loss 1.3473 (1.3473)   Prec@1 53.000 (53.000)   Prec@5 92.000 (92.000)   [2025-10-24 08:52:50]
  Epoch: [013][100/500]   Time 0.017 (0.283)   Data 0.000 (0.264)   Loss 1.3465 (1.2278)   Prec@1 51.000 (56.317)   Prec@5 91.000 (94.792)   [2025-10-24 08:52:52]
  Epoch: [013][200/500]   Time 0.015 (0.150)   Data 0.000 (0.133)   Loss 1.2782 (1.2317)   Prec@1 53.000 (55.945)   Prec@5 95.000 (94.498)   [2025-10-24 08:52:54]
  Epoch: [013][300/500]   Time 0.017 (0.106)   Data 0.000 (0.089)   Loss 1.2608 (1.2275)   Prec@1 52.000 (55.977)   Prec@5 93.000 (94.432)   [2025-10-24 08:52:56]
  Epoch: [013][400/500]   Time 0.019 (0.084)   Data 0.000 (0.067)   Loss 1.3455 (1.2245)   Prec@1 48.000 (56.077)   Prec@5 93.000 (94.474)   [2025-10-24 08:52:57]
  **Train** Prec@1 56.166 Prec@5 94.542 Error@1 43.834
  **Test** Prec@1 62.550 Prec@5 96.560 Error@1 37.450

==>>[2025-10-24 08:53:22] [Epoch=014/040] [Need: 00:29:18] [LR=0.0100] [Best : Accuracy=62.91, Error=37.09]
  Epoch: [014][000/500]   Time 21.449 (21.449)   Data 21.384 (21.384)   Loss 1.4070 (1.4070)   Prec@1 55.000 (55.000)   Prec@5 88.000 (88.000)   [2025-10-24 08:53:44]
  Epoch: [014][100/500]   Time 0.016 (0.231)   Data 0.001 (0.212)   Loss 1.0589 (1.2080)   Prec@1 61.000 (56.891)   Prec@5 97.000 (94.950)   [2025-10-24 08:53:46]
  Epoch: [014][200/500]   Time 0.016 (0.125)   Data 0.000 (0.107)   Loss 1.2234 (1.2035)   Prec@1 61.000 (57.612)   Prec@5 92.000 (94.766)   [2025-10-24 08:53:47]
  Epoch: [014][300/500]   Time 0.017 (0.090)   Data 0.001 (0.071)   Loss 1.1817 (1.1936)   Prec@1 57.000 (57.734)   Prec@5 95.000 (94.907)   [2025-10-24 08:53:49]
  Epoch: [014][400/500]   Time 0.018 (0.072)   Data 0.000 (0.054)   Loss 1.1556 (1.1897)   Prec@1 56.000 (57.823)   Prec@5 95.000 (94.875)   [2025-10-24 08:53:51]
  **Train** Prec@1 57.748 Prec@5 94.868 Error@1 42.252
  **Test** Prec@1 65.180 Prec@5 97.080 Error@1 34.820
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:54:15] [Epoch=015/040] [Need: 00:27:45] [LR=0.0100] [Best : Accuracy=65.18, Error=34.82]
  Epoch: [015][000/500]   Time 20.055 (20.055)   Data 19.991 (19.991)   Loss 1.3511 (1.3511)   Prec@1 58.000 (58.000)   Prec@5 93.000 (93.000)   [2025-10-24 08:54:35]
  Epoch: [015][100/500]   Time 0.017 (0.217)   Data 0.000 (0.198)   Loss 1.1886 (1.1760)   Prec@1 66.000 (58.624)   Prec@5 93.000 (95.277)   [2025-10-24 08:54:37]
  Epoch: [015][200/500]   Time 0.019 (0.118)   Data 0.000 (0.100)   Loss 1.0194 (1.1720)   Prec@1 65.000 (58.682)   Prec@5 94.000 (95.244)   [2025-10-24 08:54:38]
  Epoch: [015][300/500]   Time 0.016 (0.084)   Data 0.000 (0.067)   Loss 1.1745 (1.1709)   Prec@1 60.000 (58.505)   Prec@5 97.000 (95.233)   [2025-10-24 08:54:40]
  Epoch: [015][400/500]   Time 0.017 (0.068)   Data 0.000 (0.050)   Loss 1.2937 (1.1675)   Prec@1 50.000 (58.474)   Prec@5 94.000 (95.192)   [2025-10-24 08:54:42]
  **Train** Prec@1 58.572 Prec@5 95.112 Error@1 41.428
  **Test** Prec@1 66.380 Prec@5 97.310 Error@1 33.620
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:55:05] [Epoch=016/040] [Need: 00:26:14] [LR=0.0100] [Best : Accuracy=66.38, Error=33.62]
  Epoch: [016][000/500]   Time 20.909 (20.909)   Data 20.844 (20.844)   Loss 1.1111 (1.1111)   Prec@1 55.000 (55.000)   Prec@5 96.000 (96.000)   [2025-10-24 08:55:26]
  Epoch: [016][100/500]   Time 0.017 (0.226)   Data 0.000 (0.207)   Loss 1.0828 (1.1342)   Prec@1 59.000 (59.416)   Prec@5 95.000 (95.416)   [2025-10-24 08:55:28]
  Epoch: [016][200/500]   Time 0.017 (0.122)   Data 0.000 (0.104)   Loss 1.0319 (1.1392)   Prec@1 67.000 (59.607)   Prec@5 94.000 (95.308)   [2025-10-24 08:55:30]
  Epoch: [016][300/500]   Time 0.016 (0.087)   Data 0.000 (0.069)   Loss 1.3504 (1.1353)   Prec@1 51.000 (59.831)   Prec@5 95.000 (95.412)   [2025-10-24 08:55:31]
  Epoch: [016][400/500]   Time 0.020 (0.070)   Data 0.000 (0.052)   Loss 1.1651 (1.1381)   Prec@1 59.000 (59.703)   Prec@5 95.000 (95.334)   [2025-10-24 08:55:33]
  **Train** Prec@1 59.836 Prec@5 95.348 Error@1 40.164
  **Test** Prec@1 67.030 Prec@5 97.280 Error@1 32.970
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:55:58] [Epoch=017/040] [Need: 00:24:51] [LR=0.0100] [Best : Accuracy=67.03, Error=32.97]
  Epoch: [017][000/500]   Time 21.081 (21.081)   Data 21.015 (21.015)   Loss 1.1728 (1.1728)   Prec@1 58.000 (58.000)   Prec@5 95.000 (95.000)   [2025-10-24 08:56:19]
  Epoch: [017][100/500]   Time 0.019 (0.228)   Data 0.001 (0.208)   Loss 1.1398 (1.1077)   Prec@1 61.000 (60.267)   Prec@5 96.000 (95.554)   [2025-10-24 08:56:21]
  Epoch: [017][200/500]   Time 0.019 (0.124)   Data 0.000 (0.105)   Loss 0.9204 (1.1126)   Prec@1 67.000 (60.458)   Prec@5 97.000 (95.468)   [2025-10-24 08:56:23]
  Epoch: [017][300/500]   Time 0.019 (0.088)   Data 0.000 (0.070)   Loss 0.9557 (1.1123)   Prec@1 70.000 (60.618)   Prec@5 98.000 (95.468)   [2025-10-24 08:56:24]
  Epoch: [017][400/500]   Time 0.017 (0.071)   Data 0.000 (0.053)   Loss 1.0476 (1.1130)   Prec@1 62.000 (60.691)   Prec@5 96.000 (95.436)   [2025-10-24 08:56:26]
  **Train** Prec@1 60.852 Prec@5 95.432 Error@1 39.148
  **Test** Prec@1 67.750 Prec@5 97.350 Error@1 32.250
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:56:49] [Epoch=018/040] [Need: 00:23:29] [LR=0.0100] [Best : Accuracy=67.75, Error=32.25]
  Epoch: [018][000/500]   Time 20.860 (20.860)   Data 20.795 (20.795)   Loss 1.0594 (1.0594)   Prec@1 66.000 (66.000)   Prec@5 94.000 (94.000)   [2025-10-24 08:57:10]
  Epoch: [018][100/500]   Time 0.019 (0.225)   Data 0.000 (0.206)   Loss 1.1565 (1.0985)   Prec@1 59.000 (61.446)   Prec@5 95.000 (95.713)   [2025-10-24 08:57:12]
  Epoch: [018][200/500]   Time 0.018 (0.122)   Data 0.000 (0.104)   Loss 1.1268 (1.0919)   Prec@1 64.000 (61.418)   Prec@5 94.000 (95.801)   [2025-10-24 08:57:14]
  Epoch: [018][300/500]   Time 0.021 (0.087)   Data 0.001 (0.069)   Loss 1.2751 (1.0873)   Prec@1 51.000 (61.595)   Prec@5 95.000 (95.814)   [2025-10-24 08:57:15]
  Epoch: [018][400/500]   Time 0.020 (0.070)   Data 0.001 (0.052)   Loss 1.0271 (1.0873)   Prec@1 68.000 (61.546)   Prec@5 95.000 (95.758)   [2025-10-24 08:57:17]
  **Train** Prec@1 61.828 Prec@5 95.724 Error@1 38.172
  **Test** Prec@1 69.310 Prec@5 97.500 Error@1 30.690
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:57:40] [Epoch=019/040] [Need: 00:22:11] [LR=0.0100] [Best : Accuracy=69.31, Error=30.69]
  Epoch: [019][000/500]   Time 20.048 (20.048)   Data 19.982 (19.982)   Loss 1.0003 (1.0003)   Prec@1 66.000 (66.000)   Prec@5 98.000 (98.000)   [2025-10-24 08:58:00]
  Epoch: [019][100/500]   Time 0.015 (0.216)   Data 0.000 (0.198)   Loss 1.1194 (1.0754)   Prec@1 58.000 (61.931)   Prec@5 93.000 (95.950)   [2025-10-24 08:58:02]
  Epoch: [019][200/500]   Time 0.018 (0.117)   Data 0.001 (0.100)   Loss 1.1744 (1.0776)   Prec@1 59.000 (61.995)   Prec@5 95.000 (95.955)   [2025-10-24 08:58:04]
  Epoch: [019][300/500]   Time 0.016 (0.084)   Data 0.001 (0.067)   Loss 1.1370 (1.0761)   Prec@1 57.000 (62.083)   Prec@5 95.000 (95.850)   [2025-10-24 08:58:06]
  Epoch: [019][400/500]   Time 0.017 (0.068)   Data 0.000 (0.050)   Loss 1.1148 (1.0729)   Prec@1 64.000 (62.180)   Prec@5 93.000 (95.845)   [2025-10-24 08:58:07]
  **Train** Prec@1 62.394 Prec@5 95.930 Error@1 37.606
  **Test** Prec@1 69.180 Prec@5 97.620 Error@1 30.820

==>>[2025-10-24 08:58:31] [Epoch=020/040] [Need: 00:20:55] [LR=0.0100] [Best : Accuracy=69.31, Error=30.69]
  Epoch: [020][000/500]   Time 19.952 (19.952)   Data 19.888 (19.888)   Loss 1.0630 (1.0630)   Prec@1 65.000 (65.000)   Prec@5 100.000 (100.000)   [2025-10-24 08:58:51]
  Epoch: [020][100/500]   Time 0.016 (0.215)   Data 0.000 (0.197)   Loss 0.9444 (1.0443)   Prec@1 65.000 (63.683)   Prec@5 99.000 (95.891)   [2025-10-24 08:58:53]
  Epoch: [020][200/500]   Time 0.018 (0.116)   Data 0.001 (0.099)   Loss 0.9943 (1.0522)   Prec@1 64.000 (63.398)   Prec@5 99.000 (95.925)   [2025-10-24 08:58:54]
  Epoch: [020][300/500]   Time 0.016 (0.084)   Data 0.001 (0.066)   Loss 1.1479 (1.0514)   Prec@1 59.000 (63.352)   Prec@5 94.000 (96.010)   [2025-10-24 08:58:56]
  Epoch: [020][400/500]   Time 0.018 (0.067)   Data 0.000 (0.050)   Loss 0.8274 (1.0444)   Prec@1 68.000 (63.666)   Prec@5 98.000 (96.037)   [2025-10-24 08:58:58]
  **Train** Prec@1 63.610 Prec@5 96.058 Error@1 36.390
  **Test** Prec@1 70.710 Prec@5 97.740 Error@1 29.290
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:59:21] [Epoch=021/040] [Need: 00:19:41] [LR=0.0100] [Best : Accuracy=70.71, Error=29.29]
  Epoch: [021][000/500]   Time 20.345 (20.345)   Data 20.277 (20.277)   Loss 0.8936 (0.8936)   Prec@1 69.000 (69.000)   Prec@5 98.000 (98.000)   [2025-10-24 08:59:41]
  Epoch: [021][100/500]   Time 0.016 (0.220)   Data 0.000 (0.201)   Loss 0.8266 (1.0219)   Prec@1 73.000 (63.931)   Prec@5 100.000 (96.149)   [2025-10-24 08:59:43]
  Epoch: [021][200/500]   Time 0.018 (0.119)   Data 0.000 (0.101)   Loss 1.1110 (1.0215)   Prec@1 61.000 (64.219)   Prec@5 97.000 (96.144)   [2025-10-24 08:59:45]
  Epoch: [021][300/500]   Time 0.016 (0.085)   Data 0.000 (0.068)   Loss 0.8666 (1.0307)   Prec@1 72.000 (63.880)   Prec@5 97.000 (96.103)   [2025-10-24 08:59:47]
  Epoch: [021][400/500]   Time 0.019 (0.068)   Data 0.000 (0.051)   Loss 0.9286 (1.0260)   Prec@1 65.000 (64.180)   Prec@5 98.000 (96.200)   [2025-10-24 08:59:49]
  **Train** Prec@1 64.236 Prec@5 96.218 Error@1 35.764
  **Test** Prec@1 70.890 Prec@5 98.010 Error@1 29.110
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:00:13] [Epoch=022/040] [Need: 00:18:30] [LR=0.0100] [Best : Accuracy=70.89, Error=29.11]
  Epoch: [022][000/500]   Time 20.982 (20.982)   Data 20.915 (20.915)   Loss 0.9554 (0.9554)   Prec@1 63.000 (63.000)   Prec@5 97.000 (97.000)   [2025-10-24 09:00:34]
  Epoch: [022][100/500]   Time 0.016 (0.227)   Data 0.000 (0.207)   Loss 1.2026 (1.0078)   Prec@1 56.000 (64.792)   Prec@5 94.000 (96.248)   [2025-10-24 09:00:35]
  Epoch: [022][200/500]   Time 0.019 (0.123)   Data 0.000 (0.104)   Loss 1.0923 (1.0241)   Prec@1 59.000 (64.189)   Prec@5 97.000 (96.348)   [2025-10-24 09:00:37]
  Epoch: [022][300/500]   Time 0.022 (0.088)   Data 0.000 (0.070)   Loss 1.0102 (1.0131)   Prec@1 68.000 (64.645)   Prec@5 92.000 (96.435)   [2025-10-24 09:00:39]
  Epoch: [022][400/500]   Time 0.021 (0.071)   Data 0.000 (0.052)   Loss 1.0695 (1.0062)   Prec@1 62.000 (64.955)   Prec@5 99.000 (96.441)   [2025-10-24 09:00:41]
  **Train** Prec@1 64.968 Prec@5 96.502 Error@1 35.032
  **Test** Prec@1 72.100 Prec@5 97.970 Error@1 27.900
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:01:04] [Epoch=023/040] [Need: 00:17:21] [LR=0.0100] [Best : Accuracy=72.10, Error=27.90]
  Epoch: [023][000/500]   Time 20.574 (20.574)   Data 20.509 (20.509)   Loss 0.9600 (0.9600)   Prec@1 68.000 (68.000)   Prec@5 97.000 (97.000)   [2025-10-24 09:01:25]
  Epoch: [023][100/500]   Time 0.021 (0.223)   Data 0.002 (0.203)   Loss 0.9854 (0.9935)   Prec@1 67.000 (65.614)   Prec@5 97.000 (96.960)   [2025-10-24 09:01:27]
  Epoch: [023][200/500]   Time 0.019 (0.120)   Data 0.000 (0.102)   Loss 0.9470 (0.9956)   Prec@1 72.000 (65.433)   Prec@5 100.000 (96.786)   [2025-10-24 09:01:28]
  Epoch: [023][300/500]   Time 0.018 (0.086)   Data 0.000 (0.068)   Loss 1.2485 (0.9971)   Prec@1 57.000 (65.458)   Prec@5 94.000 (96.651)   [2025-10-24 09:01:30]
  Epoch: [023][400/500]   Time 0.018 (0.069)   Data 0.000 (0.051)   Loss 0.8965 (0.9967)   Prec@1 70.000 (65.491)   Prec@5 97.000 (96.633)   [2025-10-24 09:01:32]
  **Train** Prec@1 65.502 Prec@5 96.608 Error@1 34.498
  **Test** Prec@1 73.340 Prec@5 98.190 Error@1 26.660
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:01:55] [Epoch=024/040] [Need: 00:16:12] [LR=0.0100] [Best : Accuracy=73.34, Error=26.66]
  Epoch: [024][000/500]   Time 22.211 (22.211)   Data 22.144 (22.144)   Loss 0.8452 (0.8452)   Prec@1 68.000 (68.000)   Prec@5 98.000 (98.000)   [2025-10-24 09:02:17]
  Epoch: [024][100/500]   Time 0.018 (0.239)   Data 0.000 (0.219)   Loss 0.9225 (0.9834)   Prec@1 74.000 (65.990)   Prec@5 95.000 (96.663)   [2025-10-24 09:02:19]
  Epoch: [024][200/500]   Time 0.016 (0.128)   Data 0.000 (0.110)   Loss 1.0208 (0.9849)   Prec@1 64.000 (65.701)   Prec@5 99.000 (96.582)   [2025-10-24 09:02:21]
  Epoch: [024][300/500]   Time 0.019 (0.091)   Data 0.000 (0.074)   Loss 0.9936 (0.9791)   Prec@1 68.000 (65.967)   Prec@5 99.000 (96.711)   [2025-10-24 09:02:23]
  Epoch: [024][400/500]   Time 0.017 (0.073)   Data 0.000 (0.055)   Loss 0.9482 (0.9765)   Prec@1 64.000 (66.085)   Prec@5 96.000 (96.723)   [2025-10-24 09:02:24]
  **Train** Prec@1 66.234 Prec@5 96.718 Error@1 33.766
  **Test** Prec@1 73.410 Prec@5 98.250 Error@1 26.590
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:02:47] [Epoch=025/040] [Need: 00:15:07] [LR=0.0010] [Best : Accuracy=73.41, Error=26.59]
  Epoch: [025][000/500]   Time 19.478 (19.478)   Data 19.412 (19.412)   Loss 0.8414 (0.8414)   Prec@1 71.000 (71.000)   Prec@5 99.000 (99.000)   [2025-10-24 09:03:07]
  Epoch: [025][100/500]   Time 0.014 (0.211)   Data 0.000 (0.192)   Loss 0.9295 (0.9375)   Prec@1 66.000 (67.535)   Prec@5 96.000 (97.079)   [2025-10-24 09:03:09]
  Epoch: [025][200/500]   Time 0.018 (0.115)   Data 0.000 (0.097)   Loss 0.7699 (0.9134)   Prec@1 73.000 (68.259)   Prec@5 99.000 (96.975)   [2025-10-24 09:03:10]
  Epoch: [025][300/500]   Time 0.020 (0.082)   Data 0.000 (0.065)   Loss 0.8228 (0.9079)   Prec@1 68.000 (68.329)   Prec@5 97.000 (97.100)   [2025-10-24 09:03:12]
  Epoch: [025][400/500]   Time 0.017 (0.066)   Data 0.000 (0.049)   Loss 0.8783 (0.9032)   Prec@1 70.000 (68.486)   Prec@5 98.000 (97.112)   [2025-10-24 09:03:14]
  **Train** Prec@1 68.528 Prec@5 97.080 Error@1 31.472
  **Test** Prec@1 75.170 Prec@5 98.390 Error@1 24.830
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:03:38] [Epoch=026/040] [Need: 00:14:01] [LR=0.0010] [Best : Accuracy=75.17, Error=24.83]
  Epoch: [026][000/500]   Time 20.895 (20.895)   Data 20.829 (20.829)   Loss 1.1414 (1.1414)   Prec@1 65.000 (65.000)   Prec@5 96.000 (96.000)   [2025-10-24 09:03:58]
  Epoch: [026][100/500]   Time 0.017 (0.226)   Data 0.000 (0.206)   Loss 0.7793 (0.9006)   Prec@1 72.000 (69.010)   Prec@5 100.000 (97.366)   [2025-10-24 09:04:00]
  Epoch: [026][200/500]   Time 0.019 (0.122)   Data 0.000 (0.104)   Loss 0.8592 (0.8884)   Prec@1 72.000 (69.269)   Prec@5 97.000 (97.234)   [2025-10-24 09:04:02]
  Epoch: [026][300/500]   Time 0.018 (0.088)   Data 0.000 (0.069)   Loss 1.0719 (0.8905)   Prec@1 63.000 (69.236)   Prec@5 97.000 (97.206)   [2025-10-24 09:04:04]
  Epoch: [026][400/500]   Time 0.018 (0.070)   Data 0.000 (0.052)   Loss 0.9207 (0.8833)   Prec@1 68.000 (69.519)   Prec@5 97.000 (97.284)   [2025-10-24 09:04:06]
  **Train** Prec@1 69.540 Prec@5 97.352 Error@1 30.460
  **Test** Prec@1 75.570 Prec@5 98.410 Error@1 24.430
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:04:30] [Epoch=027/040] [Need: 00:12:57] [LR=0.0010] [Best : Accuracy=75.57, Error=24.43]
  Epoch: [027][000/500]   Time 20.185 (20.185)   Data 20.120 (20.120)   Loss 0.7946 (0.7946)   Prec@1 69.000 (69.000)   Prec@5 96.000 (96.000)   [2025-10-24 09:04:50]
  Epoch: [027][100/500]   Time 0.016 (0.219)   Data 0.000 (0.199)   Loss 0.8562 (0.8751)   Prec@1 72.000 (69.990)   Prec@5 96.000 (97.416)   [2025-10-24 09:04:52]
  Epoch: [027][200/500]   Time 0.016 (0.119)   Data 0.001 (0.100)   Loss 0.7141 (0.8766)   Prec@1 78.000 (69.851)   Prec@5 98.000 (97.224)   [2025-10-24 09:04:54]
  Epoch: [027][300/500]   Time 0.019 (0.085)   Data 0.000 (0.067)   Loss 0.7323 (0.8773)   Prec@1 75.000 (69.847)   Prec@5 98.000 (97.213)   [2025-10-24 09:04:56]
  Epoch: [027][400/500]   Time 0.023 (0.068)   Data 0.000 (0.050)   Loss 0.9688 (0.8772)   Prec@1 65.000 (69.858)   Prec@5 94.000 (97.239)   [2025-10-24 09:04:58]
  **Train** Prec@1 69.838 Prec@5 97.292 Error@1 30.162
  **Test** Prec@1 75.190 Prec@5 98.430 Error@1 24.810

==>>[2025-10-24 09:05:21] [Epoch=028/040] [Need: 00:11:53] [LR=0.0010] [Best : Accuracy=75.57, Error=24.43]
  Epoch: [028][000/500]   Time 21.235 (21.235)   Data 21.168 (21.168)   Loss 0.8418 (0.8418)   Prec@1 70.000 (70.000)   Prec@5 99.000 (99.000)   [2025-10-24 09:05:43]
  Epoch: [028][100/500]   Time 0.018 (0.229)   Data 0.001 (0.210)   Loss 1.0134 (0.8763)   Prec@1 67.000 (69.851)   Prec@5 97.000 (97.426)   [2025-10-24 09:05:45]
  Epoch: [028][200/500]   Time 0.017 (0.124)   Data 0.000 (0.106)   Loss 0.8281 (0.8602)   Prec@1 68.000 (70.413)   Prec@5 99.000 (97.542)   [2025-10-24 09:05:46]
  Epoch: [028][300/500]   Time 0.019 (0.089)   Data 0.000 (0.071)   Loss 0.9142 (0.8654)   Prec@1 67.000 (70.246)   Prec@5 99.000 (97.482)   [2025-10-24 09:05:48]
  Epoch: [028][400/500]   Time 0.017 (0.071)   Data 0.000 (0.053)   Loss 0.8987 (0.8627)   Prec@1 68.000 (70.279)   Prec@5 100.000 (97.536)   [2025-10-24 09:05:50]
  **Train** Prec@1 70.232 Prec@5 97.466 Error@1 29.768
  **Test** Prec@1 75.980 Prec@5 98.510 Error@1 24.020
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:06:14] [Epoch=029/040] [Need: 00:10:51] [LR=0.0010] [Best : Accuracy=75.98, Error=24.02]
  Epoch: [029][000/500]   Time 20.420 (20.420)   Data 20.354 (20.354)   Loss 0.7327 (0.7327)   Prec@1 75.000 (75.000)   Prec@5 99.000 (99.000)   [2025-10-24 09:06:35]
  Epoch: [029][100/500]   Time 0.016 (0.221)   Data 0.000 (0.202)   Loss 0.8086 (0.8410)   Prec@1 77.000 (70.554)   Prec@5 98.000 (97.604)   [2025-10-24 09:06:37]
  Epoch: [029][200/500]   Time 0.018 (0.120)   Data 0.000 (0.101)   Loss 1.0260 (0.8500)   Prec@1 63.000 (70.468)   Prec@5 97.000 (97.443)   [2025-10-24 09:06:38]
  Epoch: [029][300/500]   Time 0.019 (0.086)   Data 0.000 (0.068)   Loss 0.7042 (0.8550)   Prec@1 79.000 (70.505)   Prec@5 100.000 (97.505)   [2025-10-24 09:06:40]
  Epoch: [029][400/500]   Time 0.017 (0.069)   Data 0.000 (0.051)   Loss 0.6605 (0.8574)   Prec@1 77.000 (70.392)   Prec@5 99.000 (97.441)   [2025-10-24 09:06:42]
  **Train** Prec@1 70.334 Prec@5 97.458 Error@1 29.666
  **Test** Prec@1 75.550 Prec@5 98.400 Error@1 24.450

==>>[2025-10-24 09:07:07] [Epoch=030/040] [Need: 00:09:50] [LR=0.0010] [Best : Accuracy=75.98, Error=24.02]
  Epoch: [030][000/500]   Time 20.458 (20.458)   Data 20.391 (20.391)   Loss 0.6624 (0.6624)   Prec@1 76.000 (76.000)   Prec@5 98.000 (98.000)   [2025-10-24 09:07:27]
  Epoch: [030][100/500]   Time 0.015 (0.221)   Data 0.000 (0.202)   Loss 1.0438 (0.8368)   Prec@1 65.000 (71.436)   Prec@5 97.000 (97.277)   [2025-10-24 09:07:29]
  Epoch: [030][200/500]   Time 0.017 (0.120)   Data 0.000 (0.102)   Loss 0.8040 (0.8477)   Prec@1 77.000 (70.861)   Prec@5 98.000 (97.408)   [2025-10-24 09:07:31]
  Epoch: [030][300/500]   Time 0.018 (0.086)   Data 0.000 (0.068)   Loss 0.7375 (0.8482)   Prec@1 77.000 (70.774)   Prec@5 98.000 (97.402)   [2025-10-24 09:07:33]
  Epoch: [030][400/500]   Time 0.018 (0.069)   Data 0.000 (0.051)   Loss 0.9297 (0.8512)   Prec@1 69.000 (70.666)   Prec@5 97.000 (97.382)   [2025-10-24 09:07:34]
  **Train** Prec@1 70.612 Prec@5 97.372 Error@1 29.388
  **Test** Prec@1 76.160 Prec@5 98.450 Error@1 23.840
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:07:58] [Epoch=031/040] [Need: 00:08:49] [LR=0.0010] [Best : Accuracy=76.16, Error=23.84]
  Epoch: [031][000/500]   Time 19.858 (19.858)   Data 19.792 (19.792)   Loss 0.8757 (0.8757)   Prec@1 68.000 (68.000)   Prec@5 97.000 (97.000)   [2025-10-24 09:08:18]
  Epoch: [031][100/500]   Time 0.017 (0.216)   Data 0.000 (0.196)   Loss 0.6616 (0.8473)   Prec@1 75.000 (70.762)   Prec@5 100.000 (97.356)   [2025-10-24 09:08:20]
  Epoch: [031][200/500]   Time 0.018 (0.117)   Data 0.000 (0.099)   Loss 1.2136 (0.8497)   Prec@1 54.000 (70.627)   Prec@5 93.000 (97.398)   [2025-10-24 09:08:22]
  Epoch: [031][300/500]   Time 0.019 (0.084)   Data 0.000 (0.066)   Loss 0.7758 (0.8515)   Prec@1 76.000 (70.608)   Prec@5 100.000 (97.452)   [2025-10-24 09:08:24]
  Epoch: [031][400/500]   Time 0.018 (0.067)   Data 0.000 (0.050)   Loss 1.0149 (0.8509)   Prec@1 71.000 (70.569)   Prec@5 94.000 (97.449)   [2025-10-24 09:08:25]
  **Train** Prec@1 70.550 Prec@5 97.474 Error@1 29.450
  **Test** Prec@1 76.490 Prec@5 98.570 Error@1 23.510
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:08:49] [Epoch=032/040] [Need: 00:07:48] [LR=0.0010] [Best : Accuracy=76.49, Error=23.51]
  Epoch: [032][000/500]   Time 20.901 (20.901)   Data 20.834 (20.834)   Loss 0.8762 (0.8762)   Prec@1 70.000 (70.000)   Prec@5 99.000 (99.000)   [2025-10-24 09:09:10]
  Epoch: [032][100/500]   Time 0.018 (0.227)   Data 0.000 (0.207)   Loss 0.6366 (0.8582)   Prec@1 80.000 (70.733)   Prec@5 99.000 (97.139)   [2025-10-24 09:09:12]
  Epoch: [032][200/500]   Time 0.017 (0.123)   Data 0.000 (0.104)   Loss 0.7969 (0.8579)   Prec@1 75.000 (70.313)   Prec@5 99.000 (97.383)   [2025-10-24 09:09:14]
  Epoch: [032][300/500]   Time 0.023 (0.088)   Data 0.000 (0.069)   Loss 0.9684 (0.8552)   Prec@1 66.000 (70.409)   Prec@5 97.000 (97.462)   [2025-10-24 09:09:16]
  Epoch: [032][400/500]   Time 0.020 (0.071)   Data 0.000 (0.052)   Loss 1.0956 (0.8497)   Prec@1 57.000 (70.571)   Prec@5 96.000 (97.516)   [2025-10-24 09:09:18]
  **Train** Prec@1 70.724 Prec@5 97.520 Error@1 29.276
  **Test** Prec@1 76.610 Prec@5 98.570 Error@1 23.390
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:09:42] [Epoch=033/040] [Need: 00:06:48] [LR=0.0010] [Best : Accuracy=76.61, Error=23.39]
  Epoch: [033][000/500]   Time 20.329 (20.329)   Data 20.264 (20.264)   Loss 1.0590 (1.0590)   Prec@1 63.000 (63.000)   Prec@5 97.000 (97.000)   [2025-10-24 09:10:02]
  Epoch: [033][100/500]   Time 0.017 (0.219)   Data 0.000 (0.201)   Loss 0.8037 (0.8589)   Prec@1 76.000 (70.436)   Prec@5 98.000 (97.426)   [2025-10-24 09:10:04]
  Epoch: [033][200/500]   Time 0.016 (0.118)   Data 0.000 (0.101)   Loss 0.9213 (0.8488)   Prec@1 63.000 (70.642)   Prec@5 99.000 (97.443)   [2025-10-24 09:10:05]
  Epoch: [033][300/500]   Time 0.017 (0.085)   Data 0.000 (0.068)   Loss 0.8982 (0.8500)   Prec@1 73.000 (70.465)   Prec@5 94.000 (97.545)   [2025-10-24 09:10:07]
  Epoch: [033][400/500]   Time 0.019 (0.068)   Data 0.000 (0.051)   Loss 0.8491 (0.8453)   Prec@1 72.000 (70.726)   Prec@5 98.000 (97.541)   [2025-10-24 09:10:09]
  **Train** Prec@1 70.628 Prec@5 97.510 Error@1 29.372
  **Test** Prec@1 76.850 Prec@5 98.540 Error@1 23.150
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:10:33] [Epoch=034/040] [Need: 00:05:48] [LR=0.0010] [Best : Accuracy=76.85, Error=23.15]
  Epoch: [034][000/500]   Time 20.403 (20.403)   Data 20.338 (20.338)   Loss 0.7539 (0.7539)   Prec@1 74.000 (74.000)   Prec@5 99.000 (99.000)   [2025-10-24 09:10:53]
  Epoch: [034][100/500]   Time 0.018 (0.221)   Data 0.000 (0.202)   Loss 1.0838 (0.8392)   Prec@1 62.000 (71.535)   Prec@5 97.000 (97.436)   [2025-10-24 09:10:55]
  Epoch: [034][200/500]   Time 0.016 (0.120)   Data 0.001 (0.101)   Loss 0.8029 (0.8455)   Prec@1 71.000 (71.159)   Prec@5 98.000 (97.547)   [2025-10-24 09:10:57]
  Epoch: [034][300/500]   Time 0.019 (0.086)   Data 0.002 (0.068)   Loss 0.7526 (0.8447)   Prec@1 69.000 (70.867)   Prec@5 100.000 (97.658)   [2025-10-24 09:10:59]
  Epoch: [034][400/500]   Time 0.018 (0.069)   Data 0.000 (0.051)   Loss 0.7145 (0.8471)   Prec@1 78.000 (70.825)   Prec@5 98.000 (97.599)   [2025-10-24 09:11:01]
  **Train** Prec@1 70.854 Prec@5 97.630 Error@1 29.146
  **Test** Prec@1 76.960 Prec@5 98.600 Error@1 23.040
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:11:23] [Epoch=035/040] [Need: 00:04:49] [LR=0.0010] [Best : Accuracy=76.96, Error=23.04]
  Epoch: [035][000/500]   Time 19.490 (19.490)   Data 19.425 (19.425)   Loss 1.0423 (1.0423)   Prec@1 67.000 (67.000)   Prec@5 93.000 (93.000)   [2025-10-24 09:11:43]
  Epoch: [035][100/500]   Time 0.016 (0.212)   Data 0.000 (0.192)   Loss 0.9111 (0.8327)   Prec@1 70.000 (71.040)   Prec@5 98.000 (97.554)   [2025-10-24 09:11:45]
  Epoch: [035][200/500]   Time 0.017 (0.115)   Data 0.000 (0.097)   Loss 0.7741 (0.8276)   Prec@1 74.000 (71.159)   Prec@5 98.000 (97.692)   [2025-10-24 09:11:46]
  Epoch: [035][300/500]   Time 0.017 (0.082)   Data 0.000 (0.065)   Loss 0.7980 (0.8291)   Prec@1 72.000 (71.056)   Prec@5 98.000 (97.615)   [2025-10-24 09:11:48]
  Epoch: [035][400/500]   Time 0.018 (0.066)   Data 0.001 (0.049)   Loss 0.8714 (0.8300)   Prec@1 74.000 (71.020)   Prec@5 97.000 (97.623)   [2025-10-24 09:11:50]
  **Train** Prec@1 70.880 Prec@5 97.594 Error@1 29.120
  **Test** Prec@1 76.550 Prec@5 98.530 Error@1 23.450

==>>[2025-10-24 09:12:12] [Epoch=036/040] [Need: 00:03:50] [LR=0.0010] [Best : Accuracy=76.96, Error=23.04]
  Epoch: [036][000/500]   Time 20.006 (20.006)   Data 19.939 (19.939)   Loss 0.8349 (0.8349)   Prec@1 75.000 (75.000)   Prec@5 96.000 (96.000)   [2025-10-24 09:12:32]
  Epoch: [036][100/500]   Time 0.017 (0.217)   Data 0.000 (0.198)   Loss 0.8792 (0.8565)   Prec@1 66.000 (70.188)   Prec@5 99.000 (97.525)   [2025-10-24 09:12:34]
  Epoch: [036][200/500]   Time 0.018 (0.117)   Data 0.001 (0.099)   Loss 0.7916 (0.8400)   Prec@1 73.000 (71.090)   Prec@5 98.000 (97.597)   [2025-10-24 09:12:36]
  Epoch: [036][300/500]   Time 0.016 (0.084)   Data 0.000 (0.066)   Loss 0.7619 (0.8407)   Prec@1 74.000 (71.030)   Prec@5 98.000 (97.568)   [2025-10-24 09:12:37]
  Epoch: [036][400/500]   Time 0.019 (0.068)   Data 0.000 (0.050)   Loss 0.8159 (0.8397)   Prec@1 76.000 (71.140)   Prec@5 97.000 (97.564)   [2025-10-24 09:12:39]
  **Train** Prec@1 71.230 Prec@5 97.594 Error@1 28.770
  **Test** Prec@1 76.220 Prec@5 98.410 Error@1 23.780

==>>[2025-10-24 09:13:01] [Epoch=037/040] [Need: 00:02:52] [LR=0.0010] [Best : Accuracy=76.96, Error=23.04]
  Epoch: [037][000/500]   Time 20.445 (20.445)   Data 20.380 (20.380)   Loss 0.7464 (0.7464)   Prec@1 74.000 (74.000)   Prec@5 98.000 (98.000)   [2025-10-24 09:13:22]
  Epoch: [037][100/500]   Time 0.018 (0.222)   Data 0.001 (0.202)   Loss 0.8213 (0.8244)   Prec@1 73.000 (71.584)   Prec@5 98.000 (97.644)   [2025-10-24 09:13:24]
  Epoch: [037][200/500]   Time 0.019 (0.120)   Data 0.000 (0.102)   Loss 0.8537 (0.8346)   Prec@1 67.000 (71.378)   Prec@5 97.000 (97.502)   [2025-10-24 09:13:26]
  Epoch: [037][300/500]   Time 0.022 (0.086)   Data 0.000 (0.068)   Loss 0.8661 (0.8348)   Prec@1 66.000 (71.332)   Prec@5 99.000 (97.575)   [2025-10-24 09:13:27]
  Epoch: [037][400/500]   Time 0.018 (0.069)   Data 0.000 (0.051)   Loss 0.7835 (0.8311)   Prec@1 76.000 (71.506)   Prec@5 97.000 (97.596)   [2025-10-24 09:13:29]
  **Train** Prec@1 71.324 Prec@5 97.532 Error@1 28.676
  **Test** Prec@1 77.320 Prec@5 98.570 Error@1 22.680
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:13:52] [Epoch=038/040] [Need: 00:01:54] [LR=0.0010] [Best : Accuracy=77.32, Error=22.68]
  Epoch: [038][000/500]   Time 19.694 (19.694)   Data 19.629 (19.629)   Loss 0.9967 (0.9967)   Prec@1 65.000 (65.000)   Prec@5 97.000 (97.000)   [2025-10-24 09:14:12]
  Epoch: [038][100/500]   Time 0.019 (0.214)   Data 0.001 (0.195)   Loss 0.7297 (0.8466)   Prec@1 76.000 (71.089)   Prec@5 97.000 (97.495)   [2025-10-24 09:14:14]
  Epoch: [038][200/500]   Time 0.019 (0.116)   Data 0.001 (0.098)   Loss 0.9155 (0.8365)   Prec@1 70.000 (71.179)   Prec@5 96.000 (97.502)   [2025-10-24 09:14:16]
  Epoch: [038][300/500]   Time 0.018 (0.083)   Data 0.001 (0.065)   Loss 0.9889 (0.8301)   Prec@1 65.000 (71.468)   Prec@5 98.000 (97.532)   [2025-10-24 09:14:17]
  Epoch: [038][400/500]   Time 0.017 (0.067)   Data 0.000 (0.049)   Loss 0.8039 (0.8320)   Prec@1 74.000 (71.372)   Prec@5 97.000 (97.551)   [2025-10-24 09:14:19]
  **Train** Prec@1 71.508 Prec@5 97.594 Error@1 28.492
  **Test** Prec@1 76.930 Prec@5 98.590 Error@1 23.070

==>>[2025-10-24 09:14:42] [Epoch=039/040] [Need: 00:00:57] [LR=0.0010] [Best : Accuracy=77.32, Error=22.68]
  Epoch: [039][000/500]   Time 20.520 (20.520)   Data 20.457 (20.457)   Loss 0.7836 (0.7836)   Prec@1 69.000 (69.000)   Prec@5 97.000 (97.000)   [2025-10-24 09:15:02]
  Epoch: [039][100/500]   Time 0.016 (0.221)   Data 0.000 (0.203)   Loss 0.9498 (0.8098)   Prec@1 67.000 (72.050)   Prec@5 98.000 (97.832)   [2025-10-24 09:15:04]
  Epoch: [039][200/500]   Time 0.018 (0.120)   Data 0.000 (0.102)   Loss 0.7171 (0.8277)   Prec@1 75.000 (71.478)   Prec@5 99.000 (97.652)   [2025-10-24 09:15:06]
  Epoch: [039][300/500]   Time 0.016 (0.086)   Data 0.000 (0.068)   Loss 0.8945 (0.8215)   Prec@1 66.000 (71.694)   Prec@5 96.000 (97.681)   [2025-10-24 09:15:07]
  Epoch: [039][400/500]   Time 0.016 (0.069)   Data 0.000 (0.051)   Loss 0.7306 (0.8242)   Prec@1 74.000 (71.653)   Prec@5 99.000 (97.688)   [2025-10-24 09:15:09]
  **Train** Prec@1 71.532 Prec@5 97.678 Error@1 28.468
  **Test** Prec@1 77.220 Prec@5 98.600 Error@1 22.780
