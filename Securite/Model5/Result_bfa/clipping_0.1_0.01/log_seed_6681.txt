save path : ./save/tinyvgg_quan/clipping_0.1_0.01
{'data_path': './dataset', 'arch': 'tinyvgg_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': False, 'clipping_coeff': 0.1, 'learning_rate': 0.01, 'manualSeed': 6681, 'save_path': './save/tinyvgg_quan/clipping_0.1_0.01', 'enable_bfa': False, 'resume': '', 'quan_bitwidth': None, 'reset_weight': False, 'evaluate': False, 'n_iter': 30, 'model_only': False, 'random_bfa': False, 'use_cuda': True}
Random Seed: 6681
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'tinyvgg_quan'
=> network :
 TinyVGG(
  (features): Sequential(
    (0): quan_Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Dropout2d(p=0.3, inplace=False)
    (6): quan_Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace=True)
    (8): quan_Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace=True)
    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (11): Dropout2d(p=0.3, inplace=False)
    (12): quan_Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): quan_Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Dropout2d(p=0.3, inplace=False)
  )
  (classifier): Sequential(
    (0): quan_Linear(in_features=2048, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): quan_Linear(in_features=128, out_features=10, bias=True)
  )
)
=> do not use any checkpoint for tinyvgg_quan model

==>>[2025-10-24 09:15:47] [Epoch=000/040] [Need: 00:00:00] [LR=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/500]   Time 20.310 (20.310)   Data 19.849 (19.849)   Loss 2.3067 (2.3067)   Prec@1 7.000 (7.000)   Prec@5 44.000 (44.000)   [2025-10-24 09:16:07]
  Epoch: [000][100/500]   Time 0.016 (0.218)   Data 0.000 (0.197)   Loss 2.2995 (2.3029)   Prec@1 13.000 (10.158)   Prec@5 56.000 (50.208)   [2025-10-24 09:16:09]
  Epoch: [000][200/500]   Time 0.016 (0.118)   Data 0.000 (0.099)   Loss 2.3008 (2.3028)   Prec@1 14.000 (10.045)   Prec@5 57.000 (50.368)   [2025-10-24 09:16:11]
  Epoch: [000][300/500]   Time 0.016 (0.085)   Data 0.000 (0.066)   Loss 2.2883 (2.3014)   Prec@1 12.000 (10.628)   Prec@5 62.000 (51.811)   [2025-10-24 09:16:13]
  Epoch: [000][400/500]   Time 0.022 (0.068)   Data 0.001 (0.050)   Loss 2.0805 (2.2793)   Prec@1 20.000 (12.499)   Prec@5 75.000 (55.796)   [2025-10-24 09:16:14]
  **Train** Prec@1 14.480 Prec@5 59.594 Error@1 85.520
  **Test** Prec@1 27.350 Prec@5 77.570 Error@1 72.650
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:16:39] [Epoch=001/040] [Need: 00:33:30] [LR=0.0100] [Best : Accuracy=27.35, Error=72.65]
  Epoch: [001][000/500]   Time 20.128 (20.128)   Data 20.061 (20.061)   Loss 2.0763 (2.0763)   Prec@1 19.000 (19.000)   Prec@5 79.000 (79.000)   [2025-10-24 09:16:59]
  Epoch: [001][100/500]   Time 0.018 (0.218)   Data 0.000 (0.199)   Loss 2.1233 (2.0353)   Prec@1 26.000 (24.238)   Prec@5 67.000 (77.634)   [2025-10-24 09:17:01]
  Epoch: [001][200/500]   Time 0.018 (0.118)   Data 0.000 (0.100)   Loss 1.8956 (2.0147)   Prec@1 31.000 (25.095)   Prec@5 85.000 (79.035)   [2025-10-24 09:17:02]
  Epoch: [001][300/500]   Time 0.018 (0.085)   Data 0.000 (0.067)   Loss 1.9581 (1.9907)   Prec@1 25.000 (25.757)   Prec@5 82.000 (79.801)   [2025-10-24 09:17:04]
  Epoch: [001][400/500]   Time 0.016 (0.068)   Data 0.000 (0.050)   Loss 1.7259 (1.9732)   Prec@1 37.000 (26.072)   Prec@5 89.000 (80.506)   [2025-10-24 09:17:06]
  **Train** Prec@1 26.734 Prec@5 81.118 Error@1 73.266
  **Test** Prec@1 33.020 Prec@5 86.930 Error@1 66.980
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:17:29] [Epoch=002/040] [Need: 00:32:17] [LR=0.0100] [Best : Accuracy=33.02, Error=66.98]
  Epoch: [002][000/500]   Time 20.823 (20.823)   Data 20.757 (20.757)   Loss 1.9582 (1.9582)   Prec@1 33.000 (33.000)   Prec@5 82.000 (82.000)   [2025-10-24 09:17:50]
  Epoch: [002][100/500]   Time 0.016 (0.225)   Data 0.000 (0.206)   Loss 1.7896 (1.8718)   Prec@1 35.000 (29.327)   Prec@5 86.000 (83.980)   [2025-10-24 09:17:52]
  Epoch: [002][200/500]   Time 0.017 (0.122)   Data 0.000 (0.103)   Loss 1.8166 (1.8651)   Prec@1 36.000 (29.726)   Prec@5 86.000 (84.274)   [2025-10-24 09:17:54]
  Epoch: [002][300/500]   Time 0.020 (0.088)   Data 0.000 (0.069)   Loss 1.8042 (1.8569)   Prec@1 35.000 (29.874)   Prec@5 84.000 (84.502)   [2025-10-24 09:17:56]
  Epoch: [002][400/500]   Time 0.018 (0.071)   Data 0.000 (0.052)   Loss 1.9334 (1.8475)   Prec@1 27.000 (30.424)   Prec@5 86.000 (84.870)   [2025-10-24 09:17:57]
  **Train** Prec@1 30.566 Prec@5 85.004 Error@1 69.434
  **Test** Prec@1 36.320 Prec@5 89.240 Error@1 63.680
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:18:24] [Epoch=003/040] [Need: 00:32:09] [LR=0.0100] [Best : Accuracy=36.32, Error=63.68]
  Epoch: [003][000/500]   Time 20.996 (20.996)   Data 20.930 (20.930)   Loss 1.8270 (1.8270)   Prec@1 33.000 (33.000)   Prec@5 86.000 (86.000)   [2025-10-24 09:18:45]
  Epoch: [003][100/500]   Time 0.016 (0.226)   Data 0.000 (0.207)   Loss 1.6792 (1.7764)   Prec@1 36.000 (33.099)   Prec@5 87.000 (86.851)   [2025-10-24 09:18:46]
  Epoch: [003][200/500]   Time 0.018 (0.122)   Data 0.000 (0.104)   Loss 1.8811 (1.7749)   Prec@1 31.000 (33.294)   Prec@5 81.000 (86.925)   [2025-10-24 09:18:48]
  Epoch: [003][300/500]   Time 0.019 (0.088)   Data 0.000 (0.070)   Loss 1.7213 (1.7709)   Prec@1 33.000 (33.694)   Prec@5 87.000 (86.764)   [2025-10-24 09:18:50]
  Epoch: [003][400/500]   Time 0.020 (0.071)   Data 0.000 (0.052)   Loss 1.6551 (1.7624)   Prec@1 41.000 (34.085)   Prec@5 90.000 (87.042)   [2025-10-24 09:18:52]
  **Train** Prec@1 34.416 Prec@5 87.264 Error@1 65.584
  **Test** Prec@1 41.080 Prec@5 91.610 Error@1 58.920
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:19:17] [Epoch=004/040] [Need: 00:31:24] [LR=0.0100] [Best : Accuracy=41.08, Error=58.92]
  Epoch: [004][000/500]   Time 19.914 (19.914)   Data 19.849 (19.849)   Loss 1.7505 (1.7505)   Prec@1 34.000 (34.000)   Prec@5 92.000 (92.000)   [2025-10-24 09:19:37]
  Epoch: [004][100/500]   Time 0.019 (0.215)   Data 0.000 (0.197)   Loss 1.7009 (1.7033)   Prec@1 38.000 (35.901)   Prec@5 85.000 (88.337)   [2025-10-24 09:19:38]
  Epoch: [004][200/500]   Time 0.016 (0.117)   Data 0.000 (0.099)   Loss 1.5358 (1.6896)   Prec@1 35.000 (37.045)   Prec@5 92.000 (88.522)   [2025-10-24 09:19:40]
  Epoch: [004][300/500]   Time 0.018 (0.084)   Data 0.000 (0.066)   Loss 1.7119 (1.6808)   Prec@1 41.000 (37.575)   Prec@5 89.000 (88.615)   [2025-10-24 09:19:42]
  Epoch: [004][400/500]   Time 0.017 (0.067)   Data 0.000 (0.050)   Loss 1.5858 (1.6764)   Prec@1 40.000 (37.908)   Prec@5 93.000 (88.793)   [2025-10-24 09:19:44]
  **Train** Prec@1 38.196 Prec@5 89.012 Error@1 61.804
  **Test** Prec@1 45.980 Prec@5 93.070 Error@1 54.020
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:20:07] [Epoch=005/040] [Need: 00:30:15] [LR=0.0100] [Best : Accuracy=45.98, Error=54.02]
  Epoch: [005][000/500]   Time 19.880 (19.880)   Data 19.816 (19.816)   Loss 1.7274 (1.7274)   Prec@1 32.000 (32.000)   Prec@5 86.000 (86.000)   [2025-10-24 09:20:26]
  Epoch: [005][100/500]   Time 0.015 (0.214)   Data 0.000 (0.196)   Loss 1.5658 (1.6211)   Prec@1 39.000 (39.663)   Prec@5 92.000 (90.366)   [2025-10-24 09:20:28]
  Epoch: [005][200/500]   Time 0.015 (0.116)   Data 0.001 (0.099)   Loss 1.5523 (1.6086)   Prec@1 42.000 (40.507)   Prec@5 91.000 (90.204)   [2025-10-24 09:20:30]
  Epoch: [005][300/500]   Time 0.020 (0.083)   Data 0.000 (0.066)   Loss 1.6218 (1.6037)   Prec@1 38.000 (40.751)   Prec@5 91.000 (90.223)   [2025-10-24 09:20:32]
  Epoch: [005][400/500]   Time 0.016 (0.067)   Data 0.000 (0.050)   Loss 1.5347 (1.5975)   Prec@1 37.000 (41.035)   Prec@5 92.000 (90.329)   [2025-10-24 09:20:33]
  **Train** Prec@1 41.272 Prec@5 90.408 Error@1 58.728
  **Test** Prec@1 48.570 Prec@5 93.640 Error@1 51.430
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:20:58] [Epoch=006/040] [Need: 00:29:19] [LR=0.0100] [Best : Accuracy=48.57, Error=51.43]
  Epoch: [006][000/500]   Time 20.312 (20.312)   Data 20.248 (20.248)   Loss 1.4936 (1.4936)   Prec@1 44.000 (44.000)   Prec@5 90.000 (90.000)   [2025-10-24 09:21:18]
  Epoch: [006][100/500]   Time 0.016 (0.219)   Data 0.000 (0.201)   Loss 1.4686 (1.5615)   Prec@1 50.000 (41.950)   Prec@5 90.000 (90.832)   [2025-10-24 09:21:20]
  Epoch: [006][200/500]   Time 0.018 (0.118)   Data 0.000 (0.101)   Loss 1.5788 (1.5481)   Prec@1 44.000 (42.771)   Prec@5 91.000 (91.159)   [2025-10-24 09:21:21]
  Epoch: [006][300/500]   Time 0.017 (0.085)   Data 0.000 (0.067)   Loss 1.5607 (1.5488)   Prec@1 41.000 (43.076)   Prec@5 96.000 (91.183)   [2025-10-24 09:21:23]
  Epoch: [006][400/500]   Time 0.019 (0.068)   Data 0.000 (0.051)   Loss 1.4582 (1.5446)   Prec@1 49.000 (43.312)   Prec@5 91.000 (91.182)   [2025-10-24 09:21:25]
  **Train** Prec@1 43.602 Prec@5 91.272 Error@1 56.398
  **Test** Prec@1 50.750 Prec@5 94.590 Error@1 49.250
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:21:48] [Epoch=007/040] [Need: 00:28:20] [LR=0.0100] [Best : Accuracy=50.75, Error=49.25]
  Epoch: [007][000/500]   Time 19.858 (19.858)   Data 19.791 (19.791)   Loss 1.5287 (1.5287)   Prec@1 40.000 (40.000)   Prec@5 92.000 (92.000)   [2025-10-24 09:22:08]
  Epoch: [007][100/500]   Time 0.019 (0.217)   Data 0.000 (0.196)   Loss 1.5342 (1.4986)   Prec@1 42.000 (44.713)   Prec@5 94.000 (91.614)   [2025-10-24 09:22:10]
  Epoch: [007][200/500]   Time 0.025 (0.118)   Data 0.001 (0.099)   Loss 1.3838 (1.4901)   Prec@1 47.000 (45.159)   Prec@5 95.000 (91.746)   [2025-10-24 09:22:12]
  Epoch: [007][300/500]   Time 0.018 (0.085)   Data 0.001 (0.066)   Loss 1.4772 (1.4907)   Prec@1 46.000 (45.096)   Prec@5 96.000 (91.678)   [2025-10-24 09:22:14]
  Epoch: [007][400/500]   Time 0.017 (0.068)   Data 0.000 (0.050)   Loss 1.4470 (1.4888)   Prec@1 46.000 (45.377)   Prec@5 95.000 (91.800)   [2025-10-24 09:22:15]
  **Train** Prec@1 45.484 Prec@5 91.840 Error@1 54.516
  **Test** Prec@1 53.100 Prec@5 94.960 Error@1 46.900
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:22:38] [Epoch=008/040] [Need: 00:27:23] [LR=0.0100] [Best : Accuracy=53.10, Error=46.90]
  Epoch: [008][000/500]   Time 21.257 (21.257)   Data 21.190 (21.190)   Loss 1.5558 (1.5558)   Prec@1 45.000 (45.000)   Prec@5 90.000 (90.000)   [2025-10-24 09:22:59]
  Epoch: [008][100/500]   Time 0.016 (0.230)   Data 0.001 (0.210)   Loss 1.4320 (1.4605)   Prec@1 50.000 (46.752)   Prec@5 93.000 (91.762)   [2025-10-24 09:23:01]
  Epoch: [008][200/500]   Time 0.018 (0.125)   Data 0.000 (0.106)   Loss 1.2820 (1.4553)   Prec@1 55.000 (46.910)   Prec@5 92.000 (92.020)   [2025-10-24 09:23:03]
  Epoch: [008][300/500]   Time 0.019 (0.089)   Data 0.000 (0.071)   Loss 1.4825 (1.4471)   Prec@1 43.000 (47.179)   Prec@5 97.000 (92.229)   [2025-10-24 09:23:05]
  Epoch: [008][400/500]   Time 0.038 (0.075)   Data 0.001 (0.053)   Loss 1.6134 (1.4486)   Prec@1 49.000 (47.289)   Prec@5 86.000 (92.252)   [2025-10-24 09:23:08]
  **Train** Prec@1 47.368 Prec@5 92.378 Error@1 52.632
  **Test** Prec@1 54.810 Prec@5 95.390 Error@1 45.190
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:23:37] [Epoch=009/040] [Need: 00:26:57] [LR=0.0100] [Best : Accuracy=54.81, Error=45.19]
  Epoch: [009][000/500]   Time 27.119 (27.119)   Data 27.032 (27.032)   Loss 1.3333 (1.3333)   Prec@1 53.000 (53.000)   Prec@5 91.000 (91.000)   [2025-10-24 09:24:04]
  Epoch: [009][100/500]   Time 0.029 (0.299)   Data 0.002 (0.268)   Loss 1.3681 (1.4136)   Prec@1 55.000 (48.188)   Prec@5 93.000 (92.931)   [2025-10-24 09:24:07]
  Epoch: [009][200/500]   Time 0.037 (0.166)   Data 0.001 (0.135)   Loss 1.4275 (1.4138)   Prec@1 47.000 (48.403)   Prec@5 95.000 (92.706)   [2025-10-24 09:24:11]
  Epoch: [009][300/500]   Time 0.039 (0.122)   Data 0.000 (0.090)   Loss 1.4769 (1.4014)   Prec@1 45.000 (48.850)   Prec@5 96.000 (93.017)   [2025-10-24 09:24:14]
  Epoch: [009][400/500]   Time 0.029 (0.099)   Data 0.001 (0.068)   Loss 1.2754 (1.3998)   Prec@1 55.000 (48.943)   Prec@5 96.000 (92.998)   [2025-10-24 09:24:17]
  **Train** Prec@1 49.384 Prec@5 93.054 Error@1 50.616
  **Test** Prec@1 57.630 Prec@5 95.440 Error@1 42.370
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:24:48] [Epoch=010/040] [Need: 00:27:02] [LR=0.0100] [Best : Accuracy=57.63, Error=42.37]
  Epoch: [010][000/500]   Time 27.699 (27.699)   Data 27.621 (27.621)   Loss 1.3421 (1.3421)   Prec@1 56.000 (56.000)   Prec@5 94.000 (94.000)   [2025-10-24 09:25:16]
  Epoch: [010][100/500]   Time 0.030 (0.303)   Data 0.000 (0.274)   Loss 1.5343 (1.3546)   Prec@1 41.000 (50.366)   Prec@5 90.000 (93.485)   [2025-10-24 09:25:19]
  Epoch: [010][200/500]   Time 0.031 (0.167)   Data 0.000 (0.138)   Loss 1.1492 (1.3645)   Prec@1 60.000 (50.647)   Prec@5 96.000 (93.104)   [2025-10-24 09:25:22]
  Epoch: [010][300/500]   Time 0.025 (0.121)   Data 0.000 (0.092)   Loss 1.4241 (1.3569)   Prec@1 40.000 (51.017)   Prec@5 92.000 (93.219)   [2025-10-24 09:25:25]
  Epoch: [010][400/500]   Time 0.033 (0.098)   Data 0.003 (0.069)   Loss 1.4012 (1.3522)   Prec@1 51.000 (51.132)   Prec@5 95.000 (93.387)   [2025-10-24 09:25:28]
  **Train** Prec@1 51.158 Prec@5 93.434 Error@1 48.842
  **Test** Prec@1 57.440 Prec@5 95.750 Error@1 42.560

==>>[2025-10-24 09:25:57] [Epoch=011/040] [Need: 00:26:46] [LR=0.0100] [Best : Accuracy=57.63, Error=42.37]
  Epoch: [011][000/500]   Time 25.730 (25.730)   Data 25.644 (25.644)   Loss 1.2416 (1.2416)   Prec@1 55.000 (55.000)   Prec@5 95.000 (95.000)   [2025-10-24 09:26:23]
  Epoch: [011][100/500]   Time 0.028 (0.290)   Data 0.001 (0.254)   Loss 1.4028 (1.3199)   Prec@1 44.000 (52.485)   Prec@5 94.000 (93.535)   [2025-10-24 09:26:26]
  Epoch: [011][200/500]   Time 0.033 (0.163)   Data 0.000 (0.128)   Loss 1.2994 (1.3125)   Prec@1 51.000 (52.159)   Prec@5 94.000 (93.891)   [2025-10-24 09:26:30]
  Epoch: [011][300/500]   Time 0.031 (0.120)   Data 0.000 (0.086)   Loss 1.3129 (1.3111)   Prec@1 48.000 (52.528)   Prec@5 93.000 (93.841)   [2025-10-24 09:26:33]
  Epoch: [011][400/500]   Time 0.041 (0.099)   Data 0.002 (0.064)   Loss 1.4158 (1.3101)   Prec@1 47.000 (52.666)   Prec@5 89.000 (93.788)   [2025-10-24 09:26:36]
  **Train** Prec@1 52.938 Prec@5 93.924 Error@1 47.062
  **Test** Prec@1 61.200 Prec@5 96.110 Error@1 38.800
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:27:06] [Epoch=012/040] [Need: 00:26:22] [LR=0.0100] [Best : Accuracy=61.20, Error=38.80]
  Epoch: [012][000/500]   Time 25.384 (25.384)   Data 25.300 (25.300)   Loss 1.3675 (1.3675)   Prec@1 52.000 (52.000)   Prec@5 95.000 (95.000)   [2025-10-24 09:27:31]
  Epoch: [012][100/500]   Time 0.023 (0.283)   Data 0.000 (0.251)   Loss 1.2093 (1.2754)   Prec@1 57.000 (54.158)   Prec@5 95.000 (94.505)   [2025-10-24 09:27:34]
  Epoch: [012][200/500]   Time 0.029 (0.157)   Data 0.002 (0.126)   Loss 1.1851 (1.2692)   Prec@1 53.000 (54.269)   Prec@5 94.000 (94.388)   [2025-10-24 09:27:37]
  Epoch: [012][300/500]   Time 0.036 (0.115)   Data 0.001 (0.084)   Loss 1.0850 (1.2667)   Prec@1 57.000 (54.439)   Prec@5 98.000 (94.322)   [2025-10-24 09:27:40]
  Epoch: [012][400/500]   Time 0.028 (0.094)   Data 0.000 (0.064)   Loss 1.1424 (1.2683)   Prec@1 58.000 (54.613)   Prec@5 97.000 (94.267)   [2025-10-24 09:27:44]
  **Train** Prec@1 54.638 Prec@5 94.238 Error@1 45.362
  **Test** Prec@1 61.880 Prec@5 96.560 Error@1 38.120
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:28:12] [Epoch=013/040] [Need: 00:25:47] [LR=0.0100] [Best : Accuracy=61.88, Error=38.12]
  Epoch: [013][000/500]   Time 26.042 (26.042)   Data 25.965 (25.965)   Loss 1.3038 (1.3038)   Prec@1 56.000 (56.000)   Prec@5 91.000 (91.000)   [2025-10-24 09:28:38]
  Epoch: [013][100/500]   Time 0.031 (0.289)   Data 0.000 (0.258)   Loss 1.2702 (1.2353)   Prec@1 52.000 (55.594)   Prec@5 94.000 (94.614)   [2025-10-24 09:28:42]
  Epoch: [013][200/500]   Time 0.028 (0.161)   Data 0.000 (0.130)   Loss 1.1088 (1.2418)   Prec@1 60.000 (55.483)   Prec@5 97.000 (94.547)   [2025-10-24 09:28:45]
  Epoch: [013][300/500]   Time 0.041 (0.117)   Data 0.001 (0.087)   Loss 1.3165 (1.2399)   Prec@1 51.000 (55.648)   Prec@5 96.000 (94.508)   [2025-10-24 09:28:48]
  Epoch: [013][400/500]   Time 0.030 (0.095)   Data 0.001 (0.065)   Loss 1.0887 (1.2381)   Prec@1 59.000 (55.638)   Prec@5 98.000 (94.581)   [2025-10-24 09:28:50]
  **Train** Prec@1 55.824 Prec@5 94.684 Error@1 44.176
  **Test** Prec@1 63.660 Prec@5 96.550 Error@1 36.340
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:29:19] [Epoch=014/040] [Need: 00:25:07] [LR=0.0100] [Best : Accuracy=63.66, Error=36.34]
  Epoch: [014][000/500]   Time 28.293 (28.293)   Data 28.215 (28.215)   Loss 1.1740 (1.1740)   Prec@1 54.000 (54.000)   Prec@5 94.000 (94.000)   [2025-10-24 09:29:47]
  Epoch: [014][100/500]   Time 0.029 (0.311)   Data 0.001 (0.280)   Loss 1.0923 (1.2074)   Prec@1 56.000 (57.426)   Prec@5 96.000 (94.861)   [2025-10-24 09:29:50]
  Epoch: [014][200/500]   Time 0.026 (0.171)   Data 0.002 (0.141)   Loss 1.1310 (1.2015)   Prec@1 55.000 (57.164)   Prec@5 95.000 (94.970)   [2025-10-24 09:29:53]
  Epoch: [014][300/500]   Time 0.031 (0.123)   Data 0.000 (0.094)   Loss 1.1977 (1.1976)   Prec@1 60.000 (57.186)   Prec@5 91.000 (94.960)   [2025-10-24 09:29:56]
  Epoch: [014][400/500]   Time 0.028 (0.100)   Data 0.001 (0.071)   Loss 1.1984 (1.1953)   Prec@1 53.000 (57.352)   Prec@5 97.000 (94.928)   [2025-10-24 09:29:59]
  **Train** Prec@1 57.544 Prec@5 94.916 Error@1 42.456
  **Test** Prec@1 65.970 Prec@5 96.990 Error@1 34.030
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:30:30] [Epoch=015/040] [Need: 00:24:30] [LR=0.0100] [Best : Accuracy=65.97, Error=34.03]
  Epoch: [015][000/500]   Time 30.566 (30.566)   Data 30.484 (30.484)   Loss 1.0633 (1.0633)   Prec@1 64.000 (64.000)   Prec@5 99.000 (99.000)   [2025-10-24 09:31:01]
  Epoch: [015][100/500]   Time 0.039 (0.334)   Data 0.001 (0.302)   Loss 1.1159 (1.1777)   Prec@1 58.000 (57.941)   Prec@5 94.000 (95.505)   [2025-10-24 09:31:04]
  Epoch: [015][200/500]   Time 0.048 (0.186)   Data 0.001 (0.152)   Loss 1.1359 (1.1777)   Prec@1 56.000 (58.025)   Prec@5 96.000 (95.254)   [2025-10-24 09:31:08]
  Epoch: [015][300/500]   Time 0.045 (0.136)   Data 0.002 (0.102)   Loss 1.1813 (1.1712)   Prec@1 52.000 (58.272)   Prec@5 96.000 (95.359)   [2025-10-24 09:31:11]
  Epoch: [015][400/500]   Time 0.033 (0.111)   Data 0.000 (0.077)   Loss 1.3594 (1.1649)   Prec@1 55.000 (58.594)   Prec@5 94.000 (95.421)   [2025-10-24 09:31:15]
  **Train** Prec@1 58.656 Prec@5 95.400 Error@1 41.344
  **Test** Prec@1 65.680 Prec@5 97.220 Error@1 34.320

==>>[2025-10-24 09:31:45] [Epoch=016/040] [Need: 00:23:56] [LR=0.0100] [Best : Accuracy=65.97, Error=34.03]
  Epoch: [016][000/500]   Time 29.063 (29.063)   Data 28.976 (28.976)   Loss 1.2350 (1.2350)   Prec@1 57.000 (57.000)   Prec@5 94.000 (94.000)   [2025-10-24 09:32:14]
  Epoch: [016][100/500]   Time 0.028 (0.318)   Data 0.001 (0.287)   Loss 1.1527 (1.1471)   Prec@1 56.000 (59.287)   Prec@5 95.000 (95.079)   [2025-10-24 09:32:17]
  Epoch: [016][200/500]   Time 0.029 (0.173)   Data 0.000 (0.145)   Loss 1.1742 (1.1433)   Prec@1 60.000 (59.582)   Prec@5 94.000 (95.259)   [2025-10-24 09:32:20]
  Epoch: [016][300/500]   Time 0.025 (0.125)   Data 0.001 (0.097)   Loss 1.1284 (1.1423)   Prec@1 56.000 (59.751)   Prec@5 96.000 (95.272)   [2025-10-24 09:32:23]
  Epoch: [016][400/500]   Time 0.028 (0.101)   Data 0.000 (0.073)   Loss 1.0941 (1.1416)   Prec@1 61.000 (59.658)   Prec@5 94.000 (95.319)   [2025-10-24 09:32:25]
  **Train** Prec@1 59.584 Prec@5 95.336 Error@1 40.416
  **Test** Prec@1 67.340 Prec@5 97.470 Error@1 32.660
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:32:56] [Epoch=017/040] [Need: 00:23:10] [LR=0.0100] [Best : Accuracy=67.34, Error=32.66]
  Epoch: [017][000/500]   Time 29.337 (29.337)   Data 29.250 (29.250)   Loss 1.1982 (1.1982)   Prec@1 55.000 (55.000)   Prec@5 94.000 (94.000)   [2025-10-24 09:33:25]
  Epoch: [017][100/500]   Time 0.026 (0.320)   Data 0.001 (0.290)   Loss 0.9741 (1.1423)   Prec@1 70.000 (59.931)   Prec@5 96.000 (95.168)   [2025-10-24 09:33:28]
  Epoch: [017][200/500]   Time 0.024 (0.175)   Data 0.000 (0.146)   Loss 1.3447 (1.1231)   Prec@1 62.000 (60.368)   Prec@5 91.000 (95.562)   [2025-10-24 09:33:31]
  Epoch: [017][300/500]   Time 0.030 (0.126)   Data 0.000 (0.098)   Loss 1.0309 (1.1168)   Prec@1 61.000 (60.458)   Prec@5 98.000 (95.581)   [2025-10-24 09:33:34]
  Epoch: [017][400/500]   Time 0.031 (0.102)   Data 0.000 (0.073)   Loss 1.1004 (1.1160)   Prec@1 61.000 (60.579)   Prec@5 95.000 (95.621)   [2025-10-24 09:33:37]
  **Train** Prec@1 60.588 Prec@5 95.636 Error@1 39.412
  **Test** Prec@1 68.770 Prec@5 97.530 Error@1 31.230
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:34:08] [Epoch=018/040] [Need: 00:22:25] [LR=0.0100] [Best : Accuracy=68.77, Error=31.23]
  Epoch: [018][000/500]   Time 28.736 (28.736)   Data 28.644 (28.644)   Loss 1.0240 (1.0240)   Prec@1 62.000 (62.000)   Prec@5 97.000 (97.000)   [2025-10-24 09:34:37]
  Epoch: [018][100/500]   Time 0.039 (0.323)   Data 0.000 (0.284)   Loss 1.1041 (1.0966)   Prec@1 62.000 (61.287)   Prec@5 95.000 (95.564)   [2025-10-24 09:34:41]
  Epoch: [018][200/500]   Time 0.037 (0.182)   Data 0.000 (0.143)   Loss 0.8870 (1.0904)   Prec@1 70.000 (61.468)   Prec@5 97.000 (95.716)   [2025-10-24 09:34:45]
  Epoch: [018][300/500]   Time 0.049 (0.135)   Data 0.000 (0.096)   Loss 1.2988 (1.0980)   Prec@1 52.000 (61.385)   Prec@5 91.000 (95.568)   [2025-10-24 09:34:49]
  Epoch: [018][400/500]   Time 0.041 (0.111)   Data 0.002 (0.072)   Loss 1.2443 (1.0926)   Prec@1 51.000 (61.621)   Prec@5 96.000 (95.728)   [2025-10-24 09:34:53]
  **Train** Prec@1 61.808 Prec@5 95.786 Error@1 38.192
  **Test** Prec@1 69.560 Prec@5 97.730 Error@1 30.440
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:35:25] [Epoch=019/040] [Need: 00:21:41] [LR=0.0100] [Best : Accuracy=69.56, Error=30.44]
  Epoch: [019][000/500]   Time 28.913 (28.913)   Data 28.832 (28.832)   Loss 1.1615 (1.1615)   Prec@1 57.000 (57.000)   Prec@5 94.000 (94.000)   [2025-10-24 09:35:54]
  Epoch: [019][100/500]   Time 0.027 (0.317)   Data 0.000 (0.286)   Loss 1.2167 (1.0832)   Prec@1 55.000 (61.535)   Prec@5 95.000 (96.208)   [2025-10-24 09:35:57]
  Epoch: [019][200/500]   Time 0.028 (0.176)   Data 0.000 (0.144)   Loss 0.9745 (1.0732)   Prec@1 66.000 (61.836)   Prec@5 100.000 (96.109)   [2025-10-24 09:36:00]
  Epoch: [019][300/500]   Time 0.032 (0.130)   Data 0.000 (0.096)   Loss 0.9288 (1.0662)   Prec@1 72.000 (62.312)   Prec@5 94.000 (96.193)   [2025-10-24 09:36:04]
  Epoch: [019][400/500]   Time 0.035 (0.107)   Data 0.001 (0.072)   Loss 1.0588 (1.0624)   Prec@1 64.000 (62.424)   Prec@5 95.000 (96.197)   [2025-10-24 09:36:08]
  **Train** Prec@1 62.500 Prec@5 96.152 Error@1 37.500
  **Test** Prec@1 69.940 Prec@5 97.900 Error@1 30.060
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:36:40] [Epoch=020/040] [Need: 00:20:51] [LR=0.0100] [Best : Accuracy=69.94, Error=30.06]
  Epoch: [020][000/500]   Time 29.575 (29.575)   Data 29.490 (29.490)   Loss 1.2003 (1.2003)   Prec@1 62.000 (62.000)   Prec@5 97.000 (97.000)   [2025-10-24 09:37:09]
  Epoch: [020][100/500]   Time 0.029 (0.326)   Data 0.001 (0.293)   Loss 0.9203 (1.0400)   Prec@1 68.000 (63.356)   Prec@5 97.000 (96.386)   [2025-10-24 09:37:13]
  Epoch: [020][200/500]   Time 0.031 (0.179)   Data 0.001 (0.147)   Loss 1.0127 (1.0522)   Prec@1 68.000 (63.134)   Prec@5 95.000 (96.204)   [2025-10-24 09:37:16]
  Epoch: [020][300/500]   Time 0.026 (0.129)   Data 0.000 (0.098)   Loss 0.9812 (1.0467)   Prec@1 68.000 (63.495)   Prec@5 98.000 (96.289)   [2025-10-24 09:37:18]
  Epoch: [020][400/500]   Time 0.033 (0.104)   Data 0.000 (0.074)   Loss 1.0029 (1.0418)   Prec@1 67.000 (63.546)   Prec@5 95.000 (96.314)   [2025-10-24 09:37:21]
  **Train** Prec@1 63.666 Prec@5 96.294 Error@1 36.334
  **Test** Prec@1 71.360 Prec@5 97.940 Error@1 28.640
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:37:52] [Epoch=021/040] [Need: 00:19:58] [LR=0.0100] [Best : Accuracy=71.36, Error=28.64]
  Epoch: [021][000/500]   Time 30.074 (30.074)   Data 29.986 (29.986)   Loss 1.0206 (1.0206)   Prec@1 67.000 (67.000)   Prec@5 95.000 (95.000)   [2025-10-24 09:38:22]
  Epoch: [021][100/500]   Time 0.039 (0.341)   Data 0.000 (0.298)   Loss 1.0665 (1.0108)   Prec@1 61.000 (63.970)   Prec@5 97.000 (96.297)   [2025-10-24 09:38:27]
  Epoch: [021][200/500]   Time 0.030 (0.190)   Data 0.002 (0.150)   Loss 1.0257 (1.0265)   Prec@1 67.000 (63.711)   Prec@5 94.000 (96.194)   [2025-10-24 09:38:31]
  Epoch: [021][300/500]   Time 0.050 (0.138)   Data 0.004 (0.100)   Loss 0.8685 (1.0220)   Prec@1 70.000 (63.963)   Prec@5 98.000 (96.256)   [2025-10-24 09:38:34]
  Epoch: [021][400/500]   Time 0.029 (0.111)   Data 0.001 (0.075)   Loss 0.9455 (1.0197)   Prec@1 66.000 (63.950)   Prec@5 98.000 (96.434)   [2025-10-24 09:38:37]
  **Train** Prec@1 64.030 Prec@5 96.422 Error@1 35.970
  **Test** Prec@1 71.100 Prec@5 97.830 Error@1 28.900

==>>[2025-10-24 09:39:08] [Epoch=022/040] [Need: 00:19:06] [LR=0.0100] [Best : Accuracy=71.36, Error=28.64]
  Epoch: [022][000/500]   Time 25.627 (25.627)   Data 25.536 (25.536)   Loss 1.1517 (1.1517)   Prec@1 59.000 (59.000)   Prec@5 97.000 (97.000)   [2025-10-24 09:39:34]
  Epoch: [022][100/500]   Time 0.037 (0.292)   Data 0.001 (0.253)   Loss 1.1886 (1.0146)   Prec@1 62.000 (64.881)   Prec@5 97.000 (96.297)   [2025-10-24 09:39:38]
  Epoch: [022][200/500]   Time 0.034 (0.165)   Data 0.000 (0.128)   Loss 0.9382 (1.0092)   Prec@1 69.000 (65.005)   Prec@5 96.000 (96.483)   [2025-10-24 09:39:42]
  Epoch: [022][300/500]   Time 0.040 (0.124)   Data 0.000 (0.085)   Loss 0.9363 (1.0096)   Prec@1 67.000 (64.897)   Prec@5 97.000 (96.468)   [2025-10-24 09:39:46]
  Epoch: [022][400/500]   Time 0.036 (0.103)   Data 0.002 (0.064)   Loss 0.8153 (1.0046)   Prec@1 75.000 (64.965)   Prec@5 99.000 (96.616)   [2025-10-24 09:39:49]
  **Train** Prec@1 65.032 Prec@5 96.630 Error@1 34.968
  **Test** Prec@1 72.400 Prec@5 97.950 Error@1 27.600
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:40:20] [Epoch=023/040] [Need: 00:18:08] [LR=0.0100] [Best : Accuracy=72.40, Error=27.60]
  Epoch: [023][000/500]   Time 28.833 (28.833)   Data 28.752 (28.752)   Loss 1.0612 (1.0612)   Prec@1 64.000 (64.000)   Prec@5 93.000 (93.000)   [2025-10-24 09:40:49]
  Epoch: [023][100/500]   Time 0.032 (0.317)   Data 0.001 (0.285)   Loss 0.9097 (0.9767)   Prec@1 70.000 (65.960)   Prec@5 92.000 (96.713)   [2025-10-24 09:40:52]
  Epoch: [023][200/500]   Time 0.029 (0.173)   Data 0.000 (0.144)   Loss 1.0083 (0.9876)   Prec@1 59.000 (65.652)   Prec@5 98.000 (96.592)   [2025-10-24 09:40:55]
  Epoch: [023][300/500]   Time 0.029 (0.125)   Data 0.000 (0.096)   Loss 0.9295 (0.9842)   Prec@1 69.000 (65.794)   Prec@5 97.000 (96.581)   [2025-10-24 09:40:57]
  Epoch: [023][400/500]   Time 0.031 (0.101)   Data 0.001 (0.072)   Loss 0.9637 (0.9848)   Prec@1 63.000 (65.813)   Prec@5 95.000 (96.603)   [2025-10-24 09:41:00]
  **Train** Prec@1 65.832 Prec@5 96.644 Error@1 34.168
  **Test** Prec@1 72.530 Prec@5 98.100 Error@1 27.470
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:41:28] [Epoch=024/040] [Need: 00:17:07] [LR=0.0100] [Best : Accuracy=72.53, Error=27.47]
  Epoch: [024][000/500]   Time 26.844 (26.844)   Data 26.764 (26.764)   Loss 1.1014 (1.1014)   Prec@1 61.000 (61.000)   Prec@5 95.000 (95.000)   [2025-10-24 09:41:55]
  Epoch: [024][100/500]   Time 0.028 (0.295)   Data 0.001 (0.265)   Loss 0.9082 (0.9825)   Prec@1 68.000 (66.317)   Prec@5 96.000 (96.436)   [2025-10-24 09:41:58]
  Epoch: [024][200/500]   Time 0.030 (0.163)   Data 0.001 (0.134)   Loss 0.8698 (0.9784)   Prec@1 70.000 (66.517)   Prec@5 97.000 (96.572)   [2025-10-24 09:42:01]
  Epoch: [024][300/500]   Time 0.031 (0.118)   Data 0.000 (0.089)   Loss 0.9274 (0.9729)   Prec@1 64.000 (66.585)   Prec@5 96.000 (96.548)   [2025-10-24 09:42:04]
  Epoch: [024][400/500]   Time 0.030 (0.096)   Data 0.000 (0.067)   Loss 0.8242 (0.9706)   Prec@1 71.000 (66.589)   Prec@5 99.000 (96.581)   [2025-10-24 09:42:06]
  **Train** Prec@1 66.530 Prec@5 96.648 Error@1 33.470
  **Test** Prec@1 73.800 Prec@5 98.370 Error@1 26.200
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:42:34] [Epoch=025/040] [Need: 00:16:03] [LR=0.0010] [Best : Accuracy=73.80, Error=26.20]
  Epoch: [025][000/500]   Time 27.659 (27.659)   Data 27.576 (27.576)   Loss 0.9050 (0.9050)   Prec@1 66.000 (66.000)   Prec@5 95.000 (95.000)   [2025-10-24 09:43:02]
  Epoch: [025][100/500]   Time 0.030 (0.305)   Data 0.000 (0.273)   Loss 0.9878 (0.9176)   Prec@1 62.000 (68.574)   Prec@5 96.000 (97.129)   [2025-10-24 09:43:05]
  Epoch: [025][200/500]   Time 0.029 (0.167)   Data 0.000 (0.138)   Loss 1.0293 (0.9141)   Prec@1 59.000 (68.274)   Prec@5 97.000 (97.194)   [2025-10-24 09:43:08]
  Epoch: [025][300/500]   Time 0.025 (0.121)   Data 0.000 (0.092)   Loss 0.8389 (0.9061)   Prec@1 70.000 (68.635)   Prec@5 99.000 (97.259)   [2025-10-24 09:43:10]
  Epoch: [025][400/500]   Time 0.028 (0.098)   Data 0.000 (0.069)   Loss 0.8758 (0.9008)   Prec@1 68.000 (68.843)   Prec@5 99.000 (97.202)   [2025-10-24 09:43:13]
  **Train** Prec@1 69.162 Prec@5 97.226 Error@1 30.838
  **Test** Prec@1 75.560 Prec@5 98.510 Error@1 24.440
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:43:41] [Epoch=026/040] [Need: 00:15:00] [LR=0.0010] [Best : Accuracy=75.56, Error=24.44]
  Epoch: [026][000/500]   Time 25.745 (25.745)   Data 25.666 (25.666)   Loss 0.9789 (0.9789)   Prec@1 65.000 (65.000)   Prec@5 95.000 (95.000)   [2025-10-24 09:44:06]
  Epoch: [026][100/500]   Time 0.030 (0.285)   Data 0.000 (0.255)   Loss 0.8562 (0.8665)   Prec@1 72.000 (69.970)   Prec@5 97.000 (97.604)   [2025-10-24 09:44:10]
  Epoch: [026][200/500]   Time 0.025 (0.157)   Data 0.000 (0.128)   Loss 1.0256 (0.8686)   Prec@1 63.000 (69.871)   Prec@5 97.000 (97.577)   [2025-10-24 09:44:12]
  Epoch: [026][300/500]   Time 0.028 (0.116)   Data 0.000 (0.086)   Loss 1.0870 (0.8680)   Prec@1 62.000 (69.937)   Prec@5 94.000 (97.472)   [2025-10-24 09:44:16]
  Epoch: [026][400/500]   Time 0.028 (0.094)   Data 0.001 (0.064)   Loss 0.9079 (0.8707)   Prec@1 65.000 (69.880)   Prec@5 98.000 (97.441)   [2025-10-24 09:44:19]
  **Train** Prec@1 69.900 Prec@5 97.388 Error@1 30.100
  **Test** Prec@1 75.620 Prec@5 98.470 Error@1 24.380
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:44:46] [Epoch=027/040] [Need: 00:13:57] [LR=0.0010] [Best : Accuracy=75.62, Error=24.38]
  Epoch: [027][000/500]   Time 25.723 (25.723)   Data 25.647 (25.647)   Loss 0.7923 (0.7923)   Prec@1 72.000 (72.000)   Prec@5 98.000 (98.000)   [2025-10-24 09:45:12]
  Epoch: [027][100/500]   Time 0.041 (0.285)   Data 0.001 (0.254)   Loss 1.1071 (0.8670)   Prec@1 65.000 (70.129)   Prec@5 96.000 (97.554)   [2025-10-24 09:45:15]
  Epoch: [027][200/500]   Time 0.024 (0.158)   Data 0.001 (0.128)   Loss 0.8869 (0.8606)   Prec@1 73.000 (70.080)   Prec@5 96.000 (97.582)   [2025-10-24 09:45:18]
  Epoch: [027][300/500]   Time 0.030 (0.115)   Data 0.001 (0.086)   Loss 0.7439 (0.8620)   Prec@1 75.000 (70.093)   Prec@5 99.000 (97.631)   [2025-10-24 09:45:21]
  Epoch: [027][400/500]   Time 0.028 (0.093)   Data 0.001 (0.064)   Loss 0.7958 (0.8606)   Prec@1 75.000 (70.065)   Prec@5 97.000 (97.623)   [2025-10-24 09:45:24]
  **Train** Prec@1 70.076 Prec@5 97.602 Error@1 29.924
  **Test** Prec@1 76.150 Prec@5 98.540 Error@1 23.850
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:45:51] [Epoch=028/040] [Need: 00:12:52] [LR=0.0010] [Best : Accuracy=76.15, Error=23.85]
  Epoch: [028][000/500]   Time 26.819 (26.819)   Data 26.722 (26.722)   Loss 0.8594 (0.8594)   Prec@1 67.000 (67.000)   Prec@5 98.000 (98.000)   [2025-10-24 09:46:17]
  Epoch: [028][100/500]   Time 0.038 (0.305)   Data 0.003 (0.265)   Loss 0.8058 (0.8499)   Prec@1 76.000 (70.851)   Prec@5 97.000 (97.594)   [2025-10-24 09:46:21]
  Epoch: [028][200/500]   Time 0.026 (0.170)   Data 0.000 (0.134)   Loss 0.8426 (0.8526)   Prec@1 68.000 (70.716)   Prec@5 96.000 (97.468)   [2025-10-24 09:46:25]
  Epoch: [028][300/500]   Time 0.025 (0.123)   Data 0.002 (0.089)   Loss 0.7342 (0.8584)   Prec@1 75.000 (70.462)   Prec@5 97.000 (97.462)   [2025-10-24 09:46:27]
  Epoch: [028][400/500]   Time 0.029 (0.099)   Data 0.001 (0.067)   Loss 0.7760 (0.8573)   Prec@1 72.000 (70.459)   Prec@5 98.000 (97.521)   [2025-10-24 09:46:30]
  **Train** Prec@1 70.338 Prec@5 97.510 Error@1 29.662
  **Test** Prec@1 76.540 Prec@5 98.540 Error@1 23.460
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:46:58] [Epoch=029/040] [Need: 00:11:49] [LR=0.0010] [Best : Accuracy=76.54, Error=23.46]
  Epoch: [029][000/500]   Time 27.040 (27.040)   Data 26.955 (26.955)   Loss 0.7790 (0.7790)   Prec@1 75.000 (75.000)   Prec@5 97.000 (97.000)   [2025-10-24 09:47:25]
  Epoch: [029][100/500]   Time 0.030 (0.296)   Data 0.001 (0.267)   Loss 0.9479 (0.8338)   Prec@1 67.000 (71.158)   Prec@5 95.000 (97.802)   [2025-10-24 09:47:28]
  Epoch: [029][200/500]   Time 0.027 (0.163)   Data 0.000 (0.134)   Loss 0.8330 (0.8446)   Prec@1 69.000 (70.940)   Prec@5 97.000 (97.592)   [2025-10-24 09:47:31]
  Epoch: [029][300/500]   Time 0.029 (0.118)   Data 0.001 (0.090)   Loss 0.9649 (0.8507)   Prec@1 67.000 (70.817)   Prec@5 97.000 (97.488)   [2025-10-24 09:47:34]
  Epoch: [029][400/500]   Time 0.030 (0.096)   Data 0.000 (0.068)   Loss 0.7203 (0.8488)   Prec@1 77.000 (70.870)   Prec@5 97.000 (97.489)   [2025-10-24 09:47:37]
  **Train** Prec@1 70.846 Prec@5 97.478 Error@1 29.154
  **Test** Prec@1 76.340 Prec@5 98.540 Error@1 23.660

==>>[2025-10-24 09:48:05] [Epoch=030/040] [Need: 00:10:45] [LR=0.0010] [Best : Accuracy=76.54, Error=23.46]
  Epoch: [030][000/500]   Time 26.465 (26.465)   Data 26.387 (26.387)   Loss 0.7474 (0.7474)   Prec@1 73.000 (73.000)   Prec@5 98.000 (98.000)   [2025-10-24 09:48:32]
  Epoch: [030][100/500]   Time 0.028 (0.292)   Data 0.000 (0.262)   Loss 0.6994 (0.8308)   Prec@1 74.000 (71.208)   Prec@5 98.000 (97.446)   [2025-10-24 09:48:35]
  Epoch: [030][200/500]   Time 0.030 (0.161)   Data 0.000 (0.132)   Loss 0.7880 (0.8396)   Prec@1 76.000 (71.015)   Prec@5 99.000 (97.577)   [2025-10-24 09:48:37]
  Epoch: [030][300/500]   Time 0.024 (0.117)   Data 0.000 (0.088)   Loss 0.7514 (0.8423)   Prec@1 81.000 (71.007)   Prec@5 95.000 (97.591)   [2025-10-24 09:48:40]
  Epoch: [030][400/500]   Time 0.043 (0.095)   Data 0.000 (0.066)   Loss 0.8118 (0.8473)   Prec@1 74.000 (70.830)   Prec@5 96.000 (97.559)   [2025-10-24 09:48:43]
  **Train** Prec@1 70.760 Prec@5 97.570 Error@1 29.240
  **Test** Prec@1 76.850 Prec@5 98.540 Error@1 23.150
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:49:11] [Epoch=031/040] [Need: 00:09:41] [LR=0.0010] [Best : Accuracy=76.85, Error=23.15]
  Epoch: [031][000/500]   Time 25.647 (25.647)   Data 25.568 (25.568)   Loss 0.7785 (0.7785)   Prec@1 74.000 (74.000)   Prec@5 100.000 (100.000)   [2025-10-24 09:49:37]
  Epoch: [031][100/500]   Time 0.028 (0.283)   Data 0.001 (0.254)   Loss 0.8919 (0.8318)   Prec@1 67.000 (71.347)   Prec@5 97.000 (97.713)   [2025-10-24 09:49:40]
  Epoch: [031][200/500]   Time 0.035 (0.158)   Data 0.002 (0.128)   Loss 0.7214 (0.8378)   Prec@1 76.000 (71.075)   Prec@5 97.000 (97.652)   [2025-10-24 09:49:43]
  Epoch: [031][300/500]   Time 0.029 (0.115)   Data 0.000 (0.085)   Loss 1.0030 (0.8314)   Prec@1 70.000 (71.432)   Prec@5 98.000 (97.718)   [2025-10-24 09:49:46]
  Epoch: [031][400/500]   Time 0.028 (0.094)   Data 0.000 (0.064)   Loss 0.6847 (0.8387)   Prec@1 76.000 (71.105)   Prec@5 99.000 (97.651)   [2025-10-24 09:49:49]
  **Train** Prec@1 70.868 Prec@5 97.662 Error@1 29.132
  **Test** Prec@1 76.590 Prec@5 98.400 Error@1 23.410

==>>[2025-10-24 09:50:17] [Epoch=032/040] [Need: 00:08:37] [LR=0.0010] [Best : Accuracy=76.85, Error=23.15]
  Epoch: [032][000/500]   Time 26.632 (26.632)   Data 26.557 (26.557)   Loss 1.0226 (1.0226)   Prec@1 62.000 (62.000)   Prec@5 95.000 (95.000)   [2025-10-24 09:50:44]
  Epoch: [032][100/500]   Time 0.033 (0.295)   Data 0.000 (0.263)   Loss 0.8933 (0.8304)   Prec@1 74.000 (71.307)   Prec@5 97.000 (97.495)   [2025-10-24 09:50:47]
  Epoch: [032][200/500]   Time 0.027 (0.163)   Data 0.000 (0.133)   Loss 0.7772 (0.8403)   Prec@1 69.000 (70.701)   Prec@5 98.000 (97.423)   [2025-10-24 09:50:50]
  Epoch: [032][300/500]   Time 0.026 (0.118)   Data 0.001 (0.089)   Loss 0.9326 (0.8330)   Prec@1 65.000 (71.007)   Prec@5 97.000 (97.492)   [2025-10-24 09:50:52]
  Epoch: [032][400/500]   Time 0.022 (0.096)   Data 0.000 (0.067)   Loss 1.1017 (0.8361)   Prec@1 65.000 (71.110)   Prec@5 94.000 (97.501)   [2025-10-24 09:50:55]
  **Train** Prec@1 70.954 Prec@5 97.540 Error@1 29.046
  **Test** Prec@1 76.610 Prec@5 98.580 Error@1 23.390

==>>[2025-10-24 09:51:22] [Epoch=033/040] [Need: 00:07:32] [LR=0.0010] [Best : Accuracy=76.85, Error=23.15]
  Epoch: [033][000/500]   Time 26.520 (26.520)   Data 26.442 (26.442)   Loss 0.7840 (0.7840)   Prec@1 73.000 (73.000)   Prec@5 99.000 (99.000)   [2025-10-24 09:51:49]
  Epoch: [033][100/500]   Time 0.029 (0.292)   Data 0.000 (0.262)   Loss 0.7341 (0.8401)   Prec@1 74.000 (71.406)   Prec@5 98.000 (97.475)   [2025-10-24 09:51:52]
  Epoch: [033][200/500]   Time 0.026 (0.160)   Data 0.001 (0.132)   Loss 0.7960 (0.8354)   Prec@1 66.000 (71.204)   Prec@5 98.000 (97.617)   [2025-10-24 09:51:55]
  Epoch: [033][300/500]   Time 0.025 (0.116)   Data 0.002 (0.088)   Loss 0.8129 (0.8357)   Prec@1 73.000 (71.123)   Prec@5 98.000 (97.615)   [2025-10-24 09:51:57]
  Epoch: [033][400/500]   Time 0.030 (0.094)   Data 0.000 (0.066)   Loss 0.8752 (0.8376)   Prec@1 70.000 (71.050)   Prec@5 97.000 (97.636)   [2025-10-24 09:52:00]
  **Train** Prec@1 71.074 Prec@5 97.676 Error@1 28.926
  **Test** Prec@1 76.910 Prec@5 98.650 Error@1 23.090
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:52:28] [Epoch=034/040] [Need: 00:06:28] [LR=0.0010] [Best : Accuracy=76.91, Error=23.09]
  Epoch: [034][000/500]   Time 26.913 (26.913)   Data 26.836 (26.836)   Loss 0.9922 (0.9922)   Prec@1 69.000 (69.000)   Prec@5 96.000 (96.000)   [2025-10-24 09:52:55]
  Epoch: [034][100/500]   Time 0.028 (0.296)   Data 0.000 (0.266)   Loss 0.7568 (0.8430)   Prec@1 73.000 (70.921)   Prec@5 98.000 (97.475)   [2025-10-24 09:52:58]
  Epoch: [034][200/500]   Time 0.027 (0.163)   Data 0.001 (0.134)   Loss 0.9212 (0.8348)   Prec@1 71.000 (71.358)   Prec@5 96.000 (97.517)   [2025-10-24 09:53:01]
  Epoch: [034][300/500]   Time 0.028 (0.118)   Data 0.000 (0.090)   Loss 0.8884 (0.8364)   Prec@1 67.000 (71.312)   Prec@5 97.000 (97.538)   [2025-10-24 09:53:03]
  Epoch: [034][400/500]   Time 0.030 (0.096)   Data 0.002 (0.067)   Loss 0.9154 (0.8343)   Prec@1 61.000 (71.354)   Prec@5 100.000 (97.544)   [2025-10-24 09:53:06]
  **Train** Prec@1 71.330 Prec@5 97.562 Error@1 28.670
  **Test** Prec@1 76.830 Prec@5 98.630 Error@1 23.170

==>>[2025-10-24 09:53:36] [Epoch=035/040] [Need: 00:05:24] [LR=0.0010] [Best : Accuracy=76.91, Error=23.09]
  Epoch: [035][000/500]   Time 26.540 (26.540)   Data 26.462 (26.462)   Loss 0.7956 (0.7956)   Prec@1 76.000 (76.000)   Prec@5 98.000 (98.000)   [2025-10-24 09:54:02]
  Epoch: [035][100/500]   Time 0.034 (0.297)   Data 0.000 (0.263)   Loss 0.8741 (0.8403)   Prec@1 66.000 (70.990)   Prec@5 98.000 (97.426)   [2025-10-24 09:54:06]
  Epoch: [035][200/500]   Time 0.023 (0.165)   Data 0.000 (0.132)   Loss 0.9697 (0.8318)   Prec@1 63.000 (71.020)   Prec@5 99.000 (97.587)   [2025-10-24 09:54:09]
  Epoch: [035][300/500]   Time 0.031 (0.121)   Data 0.000 (0.088)   Loss 0.6527 (0.8261)   Prec@1 79.000 (71.405)   Prec@5 98.000 (97.645)   [2025-10-24 09:54:12]
  Epoch: [035][400/500]   Time 0.047 (0.100)   Data 0.001 (0.067)   Loss 0.6651 (0.8253)   Prec@1 79.000 (71.409)   Prec@5 98.000 (97.638)   [2025-10-24 09:54:16]
  **Train** Prec@1 71.532 Prec@5 97.692 Error@1 28.468
  **Test** Prec@1 76.880 Prec@5 98.590 Error@1 23.120

==>>[2025-10-24 09:54:49] [Epoch=036/040] [Need: 00:04:20] [LR=0.0010] [Best : Accuracy=76.91, Error=23.09]
  Epoch: [036][000/500]   Time 25.469 (25.469)   Data 25.387 (25.387)   Loss 0.6554 (0.6554)   Prec@1 79.000 (79.000)   Prec@5 100.000 (100.000)   [2025-10-24 09:55:14]
  Epoch: [036][100/500]   Time 0.032 (0.290)   Data 0.000 (0.252)   Loss 0.7439 (0.8236)   Prec@1 76.000 (71.238)   Prec@5 99.000 (97.743)   [2025-10-24 09:55:18]
  Epoch: [036][200/500]   Time 0.035 (0.163)   Data 0.000 (0.127)   Loss 0.7531 (0.8279)   Prec@1 77.000 (71.284)   Prec@5 97.000 (97.657)   [2025-10-24 09:55:21]
  Epoch: [036][300/500]   Time 0.035 (0.121)   Data 0.000 (0.085)   Loss 1.0606 (0.8247)   Prec@1 63.000 (71.375)   Prec@5 95.000 (97.711)   [2025-10-24 09:55:25]
  Epoch: [036][400/500]   Time 0.028 (0.098)   Data 0.000 (0.064)   Loss 0.7655 (0.8273)   Prec@1 78.000 (71.347)   Prec@5 97.000 (97.681)   [2025-10-24 09:55:28]
  **Train** Prec@1 71.458 Prec@5 97.714 Error@1 28.542
  **Test** Prec@1 76.990 Prec@5 98.640 Error@1 23.010
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:55:56] [Epoch=037/040] [Need: 00:03:15] [LR=0.0010] [Best : Accuracy=76.99, Error=23.01]
  Epoch: [037][000/500]   Time 27.538 (27.538)   Data 27.449 (27.449)   Loss 0.7458 (0.7458)   Prec@1 77.000 (77.000)   Prec@5 98.000 (98.000)   [2025-10-24 09:56:24]
  Epoch: [037][100/500]   Time 0.026 (0.306)   Data 0.000 (0.272)   Loss 0.7010 (0.8083)   Prec@1 78.000 (71.861)   Prec@5 98.000 (97.851)   [2025-10-24 09:56:27]
  Epoch: [037][200/500]   Time 0.033 (0.171)   Data 0.000 (0.137)   Loss 0.6736 (0.8251)   Prec@1 82.000 (71.393)   Prec@5 98.000 (97.687)   [2025-10-24 09:56:31]
  Epoch: [037][300/500]   Time 0.035 (0.126)   Data 0.002 (0.092)   Loss 0.6679 (0.8240)   Prec@1 73.000 (71.445)   Prec@5 99.000 (97.774)   [2025-10-24 09:56:34]
  Epoch: [037][400/500]   Time 0.030 (0.103)   Data 0.002 (0.069)   Loss 0.9945 (0.8218)   Prec@1 70.000 (71.526)   Prec@5 97.000 (97.751)   [2025-10-24 09:56:38]
  **Train** Prec@1 71.572 Prec@5 97.740 Error@1 28.428
  **Test** Prec@1 77.160 Prec@5 98.620 Error@1 22.840
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 09:57:06] [Epoch=038/040] [Need: 00:02:10] [LR=0.0010] [Best : Accuracy=77.16, Error=22.84]
  Epoch: [038][000/500]   Time 25.300 (25.300)   Data 25.220 (25.220)   Loss 0.6831 (0.6831)   Prec@1 76.000 (76.000)   Prec@5 99.000 (99.000)   [2025-10-24 09:57:31]
  Epoch: [038][100/500]   Time 0.026 (0.281)   Data 0.000 (0.250)   Loss 0.6822 (0.8091)   Prec@1 74.000 (72.396)   Prec@5 100.000 (97.802)   [2025-10-24 09:57:34]
  Epoch: [038][200/500]   Time 0.032 (0.156)   Data 0.001 (0.126)   Loss 0.7949 (0.8185)   Prec@1 76.000 (72.030)   Prec@5 95.000 (97.672)   [2025-10-24 09:57:37]
  Epoch: [038][300/500]   Time 0.028 (0.114)   Data 0.000 (0.084)   Loss 0.7755 (0.8199)   Prec@1 74.000 (71.837)   Prec@5 98.000 (97.654)   [2025-10-24 09:57:40]
  Epoch: [038][400/500]   Time 0.032 (0.094)   Data 0.001 (0.063)   Loss 0.9432 (0.8172)   Prec@1 64.000 (71.908)   Prec@5 99.000 (97.656)   [2025-10-24 09:57:43]
  **Train** Prec@1 71.812 Prec@5 97.592 Error@1 28.188
  **Test** Prec@1 77.060 Prec@5 98.610 Error@1 22.940

==>>[2025-10-24 09:58:11] [Epoch=039/040] [Need: 00:01:05] [LR=0.0010] [Best : Accuracy=77.16, Error=22.84]
  Epoch: [039][000/500]   Time 24.028 (24.028)   Data 23.926 (23.926)   Loss 0.9015 (0.9015)   Prec@1 67.000 (67.000)   Prec@5 98.000 (98.000)   [2025-10-24 09:58:35]
  Epoch: [039][100/500]   Time 0.035 (0.276)   Data 0.002 (0.238)   Loss 0.6375 (0.8278)   Prec@1 75.000 (71.436)   Prec@5 99.000 (97.475)   [2025-10-24 09:58:39]
  Epoch: [039][200/500]   Time 0.033 (0.157)   Data 0.002 (0.120)   Loss 0.6316 (0.8179)   Prec@1 77.000 (71.711)   Prec@5 100.000 (97.557)   [2025-10-24 09:58:43]
  Epoch: [039][300/500]   Time 0.034 (0.117)   Data 0.000 (0.080)   Loss 0.9855 (0.8201)   Prec@1 68.000 (71.748)   Prec@5 96.000 (97.638)   [2025-10-24 09:58:47]
  Epoch: [039][400/500]   Time 0.041 (0.097)   Data 0.001 (0.060)   Loss 0.7352 (0.8166)   Prec@1 76.000 (71.845)   Prec@5 98.000 (97.686)   [2025-10-24 09:58:50]
  **Train** Prec@1 71.912 Prec@5 97.684 Error@1 28.088
  **Test** Prec@1 77.040 Prec@5 98.610 Error@1 22.960
