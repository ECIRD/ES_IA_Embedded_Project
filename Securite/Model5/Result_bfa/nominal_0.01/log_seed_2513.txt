save path : ./save/tinyvgg_quan/nominal_0.01
{'data_path': './dataset', 'arch': 'tinyvgg_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': False, 'clipping_coeff': 0.0, 'learning_rate': 0.01, 'manualSeed': 2513, 'save_path': './save/tinyvgg_quan/nominal_0.01', 'enable_bfa': False, 'resume': '', 'quan_bitwidth': None, 'reset_weight': False, 'evaluate': False, 'n_iter': 30, 'model_only': False, 'random_bfa': False, 'use_cuda': True}
Random Seed: 2513
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'tinyvgg_quan'
=> network :
 TinyVGG(
  (features): Sequential(
    (0): quan_Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Dropout2d(p=0.3, inplace=False)
    (6): quan_Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace=True)
    (8): quan_Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace=True)
    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (11): Dropout2d(p=0.3, inplace=False)
    (12): quan_Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): quan_Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Dropout2d(p=0.3, inplace=False)
  )
  (classifier): Sequential(
    (0): quan_Linear(in_features=2048, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): quan_Linear(in_features=128, out_features=10, bias=True)
  )
)
=> do not use any checkpoint for tinyvgg_quan model

==>>[2025-10-24 08:00:51] [Epoch=000/040] [Need: 00:00:00] [LR=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/500]   Time 28.698 (28.698)   Data 26.839 (26.839)   Loss 2.3057 (2.3057)   Prec@1 10.000 (10.000)   Prec@5 51.000 (51.000)   [2025-10-24 08:01:20]
  Epoch: [000][100/500]   Time 0.026 (0.313)   Data 0.001 (0.266)   Loss 2.3010 (2.3032)   Prec@1 12.000 (10.030)   Prec@5 58.000 (50.594)   [2025-10-24 08:01:23]
  Epoch: [000][200/500]   Time 0.025 (0.171)   Data 0.001 (0.134)   Loss 2.3027 (2.3031)   Prec@1 9.000 (9.980)   Prec@5 46.000 (50.030)   [2025-10-24 08:01:26]
  Epoch: [000][300/500]   Time 0.028 (0.123)   Data 0.000 (0.090)   Loss 2.3031 (2.3028)   Prec@1 8.000 (10.169)   Prec@5 46.000 (50.276)   [2025-10-24 08:01:28]
  Epoch: [000][400/500]   Time 0.024 (0.099)   Data 0.000 (0.067)   Loss 2.3171 (2.3001)   Prec@1 13.000 (10.975)   Prec@5 54.000 (51.554)   [2025-10-24 08:01:31]
  **Train** Prec@1 12.836 Prec@5 55.682 Error@1 87.164
  **Test** Prec@1 22.680 Prec@5 77.460 Error@1 77.320
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:01:59] [Epoch=001/040] [Need: 00:43:52] [LR=0.0100] [Best : Accuracy=22.68, Error=77.32]
  Epoch: [001][000/500]   Time 26.720 (26.720)   Data 26.637 (26.637)   Loss 2.1049 (2.1049)   Prec@1 26.000 (26.000)   Prec@5 78.000 (78.000)   [2025-10-24 08:02:26]
  Epoch: [001][100/500]   Time 0.031 (0.295)   Data 0.001 (0.264)   Loss 2.0901 (2.0750)   Prec@1 23.000 (22.802)   Prec@5 78.000 (76.960)   [2025-10-24 08:02:29]
  Epoch: [001][200/500]   Time 0.032 (0.162)   Data 0.000 (0.133)   Loss 1.8999 (2.0406)   Prec@1 27.000 (23.408)   Prec@5 84.000 (78.104)   [2025-10-24 08:02:32]
  Epoch: [001][300/500]   Time 0.031 (0.117)   Data 0.000 (0.089)   Loss 2.0578 (2.0154)   Prec@1 22.000 (24.080)   Prec@5 78.000 (79.216)   [2025-10-24 08:02:35]
  Epoch: [001][400/500]   Time 0.028 (0.095)   Data 0.001 (0.067)   Loss 1.9765 (1.9964)   Prec@1 26.000 (24.613)   Prec@5 79.000 (80.017)   [2025-10-24 08:02:38]
  **Train** Prec@1 25.168 Prec@5 80.804 Error@1 74.832
  **Test** Prec@1 31.430 Prec@5 86.920 Error@1 68.570
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:03:06] [Epoch=002/040] [Need: 00:42:21] [LR=0.0100] [Best : Accuracy=31.43, Error=68.57]
  Epoch: [002][000/500]   Time 26.111 (26.111)   Data 26.031 (26.031)   Loss 1.7754 (1.7754)   Prec@1 28.000 (28.000)   Prec@5 86.000 (86.000)   [2025-10-24 08:03:32]
  Epoch: [002][100/500]   Time 0.027 (0.289)   Data 0.001 (0.258)   Loss 1.8014 (1.8700)   Prec@1 32.000 (28.683)   Prec@5 89.000 (85.297)   [2025-10-24 08:03:35]
  Epoch: [002][200/500]   Time 0.026 (0.160)   Data 0.000 (0.130)   Loss 1.7692 (1.8581)   Prec@1 34.000 (29.473)   Prec@5 87.000 (85.313)   [2025-10-24 08:03:38]
  Epoch: [002][300/500]   Time 0.027 (0.116)   Data 0.000 (0.087)   Loss 1.7180 (1.8477)   Prec@1 31.000 (29.704)   Prec@5 92.000 (85.449)   [2025-10-24 08:03:41]
  Epoch: [002][400/500]   Time 0.024 (0.094)   Data 0.000 (0.065)   Loss 1.6996 (1.8392)   Prec@1 40.000 (30.182)   Prec@5 89.000 (85.636)   [2025-10-24 08:03:43]
  **Train** Prec@1 30.684 Prec@5 85.880 Error@1 69.316
  **Test** Prec@1 37.840 Prec@5 90.330 Error@1 62.160
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:04:12] [Epoch=003/040] [Need: 00:41:02] [LR=0.0100] [Best : Accuracy=37.84, Error=62.16]
  Epoch: [003][000/500]   Time 26.969 (26.969)   Data 26.887 (26.887)   Loss 1.5967 (1.5967)   Prec@1 39.000 (39.000)   Prec@5 94.000 (94.000)   [2025-10-24 08:04:39]
  Epoch: [003][100/500]   Time 0.024 (0.296)   Data 0.000 (0.267)   Loss 1.8515 (1.7590)   Prec@1 31.000 (32.703)   Prec@5 84.000 (87.366)   [2025-10-24 08:04:41]
  Epoch: [003][200/500]   Time 0.033 (0.163)   Data 0.000 (0.134)   Loss 1.8854 (1.7534)   Prec@1 34.000 (33.045)   Prec@5 85.000 (87.458)   [2025-10-24 08:04:44]
  Epoch: [003][300/500]   Time 0.027 (0.118)   Data 0.001 (0.090)   Loss 1.7104 (1.7457)   Prec@1 33.000 (33.605)   Prec@5 87.000 (87.565)   [2025-10-24 08:04:47]
  Epoch: [003][400/500]   Time 0.027 (0.096)   Data 0.000 (0.067)   Loss 1.7152 (1.7388)   Prec@1 41.000 (33.773)   Prec@5 91.000 (87.776)   [2025-10-24 08:04:50]
  **Train** Prec@1 34.122 Prec@5 87.978 Error@1 65.878
  **Test** Prec@1 40.740 Prec@5 91.230 Error@1 59.260
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:05:17] [Epoch=004/040] [Need: 00:39:45] [LR=0.0100] [Best : Accuracy=40.74, Error=59.26]
  Epoch: [004][000/500]   Time 25.581 (25.581)   Data 25.498 (25.498)   Loss 1.5630 (1.5630)   Prec@1 33.000 (33.000)   Prec@5 92.000 (92.000)   [2025-10-24 08:05:43]
  Epoch: [004][100/500]   Time 0.023 (0.282)   Data 0.001 (0.253)   Loss 1.8041 (1.6760)   Prec@1 34.000 (36.248)   Prec@5 85.000 (89.139)   [2025-10-24 08:05:45]
  Epoch: [004][200/500]   Time 0.027 (0.155)   Data 0.000 (0.127)   Loss 1.6671 (1.6781)   Prec@1 41.000 (36.791)   Prec@5 90.000 (89.264)   [2025-10-24 08:05:48]
  Epoch: [004][300/500]   Time 0.028 (0.113)   Data 0.001 (0.085)   Loss 1.7498 (1.6662)   Prec@1 39.000 (37.073)   Prec@5 89.000 (89.615)   [2025-10-24 08:05:51]
  Epoch: [004][400/500]   Time 0.031 (0.092)   Data 0.001 (0.064)   Loss 1.5216 (1.6604)   Prec@1 46.000 (37.494)   Prec@5 93.000 (89.541)   [2025-10-24 08:05:54]
  **Train** Prec@1 38.004 Prec@5 89.564 Error@1 61.996
  **Test** Prec@1 46.090 Prec@5 92.790 Error@1 53.910
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:06:21] [Epoch=005/040] [Need: 00:38:22] [LR=0.0100] [Best : Accuracy=46.09, Error=53.91]
  Epoch: [005][000/500]   Time 24.696 (24.696)   Data 24.631 (24.631)   Loss 1.6134 (1.6134)   Prec@1 40.000 (40.000)   Prec@5 87.000 (87.000)   [2025-10-24 08:06:46]
  Epoch: [005][100/500]   Time 0.015 (0.261)   Data 0.000 (0.244)   Loss 1.6302 (1.6031)   Prec@1 41.000 (40.248)   Prec@5 91.000 (90.188)   [2025-10-24 08:06:47]
  Epoch: [005][200/500]   Time 0.015 (0.139)   Data 0.001 (0.123)   Loss 1.6574 (1.5903)   Prec@1 43.000 (40.687)   Prec@5 90.000 (90.577)   [2025-10-24 08:06:49]
  Epoch: [005][300/500]   Time 0.016 (0.098)   Data 0.000 (0.082)   Loss 1.6939 (1.5850)   Prec@1 33.000 (40.724)   Prec@5 85.000 (90.532)   [2025-10-24 08:06:50]
  Epoch: [005][400/500]   Time 0.016 (0.078)   Data 0.000 (0.062)   Loss 1.5978 (1.5748)   Prec@1 45.000 (41.150)   Prec@5 89.000 (90.666)   [2025-10-24 08:06:52]
  **Train** Prec@1 41.522 Prec@5 90.784 Error@1 58.478
  **Test** Prec@1 50.060 Prec@5 94.370 Error@1 49.940
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:07:14] [Epoch=006/040] [Need: 00:36:07] [LR=0.0100] [Best : Accuracy=50.06, Error=49.94]
  Epoch: [006][000/500]   Time 19.951 (19.951)   Data 19.885 (19.885)   Loss 1.4760 (1.4760)   Prec@1 47.000 (47.000)   Prec@5 95.000 (95.000)   [2025-10-24 08:07:34]
  Epoch: [006][100/500]   Time 0.015 (0.216)   Data 0.001 (0.197)   Loss 1.4625 (1.5073)   Prec@1 43.000 (44.287)   Prec@5 91.000 (91.614)   [2025-10-24 08:07:36]
  Epoch: [006][200/500]   Time 0.015 (0.117)   Data 0.000 (0.099)   Loss 1.3179 (1.5056)   Prec@1 51.000 (43.970)   Prec@5 92.000 (91.701)   [2025-10-24 08:07:38]
  Epoch: [006][300/500]   Time 0.019 (0.083)   Data 0.000 (0.066)   Loss 1.4971 (1.4969)   Prec@1 43.000 (44.309)   Prec@5 92.000 (91.761)   [2025-10-24 08:07:39]
  Epoch: [006][400/500]   Time 0.016 (0.067)   Data 0.001 (0.050)   Loss 1.4261 (1.4904)   Prec@1 49.000 (44.708)   Prec@5 94.000 (91.933)   [2025-10-24 08:07:41]
  **Train** Prec@1 45.210 Prec@5 92.028 Error@1 54.790
  **Test** Prec@1 52.160 Prec@5 94.990 Error@1 47.840
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:08:03] [Epoch=007/040] [Need: 00:33:55] [LR=0.0100] [Best : Accuracy=52.16, Error=47.84]
  Epoch: [007][000/500]   Time 19.690 (19.690)   Data 19.625 (19.625)   Loss 1.4047 (1.4047)   Prec@1 49.000 (49.000)   Prec@5 90.000 (90.000)   [2025-10-24 08:08:23]
  Epoch: [007][100/500]   Time 0.018 (0.212)   Data 0.000 (0.194)   Loss 1.4010 (1.4304)   Prec@1 50.000 (47.238)   Prec@5 89.000 (92.436)   [2025-10-24 08:08:25]
  Epoch: [007][200/500]   Time 0.016 (0.115)   Data 0.000 (0.098)   Loss 1.4264 (1.4404)   Prec@1 53.000 (46.955)   Prec@5 93.000 (92.507)   [2025-10-24 08:08:26]
  Epoch: [007][300/500]   Time 0.016 (0.082)   Data 0.000 (0.065)   Loss 1.5304 (1.4305)   Prec@1 43.000 (47.528)   Prec@5 93.000 (92.664)   [2025-10-24 08:08:28]
  Epoch: [007][400/500]   Time 0.019 (0.066)   Data 0.000 (0.049)   Loss 1.4388 (1.4269)   Prec@1 41.000 (47.840)   Prec@5 97.000 (92.571)   [2025-10-24 08:08:30]
  **Train** Prec@1 48.064 Prec@5 92.690 Error@1 51.936
  **Test** Prec@1 56.560 Prec@5 95.110 Error@1 43.440
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:08:52] [Epoch=008/040] [Need: 00:32:02] [LR=0.0100] [Best : Accuracy=56.56, Error=43.44]
  Epoch: [008][000/500]   Time 19.589 (19.589)   Data 19.526 (19.526)   Loss 1.5953 (1.5953)   Prec@1 41.000 (41.000)   Prec@5 97.000 (97.000)   [2025-10-24 08:09:12]
  Epoch: [008][100/500]   Time 0.016 (0.211)   Data 0.000 (0.193)   Loss 1.2077 (1.3686)   Prec@1 62.000 (49.366)   Prec@5 94.000 (93.416)   [2025-10-24 08:09:13]
  Epoch: [008][200/500]   Time 0.016 (0.114)   Data 0.000 (0.097)   Loss 1.4455 (1.3629)   Prec@1 43.000 (49.746)   Prec@5 91.000 (93.512)   [2025-10-24 08:09:15]
  Epoch: [008][300/500]   Time 0.016 (0.082)   Data 0.000 (0.065)   Loss 1.2501 (1.3567)   Prec@1 53.000 (50.326)   Prec@5 95.000 (93.475)   [2025-10-24 08:09:17]
  Epoch: [008][400/500]   Time 0.043 (0.071)   Data 0.002 (0.049)   Loss 1.3917 (1.3526)   Prec@1 52.000 (50.579)   Prec@5 95.000 (93.416)   [2025-10-24 08:09:21]
  **Train** Prec@1 50.570 Prec@5 93.452 Error@1 49.430
  **Test** Prec@1 59.090 Prec@5 95.800 Error@1 40.910
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:09:43] [Epoch=009/040] [Need: 00:30:32] [LR=0.0100] [Best : Accuracy=59.09, Error=40.91]
  Epoch: [009][000/500]   Time 19.701 (19.701)   Data 19.636 (19.636)   Loss 1.3469 (1.3469)   Prec@1 49.000 (49.000)   Prec@5 95.000 (95.000)   [2025-10-24 08:10:03]
  Epoch: [009][100/500]   Time 0.017 (0.214)   Data 0.001 (0.195)   Loss 1.3416 (1.3360)   Prec@1 54.000 (51.634)   Prec@5 92.000 (93.683)   [2025-10-24 08:10:05]
  Epoch: [009][200/500]   Time 0.017 (0.116)   Data 0.000 (0.098)   Loss 1.1395 (1.3166)   Prec@1 61.000 (52.308)   Prec@5 95.000 (93.935)   [2025-10-24 08:10:07]
  Epoch: [009][300/500]   Time 0.015 (0.083)   Data 0.000 (0.065)   Loss 1.5341 (1.3110)   Prec@1 47.000 (52.551)   Prec@5 92.000 (94.033)   [2025-10-24 08:10:08]
  Epoch: [009][400/500]   Time 0.015 (0.066)   Data 0.000 (0.049)   Loss 1.2789 (1.3094)   Prec@1 55.000 (52.723)   Prec@5 95.000 (94.050)   [2025-10-24 08:10:10]
  **Train** Prec@1 52.932 Prec@5 94.132 Error@1 47.068
  **Test** Prec@1 60.620 Prec@5 96.430 Error@1 39.380
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:10:33] [Epoch=010/040] [Need: 00:29:03] [LR=0.0100] [Best : Accuracy=60.62, Error=39.38]
  Epoch: [010][000/500]   Time 19.644 (19.644)   Data 19.581 (19.581)   Loss 1.3049 (1.3049)   Prec@1 55.000 (55.000)   Prec@5 97.000 (97.000)   [2025-10-24 08:10:52]
  Epoch: [010][100/500]   Time 0.018 (0.212)   Data 0.000 (0.194)   Loss 1.3028 (1.2891)   Prec@1 51.000 (53.178)   Prec@5 95.000 (94.257)   [2025-10-24 08:10:54]
  Epoch: [010][200/500]   Time 0.016 (0.115)   Data 0.000 (0.098)   Loss 1.0949 (1.2757)   Prec@1 61.000 (53.811)   Prec@5 97.000 (94.284)   [2025-10-24 08:10:56]
  Epoch: [010][300/500]   Time 0.016 (0.082)   Data 0.001 (0.065)   Loss 1.2592 (1.2710)   Prec@1 56.000 (53.960)   Prec@5 92.000 (94.439)   [2025-10-24 08:10:57]
  Epoch: [010][400/500]   Time 0.018 (0.066)   Data 0.000 (0.049)   Loss 1.3220 (1.2647)   Prec@1 49.000 (54.202)   Prec@5 91.000 (94.449)   [2025-10-24 08:10:59]
  **Train** Prec@1 54.490 Prec@5 94.418 Error@1 45.510
  **Test** Prec@1 63.090 Prec@5 96.720 Error@1 36.910
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:11:21] [Epoch=011/040] [Need: 00:27:40] [LR=0.0100] [Best : Accuracy=63.09, Error=36.91]
  Epoch: [011][000/500]   Time 19.420 (19.420)   Data 19.355 (19.355)   Loss 1.3279 (1.3279)   Prec@1 50.000 (50.000)   Prec@5 97.000 (97.000)   [2025-10-24 08:11:41]
  Epoch: [011][100/500]   Time 0.017 (0.210)   Data 0.001 (0.192)   Loss 1.3447 (1.2418)   Prec@1 54.000 (54.792)   Prec@5 92.000 (94.525)   [2025-10-24 08:11:43]
  Epoch: [011][200/500]   Time 0.016 (0.114)   Data 0.000 (0.096)   Loss 1.3830 (1.2345)   Prec@1 49.000 (55.502)   Prec@5 95.000 (94.612)   [2025-10-24 08:11:44]
  Epoch: [011][300/500]   Time 0.019 (0.081)   Data 0.000 (0.064)   Loss 1.2127 (1.2273)   Prec@1 54.000 (55.678)   Prec@5 97.000 (94.638)   [2025-10-24 08:11:46]
  Epoch: [011][400/500]   Time 0.017 (0.065)   Data 0.000 (0.048)   Loss 1.4681 (1.2225)   Prec@1 51.000 (55.741)   Prec@5 92.000 (94.718)   [2025-10-24 08:11:48]
  **Train** Prec@1 55.898 Prec@5 94.734 Error@1 44.102
  **Test** Prec@1 64.950 Prec@5 96.760 Error@1 35.050
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:12:10] [Epoch=012/040] [Need: 00:26:22] [LR=0.0100] [Best : Accuracy=64.95, Error=35.05]
  Epoch: [012][000/500]   Time 21.583 (21.583)   Data 21.517 (21.517)   Loss 1.0108 (1.0108)   Prec@1 65.000 (65.000)   Prec@5 96.000 (96.000)   [2025-10-24 08:12:32]
  Epoch: [012][100/500]   Time 0.016 (0.232)   Data 0.000 (0.213)   Loss 1.1922 (1.1830)   Prec@1 58.000 (57.683)   Prec@5 94.000 (95.436)   [2025-10-24 08:12:33]
  Epoch: [012][200/500]   Time 0.016 (0.125)   Data 0.000 (0.107)   Loss 1.2140 (1.1887)   Prec@1 62.000 (57.453)   Prec@5 96.000 (95.264)   [2025-10-24 08:12:35]
  Epoch: [012][300/500]   Time 0.018 (0.089)   Data 0.000 (0.072)   Loss 1.0797 (1.1835)   Prec@1 67.000 (57.907)   Prec@5 97.000 (95.189)   [2025-10-24 08:12:37]
  Epoch: [012][400/500]   Time 0.019 (0.071)   Data 0.000 (0.054)   Loss 0.9556 (1.1816)   Prec@1 61.000 (57.918)   Prec@5 99.000 (95.145)   [2025-10-24 08:12:38]
  **Train** Prec@1 57.880 Prec@5 95.146 Error@1 42.120
  **Test** Prec@1 65.240 Prec@5 96.930 Error@1 34.760
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:13:02] [Epoch=013/040] [Need: 00:25:16] [LR=0.0100] [Best : Accuracy=65.24, Error=34.76]
  Epoch: [013][000/500]   Time 21.845 (21.845)   Data 21.778 (21.778)   Loss 1.1259 (1.1259)   Prec@1 66.000 (66.000)   Prec@5 98.000 (98.000)   [2025-10-24 08:13:24]
  Epoch: [013][100/500]   Time 0.020 (0.235)   Data 0.000 (0.216)   Loss 1.0583 (1.1580)   Prec@1 62.000 (58.396)   Prec@5 96.000 (95.436)   [2025-10-24 08:13:26]
  Epoch: [013][200/500]   Time 0.016 (0.126)   Data 0.000 (0.109)   Loss 1.0839 (1.1469)   Prec@1 66.000 (58.478)   Prec@5 94.000 (95.567)   [2025-10-24 08:13:27]
  Epoch: [013][300/500]   Time 0.017 (0.090)   Data 0.000 (0.073)   Loss 1.1550 (1.1508)   Prec@1 61.000 (58.645)   Prec@5 94.000 (95.455)   [2025-10-24 08:13:29]
  Epoch: [013][400/500]   Time 0.017 (0.072)   Data 0.000 (0.054)   Loss 1.1361 (1.1514)   Prec@1 59.000 (58.673)   Prec@5 93.000 (95.359)   [2025-10-24 08:13:31]
  **Train** Prec@1 58.838 Prec@5 95.410 Error@1 41.162
  **Test** Prec@1 67.770 Prec@5 97.440 Error@1 32.230
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:13:54] [Epoch=014/040] [Need: 00:24:13] [LR=0.0100] [Best : Accuracy=67.77, Error=32.23]
  Epoch: [014][000/500]   Time 21.019 (21.019)   Data 20.953 (20.953)   Loss 1.1178 (1.1178)   Prec@1 57.000 (57.000)   Prec@5 98.000 (98.000)   [2025-10-24 08:14:15]
  Epoch: [014][100/500]   Time 0.017 (0.225)   Data 0.000 (0.208)   Loss 1.0097 (1.1392)   Prec@1 67.000 (59.842)   Prec@5 94.000 (95.168)   [2025-10-24 08:14:17]
  Epoch: [014][200/500]   Time 0.018 (0.122)   Data 0.000 (0.104)   Loss 1.1349 (1.1247)   Prec@1 62.000 (60.214)   Prec@5 97.000 (95.542)   [2025-10-24 08:14:19]
  Epoch: [014][300/500]   Time 0.018 (0.087)   Data 0.000 (0.070)   Loss 1.0654 (1.1199)   Prec@1 60.000 (60.445)   Prec@5 97.000 (95.631)   [2025-10-24 08:14:20]
  Epoch: [014][400/500]   Time 0.016 (0.069)   Data 0.001 (0.052)   Loss 1.0504 (1.1146)   Prec@1 63.000 (60.479)   Prec@5 95.000 (95.753)   [2025-10-24 08:14:22]
  **Train** Prec@1 60.382 Prec@5 95.736 Error@1 39.618
  **Test** Prec@1 68.550 Prec@5 97.650 Error@1 31.450
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:14:45] [Epoch=015/040] [Need: 00:23:09] [LR=0.0100] [Best : Accuracy=68.55, Error=31.45]
  Epoch: [015][000/500]   Time 19.922 (19.922)   Data 19.855 (19.855)   Loss 0.9961 (0.9961)   Prec@1 62.000 (62.000)   Prec@5 99.000 (99.000)   [2025-10-24 08:15:05]
  Epoch: [015][100/500]   Time 0.016 (0.215)   Data 0.000 (0.197)   Loss 1.1163 (1.1043)   Prec@1 59.000 (60.149)   Prec@5 99.000 (95.832)   [2025-10-24 08:15:07]
  Epoch: [015][200/500]   Time 0.015 (0.116)   Data 0.001 (0.099)   Loss 1.1211 (1.0999)   Prec@1 56.000 (60.358)   Prec@5 96.000 (95.896)   [2025-10-24 08:15:09]
  Epoch: [015][300/500]   Time 0.019 (0.083)   Data 0.000 (0.066)   Loss 1.0188 (1.0920)   Prec@1 66.000 (61.007)   Prec@5 94.000 (95.811)   [2025-10-24 08:15:11]
  Epoch: [015][400/500]   Time 0.016 (0.067)   Data 0.000 (0.050)   Loss 1.0886 (1.0902)   Prec@1 61.000 (61.080)   Prec@5 99.000 (95.875)   [2025-10-24 08:15:12]
  **Train** Prec@1 61.316 Prec@5 95.872 Error@1 38.684
  **Test** Prec@1 68.570 Prec@5 97.700 Error@1 31.430
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:15:35] [Epoch=016/040] [Need: 00:22:05] [LR=0.0100] [Best : Accuracy=68.57, Error=31.43]
  Epoch: [016][000/500]   Time 20.126 (20.126)   Data 20.062 (20.062)   Loss 1.1003 (1.1003)   Prec@1 57.000 (57.000)   Prec@5 97.000 (97.000)   [2025-10-24 08:15:55]
  Epoch: [016][100/500]   Time 0.016 (0.217)   Data 0.000 (0.199)   Loss 1.1429 (1.0506)   Prec@1 56.000 (62.218)   Prec@5 99.000 (96.129)   [2025-10-24 08:15:57]
  Epoch: [016][200/500]   Time 0.019 (0.118)   Data 0.000 (0.100)   Loss 1.0822 (1.0486)   Prec@1 60.000 (62.682)   Prec@5 96.000 (96.249)   [2025-10-24 08:15:59]
  Epoch: [016][300/500]   Time 0.015 (0.084)   Data 0.000 (0.067)   Loss 0.8986 (1.0562)   Prec@1 65.000 (62.452)   Prec@5 99.000 (96.163)   [2025-10-24 08:16:01]
  Epoch: [016][400/500]   Time 0.016 (0.067)   Data 0.000 (0.050)   Loss 1.2449 (1.0537)   Prec@1 57.000 (62.631)   Prec@5 91.000 (96.160)   [2025-10-24 08:16:02]
  **Train** Prec@1 62.788 Prec@5 96.218 Error@1 37.212
  **Test** Prec@1 70.650 Prec@5 97.840 Error@1 29.350
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:16:26] [Epoch=017/040] [Need: 00:21:03] [LR=0.0100] [Best : Accuracy=70.65, Error=29.35]
  Epoch: [017][000/500]   Time 20.347 (20.347)   Data 20.283 (20.283)   Loss 0.9249 (0.9249)   Prec@1 66.000 (66.000)   Prec@5 98.000 (98.000)   [2025-10-24 08:16:46]
  Epoch: [017][100/500]   Time 0.017 (0.219)   Data 0.000 (0.201)   Loss 0.9087 (1.0306)   Prec@1 72.000 (63.980)   Prec@5 96.000 (96.267)   [2025-10-24 08:16:48]
  Epoch: [017][200/500]   Time 0.017 (0.118)   Data 0.000 (0.101)   Loss 1.1939 (1.0449)   Prec@1 52.000 (63.498)   Prec@5 94.000 (96.080)   [2025-10-24 08:16:49]
  Epoch: [017][300/500]   Time 0.019 (0.085)   Data 0.000 (0.068)   Loss 1.0912 (1.0404)   Prec@1 63.000 (63.488)   Prec@5 95.000 (96.163)   [2025-10-24 08:16:51]
  Epoch: [017][400/500]   Time 0.019 (0.068)   Data 0.001 (0.051)   Loss 1.0278 (1.0353)   Prec@1 66.000 (63.603)   Prec@5 98.000 (96.217)   [2025-10-24 08:16:53]
  **Train** Prec@1 63.600 Prec@5 96.216 Error@1 36.400
  **Test** Prec@1 72.280 Prec@5 97.960 Error@1 27.720
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:17:17] [Epoch=018/040] [Need: 00:20:04] [LR=0.0100] [Best : Accuracy=72.28, Error=27.72]
  Epoch: [018][000/500]   Time 20.196 (20.196)   Data 20.131 (20.131)   Loss 1.2360 (1.2360)   Prec@1 60.000 (60.000)   Prec@5 95.000 (95.000)   [2025-10-24 08:17:37]
  Epoch: [018][100/500]   Time 0.014 (0.219)   Data 0.000 (0.199)   Loss 1.0416 (1.0065)   Prec@1 53.000 (64.663)   Prec@5 99.000 (96.604)   [2025-10-24 08:17:39]
  Epoch: [018][200/500]   Time 0.016 (0.118)   Data 0.000 (0.100)   Loss 1.1617 (1.0182)   Prec@1 57.000 (64.239)   Prec@5 98.000 (96.537)   [2025-10-24 08:17:41]
  Epoch: [018][300/500]   Time 0.019 (0.085)   Data 0.000 (0.067)   Loss 0.8268 (1.0111)   Prec@1 73.000 (64.551)   Prec@5 98.000 (96.445)   [2025-10-24 08:17:42]
  Epoch: [018][400/500]   Time 0.016 (0.068)   Data 0.000 (0.050)   Loss 1.0873 (1.0052)   Prec@1 62.000 (64.599)   Prec@5 92.000 (96.559)   [2025-10-24 08:17:44]
  **Train** Prec@1 64.640 Prec@5 96.492 Error@1 35.360
  **Test** Prec@1 73.130 Prec@5 98.240 Error@1 26.870
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:18:07] [Epoch=019/040] [Need: 00:19:04] [LR=0.0100] [Best : Accuracy=73.13, Error=26.87]
  Epoch: [019][000/500]   Time 20.616 (20.616)   Data 20.550 (20.550)   Loss 0.9001 (0.9001)   Prec@1 73.000 (73.000)   Prec@5 99.000 (99.000)   [2025-10-24 08:18:28]
  Epoch: [019][100/500]   Time 0.016 (0.222)   Data 0.000 (0.204)   Loss 0.8667 (0.9730)   Prec@1 68.000 (65.743)   Prec@5 98.000 (96.901)   [2025-10-24 08:18:30]
  Epoch: [019][200/500]   Time 0.015 (0.120)   Data 0.000 (0.102)   Loss 0.8842 (0.9788)   Prec@1 65.000 (65.905)   Prec@5 97.000 (96.771)   [2025-10-24 08:18:32]
  Epoch: [019][300/500]   Time 0.017 (0.086)   Data 0.000 (0.068)   Loss 0.9147 (0.9765)   Prec@1 65.000 (66.030)   Prec@5 97.000 (96.821)   [2025-10-24 08:18:33]
  Epoch: [019][400/500]   Time 0.019 (0.069)   Data 0.000 (0.051)   Loss 0.8692 (0.9801)   Prec@1 72.000 (65.853)   Prec@5 97.000 (96.778)   [2025-10-24 08:18:35]
  **Train** Prec@1 65.992 Prec@5 96.766 Error@1 34.008
  **Test** Prec@1 73.430 Prec@5 97.960 Error@1 26.570
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:18:58] [Epoch=020/040] [Need: 00:18:06] [LR=0.0100] [Best : Accuracy=73.43, Error=26.57]
  Epoch: [020][000/500]   Time 20.966 (20.966)   Data 20.901 (20.901)   Loss 0.9702 (0.9702)   Prec@1 66.000 (66.000)   Prec@5 98.000 (98.000)   [2025-10-24 08:19:19]
  Epoch: [020][100/500]   Time 0.019 (0.226)   Data 0.000 (0.207)   Loss 0.9106 (0.9786)   Prec@1 64.000 (65.822)   Prec@5 96.000 (96.505)   [2025-10-24 08:19:21]
  Epoch: [020][200/500]   Time 0.015 (0.122)   Data 0.000 (0.104)   Loss 1.0715 (0.9737)   Prec@1 58.000 (65.970)   Prec@5 96.000 (96.682)   [2025-10-24 08:19:22]
  Epoch: [020][300/500]   Time 0.016 (0.087)   Data 0.000 (0.070)   Loss 1.0083 (0.9637)   Prec@1 64.000 (66.525)   Prec@5 96.000 (96.741)   [2025-10-24 08:19:24]
  Epoch: [020][400/500]   Time 0.017 (0.070)   Data 0.000 (0.052)   Loss 1.1441 (0.9666)   Prec@1 60.000 (66.314)   Prec@5 89.000 (96.691)   [2025-10-24 08:19:26]
  **Train** Prec@1 66.370 Prec@5 96.742 Error@1 33.630
  **Test** Prec@1 74.620 Prec@5 98.360 Error@1 25.380
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:19:49] [Epoch=021/040] [Need: 00:17:09] [LR=0.0100] [Best : Accuracy=74.62, Error=25.38]
  Epoch: [021][000/500]   Time 19.845 (19.845)   Data 19.780 (19.780)   Loss 0.9667 (0.9667)   Prec@1 62.000 (62.000)   Prec@5 98.000 (98.000)   [2025-10-24 08:20:09]
  Epoch: [021][100/500]   Time 0.019 (0.215)   Data 0.000 (0.196)   Loss 0.8501 (0.9571)   Prec@1 69.000 (66.564)   Prec@5 100.000 (96.832)   [2025-10-24 08:20:11]
  Epoch: [021][200/500]   Time 0.018 (0.117)   Data 0.000 (0.099)   Loss 0.9850 (0.9503)   Prec@1 62.000 (66.741)   Prec@5 96.000 (96.826)   [2025-10-24 08:20:13]
  Epoch: [021][300/500]   Time 0.019 (0.083)   Data 0.001 (0.066)   Loss 0.8237 (0.9413)   Prec@1 76.000 (67.236)   Prec@5 99.000 (96.784)   [2025-10-24 08:20:14]
  Epoch: [021][400/500]   Time 0.016 (0.067)   Data 0.000 (0.049)   Loss 1.0615 (0.9375)   Prec@1 67.000 (67.264)   Prec@5 96.000 (96.803)   [2025-10-24 08:20:16]
  **Train** Prec@1 67.440 Prec@5 96.850 Error@1 32.560
  **Test** Prec@1 75.220 Prec@5 98.330 Error@1 24.780
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:20:39] [Epoch=022/040] [Need: 00:16:11] [LR=0.0100] [Best : Accuracy=75.22, Error=24.78]
  Epoch: [022][000/500]   Time 20.478 (20.478)   Data 20.409 (20.409)   Loss 0.9347 (0.9347)   Prec@1 69.000 (69.000)   Prec@5 96.000 (96.000)   [2025-10-24 08:21:00]
  Epoch: [022][100/500]   Time 0.019 (0.222)   Data 0.000 (0.202)   Loss 0.9275 (0.9145)   Prec@1 67.000 (68.446)   Prec@5 95.000 (97.059)   [2025-10-24 08:21:02]
  Epoch: [022][200/500]   Time 0.018 (0.120)   Data 0.000 (0.102)   Loss 0.9501 (0.9120)   Prec@1 69.000 (68.527)   Prec@5 97.000 (96.985)   [2025-10-24 08:21:03]
  Epoch: [022][300/500]   Time 0.016 (0.086)   Data 0.000 (0.068)   Loss 0.9366 (0.9171)   Prec@1 71.000 (68.326)   Prec@5 98.000 (97.040)   [2025-10-24 08:21:05]
  Epoch: [022][400/500]   Time 0.015 (0.069)   Data 0.000 (0.051)   Loss 0.9155 (0.9217)   Prec@1 63.000 (68.200)   Prec@5 99.000 (96.988)   [2025-10-24 08:21:07]
  **Train** Prec@1 68.180 Prec@5 97.008 Error@1 31.820
  **Test** Prec@1 76.020 Prec@5 98.410 Error@1 23.980
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:21:31] [Epoch=023/040] [Need: 00:15:16] [LR=0.0100] [Best : Accuracy=76.02, Error=23.98]
  Epoch: [023][000/500]   Time 21.069 (21.069)   Data 21.003 (21.003)   Loss 0.8344 (0.8344)   Prec@1 72.000 (72.000)   Prec@5 96.000 (96.000)   [2025-10-24 08:21:52]
  Epoch: [023][100/500]   Time 0.017 (0.228)   Data 0.000 (0.208)   Loss 0.9528 (0.8993)   Prec@1 67.000 (68.901)   Prec@5 98.000 (96.941)   [2025-10-24 08:21:54]
  Epoch: [023][200/500]   Time 0.020 (0.123)   Data 0.001 (0.105)   Loss 1.0941 (0.9017)   Prec@1 62.000 (68.886)   Prec@5 95.000 (96.995)   [2025-10-24 08:21:56]
  Epoch: [023][300/500]   Time 0.017 (0.088)   Data 0.001 (0.070)   Loss 0.9713 (0.9062)   Prec@1 67.000 (68.551)   Prec@5 94.000 (97.066)   [2025-10-24 08:21:57]
  Epoch: [023][400/500]   Time 0.017 (0.070)   Data 0.000 (0.053)   Loss 1.0085 (0.9003)   Prec@1 63.000 (68.701)   Prec@5 92.000 (97.105)   [2025-10-24 08:21:59]
  **Train** Prec@1 68.734 Prec@5 97.106 Error@1 31.266
  **Test** Prec@1 75.460 Prec@5 98.540 Error@1 24.540

==>>[2025-10-24 08:22:22] [Epoch=024/040] [Need: 00:14:20] [LR=0.0100] [Best : Accuracy=76.02, Error=23.98]
  Epoch: [024][000/500]   Time 20.701 (20.701)   Data 20.637 (20.637)   Loss 0.8600 (0.8600)   Prec@1 73.000 (73.000)   Prec@5 100.000 (100.000)   [2025-10-24 08:22:42]
  Epoch: [024][100/500]   Time 0.017 (0.224)   Data 0.000 (0.205)   Loss 1.0667 (0.8726)   Prec@1 62.000 (69.515)   Prec@5 97.000 (97.713)   [2025-10-24 08:22:44]
  Epoch: [024][200/500]   Time 0.018 (0.121)   Data 0.000 (0.103)   Loss 0.8794 (0.8831)   Prec@1 73.000 (69.214)   Prec@5 96.000 (97.383)   [2025-10-24 08:22:46]
  Epoch: [024][300/500]   Time 0.020 (0.087)   Data 0.001 (0.069)   Loss 0.9007 (0.8853)   Prec@1 72.000 (69.169)   Prec@5 97.000 (97.359)   [2025-10-24 08:22:48]
  Epoch: [024][400/500]   Time 0.015 (0.069)   Data 0.000 (0.052)   Loss 1.2120 (0.8870)   Prec@1 56.000 (69.192)   Prec@5 98.000 (97.267)   [2025-10-24 08:22:49]
  **Train** Prec@1 69.384 Prec@5 97.238 Error@1 30.616
  **Test** Prec@1 77.310 Prec@5 98.660 Error@1 22.690
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:23:13] [Epoch=025/040] [Need: 00:13:24] [LR=0.0010] [Best : Accuracy=77.31, Error=22.69]
  Epoch: [025][000/500]   Time 20.781 (20.781)   Data 20.716 (20.716)   Loss 0.9042 (0.9042)   Prec@1 68.000 (68.000)   Prec@5 100.000 (100.000)   [2025-10-24 08:23:34]
  Epoch: [025][100/500]   Time 0.020 (0.226)   Data 0.001 (0.205)   Loss 0.7171 (0.8289)   Prec@1 75.000 (70.970)   Prec@5 100.000 (97.455)   [2025-10-24 08:23:36]
  Epoch: [025][200/500]   Time 0.015 (0.122)   Data 0.000 (0.103)   Loss 1.0120 (0.8334)   Prec@1 64.000 (70.975)   Prec@5 96.000 (97.512)   [2025-10-24 08:23:37]
  Epoch: [025][300/500]   Time 0.018 (0.087)   Data 0.002 (0.069)   Loss 0.6529 (0.8298)   Prec@1 76.000 (71.169)   Prec@5 100.000 (97.561)   [2025-10-24 08:23:39]
  Epoch: [025][400/500]   Time 0.016 (0.070)   Data 0.000 (0.052)   Loss 0.8752 (0.8245)   Prec@1 70.000 (71.397)   Prec@5 99.000 (97.561)   [2025-10-24 08:23:41]
  **Train** Prec@1 71.436 Prec@5 97.604 Error@1 28.564
  **Test** Prec@1 78.560 Prec@5 98.780 Error@1 21.440
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:24:04] [Epoch=026/040] [Need: 00:12:29] [LR=0.0010] [Best : Accuracy=78.56, Error=21.44]
  Epoch: [026][000/500]   Time 20.019 (20.019)   Data 19.952 (19.952)   Loss 0.7641 (0.7641)   Prec@1 75.000 (75.000)   Prec@5 96.000 (96.000)   [2025-10-24 08:24:24]
  Epoch: [026][100/500]   Time 0.016 (0.216)   Data 0.000 (0.198)   Loss 0.9548 (0.8035)   Prec@1 64.000 (72.287)   Prec@5 97.000 (97.733)   [2025-10-24 08:24:25]
  Epoch: [026][200/500]   Time 0.017 (0.117)   Data 0.000 (0.099)   Loss 0.8558 (0.8001)   Prec@1 68.000 (72.527)   Prec@5 98.000 (97.731)   [2025-10-24 08:24:27]
  Epoch: [026][300/500]   Time 0.017 (0.084)   Data 0.000 (0.066)   Loss 0.9178 (0.7999)   Prec@1 67.000 (72.658)   Prec@5 98.000 (97.734)   [2025-10-24 08:24:29]
  Epoch: [026][400/500]   Time 0.015 (0.067)   Data 0.001 (0.050)   Loss 0.8440 (0.7978)   Prec@1 70.000 (72.611)   Prec@5 98.000 (97.761)   [2025-10-24 08:24:30]
  **Train** Prec@1 72.566 Prec@5 97.766 Error@1 27.434
  **Test** Prec@1 78.570 Prec@5 98.700 Error@1 21.430
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:24:54] [Epoch=027/040] [Need: 00:11:34] [LR=0.0010] [Best : Accuracy=78.57, Error=21.43]
  Epoch: [027][000/500]   Time 20.363 (20.363)   Data 20.298 (20.298)   Loss 0.8855 (0.8855)   Prec@1 73.000 (73.000)   Prec@5 97.000 (97.000)   [2025-10-24 08:25:15]
  Epoch: [027][100/500]   Time 0.016 (0.220)   Data 0.000 (0.201)   Loss 0.7942 (0.7894)   Prec@1 70.000 (72.614)   Prec@5 98.000 (97.802)   [2025-10-24 08:25:16]
  Epoch: [027][200/500]   Time 0.017 (0.118)   Data 0.000 (0.101)   Loss 0.8718 (0.7971)   Prec@1 70.000 (72.512)   Prec@5 96.000 (97.716)   [2025-10-24 08:25:18]
  Epoch: [027][300/500]   Time 0.017 (0.085)   Data 0.000 (0.068)   Loss 0.7124 (0.7940)   Prec@1 77.000 (72.751)   Prec@5 99.000 (97.734)   [2025-10-24 08:25:20]
  Epoch: [027][400/500]   Time 0.017 (0.068)   Data 0.000 (0.051)   Loss 0.7063 (0.7945)   Prec@1 80.000 (72.830)   Prec@5 100.000 (97.741)   [2025-10-24 08:25:21]
  **Train** Prec@1 72.886 Prec@5 97.794 Error@1 27.114
  **Test** Prec@1 78.660 Prec@5 98.760 Error@1 21.340
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:25:45] [Epoch=028/040] [Need: 00:10:39] [LR=0.0010] [Best : Accuracy=78.66, Error=21.34]
  Epoch: [028][000/500]   Time 20.525 (20.525)   Data 20.460 (20.460)   Loss 0.9106 (0.9106)   Prec@1 70.000 (70.000)   Prec@5 99.000 (99.000)   [2025-10-24 08:26:05]
  Epoch: [028][100/500]   Time 0.016 (0.221)   Data 0.000 (0.203)   Loss 0.6704 (0.7933)   Prec@1 75.000 (72.307)   Prec@5 99.000 (97.851)   [2025-10-24 08:26:07]
  Epoch: [028][200/500]   Time 0.016 (0.120)   Data 0.000 (0.102)   Loss 0.6884 (0.7913)   Prec@1 76.000 (72.731)   Prec@5 97.000 (97.786)   [2025-10-24 08:26:09]
  Epoch: [028][300/500]   Time 0.020 (0.086)   Data 0.000 (0.068)   Loss 0.7998 (0.7874)   Prec@1 69.000 (72.990)   Prec@5 98.000 (97.850)   [2025-10-24 08:26:11]
  Epoch: [028][400/500]   Time 0.018 (0.069)   Data 0.000 (0.051)   Loss 0.7095 (0.7855)   Prec@1 79.000 (73.010)   Prec@5 98.000 (97.845)   [2025-10-24 08:26:13]
  **Train** Prec@1 73.068 Prec@5 97.866 Error@1 26.932
  **Test** Prec@1 78.790 Prec@5 98.850 Error@1 21.210
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:26:36] [Epoch=029/040] [Need: 00:09:45] [LR=0.0010] [Best : Accuracy=78.79, Error=21.21]
  Epoch: [029][000/500]   Time 20.820 (20.820)   Data 20.751 (20.751)   Loss 0.8917 (0.8917)   Prec@1 69.000 (69.000)   Prec@5 99.000 (99.000)   [2025-10-24 08:26:57]
  Epoch: [029][100/500]   Time 0.019 (0.225)   Data 0.002 (0.206)   Loss 0.8801 (0.7746)   Prec@1 70.000 (73.129)   Prec@5 95.000 (97.911)   [2025-10-24 08:26:59]
  Epoch: [029][200/500]   Time 0.018 (0.122)   Data 0.001 (0.103)   Loss 0.7107 (0.7702)   Prec@1 74.000 (73.458)   Prec@5 99.000 (97.945)   [2025-10-24 08:27:01]
  Epoch: [029][300/500]   Time 0.021 (0.087)   Data 0.000 (0.069)   Loss 0.9470 (0.7666)   Prec@1 68.000 (73.658)   Prec@5 98.000 (97.997)   [2025-10-24 08:27:02]
  Epoch: [029][400/500]   Time 0.020 (0.070)   Data 0.000 (0.052)   Loss 0.8369 (0.7707)   Prec@1 74.000 (73.616)   Prec@5 97.000 (97.940)   [2025-10-24 08:27:04]
  **Train** Prec@1 73.470 Prec@5 97.872 Error@1 26.530
  **Test** Prec@1 78.470 Prec@5 98.760 Error@1 21.530

==>>[2025-10-24 08:27:27] [Epoch=030/040] [Need: 00:08:51] [LR=0.0010] [Best : Accuracy=78.79, Error=21.21]
  Epoch: [030][000/500]   Time 20.452 (20.452)   Data 20.387 (20.387)   Loss 0.6903 (0.6903)   Prec@1 76.000 (76.000)   Prec@5 98.000 (98.000)   [2025-10-24 08:27:48]
  Epoch: [030][100/500]   Time 0.018 (0.220)   Data 0.001 (0.202)   Loss 0.7021 (0.7835)   Prec@1 76.000 (72.762)   Prec@5 99.000 (98.020)   [2025-10-24 08:27:50]
  Epoch: [030][200/500]   Time 0.017 (0.119)   Data 0.001 (0.102)   Loss 0.6498 (0.7774)   Prec@1 77.000 (73.114)   Prec@5 99.000 (97.891)   [2025-10-24 08:27:51]
  Epoch: [030][300/500]   Time 0.018 (0.085)   Data 0.000 (0.068)   Loss 0.7513 (0.7713)   Prec@1 78.000 (73.316)   Prec@5 97.000 (97.950)   [2025-10-24 08:27:53]
  Epoch: [030][400/500]   Time 0.019 (0.068)   Data 0.000 (0.051)   Loss 0.8337 (0.7687)   Prec@1 73.000 (73.561)   Prec@5 99.000 (97.933)   [2025-10-24 08:27:55]
  **Train** Prec@1 73.520 Prec@5 97.924 Error@1 26.480
  **Test** Prec@1 79.110 Prec@5 98.770 Error@1 20.890
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:28:19] [Epoch=031/040] [Need: 00:07:58] [LR=0.0010] [Best : Accuracy=79.11, Error=20.89]
  Epoch: [031][000/500]   Time 20.784 (20.784)   Data 20.717 (20.717)   Loss 0.5989 (0.5989)   Prec@1 79.000 (79.000)   Prec@5 97.000 (97.000)   [2025-10-24 08:28:40]
  Epoch: [031][100/500]   Time 0.015 (0.225)   Data 0.000 (0.205)   Loss 0.9063 (0.7482)   Prec@1 69.000 (74.228)   Prec@5 98.000 (97.891)   [2025-10-24 08:28:41]
  Epoch: [031][200/500]   Time 0.020 (0.122)   Data 0.000 (0.103)   Loss 0.5780 (0.7550)   Prec@1 80.000 (73.950)   Prec@5 99.000 (98.050)   [2025-10-24 08:28:43]
  Epoch: [031][300/500]   Time 0.017 (0.087)   Data 0.001 (0.069)   Loss 0.6492 (0.7549)   Prec@1 77.000 (73.860)   Prec@5 98.000 (98.013)   [2025-10-24 08:28:45]
  Epoch: [031][400/500]   Time 0.018 (0.070)   Data 0.000 (0.052)   Loss 0.6482 (0.7591)   Prec@1 79.000 (73.796)   Prec@5 100.000 (97.993)   [2025-10-24 08:28:47]
  **Train** Prec@1 73.662 Prec@5 97.988 Error@1 26.338
  **Test** Prec@1 79.230 Prec@5 98.760 Error@1 20.770
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:29:11] [Epoch=032/040] [Need: 00:07:04] [LR=0.0010] [Best : Accuracy=79.23, Error=20.77]
  Epoch: [032][000/500]   Time 19.574 (19.574)   Data 19.509 (19.509)   Loss 0.8065 (0.8065)   Prec@1 71.000 (71.000)   Prec@5 98.000 (98.000)   [2025-10-24 08:29:30]
  Epoch: [032][100/500]   Time 0.013 (0.211)   Data 0.000 (0.193)   Loss 0.6406 (0.7673)   Prec@1 78.000 (73.762)   Prec@5 100.000 (98.069)   [2025-10-24 08:29:32]
  Epoch: [032][200/500]   Time 0.017 (0.114)   Data 0.000 (0.097)   Loss 0.7741 (0.7630)   Prec@1 76.000 (73.721)   Prec@5 99.000 (98.045)   [2025-10-24 08:29:34]
  Epoch: [032][300/500]   Time 0.017 (0.082)   Data 0.001 (0.065)   Loss 0.6913 (0.7625)   Prec@1 77.000 (73.731)   Prec@5 99.000 (98.056)   [2025-10-24 08:29:35]
  Epoch: [032][400/500]   Time 0.016 (0.066)   Data 0.001 (0.049)   Loss 0.7583 (0.7643)   Prec@1 74.000 (73.621)   Prec@5 98.000 (98.107)   [2025-10-24 08:29:37]
  **Train** Prec@1 73.622 Prec@5 98.080 Error@1 26.378
  **Test** Prec@1 79.230 Prec@5 98.830 Error@1 20.770
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:30:00] [Epoch=033/040] [Need: 00:06:10] [LR=0.0010] [Best : Accuracy=79.23, Error=20.77]
  Epoch: [033][000/500]   Time 20.862 (20.862)   Data 20.797 (20.797)   Loss 0.6837 (0.6837)   Prec@1 78.000 (78.000)   Prec@5 99.000 (99.000)   [2025-10-24 08:30:21]
  Epoch: [033][100/500]   Time 0.015 (0.226)   Data 0.001 (0.206)   Loss 0.8240 (0.7523)   Prec@1 75.000 (73.545)   Prec@5 97.000 (98.149)   [2025-10-24 08:30:23]
  Epoch: [033][200/500]   Time 0.017 (0.122)   Data 0.001 (0.104)   Loss 0.5423 (0.7554)   Prec@1 83.000 (73.433)   Prec@5 100.000 (98.129)   [2025-10-24 08:30:25]
  Epoch: [033][300/500]   Time 0.017 (0.087)   Data 0.000 (0.069)   Loss 0.8091 (0.7577)   Prec@1 73.000 (73.538)   Prec@5 96.000 (98.030)   [2025-10-24 08:30:26]
  Epoch: [033][400/500]   Time 0.016 (0.070)   Data 0.000 (0.052)   Loss 0.8013 (0.7600)   Prec@1 79.000 (73.579)   Prec@5 97.000 (98.015)   [2025-10-24 08:30:28]
  **Train** Prec@1 73.776 Prec@5 97.984 Error@1 26.224
  **Test** Prec@1 79.420 Prec@5 98.870 Error@1 20.580
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:30:52] [Epoch=034/040] [Need: 00:05:17] [LR=0.0010] [Best : Accuracy=79.42, Error=20.58]
  Epoch: [034][000/500]   Time 21.117 (21.117)   Data 21.050 (21.050)   Loss 0.7607 (0.7607)   Prec@1 73.000 (73.000)   Prec@5 99.000 (99.000)   [2025-10-24 08:31:13]
  Epoch: [034][100/500]   Time 0.020 (0.228)   Data 0.001 (0.209)   Loss 0.8648 (0.7610)   Prec@1 69.000 (73.762)   Prec@5 98.000 (97.723)   [2025-10-24 08:31:15]
  Epoch: [034][200/500]   Time 0.018 (0.123)   Data 0.001 (0.105)   Loss 0.8130 (0.7588)   Prec@1 75.000 (73.851)   Prec@5 99.000 (97.871)   [2025-10-24 08:31:17]
  Epoch: [034][300/500]   Time 0.020 (0.088)   Data 0.001 (0.070)   Loss 0.5615 (0.7583)   Prec@1 82.000 (73.857)   Prec@5 99.000 (97.904)   [2025-10-24 08:31:19]
  Epoch: [034][400/500]   Time 0.014 (0.071)   Data 0.000 (0.053)   Loss 0.9527 (0.7579)   Prec@1 68.000 (73.870)   Prec@5 97.000 (97.968)   [2025-10-24 08:31:21]
  **Train** Prec@1 74.036 Prec@5 97.978 Error@1 25.964
  **Test** Prec@1 79.560 Prec@5 98.840 Error@1 20.440
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:31:45] [Epoch=035/040] [Need: 00:04:24] [LR=0.0010] [Best : Accuracy=79.56, Error=20.44]
  Epoch: [035][000/500]   Time 20.500 (20.500)   Data 20.433 (20.433)   Loss 0.7597 (0.7597)   Prec@1 73.000 (73.000)   Prec@5 99.000 (99.000)   [2025-10-24 08:32:06]
  Epoch: [035][100/500]   Time 0.016 (0.220)   Data 0.000 (0.203)   Loss 0.9789 (0.7581)   Prec@1 67.000 (73.782)   Prec@5 95.000 (98.079)   [2025-10-24 08:32:08]
  Epoch: [035][200/500]   Time 0.017 (0.119)   Data 0.000 (0.102)   Loss 0.5706 (0.7496)   Prec@1 80.000 (73.965)   Prec@5 100.000 (98.119)   [2025-10-24 08:32:09]
  Epoch: [035][300/500]   Time 0.016 (0.086)   Data 0.000 (0.068)   Loss 0.7453 (0.7575)   Prec@1 74.000 (73.578)   Prec@5 98.000 (98.033)   [2025-10-24 08:32:11]
  Epoch: [035][400/500]   Time 0.016 (0.069)   Data 0.000 (0.051)   Loss 0.7412 (0.7559)   Prec@1 73.000 (73.843)   Prec@5 98.000 (97.995)   [2025-10-24 08:32:13]
  **Train** Prec@1 73.980 Prec@5 98.022 Error@1 26.020
  **Test** Prec@1 79.750 Prec@5 98.920 Error@1 20.250
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:32:36] [Epoch=036/040] [Need: 00:03:31] [LR=0.0010] [Best : Accuracy=79.75, Error=20.25]
  Epoch: [036][000/500]   Time 20.368 (20.368)   Data 20.301 (20.301)   Loss 0.6561 (0.6561)   Prec@1 77.000 (77.000)   Prec@5 100.000 (100.000)   [2025-10-24 08:32:57]
  Epoch: [036][100/500]   Time 0.018 (0.220)   Data 0.001 (0.201)   Loss 0.8180 (0.7426)   Prec@1 71.000 (74.901)   Prec@5 96.000 (98.099)   [2025-10-24 08:32:59]
  Epoch: [036][200/500]   Time 0.016 (0.119)   Data 0.000 (0.101)   Loss 0.6686 (0.7457)   Prec@1 77.000 (74.552)   Prec@5 98.000 (98.124)   [2025-10-24 08:33:00]
  Epoch: [036][300/500]   Time 0.018 (0.085)   Data 0.000 (0.068)   Loss 0.7784 (0.7462)   Prec@1 75.000 (74.435)   Prec@5 98.000 (98.116)   [2025-10-24 08:33:02]
  Epoch: [036][400/500]   Time 0.017 (0.068)   Data 0.000 (0.051)   Loss 0.6747 (0.7465)   Prec@1 73.000 (74.426)   Prec@5 100.000 (98.067)   [2025-10-24 08:33:04]
  **Train** Prec@1 74.330 Prec@5 98.016 Error@1 25.670
  **Test** Prec@1 79.780 Prec@5 98.840 Error@1 20.220
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 08:33:27] [Epoch=037/040] [Need: 00:02:38] [LR=0.0010] [Best : Accuracy=79.78, Error=20.22]
  Epoch: [037][000/500]   Time 21.834 (21.834)   Data 21.767 (21.767)   Loss 0.8050 (0.8050)   Prec@1 73.000 (73.000)   Prec@5 97.000 (97.000)   [2025-10-24 08:33:49]
  Epoch: [037][100/500]   Time 0.016 (0.235)   Data 0.000 (0.216)   Loss 0.7729 (0.7525)   Prec@1 69.000 (74.059)   Prec@5 97.000 (97.842)   [2025-10-24 08:33:51]
  Epoch: [037][200/500]   Time 0.018 (0.126)   Data 0.000 (0.108)   Loss 0.7578 (0.7514)   Prec@1 78.000 (73.945)   Prec@5 99.000 (97.945)   [2025-10-24 08:33:53]
  Epoch: [037][300/500]   Time 0.020 (0.090)   Data 0.000 (0.073)   Loss 0.7529 (0.7522)   Prec@1 70.000 (74.056)   Prec@5 98.000 (97.973)   [2025-10-24 08:33:54]
  Epoch: [037][400/500]   Time 0.016 (0.072)   Data 0.001 (0.054)   Loss 0.6500 (0.7498)   Prec@1 77.000 (74.334)   Prec@5 99.000 (97.980)   [2025-10-24 08:33:56]
  **Train** Prec@1 74.402 Prec@5 98.034 Error@1 25.598
  **Test** Prec@1 79.770 Prec@5 98.950 Error@1 20.230

==>>[2025-10-24 08:34:22] [Epoch=038/040] [Need: 00:01:45] [LR=0.0010] [Best : Accuracy=79.78, Error=20.22]
  Epoch: [038][000/500]   Time 22.738 (22.738)   Data 22.671 (22.671)   Loss 0.9196 (0.9196)   Prec@1 63.000 (63.000)   Prec@5 96.000 (96.000)   [2025-10-24 08:34:45]
  Epoch: [038][100/500]   Time 0.017 (0.245)   Data 0.000 (0.225)   Loss 0.6618 (0.7391)   Prec@1 75.000 (74.921)   Prec@5 98.000 (98.218)   [2025-10-24 08:34:47]
  Epoch: [038][200/500]   Time 0.016 (0.132)   Data 0.000 (0.113)   Loss 0.6446 (0.7332)   Prec@1 75.000 (74.886)   Prec@5 98.000 (98.214)   [2025-10-24 08:34:48]
  Epoch: [038][300/500]   Time 0.018 (0.094)   Data 0.000 (0.076)   Loss 0.8198 (0.7367)   Prec@1 71.000 (74.841)   Prec@5 97.000 (98.116)   [2025-10-24 08:34:50]
  Epoch: [038][400/500]   Time 0.020 (0.075)   Data 0.000 (0.057)   Loss 0.6954 (0.7399)   Prec@1 80.000 (74.758)   Prec@5 98.000 (98.057)   [2025-10-24 08:34:52]
  **Train** Prec@1 74.734 Prec@5 98.018 Error@1 25.266
  **Test** Prec@1 79.630 Prec@5 98.940 Error@1 20.370

==>>[2025-10-24 08:35:18] [Epoch=039/040] [Need: 00:00:52] [LR=0.0010] [Best : Accuracy=79.78, Error=20.22]
  Epoch: [039][000/500]   Time 22.091 (22.091)   Data 22.026 (22.026)   Loss 1.0154 (1.0154)   Prec@1 70.000 (70.000)   Prec@5 98.000 (98.000)   [2025-10-24 08:35:40]
  Epoch: [039][100/500]   Time 0.018 (0.237)   Data 0.000 (0.218)   Loss 0.6345 (0.7417)   Prec@1 82.000 (74.792)   Prec@5 98.000 (98.079)   [2025-10-24 08:35:42]
  Epoch: [039][200/500]   Time 0.017 (0.128)   Data 0.000 (0.110)   Loss 0.9725 (0.7447)   Prec@1 73.000 (74.333)   Prec@5 96.000 (98.060)   [2025-10-24 08:35:44]
  Epoch: [039][300/500]   Time 0.017 (0.091)   Data 0.000 (0.073)   Loss 0.9177 (0.7406)   Prec@1 72.000 (74.555)   Prec@5 95.000 (98.083)   [2025-10-24 08:35:45]
  Epoch: [039][400/500]   Time 0.017 (0.073)   Data 0.000 (0.055)   Loss 0.8942 (0.7398)   Prec@1 67.000 (74.693)   Prec@5 96.000 (98.115)   [2025-10-24 08:35:47]
  **Train** Prec@1 74.806 Prec@5 98.116 Error@1 25.194
  **Test** Prec@1 80.070 Prec@5 98.880 Error@1 19.930
=> Obtain best accuracy, and update the best model
