save path : ./save/tinyvgg_quan/nominal_0.01
{'data_path': './dataset', 'arch': 'tinyvgg_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': False, 'clipping_coeff': 0.0, 'learning_rate': 0.01, 'manualSeed': 3970, 'save_path': './save/tinyvgg_quan/nominal_0.01', 'enable_bfa': False, 'resume': '', 'quan_bitwidth': None, 'reset_weight': False, 'evaluate': False, 'n_iter': 30, 'model_only': False, 'random_bfa': False, 'use_cuda': True}
Random Seed: 3970
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'tinyvgg_quan'
=> network :
 TinyVGG(
  (features): Sequential(
    (0): quan_Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Dropout2d(p=0.3, inplace=False)
    (6): quan_Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace=True)
    (8): quan_Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace=True)
    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (11): Dropout2d(p=0.3, inplace=False)
    (12): quan_Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): quan_Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Dropout2d(p=0.3, inplace=False)
  )
  (classifier): Sequential(
    (0): quan_Linear(in_features=2048, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): quan_Linear(in_features=128, out_features=10, bias=True)
  )
)
=> do not use any checkpoint for tinyvgg_quan model

==>>[2025-10-24 06:58:15] [Epoch=000/040] [Need: 00:00:00] [LR=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/500]   Time 25.375 (25.375)   Data 23.982 (23.982)   Loss 2.3011 (2.3011)   Prec@1 10.000 (10.000)   Prec@5 59.000 (59.000)   [2025-10-24 06:58:40]
  Epoch: [000][100/500]   Time 0.018 (0.270)   Data 0.000 (0.238)   Loss 2.3023 (2.3034)   Prec@1 11.000 (10.099)   Prec@5 50.000 (49.624)   [2025-10-24 06:58:42]
  Epoch: [000][200/500]   Time 0.017 (0.145)   Data 0.001 (0.120)   Loss 2.3050 (2.3030)   Prec@1 8.000 (9.900)   Prec@5 41.000 (49.930)   [2025-10-24 06:58:44]
  Epoch: [000][300/500]   Time 0.019 (0.103)   Data 0.000 (0.080)   Loss 2.3059 (2.3028)   Prec@1 8.000 (10.096)   Prec@5 44.000 (50.163)   [2025-10-24 06:58:46]
  Epoch: [000][400/500]   Time 0.019 (0.082)   Data 0.001 (0.060)   Loss 2.2688 (2.2992)   Prec@1 20.000 (10.810)   Prec@5 56.000 (51.761)   [2025-10-24 06:58:48]
  **Train** Prec@1 12.798 Prec@5 55.638 Error@1 87.202
  **Test** Prec@1 24.060 Prec@5 77.470 Error@1 75.940
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 06:59:14] [Epoch=001/040] [Need: 00:38:33] [LR=0.0100] [Best : Accuracy=24.06, Error=75.94]
  Epoch: [001][000/500]   Time 27.613 (27.613)   Data 27.548 (27.548)   Loss 2.0970 (2.0970)   Prec@1 30.000 (30.000)   Prec@5 76.000 (76.000)   [2025-10-24 06:59:42]
  Epoch: [001][100/500]   Time 0.017 (0.293)   Data 0.000 (0.273)   Loss 2.0324 (2.0651)   Prec@1 26.000 (24.149)   Prec@5 85.000 (76.931)   [2025-10-24 06:59:44]
  Epoch: [001][200/500]   Time 0.020 (0.157)   Data 0.000 (0.137)   Loss 1.9027 (2.0337)   Prec@1 30.000 (24.871)   Prec@5 87.000 (78.269)   [2025-10-24 06:59:46]
  Epoch: [001][300/500]   Time 0.018 (0.111)   Data 0.000 (0.092)   Loss 1.9424 (2.0073)   Prec@1 27.000 (24.924)   Prec@5 78.000 (79.708)   [2025-10-24 06:59:48]
  Epoch: [001][400/500]   Time 0.019 (0.088)   Data 0.001 (0.069)   Loss 1.9616 (1.9847)   Prec@1 28.000 (25.506)   Prec@5 83.000 (80.524)   [2025-10-24 06:59:50]
  **Train** Prec@1 26.136 Prec@5 81.256 Error@1 73.864
  **Test** Prec@1 32.050 Prec@5 86.700 Error@1 67.950
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 07:00:18] [Epoch=002/040] [Need: 00:38:57] [LR=0.0100] [Best : Accuracy=32.05, Error=67.95]
  Epoch: [002][000/500]   Time 22.669 (22.669)   Data 22.605 (22.605)   Loss 1.8873 (1.8873)   Prec@1 26.000 (26.000)   Prec@5 85.000 (85.000)   [2025-10-24 07:00:41]
  Epoch: [002][100/500]   Time 0.021 (0.245)   Data 0.001 (0.224)   Loss 1.8660 (1.8382)   Prec@1 35.000 (30.099)   Prec@5 81.000 (85.455)   [2025-10-24 07:00:43]
  Epoch: [002][200/500]   Time 0.020 (0.133)   Data 0.000 (0.113)   Loss 1.8276 (1.8216)   Prec@1 27.000 (30.776)   Prec@5 88.000 (85.881)   [2025-10-24 07:00:45]
  Epoch: [002][300/500]   Time 0.020 (0.095)   Data 0.000 (0.075)   Loss 1.7056 (1.8075)   Prec@1 40.000 (31.203)   Prec@5 82.000 (86.209)   [2025-10-24 07:00:47]
  Epoch: [002][400/500]   Time 0.019 (0.076)   Data 0.000 (0.057)   Loss 1.7649 (1.7964)   Prec@1 34.000 (31.746)   Prec@5 88.000 (86.441)   [2025-10-24 07:00:49]
  **Train** Prec@1 32.212 Prec@5 86.766 Error@1 67.788
  **Test** Prec@1 42.000 Prec@5 91.560 Error@1 58.000
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 07:01:14] [Epoch=003/040] [Need: 00:36:51] [LR=0.0100] [Best : Accuracy=42.00, Error=58.00]
  Epoch: [003][000/500]   Time 24.185 (24.185)   Data 24.106 (24.106)   Loss 1.8168 (1.8168)   Prec@1 32.000 (32.000)   Prec@5 88.000 (88.000)   [2025-10-24 07:01:39]
  Epoch: [003][100/500]   Time 0.018 (0.260)   Data 0.000 (0.239)   Loss 1.8277 (1.7169)   Prec@1 25.000 (35.554)   Prec@5 83.000 (87.980)   [2025-10-24 07:01:41]
  Epoch: [003][200/500]   Time 0.021 (0.140)   Data 0.000 (0.120)   Loss 1.9271 (1.7019)   Prec@1 28.000 (36.388)   Prec@5 80.000 (88.323)   [2025-10-24 07:01:43]
  Epoch: [003][300/500]   Time 0.023 (0.100)   Data 0.000 (0.081)   Loss 1.5924 (1.6913)   Prec@1 43.000 (36.721)   Prec@5 93.000 (88.578)   [2025-10-24 07:01:45]
  Epoch: [003][400/500]   Time 0.018 (0.080)   Data 0.000 (0.061)   Loss 1.6554 (1.6827)   Prec@1 33.000 (36.975)   Prec@5 95.000 (88.676)   [2025-10-24 07:01:47]
  **Train** Prec@1 37.396 Prec@5 88.798 Error@1 62.604
  **Test** Prec@1 44.780 Prec@5 92.690 Error@1 55.220
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 07:02:14] [Epoch=004/040] [Need: 00:35:47] [LR=0.0100] [Best : Accuracy=44.78, Error=55.22]
  Epoch: [004][000/500]   Time 23.592 (23.592)   Data 23.526 (23.526)   Loss 1.6616 (1.6616)   Prec@1 35.000 (35.000)   Prec@5 86.000 (86.000)   [2025-10-24 07:02:37]
  Epoch: [004][100/500]   Time 0.021 (0.254)   Data 0.001 (0.233)   Loss 1.7062 (1.6161)   Prec@1 36.000 (40.465)   Prec@5 88.000 (90.257)   [2025-10-24 07:02:39]
  Epoch: [004][200/500]   Time 0.018 (0.137)   Data 0.000 (0.117)   Loss 1.6462 (1.6058)   Prec@1 36.000 (40.637)   Prec@5 84.000 (90.433)   [2025-10-24 07:02:41]
  Epoch: [004][300/500]   Time 0.020 (0.098)   Data 0.000 (0.078)   Loss 1.4559 (1.5959)   Prec@1 47.000 (40.993)   Prec@5 92.000 (90.425)   [2025-10-24 07:02:43]
  Epoch: [004][400/500]   Time 0.018 (0.078)   Data 0.000 (0.059)   Loss 1.5814 (1.5872)   Prec@1 38.000 (41.157)   Prec@5 92.000 (90.574)   [2025-10-24 07:02:45]
  **Train** Prec@1 41.656 Prec@5 90.758 Error@1 58.344
  **Test** Prec@1 49.540 Prec@5 94.080 Error@1 50.460
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 07:03:12] [Epoch=005/040] [Need: 00:34:40] [LR=0.0100] [Best : Accuracy=49.54, Error=50.46]
  Epoch: [005][000/500]   Time 24.475 (24.475)   Data 24.397 (24.397)   Loss 1.5029 (1.5029)   Prec@1 43.000 (43.000)   Prec@5 93.000 (93.000)   [2025-10-24 07:03:37]
  Epoch: [005][100/500]   Time 0.020 (0.264)   Data 0.001 (0.242)   Loss 1.5565 (1.5132)   Prec@1 46.000 (44.218)   Prec@5 86.000 (91.396)   [2025-10-24 07:03:39]
  Epoch: [005][200/500]   Time 0.018 (0.142)   Data 0.000 (0.122)   Loss 1.4731 (1.5069)   Prec@1 45.000 (44.318)   Prec@5 91.000 (91.408)   [2025-10-24 07:03:41]
  Epoch: [005][300/500]   Time 0.021 (0.101)   Data 0.000 (0.081)   Loss 1.6277 (1.5020)   Prec@1 42.000 (44.435)   Prec@5 90.000 (91.535)   [2025-10-24 07:03:43]
  Epoch: [005][400/500]   Time 0.021 (0.081)   Data 0.000 (0.061)   Loss 1.4765 (1.4947)   Prec@1 45.000 (44.788)   Prec@5 89.000 (91.574)   [2025-10-24 07:03:45]
  **Train** Prec@1 45.210 Prec@5 91.840 Error@1 54.790
  **Test** Prec@1 54.150 Prec@5 95.250 Error@1 45.850
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 07:04:12] [Epoch=006/040] [Need: 00:33:40] [LR=0.0100] [Best : Accuracy=54.15, Error=45.85]
  Epoch: [006][000/500]   Time 23.188 (23.188)   Data 23.124 (23.124)   Loss 1.3371 (1.3371)   Prec@1 52.000 (52.000)   Prec@5 97.000 (97.000)   [2025-10-24 07:04:35]
  Epoch: [006][100/500]   Time 0.022 (0.250)   Data 0.000 (0.229)   Loss 1.3346 (1.4431)   Prec@1 52.000 (46.149)   Prec@5 95.000 (92.614)   [2025-10-24 07:04:37]
  Epoch: [006][200/500]   Time 0.019 (0.135)   Data 0.000 (0.115)   Loss 1.5111 (1.4255)   Prec@1 44.000 (47.289)   Prec@5 94.000 (92.687)   [2025-10-24 07:04:39]
  Epoch: [006][300/500]   Time 0.020 (0.098)   Data 0.001 (0.077)   Loss 1.4761 (1.4196)   Prec@1 43.000 (47.857)   Prec@5 95.000 (92.874)   [2025-10-24 07:04:41]
  Epoch: [006][400/500]   Time 0.020 (0.078)   Data 0.001 (0.058)   Loss 1.1757 (1.4081)   Prec@1 62.000 (48.217)   Prec@5 98.000 (93.047)   [2025-10-24 07:04:43]
  **Train** Prec@1 48.506 Prec@5 93.026 Error@1 51.494
  **Test** Prec@1 57.090 Prec@5 95.570 Error@1 42.910
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 07:05:19] [Epoch=007/040] [Need: 00:33:17] [LR=0.0100] [Best : Accuracy=57.09, Error=42.91]
  Epoch: [007][000/500]   Time 37.250 (37.250)   Data 36.470 (36.470)   Loss 1.5123 (1.5123)   Prec@1 47.000 (47.000)   Prec@5 92.000 (92.000)   [2025-10-24 07:05:56]
  Epoch: [007][100/500]   Time 0.021 (0.408)   Data 0.000 (0.361)   Loss 1.4085 (1.3610)   Prec@1 54.000 (50.673)   Prec@5 94.000 (93.485)   [2025-10-24 07:06:00]
  Epoch: [007][200/500]   Time 0.072 (0.231)   Data 0.001 (0.182)   Loss 1.2784 (1.3571)   Prec@1 57.000 (50.821)   Prec@5 95.000 (93.502)   [2025-10-24 07:06:06]
  Epoch: [007][300/500]   Time 0.070 (0.171)   Data 0.001 (0.122)   Loss 1.3800 (1.3528)   Prec@1 40.000 (50.827)   Prec@5 94.000 (93.429)   [2025-10-24 07:06:11]
  Epoch: [007][400/500]   Time 0.018 (0.141)   Data 0.000 (0.091)   Loss 1.2078 (1.3506)   Prec@1 50.000 (50.985)   Prec@5 96.000 (93.411)   [2025-10-24 07:06:16]
  **Train** Prec@1 51.282 Prec@5 93.528 Error@1 48.718
  **Test** Prec@1 58.720 Prec@5 95.690 Error@1 41.280
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 07:06:50] [Epoch=008/040] [Need: 00:34:20] [LR=0.0100] [Best : Accuracy=58.72, Error=41.28]
  Epoch: [008][000/500]   Time 25.032 (25.032)   Data 24.966 (24.966)   Loss 1.1980 (1.1980)   Prec@1 60.000 (60.000)   Prec@5 95.000 (95.000)   [2025-10-24 07:07:15]
  Epoch: [008][100/500]   Time 0.019 (0.269)   Data 0.000 (0.248)   Loss 1.1334 (1.3057)   Prec@1 61.000 (53.178)   Prec@5 99.000 (93.881)   [2025-10-24 07:07:17]
  Epoch: [008][200/500]   Time 0.080 (0.158)   Data 0.000 (0.125)   Loss 1.3169 (1.2998)   Prec@1 44.000 (53.060)   Prec@5 99.000 (94.109)   [2025-10-24 07:07:22]
  Epoch: [008][300/500]   Time 0.020 (0.123)   Data 0.000 (0.083)   Loss 1.2060 (1.2936)   Prec@1 57.000 (53.329)   Prec@5 93.000 (94.086)   [2025-10-24 07:07:27]
  Epoch: [008][400/500]   Time 0.022 (0.103)   Data 0.000 (0.063)   Loss 1.1604 (1.2847)   Prec@1 63.000 (53.509)   Prec@5 99.000 (94.195)   [2025-10-24 07:07:31]
  **Train** Prec@1 53.422 Prec@5 94.216 Error@1 46.578
  **Test** Prec@1 62.310 Prec@5 96.500 Error@1 37.690
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 07:08:00] [Epoch=009/040] [Need: 00:33:33] [LR=0.0100] [Best : Accuracy=62.31, Error=37.69]
  Epoch: [009][000/500]   Time 22.229 (22.229)   Data 22.163 (22.163)   Loss 1.1361 (1.1361)   Prec@1 61.000 (61.000)   Prec@5 93.000 (93.000)   [2025-10-24 07:08:22]
  Epoch: [009][100/500]   Time 0.021 (0.250)   Data 0.001 (0.220)   Loss 1.1449 (1.2391)   Prec@1 58.000 (55.228)   Prec@5 97.000 (94.792)   [2025-10-24 07:08:25]
  Epoch: [009][200/500]   Time 0.020 (0.140)   Data 0.000 (0.111)   Loss 1.4747 (1.2466)   Prec@1 45.000 (54.756)   Prec@5 90.000 (94.498)   [2025-10-24 07:08:28]
  Epoch: [009][300/500]   Time 0.066 (0.106)   Data 0.000 (0.074)   Loss 1.0989 (1.2381)   Prec@1 58.000 (55.163)   Prec@5 96.000 (94.555)   [2025-10-24 07:08:31]
  Epoch: [009][400/500]   Time 0.022 (0.089)   Data 0.000 (0.056)   Loss 1.0101 (1.2320)   Prec@1 63.000 (55.489)   Prec@5 97.000 (94.656)   [2025-10-24 07:08:35]
  **Train** Prec@1 55.618 Prec@5 94.736 Error@1 44.382
  **Test** Prec@1 64.080 Prec@5 96.820 Error@1 35.920
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 07:09:03] [Epoch=010/040] [Need: 00:32:23] [LR=0.0100] [Best : Accuracy=64.08, Error=35.92]
  Epoch: [010][000/500]   Time 21.993 (21.993)   Data 21.926 (21.926)   Loss 1.3431 (1.3431)   Prec@1 50.000 (50.000)   Prec@5 91.000 (91.000)   [2025-10-24 07:09:25]
  Epoch: [010][100/500]   Time 0.022 (0.247)   Data 0.000 (0.217)   Loss 1.2516 (1.2072)   Prec@1 57.000 (56.465)   Prec@5 94.000 (95.149)   [2025-10-24 07:09:28]
  Epoch: [010][200/500]   Time 0.020 (0.139)   Data 0.000 (0.109)   Loss 0.9241 (1.2004)   Prec@1 63.000 (57.114)   Prec@5 96.000 (95.124)   [2025-10-24 07:09:31]
  Epoch: [010][300/500]   Time 0.020 (0.103)   Data 0.000 (0.073)   Loss 1.2940 (1.1977)   Prec@1 45.000 (56.993)   Prec@5 94.000 (95.133)   [2025-10-24 07:09:34]
  Epoch: [010][400/500]   Time 0.021 (0.084)   Data 0.000 (0.055)   Loss 1.0639 (1.1879)   Prec@1 63.000 (57.382)   Prec@5 96.000 (95.155)   [2025-10-24 07:09:37]
  **Train** Prec@1 57.556 Prec@5 95.234 Error@1 42.444
  **Test** Prec@1 66.250 Prec@5 97.200 Error@1 33.750
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 07:10:06] [Epoch=011/040] [Need: 00:31:11] [LR=0.0100] [Best : Accuracy=66.25, Error=33.75]
  Epoch: [011][000/500]   Time 24.658 (24.658)   Data 24.592 (24.592)   Loss 1.3656 (1.3656)   Prec@1 52.000 (52.000)   Prec@5 95.000 (95.000)   [2025-10-24 07:10:30]
  Epoch: [011][100/500]   Time 0.027 (0.276)   Data 0.000 (0.244)   Loss 1.0619 (1.1675)   Prec@1 62.000 (58.386)   Prec@5 95.000 (95.218)   [2025-10-24 07:10:34]
  Epoch: [011][200/500]   Time 0.070 (0.160)   Data 0.000 (0.123)   Loss 1.0369 (1.1470)   Prec@1 62.000 (59.129)   Prec@5 97.000 (95.433)   [2025-10-24 07:10:38]
  Epoch: [011][300/500]   Time 0.020 (0.118)   Data 0.000 (0.082)   Loss 1.1910 (1.1456)   Prec@1 57.000 (58.983)   Prec@5 93.000 (95.512)   [2025-10-24 07:10:41]
  Epoch: [011][400/500]   Time 0.074 (0.099)   Data 0.000 (0.062)   Loss 1.2671 (1.1387)   Prec@1 64.000 (59.307)   Prec@5 95.000 (95.571)   [2025-10-24 07:10:46]
  **Train** Prec@1 59.454 Prec@5 95.546 Error@1 40.546
  **Test** Prec@1 68.520 Prec@5 97.430 Error@1 31.480
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 07:11:14] [Epoch=012/040] [Need: 00:30:16] [LR=0.0100] [Best : Accuracy=68.52, Error=31.48]
  Epoch: [012][000/500]   Time 23.616 (23.616)   Data 23.549 (23.549)   Loss 1.0042 (1.0042)   Prec@1 64.000 (64.000)   Prec@5 96.000 (96.000)   [2025-10-24 07:11:37]
  Epoch: [012][100/500]   Time 0.023 (0.265)   Data 0.000 (0.233)   Loss 1.4400 (1.1099)   Prec@1 52.000 (60.931)   Prec@5 92.000 (95.594)   [2025-10-24 07:11:41]
  Epoch: [012][200/500]   Time 0.067 (0.151)   Data 0.000 (0.118)   Loss 1.2391 (1.1011)   Prec@1 53.000 (60.891)   Prec@5 98.000 (95.786)   [2025-10-24 07:11:44]
  Epoch: [012][300/500]   Time 0.020 (0.115)   Data 0.000 (0.079)   Loss 1.0528 (1.1030)   Prec@1 62.000 (60.784)   Prec@5 97.000 (95.738)   [2025-10-24 07:11:48]
  Epoch: [012][400/500]   Time 0.020 (0.097)   Data 0.001 (0.059)   Loss 1.0136 (1.1020)   Prec@1 62.000 (60.845)   Prec@5 95.000 (95.783)   [2025-10-24 07:11:53]
  **Train** Prec@1 60.936 Prec@5 95.756 Error@1 39.064
  **Test** Prec@1 68.750 Prec@5 97.580 Error@1 31.250
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 07:12:20] [Epoch=013/040] [Need: 00:29:14] [LR=0.0100] [Best : Accuracy=68.75, Error=31.25]
  Epoch: [013][000/500]   Time 25.262 (25.262)   Data 25.195 (25.195)   Loss 1.1088 (1.1088)   Prec@1 56.000 (56.000)   Prec@5 96.000 (96.000)   [2025-10-24 07:12:45]
  Epoch: [013][100/500]   Time 0.021 (0.283)   Data 0.000 (0.250)   Loss 1.2197 (1.0644)   Prec@1 56.000 (62.782)   Prec@5 96.000 (95.881)   [2025-10-24 07:12:49]
  Epoch: [013][200/500]   Time 0.069 (0.161)   Data 0.000 (0.126)   Loss 0.8778 (1.0670)   Prec@1 68.000 (62.234)   Prec@5 95.000 (95.935)   [2025-10-24 07:12:52]
  Epoch: [013][300/500]   Time 0.020 (0.120)   Data 0.000 (0.084)   Loss 1.0997 (1.0707)   Prec@1 61.000 (62.083)   Prec@5 95.000 (95.960)   [2025-10-24 07:12:56]
  Epoch: [013][400/500]   Time 0.023 (0.102)   Data 0.000 (0.063)   Loss 1.0131 (1.0617)   Prec@1 61.000 (62.521)   Prec@5 98.000 (96.027)   [2025-10-24 07:13:01]
  **Train** Prec@1 62.484 Prec@5 96.096 Error@1 37.516
  **Test** Prec@1 70.530 Prec@5 97.810 Error@1 29.470
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 07:13:30] [Epoch=014/040] [Need: 00:28:18] [LR=0.0100] [Best : Accuracy=70.53, Error=29.47]
  Epoch: [014][000/500]   Time 23.106 (23.106)   Data 23.039 (23.039)   Loss 1.0509 (1.0509)   Prec@1 66.000 (66.000)   Prec@5 98.000 (98.000)   [2025-10-24 07:13:53]
  Epoch: [014][100/500]   Time 0.025 (0.264)   Data 0.000 (0.228)   Loss 1.0341 (1.0433)   Prec@1 63.000 (63.287)   Prec@5 96.000 (96.069)   [2025-10-24 07:13:56]
  Epoch: [014][200/500]   Time 0.066 (0.149)   Data 0.001 (0.115)   Loss 0.9957 (1.0448)   Prec@1 59.000 (63.095)   Prec@5 98.000 (96.109)   [2025-10-24 07:14:00]
  Epoch: [014][300/500]   Time 0.022 (0.109)   Data 0.000 (0.077)   Loss 1.2885 (1.0388)   Prec@1 52.000 (63.259)   Prec@5 94.000 (96.209)   [2025-10-24 07:14:02]
  Epoch: [014][400/500]   Time 0.019 (0.092)   Data 0.000 (0.058)   Loss 0.9503 (1.0345)   Prec@1 71.000 (63.479)   Prec@5 98.000 (96.332)   [2025-10-24 07:14:07]
  **Train** Prec@1 63.648 Prec@5 96.280 Error@1 36.352
  **Test** Prec@1 71.490 Prec@5 98.110 Error@1 28.510
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 07:14:36] [Epoch=015/040] [Need: 00:27:14] [LR=0.0100] [Best : Accuracy=71.49, Error=28.51]
  Epoch: [015][000/500]   Time 23.710 (23.710)   Data 23.645 (23.645)   Loss 0.8718 (0.8718)   Prec@1 71.000 (71.000)   Prec@5 97.000 (97.000)   [2025-10-24 07:15:00]
  Epoch: [015][100/500]   Time 0.059 (0.267)   Data 0.000 (0.234)   Loss 0.8241 (1.0028)   Prec@1 71.000 (64.752)   Prec@5 100.000 (96.584)   [2025-10-24 07:15:03]
  Epoch: [015][200/500]   Time 0.017 (0.149)   Data 0.000 (0.118)   Loss 0.8219 (1.0047)   Prec@1 76.000 (64.741)   Prec@5 98.000 (96.582)   [2025-10-24 07:15:06]
  Epoch: [015][300/500]   Time 0.069 (0.108)   Data 0.000 (0.079)   Loss 0.8404 (1.0105)   Prec@1 65.000 (64.498)   Prec@5 99.000 (96.429)   [2025-10-24 07:15:09]
  Epoch: [015][400/500]   Time 0.019 (0.089)   Data 0.000 (0.059)   Loss 1.1081 (1.0084)   Prec@1 61.000 (64.688)   Prec@5 94.000 (96.434)   [2025-10-24 07:15:12]
  **Train** Prec@1 64.674 Prec@5 96.444 Error@1 35.326
  **Test** Prec@1 72.140 Prec@5 98.070 Error@1 27.860
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 07:15:40] [Epoch=016/040] [Need: 00:26:06] [LR=0.0100] [Best : Accuracy=72.14, Error=27.86]
  Epoch: [016][000/500]   Time 22.864 (22.864)   Data 22.794 (22.794)   Loss 0.9061 (0.9061)   Prec@1 68.000 (68.000)   Prec@5 99.000 (99.000)   [2025-10-24 07:16:02]
  Epoch: [016][100/500]   Time 0.019 (0.255)   Data 0.000 (0.226)   Loss 0.8918 (0.9878)   Prec@1 63.000 (64.752)   Prec@5 100.000 (96.545)   [2025-10-24 07:16:05]
  Epoch: [016][200/500]   Time 0.018 (0.138)   Data 0.000 (0.114)   Loss 0.8023 (0.9874)   Prec@1 73.000 (65.075)   Prec@5 95.000 (96.592)   [2025-10-24 07:16:07]
  Epoch: [016][300/500]   Time 0.066 (0.099)   Data 0.000 (0.076)   Loss 0.9205 (0.9861)   Prec@1 69.000 (65.472)   Prec@5 99.000 (96.618)   [2025-10-24 07:16:09]
  Epoch: [016][400/500]   Time 0.070 (0.081)   Data 0.000 (0.057)   Loss 0.9404 (0.9812)   Prec@1 71.000 (65.666)   Prec@5 95.000 (96.746)   [2025-10-24 07:16:12]
  **Train** Prec@1 65.700 Prec@5 96.700 Error@1 34.300
  **Test** Prec@1 74.610 Prec@5 98.170 Error@1 25.390
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 07:17:18] [Epoch=017/040] [Need: 00:25:44] [LR=0.0100] [Best : Accuracy=74.61, Error=25.39]
  Epoch: [017][000/500]   Time 75.509 (75.509)   Data 74.618 (74.618)   Loss 1.1577 (1.1577)   Prec@1 55.000 (55.000)   Prec@5 95.000 (95.000)   [2025-10-24 07:18:34]
  Epoch: [017][100/500]   Time 0.117 (0.811)   Data 0.000 (0.740)   Loss 0.8644 (0.9577)   Prec@1 70.000 (66.861)   Prec@5 98.000 (96.733)   [2025-10-24 07:18:40]
  Epoch: [017][200/500]   Time 0.121 (0.443)   Data 0.002 (0.374)   Loss 0.9244 (0.9661)   Prec@1 75.000 (66.716)   Prec@5 97.000 (96.687)   [2025-10-24 07:18:47]
  Epoch: [017][300/500]   Time 0.030 (0.320)   Data 0.001 (0.251)   Loss 0.7420 (0.9640)   Prec@1 69.000 (66.581)   Prec@5 97.000 (96.804)   [2025-10-24 07:18:55]
  Epoch: [017][400/500]   Time 0.027 (0.250)   Data 0.000 (0.189)   Loss 0.8161 (0.9661)   Prec@1 72.000 (66.526)   Prec@5 99.000 (96.741)   [2025-10-24 07:18:59]
  **Train** Prec@1 66.616 Prec@5 96.836 Error@1 33.384
  **Test** Prec@1 73.590 Prec@5 98.260 Error@1 26.410

==>>[2025-10-24 07:19:38] [Epoch=018/040] [Need: 00:26:07] [LR=0.0100] [Best : Accuracy=74.61, Error=25.39]
  Epoch: [018][000/500]   Time 27.724 (27.724)   Data 27.546 (27.546)   Loss 0.9583 (0.9583)   Prec@1 71.000 (71.000)   Prec@5 97.000 (97.000)   [2025-10-24 07:20:06]
  Epoch: [018][100/500]   Time 0.082 (0.347)   Data 0.002 (0.275)   Loss 0.8776 (0.9516)   Prec@1 66.000 (66.485)   Prec@5 98.000 (97.109)   [2025-10-24 07:20:13]
  Epoch: [018][200/500]   Time 0.029 (0.206)   Data 0.000 (0.139)   Loss 0.8766 (0.9539)   Prec@1 70.000 (66.577)   Prec@5 97.000 (96.985)   [2025-10-24 07:20:20]
  Epoch: [018][300/500]   Time 0.167 (0.161)   Data 0.000 (0.093)   Loss 0.9150 (0.9424)   Prec@1 70.000 (67.083)   Prec@5 97.000 (96.980)   [2025-10-24 07:20:27]
  Epoch: [018][400/500]   Time 0.109 (0.140)   Data 0.078 (0.072)   Loss 0.8045 (0.9388)   Prec@1 72.000 (67.249)   Prec@5 97.000 (96.923)   [2025-10-24 07:20:35]
  **Train** Prec@1 67.382 Prec@5 96.910 Error@1 32.618
  **Test** Prec@1 75.430 Prec@5 98.350 Error@1 24.570
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 07:21:21] [Epoch=019/040] [Need: 00:25:31] [LR=0.0100] [Best : Accuracy=75.43, Error=24.57]
  Epoch: [019][000/500]   Time 56.872 (56.872)   Data 56.470 (56.470)   Loss 0.9619 (0.9619)   Prec@1 66.000 (66.000)   Prec@5 97.000 (97.000)   [2025-10-24 07:22:18]
  Epoch: [019][100/500]   Time 0.045 (0.655)   Data 0.002 (0.562)   Loss 0.9823 (0.9211)   Prec@1 64.000 (68.109)   Prec@5 97.000 (96.950)   [2025-10-24 07:22:28]
  Epoch: [019][200/500]   Time 0.049 (0.382)   Data 0.000 (0.285)   Loss 0.7907 (0.9288)   Prec@1 75.000 (67.662)   Prec@5 97.000 (96.697)   [2025-10-24 07:22:38]
  Epoch: [019][300/500]   Time 0.169 (0.301)   Data 0.001 (0.192)   Loss 0.8881 (0.9207)   Prec@1 70.000 (68.010)   Prec@5 96.000 (96.827)   [2025-10-24 07:22:52]
  Epoch: [019][400/500]   Time 0.107 (0.253)   Data 0.000 (0.145)   Loss 0.8926 (0.9182)   Prec@1 68.000 (67.965)   Prec@5 98.000 (96.923)   [2025-10-24 07:23:03]
  **Train** Prec@1 67.870 Prec@5 96.964 Error@1 32.130
  **Test** Prec@1 73.870 Prec@5 98.160 Error@1 26.130

==>>[2025-10-24 07:24:00] [Epoch=020/040] [Need: 00:25:43] [LR=0.0100] [Best : Accuracy=75.43, Error=24.57]
  Epoch: [020][000/500]   Time 28.601 (28.601)   Data 28.438 (28.438)   Loss 0.9633 (0.9633)   Prec@1 68.000 (68.000)   Prec@5 99.000 (99.000)   [2025-10-24 07:24:29]
  Epoch: [020][100/500]   Time 0.133 (0.386)   Data 0.000 (0.284)   Loss 0.8658 (0.9134)   Prec@1 69.000 (68.475)   Prec@5 95.000 (97.030)   [2025-10-24 07:24:39]
  Epoch: [020][200/500]   Time 0.124 (0.243)   Data 0.000 (0.145)   Loss 0.9415 (0.9094)   Prec@1 66.000 (68.682)   Prec@5 95.000 (97.100)   [2025-10-24 07:24:49]
  Epoch: [020][300/500]   Time 0.123 (0.181)   Data 0.002 (0.097)   Loss 1.0337 (0.9069)   Prec@1 65.000 (68.528)   Prec@5 93.000 (96.987)   [2025-10-24 07:24:55]
  Epoch: [020][400/500]   Time 0.126 (0.158)   Data 0.000 (0.074)   Loss 0.8767 (0.9032)   Prec@1 67.000 (68.711)   Prec@5 98.000 (97.005)   [2025-10-24 07:25:04]
  **Train** Prec@1 68.942 Prec@5 97.042 Error@1 31.058
  **Test** Prec@1 76.500 Prec@5 98.570 Error@1 23.500
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 07:25:44] [Epoch=021/040] [Need: 00:24:51] [LR=0.0100] [Best : Accuracy=76.50, Error=23.50]
  Epoch: [021][000/500]   Time 28.264 (28.264)   Data 28.085 (28.085)   Loss 0.9399 (0.9399)   Prec@1 70.000 (70.000)   Prec@5 98.000 (98.000)   [2025-10-24 07:26:13]
  Epoch: [021][100/500]   Time 0.137 (0.370)   Data 0.000 (0.280)   Loss 0.7773 (0.9010)   Prec@1 72.000 (68.960)   Prec@5 97.000 (97.069)   [2025-10-24 07:26:22]
  Epoch: [021][200/500]   Time 0.135 (0.227)   Data 0.001 (0.142)   Loss 0.7961 (0.8954)   Prec@1 72.000 (69.114)   Prec@5 98.000 (97.144)   [2025-10-24 07:26:30]
  Epoch: [021][300/500]   Time 0.032 (0.176)   Data 0.000 (0.096)   Loss 1.0094 (0.8934)   Prec@1 63.000 (69.050)   Prec@5 97.000 (97.219)   [2025-10-24 07:26:37]
  Epoch: [021][400/500]   Time 0.030 (0.149)   Data 0.001 (0.073)   Loss 0.8639 (0.8899)   Prec@1 65.000 (69.219)   Prec@5 98.000 (97.202)   [2025-10-24 07:26:44]
  **Train** Prec@1 69.310 Prec@5 97.196 Error@1 30.690
  **Test** Prec@1 77.040 Prec@5 98.420 Error@1 22.960
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 07:27:28] [Epoch=022/040] [Need: 00:23:53] [LR=0.0100] [Best : Accuracy=77.04, Error=22.96]
  Epoch: [022][000/500]   Time 47.780 (47.780)   Data 47.197 (47.197)   Loss 1.0060 (1.0060)   Prec@1 69.000 (69.000)   Prec@5 96.000 (96.000)   [2025-10-24 07:28:16]
  Epoch: [022][100/500]   Time 0.075 (0.643)   Data 0.003 (0.471)   Loss 0.7901 (0.8582)   Prec@1 71.000 (70.455)   Prec@5 98.000 (97.356)   [2025-10-24 07:28:33]
  Epoch: [022][200/500]   Time 0.098 (0.394)   Data 0.000 (0.252)   Loss 0.9640 (0.8688)   Prec@1 69.000 (70.020)   Prec@5 95.000 (97.239)   [2025-10-24 07:28:47]
  Epoch: [022][300/500]   Time 0.123 (0.347)   Data 0.007 (0.194)   Loss 0.8869 (0.8682)   Prec@1 70.000 (70.120)   Prec@5 98.000 (97.229)   [2025-10-24 07:29:12]
  Epoch: [022][400/500]   Time 0.090 (0.305)   Data 0.041 (0.147)   Loss 0.8359 (0.8684)   Prec@1 74.000 (70.202)   Prec@5 97.000 (97.209)   [2025-10-24 07:29:30]
  **Train** Prec@1 70.206 Prec@5 97.166 Error@1 29.794
  **Test** Prec@1 77.050 Prec@5 98.760 Error@1 22.950
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 07:32:21] [Epoch=023/040] [Need: 00:25:12] [LR=0.0100] [Best : Accuracy=77.05, Error=22.95]
  Epoch: [023][000/500]   Time 53.404 (53.404)   Data 52.674 (52.674)   Loss 0.7560 (0.7560)   Prec@1 70.000 (70.000)   Prec@5 99.000 (99.000)   [2025-10-24 07:33:15]
  Epoch: [023][100/500]   Time 0.142 (0.683)   Data 0.001 (0.525)   Loss 0.7619 (0.8436)   Prec@1 77.000 (70.693)   Prec@5 98.000 (97.614)   [2025-10-24 07:33:30]
  Epoch: [023][200/500]   Time 0.153 (0.417)   Data 0.002 (0.266)   Loss 0.8821 (0.8496)   Prec@1 68.000 (70.736)   Prec@5 97.000 (97.547)   [2025-10-24 07:33:45]
  Epoch: [023][300/500]   Time 0.229 (0.345)   Data 0.000 (0.179)   Loss 0.7765 (0.8501)   Prec@1 71.000 (70.781)   Prec@5 98.000 (97.508)   [2025-10-24 07:34:05]
  Epoch: [023][400/500]   Time 0.157 (0.310)   Data 0.000 (0.136)   Loss 0.7279 (0.8533)   Prec@1 75.000 (70.623)   Prec@5 100.000 (97.454)   [2025-10-24 07:34:26]
  **Train** Prec@1 70.572 Prec@5 97.418 Error@1 29.428
  **Test** Prec@1 78.200 Prec@5 98.640 Error@1 21.800
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 07:35:27] [Epoch=024/040] [Need: 00:24:47] [LR=0.0100] [Best : Accuracy=78.20, Error=21.80]
  Epoch: [024][000/500]   Time 40.988 (40.988)   Data 40.664 (40.664)   Loss 0.9085 (0.9085)   Prec@1 71.000 (71.000)   Prec@5 98.000 (98.000)   [2025-10-24 07:36:08]
  Epoch: [024][100/500]   Time 0.019 (0.428)   Data 0.000 (0.403)   Loss 0.8189 (0.8308)   Prec@1 74.000 (71.525)   Prec@5 95.000 (97.347)   [2025-10-24 07:36:10]
  Epoch: [024][200/500]   Time 0.022 (0.225)   Data 0.000 (0.203)   Loss 0.9547 (0.8366)   Prec@1 66.000 (71.333)   Prec@5 97.000 (97.413)   [2025-10-24 07:36:12]
  Epoch: [024][300/500]   Time 0.022 (0.157)   Data 0.000 (0.135)   Loss 0.8319 (0.8400)   Prec@1 67.000 (71.196)   Prec@5 99.000 (97.449)   [2025-10-24 07:36:14]
  Epoch: [024][400/500]   Time 0.022 (0.123)   Data 0.001 (0.102)   Loss 0.8216 (0.8415)   Prec@1 69.000 (71.175)   Prec@5 97.000 (97.404)   [2025-10-24 07:36:16]
  **Train** Prec@1 71.196 Prec@5 97.388 Error@1 28.804
  **Test** Prec@1 77.960 Prec@5 98.690 Error@1 22.040

==>>[2025-10-24 07:36:59] [Epoch=025/040] [Need: 00:23:14] [LR=0.0010] [Best : Accuracy=78.20, Error=21.80]
  Epoch: [025][000/500]   Time 42.631 (42.631)   Data 42.319 (42.319)   Loss 0.8824 (0.8824)   Prec@1 70.000 (70.000)   Prec@5 97.000 (97.000)   [2025-10-24 07:37:42]
  Epoch: [025][100/500]   Time 0.020 (0.443)   Data 0.000 (0.419)   Loss 0.6814 (0.8056)   Prec@1 81.000 (72.554)   Prec@5 98.000 (97.564)   [2025-10-24 07:37:44]
  Epoch: [025][200/500]   Time 0.022 (0.233)   Data 0.000 (0.211)   Loss 0.7419 (0.7971)   Prec@1 74.000 (72.706)   Prec@5 97.000 (97.697)   [2025-10-24 07:37:46]
  Epoch: [025][300/500]   Time 0.021 (0.163)   Data 0.000 (0.141)   Loss 0.7106 (0.7856)   Prec@1 79.000 (72.997)   Prec@5 98.000 (97.864)   [2025-10-24 07:37:48]
  Epoch: [025][400/500]   Time 0.062 (0.128)   Data 0.000 (0.106)   Loss 0.7329 (0.7809)   Prec@1 72.000 (73.272)   Prec@5 97.000 (97.893)   [2025-10-24 07:37:50]
  **Train** Prec@1 73.358 Prec@5 97.858 Error@1 26.642
  **Test** Prec@1 79.330 Prec@5 98.860 Error@1 20.670
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 07:38:35] [Epoch=026/040] [Need: 00:21:42] [LR=0.0010] [Best : Accuracy=79.33, Error=20.67]
  Epoch: [026][000/500]   Time 41.364 (41.364)   Data 41.030 (41.030)   Loss 0.9465 (0.9465)   Prec@1 66.000 (66.000)   Prec@5 94.000 (94.000)   [2025-10-24 07:39:16]
  Epoch: [026][100/500]   Time 0.021 (0.432)   Data 0.002 (0.407)   Loss 0.8441 (0.7467)   Prec@1 71.000 (74.416)   Prec@5 98.000 (98.040)   [2025-10-24 07:39:19]
  Epoch: [026][200/500]   Time 0.019 (0.227)   Data 0.000 (0.204)   Loss 0.7772 (0.7488)   Prec@1 69.000 (74.144)   Prec@5 98.000 (98.159)   [2025-10-24 07:39:21]
  Epoch: [026][300/500]   Time 0.019 (0.159)   Data 0.000 (0.137)   Loss 0.5889 (0.7549)   Prec@1 75.000 (73.821)   Prec@5 99.000 (98.126)   [2025-10-24 07:39:23]
  Epoch: [026][400/500]   Time 0.023 (0.124)   Data 0.000 (0.103)   Loss 0.8150 (0.7537)   Prec@1 74.000 (73.910)   Prec@5 96.000 (98.137)   [2025-10-24 07:39:25]
  **Train** Prec@1 73.966 Prec@5 98.078 Error@1 26.034
  **Test** Prec@1 79.780 Prec@5 98.900 Error@1 20.220
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 07:40:11] [Epoch=027/040] [Need: 00:20:11] [LR=0.0010] [Best : Accuracy=79.78, Error=20.22]
  Epoch: [027][000/500]   Time 45.069 (45.069)   Data 44.736 (44.736)   Loss 0.9626 (0.9626)   Prec@1 68.000 (68.000)   Prec@5 96.000 (96.000)   [2025-10-24 07:40:56]
  Epoch: [027][100/500]   Time 0.018 (0.467)   Data 0.000 (0.443)   Loss 0.9470 (0.7539)   Prec@1 66.000 (74.277)   Prec@5 97.000 (97.921)   [2025-10-24 07:40:58]
  Epoch: [027][200/500]   Time 0.019 (0.245)   Data 0.000 (0.223)   Loss 0.9420 (0.7487)   Prec@1 71.000 (74.294)   Prec@5 97.000 (98.020)   [2025-10-24 07:41:00]
  Epoch: [027][300/500]   Time 0.020 (0.171)   Data 0.000 (0.149)   Loss 0.7786 (0.7447)   Prec@1 73.000 (74.272)   Prec@5 99.000 (98.080)   [2025-10-24 07:41:02]
  Epoch: [027][400/500]   Time 0.021 (0.134)   Data 0.001 (0.112)   Loss 0.7394 (0.7481)   Prec@1 70.000 (74.224)   Prec@5 99.000 (98.070)   [2025-10-24 07:41:04]
  **Train** Prec@1 74.236 Prec@5 98.076 Error@1 25.764
  **Test** Prec@1 79.830 Prec@5 98.930 Error@1 20.170
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 07:41:52] [Epoch=028/040] [Need: 00:18:41] [LR=0.0010] [Best : Accuracy=79.83, Error=20.17]
  Epoch: [028][000/500]   Time 55.059 (55.059)   Data 54.058 (54.058)   Loss 0.7311 (0.7311)   Prec@1 73.000 (73.000)   Prec@5 99.000 (99.000)   [2025-10-24 07:42:47]
  Epoch: [028][100/500]   Time 0.023 (0.572)   Data 0.002 (0.537)   Loss 0.7877 (0.7349)   Prec@1 71.000 (74.554)   Prec@5 99.000 (98.149)   [2025-10-24 07:42:50]
  Epoch: [028][200/500]   Time 0.034 (0.299)   Data 0.021 (0.271)   Loss 0.9395 (0.7370)   Prec@1 63.000 (74.463)   Prec@5 97.000 (98.124)   [2025-10-24 07:42:52]
  Epoch: [028][300/500]   Time 0.078 (0.216)   Data 0.000 (0.182)   Loss 1.0896 (0.7413)   Prec@1 67.000 (74.439)   Prec@5 95.000 (98.040)   [2025-10-24 07:42:57]
  Epoch: [028][400/500]   Time 0.065 (0.171)   Data 0.000 (0.138)   Loss 0.7192 (0.7420)   Prec@1 75.000 (74.476)   Prec@5 96.000 (98.072)   [2025-10-24 07:43:00]
  **Train** Prec@1 74.470 Prec@5 98.048 Error@1 25.530
  **Test** Prec@1 80.060 Prec@5 98.880 Error@1 19.940
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 07:43:58] [Epoch=029/040] [Need: 00:17:20] [LR=0.0010] [Best : Accuracy=80.06, Error=19.94]
  Epoch: [029][000/500]   Time 38.807 (38.807)   Data 38.417 (38.417)   Loss 0.7437 (0.7437)   Prec@1 71.000 (71.000)   Prec@5 98.000 (98.000)   [2025-10-24 07:44:37]
  Epoch: [029][100/500]   Time 0.023 (0.408)   Data 0.000 (0.381)   Loss 0.6929 (0.7346)   Prec@1 71.000 (74.921)   Prec@5 100.000 (98.069)   [2025-10-24 07:44:39]
  Epoch: [029][200/500]   Time 0.020 (0.216)   Data 0.000 (0.191)   Loss 0.7480 (0.7321)   Prec@1 73.000 (74.811)   Prec@5 99.000 (98.209)   [2025-10-24 07:44:42]
  Epoch: [029][300/500]   Time 0.025 (0.151)   Data 0.000 (0.128)   Loss 0.7699 (0.7284)   Prec@1 76.000 (74.801)   Prec@5 99.000 (98.209)   [2025-10-24 07:44:44]
  Epoch: [029][400/500]   Time 0.020 (0.119)   Data 0.000 (0.096)   Loss 0.8426 (0.7299)   Prec@1 70.000 (74.798)   Prec@5 99.000 (98.212)   [2025-10-24 07:44:46]
  **Train** Prec@1 74.858 Prec@5 98.154 Error@1 25.142
  **Test** Prec@1 79.910 Prec@5 98.890 Error@1 20.090

==>>[2025-10-24 07:45:34] [Epoch=030/040] [Need: 00:15:46] [LR=0.0010] [Best : Accuracy=80.06, Error=19.94]
  Epoch: [030][000/500]   Time 42.865 (42.865)   Data 42.350 (42.350)   Loss 0.7409 (0.7409)   Prec@1 77.000 (77.000)   Prec@5 98.000 (98.000)   [2025-10-24 07:46:17]
  Epoch: [030][100/500]   Time 0.064 (0.567)   Data 0.001 (0.426)   Loss 0.5904 (0.7382)   Prec@1 79.000 (74.644)   Prec@5 99.000 (98.168)   [2025-10-24 07:46:31]
  Epoch: [030][200/500]   Time 0.155 (0.357)   Data 0.001 (0.217)   Loss 0.8484 (0.7321)   Prec@1 73.000 (75.100)   Prec@5 99.000 (98.144)   [2025-10-24 07:46:45]
  Epoch: [030][300/500]   Time 0.154 (0.297)   Data 0.001 (0.147)   Loss 0.5606 (0.7330)   Prec@1 79.000 (74.960)   Prec@5 100.000 (98.126)   [2025-10-24 07:47:03]
  Epoch: [030][400/500]   Time 0.129 (0.254)   Data 0.001 (0.112)   Loss 0.7712 (0.7269)   Prec@1 75.000 (75.212)   Prec@5 96.000 (98.105)   [2025-10-24 07:47:15]
  **Train** Prec@1 75.044 Prec@5 98.082 Error@1 24.956
  **Test** Prec@1 80.130 Prec@5 98.940 Error@1 19.870
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 07:48:39] [Epoch=031/040] [Need: 00:14:37] [LR=0.0010] [Best : Accuracy=80.13, Error=19.87]
  Epoch: [031][000/500]   Time 52.956 (52.956)   Data 52.427 (52.427)   Loss 0.6424 (0.6424)   Prec@1 77.000 (77.000)   Prec@5 98.000 (98.000)   [2025-10-24 07:49:32]
  Epoch: [031][100/500]   Time 0.258 (0.661)   Data 0.001 (0.525)   Loss 0.6748 (0.7290)   Prec@1 75.000 (74.663)   Prec@5 100.000 (98.139)   [2025-10-24 07:49:46]
  Epoch: [031][200/500]   Time 0.165 (0.406)   Data 0.000 (0.267)   Loss 0.9474 (0.7254)   Prec@1 65.000 (75.030)   Prec@5 98.000 (98.080)   [2025-10-24 07:50:01]
  Epoch: [031][300/500]   Time 0.067 (0.307)   Data 0.000 (0.181)   Loss 0.8732 (0.7243)   Prec@1 76.000 (75.060)   Prec@5 98.000 (98.113)   [2025-10-24 07:50:12]
  Epoch: [031][400/500]   Time 0.038 (0.255)   Data 0.000 (0.137)   Loss 0.6673 (0.7211)   Prec@1 77.000 (75.202)   Prec@5 100.000 (98.110)   [2025-10-24 07:50:22]
  **Train** Prec@1 75.188 Prec@5 98.072 Error@1 24.812
  **Test** Prec@1 80.060 Prec@5 98.900 Error@1 19.940

==>>[2025-10-24 07:50:59] [Epoch=032/040] [Need: 00:13:10] [LR=0.0010] [Best : Accuracy=80.13, Error=19.87]
  Epoch: [032][000/500]   Time 26.129 (26.129)   Data 26.042 (26.042)   Loss 0.5944 (0.5944)   Prec@1 75.000 (75.000)   Prec@5 99.000 (99.000)   [2025-10-24 07:51:25]
  Epoch: [032][100/500]   Time 0.028 (0.287)   Data 0.000 (0.259)   Loss 0.6394 (0.7211)   Prec@1 82.000 (75.327)   Prec@5 99.000 (98.158)   [2025-10-24 07:51:28]
  Epoch: [032][200/500]   Time 0.026 (0.158)   Data 0.000 (0.130)   Loss 0.6103 (0.7209)   Prec@1 81.000 (75.313)   Prec@5 97.000 (98.214)   [2025-10-24 07:51:30]
  Epoch: [032][300/500]   Time 0.028 (0.115)   Data 0.001 (0.087)   Loss 0.6631 (0.7240)   Prec@1 78.000 (75.060)   Prec@5 96.000 (98.183)   [2025-10-24 07:51:33]
  Epoch: [032][400/500]   Time 0.028 (0.093)   Data 0.001 (0.066)   Loss 0.5087 (0.7231)   Prec@1 84.000 (75.127)   Prec@5 99.000 (98.197)   [2025-10-24 07:51:36]
  **Train** Prec@1 75.112 Prec@5 98.184 Error@1 24.888
  **Test** Prec@1 80.670 Prec@5 98.920 Error@1 19.330
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 07:52:04] [Epoch=033/040] [Need: 00:11:24] [LR=0.0010] [Best : Accuracy=80.67, Error=19.33]
  Epoch: [033][000/500]   Time 27.893 (27.893)   Data 27.813 (27.813)   Loss 0.5067 (0.5067)   Prec@1 85.000 (85.000)   Prec@5 100.000 (100.000)   [2025-10-24 07:52:32]
  Epoch: [033][100/500]   Time 0.029 (0.306)   Data 0.002 (0.276)   Loss 0.6049 (0.7355)   Prec@1 77.000 (75.168)   Prec@5 100.000 (97.990)   [2025-10-24 07:52:35]
  Epoch: [033][200/500]   Time 0.029 (0.168)   Data 0.000 (0.139)   Loss 0.6458 (0.7208)   Prec@1 77.000 (75.448)   Prec@5 100.000 (98.144)   [2025-10-24 07:52:38]
  Epoch: [033][300/500]   Time 0.024 (0.121)   Data 0.001 (0.093)   Loss 0.8018 (0.7209)   Prec@1 78.000 (75.389)   Prec@5 98.000 (98.130)   [2025-10-24 07:52:41]
  Epoch: [033][400/500]   Time 0.024 (0.098)   Data 0.001 (0.070)   Loss 0.5683 (0.7188)   Prec@1 82.000 (75.367)   Prec@5 100.000 (98.162)   [2025-10-24 07:52:44]
  **Train** Prec@1 75.374 Prec@5 98.152 Error@1 24.626
  **Test** Prec@1 80.660 Prec@5 98.950 Error@1 19.340

==>>[2025-10-24 07:53:13] [Epoch=034/040] [Need: 00:09:41] [LR=0.0010] [Best : Accuracy=80.67, Error=19.33]
  Epoch: [034][000/500]   Time 26.825 (26.825)   Data 26.747 (26.747)   Loss 0.5924 (0.5924)   Prec@1 80.000 (80.000)   Prec@5 98.000 (98.000)   [2025-10-24 07:53:40]
  Epoch: [034][100/500]   Time 0.034 (0.304)   Data 0.004 (0.266)   Loss 0.6517 (0.7191)   Prec@1 80.000 (75.594)   Prec@5 100.000 (98.109)   [2025-10-24 07:53:44]
  Epoch: [034][200/500]   Time 0.042 (0.170)   Data 0.005 (0.134)   Loss 0.6392 (0.7102)   Prec@1 81.000 (75.612)   Prec@5 98.000 (98.214)   [2025-10-24 07:53:47]
  Epoch: [034][300/500]   Time 0.036 (0.125)   Data 0.001 (0.090)   Loss 0.6586 (0.7149)   Prec@1 79.000 (75.432)   Prec@5 99.000 (98.243)   [2025-10-24 07:53:51]
  Epoch: [034][400/500]   Time 0.058 (0.104)   Data 0.003 (0.069)   Loss 0.6327 (0.7182)   Prec@1 77.000 (75.257)   Prec@5 98.000 (98.262)   [2025-10-24 07:53:55]
  **Train** Prec@1 75.446 Prec@5 98.236 Error@1 24.554
  **Test** Prec@1 80.250 Prec@5 98.980 Error@1 19.750

==>>[2025-10-24 07:54:47] [Epoch=035/040] [Need: 00:08:04] [LR=0.0010] [Best : Accuracy=80.67, Error=19.33]
  Epoch: [035][000/500]   Time 28.373 (28.373)   Data 28.217 (28.217)   Loss 0.5328 (0.5328)   Prec@1 87.000 (87.000)   Prec@5 99.000 (99.000)   [2025-10-24 07:55:15]
  Epoch: [035][100/500]   Time 0.024 (0.311)   Data 0.001 (0.280)   Loss 0.7322 (0.6966)   Prec@1 73.000 (76.218)   Prec@5 98.000 (98.139)   [2025-10-24 07:55:18]
  Epoch: [035][200/500]   Time 0.027 (0.171)   Data 0.001 (0.141)   Loss 0.5816 (0.7175)   Prec@1 83.000 (75.507)   Prec@5 100.000 (98.209)   [2025-10-24 07:55:21]
  Epoch: [035][300/500]   Time 0.025 (0.123)   Data 0.000 (0.094)   Loss 0.8894 (0.7192)   Prec@1 70.000 (75.638)   Prec@5 96.000 (98.193)   [2025-10-24 07:55:24]
  Epoch: [035][400/500]   Time 0.030 (0.099)   Data 0.000 (0.071)   Loss 0.6688 (0.7222)   Prec@1 81.000 (75.514)   Prec@5 98.000 (98.152)   [2025-10-24 07:55:27]
  **Train** Prec@1 75.570 Prec@5 98.132 Error@1 24.430
  **Test** Prec@1 80.520 Prec@5 98.920 Error@1 19.480

==>>[2025-10-24 07:55:56] [Epoch=036/040] [Need: 00:06:24] [LR=0.0010] [Best : Accuracy=80.67, Error=19.33]
  Epoch: [036][000/500]   Time 26.062 (26.062)   Data 25.981 (25.981)   Loss 0.7440 (0.7440)   Prec@1 75.000 (75.000)   Prec@5 96.000 (96.000)   [2025-10-24 07:56:22]
  Epoch: [036][100/500]   Time 0.028 (0.288)   Data 0.001 (0.258)   Loss 0.6801 (0.7121)   Prec@1 78.000 (75.416)   Prec@5 97.000 (98.218)   [2025-10-24 07:56:25]
  Epoch: [036][200/500]   Time 0.028 (0.159)   Data 0.000 (0.130)   Loss 0.8031 (0.7160)   Prec@1 75.000 (75.383)   Prec@5 97.000 (98.219)   [2025-10-24 07:56:28]
  Epoch: [036][300/500]   Time 0.026 (0.115)   Data 0.000 (0.087)   Loss 0.8460 (0.7156)   Prec@1 75.000 (75.286)   Prec@5 97.000 (98.103)   [2025-10-24 07:56:31]
  Epoch: [036][400/500]   Time 0.032 (0.094)   Data 0.001 (0.065)   Loss 0.8264 (0.7156)   Prec@1 73.000 (75.389)   Prec@5 100.000 (98.052)   [2025-10-24 07:56:34]
  **Train** Prec@1 75.628 Prec@5 98.110 Error@1 24.372
  **Test** Prec@1 80.570 Prec@5 98.920 Error@1 19.430

==>>[2025-10-24 07:57:03] [Epoch=037/040] [Need: 00:04:45] [LR=0.0010] [Best : Accuracy=80.67, Error=19.33]
  Epoch: [037][000/500]   Time 26.767 (26.767)   Data 26.687 (26.687)   Loss 0.6754 (0.6754)   Prec@1 80.000 (80.000)   Prec@5 98.000 (98.000)   [2025-10-24 07:57:30]
  Epoch: [037][100/500]   Time 0.032 (0.296)   Data 0.001 (0.265)   Loss 0.6419 (0.6935)   Prec@1 77.000 (76.119)   Prec@5 99.000 (98.396)   [2025-10-24 07:57:33]
  Epoch: [037][200/500]   Time 0.027 (0.163)   Data 0.000 (0.133)   Loss 0.7270 (0.6911)   Prec@1 73.000 (76.318)   Prec@5 97.000 (98.388)   [2025-10-24 07:57:36]
  Epoch: [037][300/500]   Time 0.025 (0.118)   Data 0.001 (0.089)   Loss 0.7776 (0.6983)   Prec@1 76.000 (76.100)   Prec@5 98.000 (98.302)   [2025-10-24 07:57:38]
  Epoch: [037][400/500]   Time 0.029 (0.096)   Data 0.000 (0.067)   Loss 0.7487 (0.6984)   Prec@1 67.000 (76.140)   Prec@5 97.000 (98.252)   [2025-10-24 07:57:41]
  **Train** Prec@1 76.064 Prec@5 98.250 Error@1 23.936
  **Test** Prec@1 80.340 Prec@5 98.910 Error@1 19.660

==>>[2025-10-24 07:58:09] [Epoch=038/040] [Need: 00:03:09] [LR=0.0010] [Best : Accuracy=80.67, Error=19.33]
  Epoch: [038][000/500]   Time 27.692 (27.692)   Data 27.608 (27.608)   Loss 0.5698 (0.5698)   Prec@1 80.000 (80.000)   Prec@5 100.000 (100.000)   [2025-10-24 07:58:37]
  Epoch: [038][100/500]   Time 0.028 (0.302)   Data 0.000 (0.274)   Loss 0.6438 (0.6897)   Prec@1 77.000 (76.327)   Prec@5 99.000 (98.307)   [2025-10-24 07:58:40]
  Epoch: [038][200/500]   Time 0.027 (0.165)   Data 0.000 (0.138)   Loss 0.8338 (0.7017)   Prec@1 68.000 (75.950)   Prec@5 99.000 (98.249)   [2025-10-24 07:58:43]
  Epoch: [038][300/500]   Time 0.028 (0.120)   Data 0.000 (0.092)   Loss 0.8278 (0.6990)   Prec@1 71.000 (76.060)   Prec@5 98.000 (98.183)   [2025-10-24 07:58:45]
  Epoch: [038][400/500]   Time 0.032 (0.097)   Data 0.000 (0.070)   Loss 0.5404 (0.6990)   Prec@1 84.000 (76.102)   Prec@5 99.000 (98.222)   [2025-10-24 07:58:48]
  **Train** Prec@1 75.968 Prec@5 98.196 Error@1 24.032
  **Test** Prec@1 80.830 Prec@5 99.020 Error@1 19.170
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 07:59:15] [Epoch=039/040] [Need: 00:01:33] [LR=0.0010] [Best : Accuracy=80.83, Error=19.17]
  Epoch: [039][000/500]   Time 25.725 (25.725)   Data 25.644 (25.644)   Loss 0.7008 (0.7008)   Prec@1 78.000 (78.000)   Prec@5 99.000 (99.000)   [2025-10-24 07:59:41]
  Epoch: [039][100/500]   Time 0.029 (0.284)   Data 0.003 (0.255)   Loss 0.8688 (0.6851)   Prec@1 73.000 (76.525)   Prec@5 97.000 (98.287)   [2025-10-24 07:59:44]
  Epoch: [039][200/500]   Time 0.029 (0.157)   Data 0.000 (0.128)   Loss 0.7867 (0.6919)   Prec@1 75.000 (76.139)   Prec@5 98.000 (98.294)   [2025-10-24 07:59:47]
  Epoch: [039][300/500]   Time 0.028 (0.114)   Data 0.000 (0.086)   Loss 0.7446 (0.7038)   Prec@1 72.000 (75.681)   Prec@5 99.000 (98.266)   [2025-10-24 07:59:50]
  Epoch: [039][400/500]   Time 0.030 (0.092)   Data 0.001 (0.065)   Loss 0.7342 (0.7026)   Prec@1 71.000 (75.683)   Prec@5 98.000 (98.292)   [2025-10-24 07:59:53]
  **Train** Prec@1 75.624 Prec@5 98.224 Error@1 24.376
  **Test** Prec@1 80.740 Prec@5 98.990 Error@1 19.260
