save path : ./save/tinyvgg_quan/randbet_0.1_0.01_10_-1
{'data_path': './dataset', 'arch': 'tinyvgg_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': True, 'clipping_coeff': 0.1, 'learning_rate': 0.01, 'manualSeed': 2415, 'save_path': './save/tinyvgg_quan/randbet_0.1_0.01_10_-1', 'enable_bfa': False, 'resume': '', 'quan_bitwidth': None, 'reset_weight': False, 'evaluate': False, 'n_iter': 30, 'model_only': False, 'random_bfa': False, 'use_cuda': True}
Random Seed: 2415
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'tinyvgg_quan'
=> network :
 TinyVGG(
  (features): Sequential(
    (0): quan_Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Dropout2d(p=0.3, inplace=False)
    (6): quan_Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace=True)
    (8): quan_Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace=True)
    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (11): Dropout2d(p=0.3, inplace=False)
    (12): quan_Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): quan_Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Dropout2d(p=0.3, inplace=False)
  )
  (classifier): Sequential(
    (0): quan_Linear(in_features=2048, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): quan_Linear(in_features=128, out_features=10, bias=True)
  )
)
=> do not use any checkpoint for tinyvgg_quan model

==>>[2025-10-24 23:28:00] [Epoch=000/040] [Need: 00:00:00] [LR=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/500]   Time 28.005 (28.005)   Data 24.835 (24.835)   Loss 2.3098 (2.3098)   Prec@1 10.000 (10.000)   Prec@5 51.000 (51.000)   [2025-10-24 23:28:28]
  Epoch: [000][100/500]   Time 0.387 (0.657)   Data 0.002 (0.248)   Loss 2.3004 (2.3034)   Prec@1 10.000 (9.950)   Prec@5 58.000 (50.149)   [2025-10-24 23:29:07]
  Epoch: [000][200/500]   Time 0.384 (0.516)   Data 0.001 (0.126)   Loss 2.3065 (2.3030)   Prec@1 7.000 (10.184)   Prec@5 46.000 (50.244)   [2025-10-24 23:29:44]
  Epoch: [000][300/500]   Time 0.406 (0.465)   Data 0.002 (0.084)   Loss 2.2959 (2.3027)   Prec@1 11.000 (10.462)   Prec@5 61.000 (50.512)   [2025-10-24 23:30:20]
  Epoch: [000][400/500]   Time 0.427 (0.444)   Data 0.004 (0.064)   Loss 2.2371 (2.3000)   Prec@1 14.000 (10.973)   Prec@5 70.000 (52.257)   [2025-10-24 23:30:58]
  **Train** Prec@1 12.790 Prec@5 56.254 Error@1 87.210
  **Test** Prec@1 23.270 Prec@5 78.710 Error@1 76.730
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 23:31:53] [Epoch=001/040] [Need: 02:31:30] [LR=0.0100] [Best : Accuracy=23.27, Error=76.73]
  Epoch: [001][000/500]   Time 19.806 (19.806)   Data 19.262 (19.262)   Loss 2.1871 (2.1871)   Prec@1 22.000 (22.000)   Prec@5 67.000 (67.000)   [2025-10-24 23:32:13]
  Epoch: [001][100/500]   Time 0.445 (0.578)   Data 0.002 (0.192)   Loss 2.0760 (2.0626)   Prec@1 21.000 (22.525)   Prec@5 71.000 (77.178)   [2025-10-24 23:32:52]
  Epoch: [001][200/500]   Time 0.454 (0.492)   Data 0.002 (0.097)   Loss 1.9314 (2.0371)   Prec@1 26.000 (23.791)   Prec@5 78.000 (78.080)   [2025-10-24 23:33:32]
  Epoch: [001][300/500]   Time 0.337 (0.455)   Data 0.001 (0.065)   Loss 1.8935 (2.0160)   Prec@1 26.000 (24.581)   Prec@5 85.000 (78.874)   [2025-10-24 23:34:11]
  Epoch: [001][400/500]   Time 0.359 (0.438)   Data 0.001 (0.049)   Loss 1.8136 (1.9953)   Prec@1 33.000 (25.347)   Prec@5 87.000 (79.589)   [2025-10-24 23:34:49]
  **Train** Prec@1 25.782 Prec@5 80.254 Error@1 74.218
  **Test** Prec@1 32.000 Prec@5 86.130 Error@1 68.000
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 23:35:46] [Epoch=002/040] [Need: 02:27:27] [LR=0.0100] [Best : Accuracy=32.00, Error=68.00]
  Epoch: [002][000/500]   Time 18.569 (18.569)   Data 18.022 (18.022)   Loss 2.0560 (2.0560)   Prec@1 32.000 (32.000)   Prec@5 79.000 (79.000)   [2025-10-24 23:36:05]
  Epoch: [002][100/500]   Time 0.368 (0.556)   Data 0.001 (0.180)   Loss 1.9759 (1.8955)   Prec@1 27.000 (28.248)   Prec@5 88.000 (83.832)   [2025-10-24 23:36:42]
  Epoch: [002][200/500]   Time 0.394 (0.467)   Data 0.001 (0.091)   Loss 1.8074 (1.8779)   Prec@1 32.000 (28.871)   Prec@5 85.000 (84.139)   [2025-10-24 23:37:20]
  Epoch: [002][300/500]   Time 0.337 (0.436)   Data 0.001 (0.061)   Loss 1.8435 (1.8645)   Prec@1 28.000 (29.495)   Prec@5 88.000 (84.668)   [2025-10-24 23:37:57]
  Epoch: [002][400/500]   Time 0.394 (0.423)   Data 0.001 (0.046)   Loss 1.8653 (1.8505)   Prec@1 27.000 (29.875)   Prec@5 88.000 (85.107)   [2025-10-24 23:38:36]
  **Train** Prec@1 30.208 Prec@5 85.346 Error@1 69.792
  **Test** Prec@1 36.980 Prec@5 90.030 Error@1 63.020
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 23:39:32] [Epoch=003/040] [Need: 02:22:06] [LR=0.0100] [Best : Accuracy=36.98, Error=63.02]
  Epoch: [003][000/500]   Time 19.514 (19.514)   Data 18.980 (18.980)   Loss 1.7036 (1.7036)   Prec@1 29.000 (29.000)   Prec@5 95.000 (95.000)   [2025-10-24 23:39:51]
  Epoch: [003][100/500]   Time 0.327 (0.571)   Data 0.000 (0.189)   Loss 1.8410 (1.7619)   Prec@1 33.000 (33.257)   Prec@5 82.000 (87.386)   [2025-10-24 23:40:29]
  Epoch: [003][200/500]   Time 0.414 (0.476)   Data 0.002 (0.096)   Loss 1.7623 (1.7686)   Prec@1 31.000 (33.134)   Prec@5 90.000 (87.095)   [2025-10-24 23:41:07]
  Epoch: [003][300/500]   Time 0.402 (0.443)   Data 0.002 (0.064)   Loss 1.7170 (1.7565)   Prec@1 34.000 (33.658)   Prec@5 93.000 (87.286)   [2025-10-24 23:41:45]
  Epoch: [003][400/500]   Time 0.344 (0.426)   Data 0.001 (0.049)   Loss 1.7984 (1.7500)   Prec@1 28.000 (33.910)   Prec@5 90.000 (87.551)   [2025-10-24 23:42:23]
  **Train** Prec@1 34.232 Prec@5 87.808 Error@1 65.768
  **Test** Prec@1 42.400 Prec@5 91.910 Error@1 57.600
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 23:43:19] [Epoch=004/040] [Need: 02:17:45] [LR=0.0100] [Best : Accuracy=42.40, Error=57.60]
  Epoch: [004][000/500]   Time 18.388 (18.388)   Data 17.826 (17.826)   Loss 1.6480 (1.6480)   Prec@1 35.000 (35.000)   Prec@5 93.000 (93.000)   [2025-10-24 23:43:37]
  Epoch: [004][100/500]   Time 0.401 (0.552)   Data 0.002 (0.178)   Loss 1.5457 (1.6831)   Prec@1 40.000 (36.317)   Prec@5 92.000 (89.228)   [2025-10-24 23:44:14]
  Epoch: [004][200/500]   Time 0.338 (0.467)   Data 0.001 (0.090)   Loss 1.7254 (1.6867)   Prec@1 30.000 (36.657)   Prec@5 93.000 (88.801)   [2025-10-24 23:44:53]
  Epoch: [004][300/500]   Time 0.352 (0.437)   Data 0.002 (0.061)   Loss 1.7300 (1.6801)   Prec@1 36.000 (37.166)   Prec@5 88.000 (88.827)   [2025-10-24 23:45:30]
  Epoch: [004][400/500]   Time 0.352 (0.422)   Data 0.002 (0.046)   Loss 1.7492 (1.6780)   Prec@1 32.000 (37.317)   Prec@5 87.000 (88.840)   [2025-10-24 23:46:08]
  **Train** Prec@1 37.408 Prec@5 88.960 Error@1 62.592
  **Test** Prec@1 44.960 Prec@5 92.000 Error@1 55.040
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 23:47:04] [Epoch=005/040] [Need: 02:13:25] [LR=0.0100] [Best : Accuracy=44.96, Error=55.04]
  Epoch: [005][000/500]   Time 18.212 (18.212)   Data 17.672 (17.672)   Loss 1.5921 (1.5921)   Prec@1 37.000 (37.000)   Prec@5 92.000 (92.000)   [2025-10-24 23:47:22]
  Epoch: [005][100/500]   Time 0.388 (0.551)   Data 0.001 (0.176)   Loss 1.6623 (1.6342)   Prec@1 39.000 (39.653)   Prec@5 92.000 (89.584)   [2025-10-24 23:48:00]
  Epoch: [005][200/500]   Time 0.417 (0.464)   Data 0.002 (0.089)   Loss 1.5080 (1.6243)   Prec@1 41.000 (39.682)   Prec@5 92.000 (89.960)   [2025-10-24 23:48:37]
  Epoch: [005][300/500]   Time 0.416 (0.436)   Data 0.002 (0.060)   Loss 1.6810 (1.6243)   Prec@1 33.000 (39.598)   Prec@5 90.000 (90.030)   [2025-10-24 23:49:15]
  Epoch: [005][400/500]   Time 0.412 (0.422)   Data 0.002 (0.045)   Loss 1.6144 (1.6177)   Prec@1 37.000 (39.828)   Prec@5 87.000 (90.075)   [2025-10-24 23:49:53]
  **Train** Prec@1 40.060 Prec@5 90.146 Error@1 59.940
  **Test** Prec@1 46.650 Prec@5 93.120 Error@1 53.350
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 23:50:52] [Epoch=006/040] [Need: 02:09:33] [LR=0.0100] [Best : Accuracy=46.65, Error=53.35]
  Epoch: [006][000/500]   Time 19.519 (19.519)   Data 18.950 (18.950)   Loss 1.5195 (1.5195)   Prec@1 42.000 (42.000)   Prec@5 95.000 (95.000)   [2025-10-24 23:51:12]
  Epoch: [006][100/500]   Time 0.327 (0.565)   Data 0.000 (0.189)   Loss 1.6251 (1.5945)   Prec@1 42.000 (40.861)   Prec@5 89.000 (90.079)   [2025-10-24 23:51:49]
  Epoch: [006][200/500]   Time 0.337 (0.471)   Data 0.000 (0.096)   Loss 1.4917 (1.5831)   Prec@1 45.000 (41.592)   Prec@5 92.000 (90.303)   [2025-10-24 23:52:27]
  Epoch: [006][300/500]   Time 0.447 (0.441)   Data 0.002 (0.064)   Loss 1.4401 (1.5761)   Prec@1 38.000 (42.037)   Prec@5 95.000 (90.449)   [2025-10-24 23:53:05]
  Epoch: [006][400/500]   Time 0.402 (0.427)   Data 0.002 (0.049)   Loss 1.4895 (1.5714)   Prec@1 46.000 (42.092)   Prec@5 92.000 (90.504)   [2025-10-24 23:53:43]
  **Train** Prec@1 42.404 Prec@5 90.688 Error@1 57.596
  **Test** Prec@1 50.270 Prec@5 93.800 Error@1 49.730
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 23:54:39] [Epoch=007/040] [Need: 02:05:37] [LR=0.0100] [Best : Accuracy=50.27, Error=49.73]
  Epoch: [007][000/500]   Time 19.287 (19.287)   Data 18.743 (18.743)   Loss 1.4735 (1.4735)   Prec@1 49.000 (49.000)   Prec@5 94.000 (94.000)   [2025-10-24 23:54:59]
  Epoch: [007][100/500]   Time 0.412 (0.565)   Data 0.002 (0.187)   Loss 1.7137 (1.5303)   Prec@1 37.000 (43.673)   Prec@5 89.000 (91.000)   [2025-10-24 23:55:36]
  Epoch: [007][200/500]   Time 0.392 (0.475)   Data 0.000 (0.095)   Loss 1.5271 (1.5186)   Prec@1 37.000 (43.995)   Prec@5 95.000 (91.418)   [2025-10-24 23:56:15]
  Epoch: [007][300/500]   Time 0.400 (0.446)   Data 0.002 (0.064)   Loss 1.5607 (1.5102)   Prec@1 38.000 (44.508)   Prec@5 94.000 (91.565)   [2025-10-24 23:56:53]
  Epoch: [007][400/500]   Time 0.395 (0.428)   Data 0.002 (0.048)   Loss 1.4648 (1.5097)   Prec@1 47.000 (44.406)   Prec@5 93.000 (91.621)   [2025-10-24 23:57:31]
  **Train** Prec@1 44.606 Prec@5 91.554 Error@1 55.394
  **Test** Prec@1 51.540 Prec@5 94.460 Error@1 48.460
=> Obtain best accuracy, and update the best model

==>>[2025-10-24 23:58:26] [Epoch=008/040] [Need: 02:01:43] [LR=0.0100] [Best : Accuracy=51.54, Error=48.46]
  Epoch: [008][000/500]   Time 18.199 (18.199)   Data 17.654 (17.654)   Loss 1.5254 (1.5254)   Prec@1 40.000 (40.000)   Prec@5 91.000 (91.000)   [2025-10-24 23:58:44]
  Epoch: [008][100/500]   Time 0.336 (0.549)   Data 0.000 (0.176)   Loss 1.6975 (1.4925)   Prec@1 37.000 (45.000)   Prec@5 89.000 (91.911)   [2025-10-24 23:59:22]
  Epoch: [008][200/500]   Time 0.431 (0.467)   Data 0.004 (0.089)   Loss 1.3714 (1.4831)   Prec@1 46.000 (45.104)   Prec@5 93.000 (91.930)   [2025-10-25 00:00:00]
  Epoch: [008][300/500]   Time 0.427 (0.438)   Data 0.002 (0.060)   Loss 1.4407 (1.4731)   Prec@1 48.000 (45.757)   Prec@5 93.000 (92.056)   [2025-10-25 00:00:38]
  Epoch: [008][400/500]   Time 0.413 (0.424)   Data 0.002 (0.045)   Loss 1.3697 (1.4685)   Prec@1 51.000 (46.130)   Prec@5 94.000 (92.095)   [2025-10-25 00:01:16]
  **Train** Prec@1 46.440 Prec@5 92.200 Error@1 53.560
  **Test** Prec@1 53.400 Prec@5 94.570 Error@1 46.600
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 00:02:13] [Epoch=009/040] [Need: 01:57:49] [LR=0.0100] [Best : Accuracy=53.40, Error=46.60]
  Epoch: [009][000/500]   Time 18.253 (18.253)   Data 17.703 (17.703)   Loss 1.5483 (1.5483)   Prec@1 41.000 (41.000)   Prec@5 91.000 (91.000)   [2025-10-25 00:02:31]
  Epoch: [009][100/500]   Time 0.400 (0.550)   Data 0.002 (0.177)   Loss 1.4433 (1.4209)   Prec@1 43.000 (48.109)   Prec@5 95.000 (92.842)   [2025-10-25 00:03:08]
  Epoch: [009][200/500]   Time 0.333 (0.463)   Data 0.001 (0.089)   Loss 1.4134 (1.4184)   Prec@1 46.000 (48.095)   Prec@5 95.000 (92.920)   [2025-10-25 00:03:46]
  Epoch: [009][300/500]   Time 0.414 (0.433)   Data 0.002 (0.060)   Loss 1.5085 (1.4231)   Prec@1 43.000 (47.930)   Prec@5 95.000 (92.691)   [2025-10-25 00:04:23]
  Epoch: [009][400/500]   Time 0.335 (0.420)   Data 0.000 (0.045)   Loss 1.5165 (1.4175)   Prec@1 40.000 (48.117)   Prec@5 94.000 (92.721)   [2025-10-25 00:05:01]
  **Train** Prec@1 48.484 Prec@5 92.792 Error@1 51.516
  **Test** Prec@1 54.580 Prec@5 95.200 Error@1 45.420
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 00:05:58] [Epoch=010/040] [Need: 01:53:51] [LR=0.0100] [Best : Accuracy=54.58, Error=45.42]
  Epoch: [010][000/500]   Time 18.378 (18.378)   Data 17.821 (17.821)   Loss 1.2494 (1.2494)   Prec@1 54.000 (54.000)   Prec@5 93.000 (93.000)   [2025-10-25 00:06:16]
  Epoch: [010][100/500]   Time 0.387 (0.551)   Data 0.000 (0.178)   Loss 1.4751 (1.3687)   Prec@1 55.000 (49.406)   Prec@5 89.000 (93.347)   [2025-10-25 00:06:53]
  Epoch: [010][200/500]   Time 0.332 (0.462)   Data 0.000 (0.090)   Loss 1.3533 (1.3765)   Prec@1 48.000 (49.468)   Prec@5 92.000 (93.035)   [2025-10-25 00:07:31]
  Epoch: [010][300/500]   Time 0.395 (0.433)   Data 0.002 (0.060)   Loss 1.5274 (1.3789)   Prec@1 45.000 (49.565)   Prec@5 89.000 (93.060)   [2025-10-25 00:08:08]
  Epoch: [010][400/500]   Time 0.354 (0.419)   Data 0.000 (0.046)   Loss 1.2940 (1.3747)   Prec@1 51.000 (49.611)   Prec@5 96.000 (93.127)   [2025-10-25 00:08:46]
  **Train** Prec@1 49.524 Prec@5 93.170 Error@1 50.476
  **Test** Prec@1 58.230 Prec@5 95.600 Error@1 41.770
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 00:09:44] [Epoch=011/040] [Need: 01:50:00] [LR=0.0100] [Best : Accuracy=58.23, Error=41.77]
  Epoch: [011][000/500]   Time 19.821 (19.821)   Data 19.244 (19.244)   Loss 1.3632 (1.3632)   Prec@1 52.000 (52.000)   Prec@5 92.000 (92.000)   [2025-10-25 00:10:04]
  Epoch: [011][100/500]   Time 0.417 (0.581)   Data 0.002 (0.192)   Loss 1.3400 (1.3517)   Prec@1 49.000 (51.703)   Prec@5 94.000 (93.208)   [2025-10-25 00:10:43]
  Epoch: [011][200/500]   Time 0.362 (0.484)   Data 0.001 (0.097)   Loss 1.2042 (1.3471)   Prec@1 57.000 (51.776)   Prec@5 95.000 (93.443)   [2025-10-25 00:11:22]
  Epoch: [011][300/500]   Time 0.418 (0.451)   Data 0.002 (0.065)   Loss 1.4764 (1.3498)   Prec@1 46.000 (51.276)   Prec@5 85.000 (93.432)   [2025-10-25 00:12:00]
  Epoch: [011][400/500]   Time 0.408 (0.434)   Data 0.002 (0.049)   Loss 1.2101 (1.3471)   Prec@1 51.000 (51.272)   Prec@5 95.000 (93.499)   [2025-10-25 00:12:38]
  **Train** Prec@1 51.612 Prec@5 93.498 Error@1 48.388
  **Test** Prec@1 58.920 Prec@5 95.420 Error@1 41.080
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 00:13:34] [Epoch=012/040] [Need: 01:46:18] [LR=0.0100] [Best : Accuracy=58.92, Error=41.08]
  Epoch: [012][000/500]   Time 18.399 (18.399)   Data 17.864 (17.864)   Loss 1.3369 (1.3369)   Prec@1 48.000 (48.000)   Prec@5 95.000 (95.000)   [2025-10-25 00:13:53]
  Epoch: [012][100/500]   Time 0.403 (0.553)   Data 0.001 (0.178)   Loss 1.4341 (1.3207)   Prec@1 50.000 (52.802)   Prec@5 93.000 (93.545)   [2025-10-25 00:14:30]
  Epoch: [012][200/500]   Time 0.363 (0.467)   Data 0.001 (0.090)   Loss 1.3109 (1.3026)   Prec@1 50.000 (52.975)   Prec@5 96.000 (93.861)   [2025-10-25 00:15:08]
  Epoch: [012][300/500]   Time 0.330 (0.437)   Data 0.001 (0.061)   Loss 1.1564 (1.3028)   Prec@1 66.000 (52.944)   Prec@5 95.000 (93.877)   [2025-10-25 00:15:46]
  Epoch: [012][400/500]   Time 0.323 (0.425)   Data 0.000 (0.046)   Loss 1.3176 (1.3002)   Prec@1 60.000 (52.933)   Prec@5 93.000 (93.910)   [2025-10-25 00:16:25]
  **Train** Prec@1 53.088 Prec@5 93.882 Error@1 46.912
  **Test** Prec@1 60.880 Prec@5 96.270 Error@1 39.120
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 00:17:20] [Epoch=013/040] [Need: 01:42:27] [LR=0.0100] [Best : Accuracy=60.88, Error=39.12]
  Epoch: [013][000/500]   Time 18.399 (18.399)   Data 17.863 (17.863)   Loss 1.3467 (1.3467)   Prec@1 44.000 (44.000)   Prec@5 98.000 (98.000)   [2025-10-25 00:17:39]
  Epoch: [013][100/500]   Time 0.409 (0.542)   Data 0.001 (0.178)   Loss 1.0981 (1.2763)   Prec@1 56.000 (53.515)   Prec@5 96.000 (94.178)   [2025-10-25 00:18:15]
  Epoch: [013][200/500]   Time 0.348 (0.457)   Data 0.002 (0.090)   Loss 1.1696 (1.2829)   Prec@1 67.000 (53.552)   Prec@5 92.000 (94.323)   [2025-10-25 00:18:52]
  Epoch: [013][300/500]   Time 0.392 (0.430)   Data 0.002 (0.061)   Loss 1.2334 (1.2764)   Prec@1 60.000 (53.957)   Prec@5 95.000 (94.322)   [2025-10-25 00:19:30]
  Epoch: [013][400/500]   Time 0.402 (0.416)   Data 0.002 (0.046)   Loss 1.3647 (1.2690)   Prec@1 56.000 (54.229)   Prec@5 95.000 (94.411)   [2025-10-25 00:20:07]
  **Train** Prec@1 54.430 Prec@5 94.416 Error@1 45.570
  **Test** Prec@1 61.120 Prec@5 95.920 Error@1 38.880
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 00:21:04] [Epoch=014/040] [Need: 01:38:32] [LR=0.0100] [Best : Accuracy=61.12, Error=38.88]
  Epoch: [014][000/500]   Time 18.339 (18.339)   Data 17.807 (17.807)   Loss 1.2484 (1.2484)   Prec@1 54.000 (54.000)   Prec@5 95.000 (95.000)   [2025-10-25 00:21:22]
  Epoch: [014][100/500]   Time 0.326 (0.548)   Data 0.001 (0.178)   Loss 1.1445 (1.2322)   Prec@1 63.000 (55.911)   Prec@5 97.000 (94.208)   [2025-10-25 00:21:59]
  Epoch: [014][200/500]   Time 0.404 (0.460)   Data 0.001 (0.090)   Loss 1.3257 (1.2429)   Prec@1 49.000 (55.403)   Prec@5 98.000 (94.294)   [2025-10-25 00:22:36]
  Epoch: [014][300/500]   Time 0.387 (0.432)   Data 0.001 (0.061)   Loss 1.4571 (1.2390)   Prec@1 45.000 (55.495)   Prec@5 91.000 (94.429)   [2025-10-25 00:23:14]
  Epoch: [014][400/500]   Time 0.329 (0.419)   Data 0.000 (0.046)   Loss 1.0398 (1.2399)   Prec@1 60.000 (55.544)   Prec@5 95.000 (94.397)   [2025-10-25 00:23:52]
  **Train** Prec@1 55.760 Prec@5 94.422 Error@1 44.240
  **Test** Prec@1 63.790 Prec@5 96.710 Error@1 36.210
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 00:24:50] [Epoch=015/040] [Need: 01:34:42] [LR=0.0100] [Best : Accuracy=63.79, Error=36.21]
  Epoch: [015][000/500]   Time 18.574 (18.574)   Data 18.029 (18.029)   Loss 1.0873 (1.0873)   Prec@1 61.000 (61.000)   Prec@5 96.000 (96.000)   [2025-10-25 00:25:08]
  Epoch: [015][100/500]   Time 0.335 (0.558)   Data 0.000 (0.180)   Loss 1.1595 (1.2198)   Prec@1 60.000 (56.376)   Prec@5 93.000 (94.713)   [2025-10-25 00:25:46]
  Epoch: [015][200/500]   Time 0.401 (0.469)   Data 0.002 (0.091)   Loss 1.3310 (1.2140)   Prec@1 51.000 (56.557)   Prec@5 95.000 (94.771)   [2025-10-25 00:26:24]
  Epoch: [015][300/500]   Time 0.322 (0.437)   Data 0.001 (0.061)   Loss 1.2447 (1.2136)   Prec@1 60.000 (57.007)   Prec@5 96.000 (94.837)   [2025-10-25 00:27:01]
  Epoch: [015][400/500]   Time 0.327 (0.421)   Data 0.000 (0.046)   Loss 1.2551 (1.2091)   Prec@1 56.000 (57.065)   Prec@5 98.000 (94.820)   [2025-10-25 00:27:39]
  **Train** Prec@1 57.142 Prec@5 94.812 Error@1 42.858
  **Test** Prec@1 64.770 Prec@5 96.940 Error@1 35.230
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 00:28:35] [Epoch=016/040] [Need: 01:30:51] [LR=0.0100] [Best : Accuracy=64.77, Error=35.23]
  Epoch: [016][000/500]   Time 18.389 (18.389)   Data 17.849 (17.849)   Loss 1.0822 (1.0822)   Prec@1 61.000 (61.000)   Prec@5 96.000 (96.000)   [2025-10-25 00:28:53]
  Epoch: [016][100/500]   Time 0.320 (0.550)   Data 0.000 (0.178)   Loss 1.3162 (1.1782)   Prec@1 52.000 (57.950)   Prec@5 94.000 (94.752)   [2025-10-25 00:29:30]
  Epoch: [016][200/500]   Time 0.328 (0.460)   Data 0.000 (0.090)   Loss 1.0517 (1.1767)   Prec@1 64.000 (58.075)   Prec@5 97.000 (94.876)   [2025-10-25 00:30:07]
  Epoch: [016][300/500]   Time 0.336 (0.430)   Data 0.001 (0.061)   Loss 1.2047 (1.1806)   Prec@1 56.000 (57.910)   Prec@5 94.000 (94.894)   [2025-10-25 00:30:44]
  Epoch: [016][400/500]   Time 0.346 (0.418)   Data 0.002 (0.046)   Loss 1.1502 (1.1788)   Prec@1 65.000 (58.080)   Prec@5 93.000 (94.945)   [2025-10-25 00:31:22]
  **Train** Prec@1 58.218 Prec@5 95.034 Error@1 41.782
  **Test** Prec@1 65.750 Prec@5 96.970 Error@1 34.250
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 00:32:17] [Epoch=017/040] [Need: 01:26:58] [LR=0.0100] [Best : Accuracy=65.75, Error=34.25]
  Epoch: [017][000/500]   Time 18.433 (18.433)   Data 17.865 (17.865)   Loss 1.1570 (1.1570)   Prec@1 61.000 (61.000)   Prec@5 95.000 (95.000)   [2025-10-25 00:32:36]
  Epoch: [017][100/500]   Time 0.343 (0.553)   Data 0.001 (0.178)   Loss 1.1715 (1.1463)   Prec@1 63.000 (59.158)   Prec@5 92.000 (94.990)   [2025-10-25 00:33:13]
  Epoch: [017][200/500]   Time 0.399 (0.464)   Data 0.002 (0.090)   Loss 1.1167 (1.1520)   Prec@1 61.000 (59.114)   Prec@5 95.000 (94.995)   [2025-10-25 00:33:50]
  Epoch: [017][300/500]   Time 0.393 (0.434)   Data 0.002 (0.061)   Loss 1.1215 (1.1562)   Prec@1 60.000 (59.040)   Prec@5 93.000 (95.013)   [2025-10-25 00:34:28]
  Epoch: [017][400/500]   Time 0.419 (0.419)   Data 0.002 (0.046)   Loss 1.1245 (1.1531)   Prec@1 60.000 (59.070)   Prec@5 95.000 (95.060)   [2025-10-25 00:35:05]
  **Train** Prec@1 59.190 Prec@5 95.182 Error@1 40.810
  **Test** Prec@1 67.310 Prec@5 97.330 Error@1 32.690
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 00:36:01] [Epoch=018/040] [Need: 01:23:07] [LR=0.0100] [Best : Accuracy=67.31, Error=32.69]
  Epoch: [018][000/500]   Time 18.307 (18.307)   Data 17.768 (17.768)   Loss 1.0049 (1.0049)   Prec@1 69.000 (69.000)   Prec@5 95.000 (95.000)   [2025-10-25 00:36:20]
  Epoch: [018][100/500]   Time 0.347 (0.540)   Data 0.002 (0.177)   Loss 1.1354 (1.1438)   Prec@1 63.000 (59.723)   Prec@5 97.000 (95.149)   [2025-10-25 00:36:56]
  Epoch: [018][200/500]   Time 0.399 (0.455)   Data 0.002 (0.090)   Loss 1.2444 (1.1389)   Prec@1 59.000 (59.697)   Prec@5 96.000 (95.219)   [2025-10-25 00:37:33]
  Epoch: [018][300/500]   Time 0.404 (0.427)   Data 0.002 (0.060)   Loss 1.1497 (1.1343)   Prec@1 61.000 (59.897)   Prec@5 95.000 (95.233)   [2025-10-25 00:38:10]
  Epoch: [018][400/500]   Time 0.341 (0.411)   Data 0.002 (0.046)   Loss 1.1548 (1.1291)   Prec@1 61.000 (60.247)   Prec@5 96.000 (95.334)   [2025-10-25 00:38:46]
  **Train** Prec@1 60.362 Prec@5 95.410 Error@1 39.638
  **Test** Prec@1 68.640 Prec@5 97.460 Error@1 31.360
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 00:39:41] [Epoch=019/040] [Need: 01:19:13] [LR=0.0100] [Best : Accuracy=68.64, Error=31.36]
  Epoch: [019][000/500]   Time 18.685 (18.685)   Data 18.153 (18.153)   Loss 0.9994 (0.9994)   Prec@1 59.000 (59.000)   Prec@5 96.000 (96.000)   [2025-10-25 00:40:00]
  Epoch: [019][100/500]   Time 0.374 (0.553)   Data 0.001 (0.181)   Loss 1.1506 (1.1100)   Prec@1 62.000 (60.554)   Prec@5 95.000 (95.366)   [2025-10-25 00:40:37]
  Epoch: [019][200/500]   Time 0.391 (0.467)   Data 0.001 (0.092)   Loss 1.1617 (1.1008)   Prec@1 61.000 (60.985)   Prec@5 94.000 (95.622)   [2025-10-25 00:41:15]
  Epoch: [019][300/500]   Time 0.355 (0.436)   Data 0.001 (0.062)   Loss 1.0805 (1.1078)   Prec@1 59.000 (60.611)   Prec@5 94.000 (95.611)   [2025-10-25 00:41:53]
  Epoch: [019][400/500]   Time 0.342 (0.421)   Data 0.002 (0.047)   Loss 1.2093 (1.1044)   Prec@1 56.000 (60.668)   Prec@5 94.000 (95.688)   [2025-10-25 00:42:30]
  **Train** Prec@1 60.734 Prec@5 95.694 Error@1 39.266
  **Test** Prec@1 68.730 Prec@5 97.690 Error@1 31.270
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 00:43:27] [Epoch=020/040] [Need: 01:15:26] [LR=0.0100] [Best : Accuracy=68.73, Error=31.27]
  Epoch: [020][000/500]   Time 18.272 (18.272)   Data 17.737 (17.737)   Loss 1.1726 (1.1726)   Prec@1 63.000 (63.000)   Prec@5 95.000 (95.000)   [2025-10-25 00:43:45]
  Epoch: [020][100/500]   Time 0.409 (0.554)   Data 0.002 (0.177)   Loss 1.1103 (1.1065)   Prec@1 66.000 (61.257)   Prec@5 94.000 (95.653)   [2025-10-25 00:44:23]
  Epoch: [020][200/500]   Time 0.393 (0.465)   Data 0.002 (0.090)   Loss 1.2358 (1.0966)   Prec@1 56.000 (61.428)   Prec@5 94.000 (95.602)   [2025-10-25 00:45:00]
  Epoch: [020][300/500]   Time 0.340 (0.435)   Data 0.001 (0.060)   Loss 1.0498 (1.0891)   Prec@1 63.000 (61.701)   Prec@5 97.000 (95.714)   [2025-10-25 00:45:38]
  Epoch: [020][400/500]   Time 0.331 (0.418)   Data 0.000 (0.046)   Loss 1.1074 (1.0921)   Prec@1 63.000 (61.636)   Prec@5 93.000 (95.768)   [2025-10-25 00:46:14]
  **Train** Prec@1 61.700 Prec@5 95.780 Error@1 38.300
  **Test** Prec@1 68.840 Prec@5 97.610 Error@1 31.160
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 00:47:11] [Epoch=021/040] [Need: 01:11:37] [LR=0.0100] [Best : Accuracy=68.84, Error=31.16]
  Epoch: [021][000/500]   Time 18.297 (18.297)   Data 17.767 (17.767)   Loss 0.9111 (0.9111)   Prec@1 65.000 (65.000)   Prec@5 99.000 (99.000)   [2025-10-25 00:47:29]
  Epoch: [021][100/500]   Time 0.342 (0.556)   Data 0.002 (0.177)   Loss 1.1849 (1.0628)   Prec@1 58.000 (62.525)   Prec@5 94.000 (96.218)   [2025-10-25 00:48:07]
  Epoch: [021][200/500]   Time 0.328 (0.463)   Data 0.000 (0.090)   Loss 1.0629 (1.0589)   Prec@1 72.000 (62.726)   Prec@5 92.000 (96.030)   [2025-10-25 00:48:44]
  Epoch: [021][300/500]   Time 0.400 (0.435)   Data 0.002 (0.060)   Loss 0.9417 (1.0690)   Prec@1 69.000 (62.336)   Prec@5 98.000 (96.033)   [2025-10-25 00:49:22]
  Epoch: [021][400/500]   Time 0.323 (0.422)   Data 0.000 (0.046)   Loss 1.0229 (1.0702)   Prec@1 66.000 (62.272)   Prec@5 98.000 (96.000)   [2025-10-25 00:50:00]
  **Train** Prec@1 62.286 Prec@5 95.960 Error@1 37.714
  **Test** Prec@1 69.670 Prec@5 97.720 Error@1 30.330
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 00:50:56] [Epoch=022/040] [Need: 01:07:51] [LR=0.0100] [Best : Accuracy=69.67, Error=30.33]
  Epoch: [022][000/500]   Time 18.348 (18.348)   Data 17.794 (17.794)   Loss 1.2237 (1.2237)   Prec@1 56.000 (56.000)   Prec@5 96.000 (96.000)   [2025-10-25 00:51:14]
  Epoch: [022][100/500]   Time 0.353 (0.549)   Data 0.002 (0.178)   Loss 0.8721 (1.0441)   Prec@1 70.000 (62.941)   Prec@5 97.000 (96.347)   [2025-10-25 00:51:52]
  Epoch: [022][200/500]   Time 0.394 (0.463)   Data 0.002 (0.090)   Loss 1.1859 (1.0508)   Prec@1 62.000 (62.910)   Prec@5 92.000 (96.060)   [2025-10-25 00:52:29]
  Epoch: [022][300/500]   Time 0.400 (0.432)   Data 0.001 (0.060)   Loss 1.1380 (1.0498)   Prec@1 59.000 (62.870)   Prec@5 93.000 (96.033)   [2025-10-25 00:53:06]
  Epoch: [022][400/500]   Time 0.394 (0.418)   Data 0.002 (0.046)   Loss 0.9790 (1.0501)   Prec@1 65.000 (62.863)   Prec@5 97.000 (96.065)   [2025-10-25 00:53:44]
  **Train** Prec@1 62.954 Prec@5 96.038 Error@1 37.046
  **Test** Prec@1 70.420 Prec@5 97.840 Error@1 29.580
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 00:54:39] [Epoch=023/040] [Need: 01:04:02] [LR=0.0100] [Best : Accuracy=70.42, Error=29.58]
  Epoch: [023][000/500]   Time 19.621 (19.621)   Data 19.083 (19.083)   Loss 1.1107 (1.1107)   Prec@1 59.000 (59.000)   Prec@5 94.000 (94.000)   [2025-10-25 00:54:59]
  Epoch: [023][100/500]   Time 0.354 (0.558)   Data 0.001 (0.190)   Loss 1.0533 (1.0422)   Prec@1 63.000 (63.337)   Prec@5 97.000 (96.109)   [2025-10-25 00:55:36]
  Epoch: [023][200/500]   Time 0.392 (0.466)   Data 0.002 (0.096)   Loss 1.1196 (1.0404)   Prec@1 59.000 (63.478)   Prec@5 96.000 (96.224)   [2025-10-25 00:56:13]
  Epoch: [023][300/500]   Time 0.413 (0.435)   Data 0.002 (0.065)   Loss 0.9688 (1.0350)   Prec@1 63.000 (63.542)   Prec@5 96.000 (96.229)   [2025-10-25 00:56:50]
  Epoch: [023][400/500]   Time 0.398 (0.421)   Data 0.002 (0.049)   Loss 0.9009 (1.0361)   Prec@1 66.000 (63.496)   Prec@5 98.000 (96.177)   [2025-10-25 00:57:28]
  **Train** Prec@1 63.806 Prec@5 96.198 Error@1 36.194
  **Test** Prec@1 72.000 Prec@5 98.040 Error@1 28.000
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 00:58:24] [Epoch=024/040] [Need: 01:00:15] [LR=0.0100] [Best : Accuracy=72.00, Error=28.00]
  Epoch: [024][000/500]   Time 18.332 (18.332)   Data 17.791 (17.791)   Loss 1.0176 (1.0176)   Prec@1 62.000 (62.000)   Prec@5 97.000 (97.000)   [2025-10-25 00:58:42]
  Epoch: [024][100/500]   Time 0.366 (0.551)   Data 0.001 (0.177)   Loss 0.8833 (1.0081)   Prec@1 68.000 (65.515)   Prec@5 97.000 (96.327)   [2025-10-25 00:59:20]
  Epoch: [024][200/500]   Time 0.350 (0.463)   Data 0.001 (0.090)   Loss 1.1617 (0.9946)   Prec@1 61.000 (65.697)   Prec@5 95.000 (96.468)   [2025-10-25 00:59:57]
  Epoch: [024][300/500]   Time 0.325 (0.434)   Data 0.001 (0.060)   Loss 1.0310 (1.0021)   Prec@1 63.000 (65.163)   Prec@5 93.000 (96.382)   [2025-10-25 01:00:35]
  Epoch: [024][400/500]   Time 0.391 (0.421)   Data 0.001 (0.046)   Loss 1.1894 (1.0078)   Prec@1 61.000 (64.855)   Prec@5 94.000 (96.389)   [2025-10-25 01:01:13]
  **Train** Prec@1 64.798 Prec@5 96.366 Error@1 35.202
  **Test** Prec@1 70.770 Prec@5 98.040 Error@1 29.230

==>>[2025-10-25 01:02:09] [Epoch=025/040] [Need: 00:56:29] [LR=0.0010] [Best : Accuracy=72.00, Error=28.00]
  Epoch: [025][000/500]   Time 18.279 (18.279)   Data 17.734 (17.734)   Loss 0.8568 (0.8568)   Prec@1 72.000 (72.000)   Prec@5 98.000 (98.000)   [2025-10-25 01:02:27]
  Epoch: [025][100/500]   Time 0.340 (0.546)   Data 0.001 (0.177)   Loss 0.7876 (0.9683)   Prec@1 71.000 (66.455)   Prec@5 100.000 (96.713)   [2025-10-25 01:03:04]
  Epoch: [025][200/500]   Time 0.410 (0.458)   Data 0.001 (0.089)   Loss 0.9129 (0.9613)   Prec@1 62.000 (66.448)   Prec@5 98.000 (96.846)   [2025-10-25 01:03:41]
  Epoch: [025][300/500]   Time 0.397 (0.432)   Data 0.002 (0.060)   Loss 0.8747 (0.9509)   Prec@1 70.000 (66.834)   Prec@5 96.000 (96.907)   [2025-10-25 01:04:19]
  Epoch: [025][400/500]   Time 0.400 (0.419)   Data 0.002 (0.046)   Loss 0.9219 (0.9425)   Prec@1 62.000 (67.140)   Prec@5 98.000 (96.978)   [2025-10-25 01:04:57]
  **Train** Prec@1 67.318 Prec@5 97.022 Error@1 32.682
  **Test** Prec@1 74.240 Prec@5 98.380 Error@1 25.760
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 01:05:54] [Epoch=026/040] [Need: 00:52:42] [LR=0.0010] [Best : Accuracy=74.24, Error=25.76]
  Epoch: [026][000/500]   Time 18.297 (18.297)   Data 17.760 (17.760)   Loss 0.9088 (0.9088)   Prec@1 68.000 (68.000)   Prec@5 98.000 (98.000)   [2025-10-25 01:06:12]
  Epoch: [026][100/500]   Time 0.325 (0.551)   Data 0.000 (0.177)   Loss 0.9513 (0.9039)   Prec@1 70.000 (68.733)   Prec@5 96.000 (96.861)   [2025-10-25 01:06:49]
  Epoch: [026][200/500]   Time 0.334 (0.464)   Data 0.000 (0.090)   Loss 0.8194 (0.9145)   Prec@1 72.000 (68.458)   Prec@5 98.000 (96.950)   [2025-10-25 01:07:27]
  Epoch: [026][300/500]   Time 0.326 (0.435)   Data 0.001 (0.060)   Loss 0.9209 (0.9164)   Prec@1 68.000 (68.206)   Prec@5 99.000 (97.027)   [2025-10-25 01:08:05]
  Epoch: [026][400/500]   Time 0.425 (0.426)   Data 0.002 (0.046)   Loss 1.1575 (0.9209)   Prec@1 62.000 (68.012)   Prec@5 95.000 (97.020)   [2025-10-25 01:08:44]
  **Train** Prec@1 68.050 Prec@5 97.054 Error@1 31.950
  **Test** Prec@1 74.500 Prec@5 98.460 Error@1 25.500
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 01:09:41] [Epoch=027/040] [Need: 00:48:57] [LR=0.0010] [Best : Accuracy=74.50, Error=25.50]
  Epoch: [027][000/500]   Time 19.034 (19.034)   Data 18.492 (18.492)   Loss 0.9230 (0.9230)   Prec@1 69.000 (69.000)   Prec@5 97.000 (97.000)   [2025-10-25 01:10:00]
  Epoch: [027][100/500]   Time 0.398 (0.559)   Data 0.002 (0.185)   Loss 0.8676 (0.9016)   Prec@1 73.000 (68.446)   Prec@5 97.000 (97.050)   [2025-10-25 01:10:37]
  Epoch: [027][200/500]   Time 0.367 (0.469)   Data 0.001 (0.093)   Loss 0.8477 (0.9010)   Prec@1 71.000 (68.493)   Prec@5 99.000 (97.134)   [2025-10-25 01:11:15]
  Epoch: [027][300/500]   Time 0.347 (0.436)   Data 0.002 (0.063)   Loss 1.0627 (0.9035)   Prec@1 66.000 (68.458)   Prec@5 98.000 (97.163)   [2025-10-25 01:11:52]
  Epoch: [027][400/500]   Time 0.311 (0.422)   Data 0.001 (0.047)   Loss 0.8127 (0.9014)   Prec@1 76.000 (68.556)   Prec@5 97.000 (97.297)   [2025-10-25 01:12:30]
  **Train** Prec@1 68.432 Prec@5 97.272 Error@1 31.568
  **Test** Prec@1 74.440 Prec@5 98.420 Error@1 25.560

==>>[2025-10-25 01:13:26] [Epoch=028/040] [Need: 00:45:10] [LR=0.0010] [Best : Accuracy=74.50, Error=25.50]
  Epoch: [028][000/500]   Time 18.353 (18.353)   Data 17.809 (17.809)   Loss 0.9350 (0.9350)   Prec@1 68.000 (68.000)   Prec@5 94.000 (94.000)   [2025-10-25 01:13:44]
  Epoch: [028][100/500]   Time 0.332 (0.548)   Data 0.000 (0.177)   Loss 0.7889 (0.9096)   Prec@1 70.000 (68.337)   Prec@5 97.000 (97.188)   [2025-10-25 01:14:21]
  Epoch: [028][200/500]   Time 0.339 (0.462)   Data 0.001 (0.090)   Loss 0.8845 (0.9098)   Prec@1 66.000 (68.164)   Prec@5 97.000 (97.085)   [2025-10-25 01:14:59]
  Epoch: [028][300/500]   Time 0.382 (0.433)   Data 0.001 (0.060)   Loss 0.7744 (0.9037)   Prec@1 69.000 (68.326)   Prec@5 99.000 (97.193)   [2025-10-25 01:15:36]
  Epoch: [028][400/500]   Time 0.436 (0.420)   Data 0.005 (0.046)   Loss 0.8949 (0.9049)   Prec@1 70.000 (68.297)   Prec@5 98.000 (97.237)   [2025-10-25 01:16:14]
  **Train** Prec@1 68.392 Prec@5 97.252 Error@1 31.608
  **Test** Prec@1 75.070 Prec@5 98.400 Error@1 24.930
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 01:17:11] [Epoch=029/040] [Need: 00:41:24] [LR=0.0010] [Best : Accuracy=75.07, Error=24.93]
  Epoch: [029][000/500]   Time 18.381 (18.381)   Data 17.838 (17.838)   Loss 0.8220 (0.8220)   Prec@1 68.000 (68.000)   Prec@5 99.000 (99.000)   [2025-10-25 01:17:29]
  Epoch: [029][100/500]   Time 0.401 (0.555)   Data 0.001 (0.178)   Loss 0.9078 (0.9038)   Prec@1 66.000 (68.535)   Prec@5 98.000 (97.347)   [2025-10-25 01:18:07]
  Epoch: [029][200/500]   Time 0.390 (0.465)   Data 0.002 (0.090)   Loss 0.7719 (0.9055)   Prec@1 73.000 (68.468)   Prec@5 99.000 (97.323)   [2025-10-25 01:18:44]
  Epoch: [029][300/500]   Time 0.399 (0.434)   Data 0.002 (0.060)   Loss 0.8430 (0.8968)   Prec@1 70.000 (68.774)   Prec@5 97.000 (97.375)   [2025-10-25 01:19:21]
  Epoch: [029][400/500]   Time 0.355 (0.421)   Data 0.002 (0.046)   Loss 0.6939 (0.8993)   Prec@1 73.000 (68.569)   Prec@5 98.000 (97.397)   [2025-10-25 01:20:00]
  **Train** Prec@1 68.674 Prec@5 97.348 Error@1 31.326
  **Test** Prec@1 75.060 Prec@5 98.500 Error@1 24.940

==>>[2025-10-25 01:20:56] [Epoch=030/040] [Need: 00:37:38] [LR=0.0010] [Best : Accuracy=75.07, Error=24.93]
  Epoch: [030][000/500]   Time 18.330 (18.330)   Data 17.793 (17.793)   Loss 0.8858 (0.8858)   Prec@1 73.000 (73.000)   Prec@5 99.000 (99.000)   [2025-10-25 01:21:15]
  Epoch: [030][100/500]   Time 0.330 (0.553)   Data 0.000 (0.178)   Loss 0.8609 (0.8972)   Prec@1 65.000 (68.386)   Prec@5 98.000 (97.248)   [2025-10-25 01:21:52]
  Epoch: [030][200/500]   Time 0.337 (0.463)   Data 0.001 (0.090)   Loss 0.9369 (0.8909)   Prec@1 65.000 (68.746)   Prec@5 98.000 (97.363)   [2025-10-25 01:22:30]
  Epoch: [030][300/500]   Time 0.422 (0.435)   Data 0.002 (0.060)   Loss 0.8930 (0.8886)   Prec@1 71.000 (68.904)   Prec@5 96.000 (97.252)   [2025-10-25 01:23:07]
  Epoch: [030][400/500]   Time 0.419 (0.420)   Data 0.001 (0.046)   Loss 0.9679 (0.8897)   Prec@1 66.000 (68.838)   Prec@5 96.000 (97.259)   [2025-10-25 01:23:45]
  **Train** Prec@1 68.820 Prec@5 97.236 Error@1 31.180
  **Test** Prec@1 74.910 Prec@5 98.410 Error@1 25.090

==>>[2025-10-25 01:24:41] [Epoch=031/040] [Need: 00:33:52] [LR=0.0010] [Best : Accuracy=75.07, Error=24.93]
  Epoch: [031][000/500]   Time 19.110 (19.110)   Data 18.567 (18.567)   Loss 0.9006 (0.9006)   Prec@1 71.000 (71.000)   Prec@5 99.000 (99.000)   [2025-10-25 01:25:00]
  Epoch: [031][100/500]   Time 0.419 (0.556)   Data 0.003 (0.185)   Loss 0.5767 (0.8672)   Prec@1 76.000 (69.545)   Prec@5 100.000 (97.366)   [2025-10-25 01:25:37]
  Epoch: [031][200/500]   Time 0.389 (0.465)   Data 0.001 (0.094)   Loss 0.9928 (0.8817)   Prec@1 64.000 (69.065)   Prec@5 94.000 (97.323)   [2025-10-25 01:26:14]
  Epoch: [031][300/500]   Time 0.336 (0.435)   Data 0.001 (0.063)   Loss 0.9144 (0.8909)   Prec@1 69.000 (68.900)   Prec@5 95.000 (97.259)   [2025-10-25 01:26:52]
  Epoch: [031][400/500]   Time 0.360 (0.422)   Data 0.001 (0.048)   Loss 1.1684 (0.8883)   Prec@1 64.000 (69.022)   Prec@5 95.000 (97.249)   [2025-10-25 01:27:30]
  **Train** Prec@1 69.006 Prec@5 97.214 Error@1 30.994
  **Test** Prec@1 75.280 Prec@5 98.440 Error@1 24.720
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 01:28:27] [Epoch=032/040] [Need: 00:30:06] [LR=0.0010] [Best : Accuracy=75.28, Error=24.72]
  Epoch: [032][000/500]   Time 18.428 (18.428)   Data 17.886 (17.886)   Loss 0.8235 (0.8235)   Prec@1 74.000 (74.000)   Prec@5 98.000 (98.000)   [2025-10-25 01:28:45]
  Epoch: [032][100/500]   Time 0.363 (0.549)   Data 0.000 (0.178)   Loss 0.9490 (0.8932)   Prec@1 63.000 (68.812)   Prec@5 99.000 (97.426)   [2025-10-25 01:29:22]
  Epoch: [032][200/500]   Time 0.376 (0.463)   Data 0.001 (0.090)   Loss 0.8654 (0.8878)   Prec@1 76.000 (69.144)   Prec@5 97.000 (97.363)   [2025-10-25 01:30:00]
  Epoch: [032][300/500]   Time 0.336 (0.433)   Data 0.000 (0.061)   Loss 0.8009 (0.8809)   Prec@1 72.000 (69.130)   Prec@5 98.000 (97.449)   [2025-10-25 01:30:37]
  Epoch: [032][400/500]   Time 0.411 (0.420)   Data 0.002 (0.046)   Loss 0.6983 (0.8817)   Prec@1 73.000 (69.092)   Prec@5 98.000 (97.399)   [2025-10-25 01:31:15]
  **Train** Prec@1 69.012 Prec@5 97.338 Error@1 30.988
  **Test** Prec@1 75.210 Prec@5 98.460 Error@1 24.790

==>>[2025-10-25 01:32:11] [Epoch=033/040] [Need: 00:26:20] [LR=0.0010] [Best : Accuracy=75.28, Error=24.72]
  Epoch: [033][000/500]   Time 18.491 (18.491)   Data 17.928 (17.928)   Loss 0.8356 (0.8356)   Prec@1 70.000 (70.000)   Prec@5 97.000 (97.000)   [2025-10-25 01:32:30]
  Epoch: [033][100/500]   Time 0.351 (0.553)   Data 0.001 (0.179)   Loss 0.9068 (0.8448)   Prec@1 67.000 (70.495)   Prec@5 97.000 (97.614)   [2025-10-25 01:33:07]
  Epoch: [033][200/500]   Time 0.396 (0.462)   Data 0.001 (0.090)   Loss 0.9505 (0.8611)   Prec@1 68.000 (69.955)   Prec@5 99.000 (97.502)   [2025-10-25 01:33:44]
  Epoch: [033][300/500]   Time 0.417 (0.433)   Data 0.002 (0.061)   Loss 0.9617 (0.8681)   Prec@1 62.000 (69.787)   Prec@5 98.000 (97.372)   [2025-10-25 01:34:22]
  Epoch: [033][400/500]   Time 0.336 (0.419)   Data 0.001 (0.046)   Loss 0.9195 (0.8726)   Prec@1 68.000 (69.539)   Prec@5 98.000 (97.394)   [2025-10-25 01:34:59]
  **Train** Prec@1 69.548 Prec@5 97.358 Error@1 30.452
  **Test** Prec@1 75.310 Prec@5 98.480 Error@1 24.690
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 01:35:56] [Epoch=034/040] [Need: 00:22:34] [LR=0.0010] [Best : Accuracy=75.31, Error=24.69]
  Epoch: [034][000/500]   Time 18.228 (18.228)   Data 17.687 (17.687)   Loss 0.8890 (0.8890)   Prec@1 70.000 (70.000)   Prec@5 96.000 (96.000)   [2025-10-25 01:36:14]
  Epoch: [034][100/500]   Time 0.395 (0.550)   Data 0.002 (0.176)   Loss 1.0199 (0.8922)   Prec@1 62.000 (68.842)   Prec@5 98.000 (97.168)   [2025-10-25 01:36:51]
  Epoch: [034][200/500]   Time 0.400 (0.464)   Data 0.002 (0.089)   Loss 0.9816 (0.8911)   Prec@1 65.000 (68.701)   Prec@5 96.000 (97.209)   [2025-10-25 01:37:29]
  Epoch: [034][300/500]   Time 0.370 (0.432)   Data 0.001 (0.060)   Loss 0.9229 (0.8812)   Prec@1 68.000 (69.203)   Prec@5 97.000 (97.319)   [2025-10-25 01:38:06]
  Epoch: [034][400/500]   Time 0.406 (0.418)   Data 0.002 (0.045)   Loss 0.9999 (0.8776)   Prec@1 67.000 (69.411)   Prec@5 99.000 (97.377)   [2025-10-25 01:38:44]
  **Train** Prec@1 69.432 Prec@5 97.352 Error@1 30.568
  **Test** Prec@1 75.790 Prec@5 98.450 Error@1 24.210
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 01:39:40] [Epoch=035/040] [Need: 00:18:48] [LR=0.0010] [Best : Accuracy=75.79, Error=24.21]
  Epoch: [035][000/500]   Time 19.007 (19.007)   Data 18.474 (18.474)   Loss 0.8072 (0.8072)   Prec@1 77.000 (77.000)   Prec@5 98.000 (98.000)   [2025-10-25 01:39:59]
  Epoch: [035][100/500]   Time 0.323 (0.560)   Data 0.001 (0.184)   Loss 0.8208 (0.8622)   Prec@1 71.000 (70.455)   Prec@5 97.000 (97.188)   [2025-10-25 01:40:36]
  Epoch: [035][200/500]   Time 0.328 (0.470)   Data 0.000 (0.093)   Loss 0.7719 (0.8735)   Prec@1 75.000 (69.896)   Prec@5 99.000 (97.259)   [2025-10-25 01:41:14]
  Epoch: [035][300/500]   Time 0.393 (0.439)   Data 0.002 (0.063)   Loss 0.9938 (0.8803)   Prec@1 66.000 (69.618)   Prec@5 95.000 (97.296)   [2025-10-25 01:41:52]
  Epoch: [035][400/500]   Time 0.404 (0.422)   Data 0.002 (0.047)   Loss 0.8386 (0.8762)   Prec@1 70.000 (69.599)   Prec@5 97.000 (97.334)   [2025-10-25 01:42:29]
  **Train** Prec@1 69.706 Prec@5 97.396 Error@1 30.294
  **Test** Prec@1 75.550 Prec@5 98.560 Error@1 24.450

==>>[2025-10-25 01:43:25] [Epoch=036/040] [Need: 00:15:02] [LR=0.0010] [Best : Accuracy=75.79, Error=24.21]
  Epoch: [036][000/500]   Time 18.319 (18.319)   Data 17.767 (17.767)   Loss 0.6116 (0.6116)   Prec@1 76.000 (76.000)   Prec@5 100.000 (100.000)   [2025-10-25 01:43:43]
  Epoch: [036][100/500]   Time 0.414 (0.549)   Data 0.002 (0.177)   Loss 0.9216 (0.8760)   Prec@1 64.000 (69.792)   Prec@5 97.000 (97.416)   [2025-10-25 01:44:20]
  Epoch: [036][200/500]   Time 0.359 (0.463)   Data 0.000 (0.090)   Loss 0.8402 (0.8703)   Prec@1 71.000 (69.791)   Prec@5 96.000 (97.423)   [2025-10-25 01:44:58]
  Epoch: [036][300/500]   Time 0.429 (0.434)   Data 0.002 (0.060)   Loss 0.7072 (0.8675)   Prec@1 79.000 (69.837)   Prec@5 98.000 (97.532)   [2025-10-25 01:45:35]
  Epoch: [036][400/500]   Time 0.394 (0.419)   Data 0.002 (0.046)   Loss 0.7660 (0.8675)   Prec@1 73.000 (69.793)   Prec@5 99.000 (97.494)   [2025-10-25 01:46:13]
  **Train** Prec@1 69.734 Prec@5 97.408 Error@1 30.266
  **Test** Prec@1 75.720 Prec@5 98.500 Error@1 24.280

==>>[2025-10-25 01:47:08] [Epoch=037/040] [Need: 00:11:16] [LR=0.0010] [Best : Accuracy=75.79, Error=24.21]
  Epoch: [037][000/500]   Time 18.225 (18.225)   Data 17.686 (17.686)   Loss 0.6826 (0.6826)   Prec@1 79.000 (79.000)   Prec@5 98.000 (98.000)   [2025-10-25 01:47:26]
  Epoch: [037][100/500]   Time 0.397 (0.555)   Data 0.002 (0.177)   Loss 0.8100 (0.8596)   Prec@1 71.000 (70.069)   Prec@5 100.000 (97.495)   [2025-10-25 01:48:04]
  Epoch: [037][200/500]   Time 0.338 (0.465)   Data 0.000 (0.089)   Loss 0.8735 (0.8677)   Prec@1 70.000 (69.771)   Prec@5 96.000 (97.378)   [2025-10-25 01:48:41]
  Epoch: [037][300/500]   Time 0.390 (0.436)   Data 0.002 (0.060)   Loss 1.0228 (0.8659)   Prec@1 68.000 (69.804)   Prec@5 94.000 (97.429)   [2025-10-25 01:49:19]
  Epoch: [037][400/500]   Time 0.329 (0.420)   Data 0.001 (0.045)   Loss 0.8050 (0.8654)   Prec@1 72.000 (69.873)   Prec@5 97.000 (97.397)   [2025-10-25 01:49:57]
  **Train** Prec@1 69.856 Prec@5 97.374 Error@1 30.144
  **Test** Prec@1 76.130 Prec@5 98.510 Error@1 23.870
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 01:50:53] [Epoch=038/040] [Need: 00:07:31] [LR=0.0010] [Best : Accuracy=76.13, Error=23.87]
  Epoch: [038][000/500]   Time 18.258 (18.258)   Data 17.713 (17.713)   Loss 0.9509 (0.9509)   Prec@1 66.000 (66.000)   Prec@5 99.000 (99.000)   [2025-10-25 01:51:11]
  Epoch: [038][100/500]   Time 0.397 (0.552)   Data 0.002 (0.177)   Loss 0.9739 (0.8648)   Prec@1 65.000 (69.842)   Prec@5 97.000 (97.703)   [2025-10-25 01:51:49]
  Epoch: [038][200/500]   Time 0.322 (0.465)   Data 0.001 (0.089)   Loss 0.7891 (0.8729)   Prec@1 75.000 (69.438)   Prec@5 98.000 (97.542)   [2025-10-25 01:52:26]
  Epoch: [038][300/500]   Time 0.412 (0.434)   Data 0.002 (0.060)   Loss 0.6612 (0.8689)   Prec@1 76.000 (69.767)   Prec@5 100.000 (97.472)   [2025-10-25 01:53:04]
  Epoch: [038][400/500]   Time 0.348 (0.420)   Data 0.002 (0.045)   Loss 0.7531 (0.8643)   Prec@1 77.000 (69.923)   Prec@5 98.000 (97.506)   [2025-10-25 01:53:41]
  **Train** Prec@1 69.938 Prec@5 97.510 Error@1 30.062
  **Test** Prec@1 75.720 Prec@5 98.580 Error@1 24.280

==>>[2025-10-25 01:54:36] [Epoch=039/040] [Need: 00:03:45] [LR=0.0010] [Best : Accuracy=76.13, Error=23.87]
  Epoch: [039][000/500]   Time 20.492 (20.492)   Data 19.951 (19.951)   Loss 1.0208 (1.0208)   Prec@1 68.000 (68.000)   Prec@5 97.000 (97.000)   [2025-10-25 01:54:57]
  Epoch: [039][100/500]   Time 0.356 (0.572)   Data 0.001 (0.199)   Loss 0.7525 (0.8708)   Prec@1 73.000 (69.703)   Prec@5 99.000 (97.455)   [2025-10-25 01:55:34]
  Epoch: [039][200/500]   Time 0.396 (0.474)   Data 0.002 (0.101)   Loss 1.0043 (0.8621)   Prec@1 67.000 (69.925)   Prec@5 96.000 (97.493)   [2025-10-25 01:56:12]
  Epoch: [039][300/500]   Time 0.390 (0.441)   Data 0.001 (0.068)   Loss 0.8568 (0.8603)   Prec@1 74.000 (69.987)   Prec@5 97.000 (97.498)   [2025-10-25 01:56:49]
  Epoch: [039][400/500]   Time 0.427 (0.425)   Data 0.002 (0.051)   Loss 0.7532 (0.8594)   Prec@1 72.000 (69.983)   Prec@5 98.000 (97.491)   [2025-10-25 01:57:27]
  **Train** Prec@1 69.900 Prec@5 97.456 Error@1 30.100
  **Test** Prec@1 76.610 Prec@5 98.620 Error@1 23.390
=> Obtain best accuracy, and update the best model
