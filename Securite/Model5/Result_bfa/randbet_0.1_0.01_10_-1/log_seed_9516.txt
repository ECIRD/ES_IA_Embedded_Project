save path : ./save/tinyvgg_quan/randbet_0.1_0.01_10_-1
{'data_path': './dataset', 'arch': 'tinyvgg_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': True, 'clipping_coeff': 0.1, 'learning_rate': 0.01, 'manualSeed': 9516, 'save_path': './save/tinyvgg_quan/randbet_0.1_0.01_10_-1', 'enable_bfa': False, 'resume': '', 'quan_bitwidth': None, 'reset_weight': False, 'evaluate': False, 'n_iter': 30, 'model_only': False, 'random_bfa': False, 'use_cuda': True}
Random Seed: 9516
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'tinyvgg_quan'
=> network :
 TinyVGG(
  (features): Sequential(
    (0): quan_Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Dropout2d(p=0.3, inplace=False)
    (6): quan_Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace=True)
    (8): quan_Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace=True)
    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (11): Dropout2d(p=0.3, inplace=False)
    (12): quan_Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): quan_Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Dropout2d(p=0.3, inplace=False)
  )
  (classifier): Sequential(
    (0): quan_Linear(in_features=2048, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): quan_Linear(in_features=128, out_features=10, bias=True)
  )
)
=> do not use any checkpoint for tinyvgg_quan model

==>>[2025-10-25 01:58:35] [Epoch=000/040] [Need: 00:00:00] [LR=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/500]   Time 18.399 (18.399)   Data 17.544 (17.544)   Loss 2.3020 (2.3020)   Prec@1 11.000 (11.000)   Prec@5 50.000 (50.000)   [2025-10-25 01:58:54]
  Epoch: [000][100/500]   Time 0.330 (0.552)   Data 0.001 (0.175)   Loss 2.3043 (2.3030)   Prec@1 6.000 (9.257)   Prec@5 48.000 (50.030)   [2025-10-25 01:59:31]
  Epoch: [000][200/500]   Time 0.348 (0.466)   Data 0.002 (0.089)   Loss 2.3010 (2.3028)   Prec@1 7.000 (9.642)   Prec@5 52.000 (50.169)   [2025-10-25 02:00:09]
  Epoch: [000][300/500]   Time 0.331 (0.441)   Data 0.001 (0.060)   Loss 2.2976 (2.3027)   Prec@1 9.000 (9.877)   Prec@5 66.000 (50.206)   [2025-10-25 02:00:48]
  Epoch: [000][400/500]   Time 0.415 (0.425)   Data 0.002 (0.045)   Loss 2.2974 (2.3019)   Prec@1 14.000 (10.516)   Prec@5 54.000 (50.990)   [2025-10-25 02:01:26]
  **Train** Prec@1 11.656 Prec@5 53.244 Error@1 88.344
  **Test** Prec@1 24.430 Prec@5 74.800 Error@1 75.570
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 02:02:22] [Epoch=001/040] [Need: 02:27:23] [LR=0.0100] [Best : Accuracy=24.43, Error=75.57]
  Epoch: [001][000/500]   Time 18.203 (18.203)   Data 17.638 (17.638)   Loss 2.1134 (2.1134)   Prec@1 23.000 (23.000)   Prec@5 72.000 (72.000)   [2025-10-25 02:02:40]
  Epoch: [001][100/500]   Time 0.398 (0.550)   Data 0.002 (0.176)   Loss 2.0248 (2.1204)   Prec@1 25.000 (21.188)   Prec@5 78.000 (73.287)   [2025-10-25 02:03:18]
  Epoch: [001][200/500]   Time 0.353 (0.460)   Data 0.001 (0.089)   Loss 1.9964 (2.0800)   Prec@1 31.000 (22.408)   Prec@5 81.000 (75.652)   [2025-10-25 02:03:55]
  Epoch: [001][300/500]   Time 0.344 (0.433)   Data 0.002 (0.060)   Loss 1.8320 (2.0441)   Prec@1 37.000 (23.671)   Prec@5 88.000 (77.498)   [2025-10-25 02:04:32]
  Epoch: [001][400/500]   Time 0.335 (0.420)   Data 0.001 (0.045)   Loss 1.9779 (2.0151)   Prec@1 19.000 (24.601)   Prec@5 81.000 (78.708)   [2025-10-25 02:05:11]
  **Train** Prec@1 25.374 Prec@5 79.704 Error@1 74.626
  **Test** Prec@1 35.170 Prec@5 86.620 Error@1 64.830
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 02:06:07] [Epoch=002/040] [Need: 02:22:55] [LR=0.0100] [Best : Accuracy=35.17, Error=64.83]
  Epoch: [002][000/500]   Time 18.291 (18.291)   Data 17.727 (17.727)   Loss 1.8320 (1.8320)   Prec@1 32.000 (32.000)   Prec@5 83.000 (83.000)   [2025-10-25 02:06:25]
  Epoch: [002][100/500]   Time 0.333 (0.549)   Data 0.001 (0.177)   Loss 1.9053 (1.8464)   Prec@1 28.000 (30.713)   Prec@5 83.000 (84.970)   [2025-10-25 02:07:02]
  Epoch: [002][200/500]   Time 0.401 (0.461)   Data 0.002 (0.089)   Loss 1.7355 (1.8344)   Prec@1 45.000 (31.313)   Prec@5 87.000 (85.313)   [2025-10-25 02:07:39]
  Epoch: [002][300/500]   Time 0.335 (0.432)   Data 0.001 (0.060)   Loss 1.6559 (1.8106)   Prec@1 41.000 (31.920)   Prec@5 89.000 (85.907)   [2025-10-25 02:08:17]
  Epoch: [002][400/500]   Time 0.398 (0.419)   Data 0.002 (0.045)   Loss 1.7933 (1.7981)   Prec@1 35.000 (32.299)   Prec@5 87.000 (86.112)   [2025-10-25 02:08:55]
  **Train** Prec@1 32.640 Prec@5 86.330 Error@1 67.360
  **Test** Prec@1 40.150 Prec@5 90.510 Error@1 59.850
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 02:09:52] [Epoch=003/040] [Need: 02:19:02] [LR=0.0100] [Best : Accuracy=40.15, Error=59.85]
  Epoch: [003][000/500]   Time 18.590 (18.590)   Data 18.026 (18.026)   Loss 1.7057 (1.7057)   Prec@1 33.000 (33.000)   Prec@5 88.000 (88.000)   [2025-10-25 02:10:10]
  Epoch: [003][100/500]   Time 0.395 (0.557)   Data 0.001 (0.180)   Loss 1.7436 (1.7235)   Prec@1 37.000 (35.396)   Prec@5 90.000 (87.426)   [2025-10-25 02:10:48]
  Epoch: [003][200/500]   Time 0.399 (0.464)   Data 0.002 (0.091)   Loss 1.6178 (1.7162)   Prec@1 36.000 (35.652)   Prec@5 88.000 (87.672)   [2025-10-25 02:11:25]
  Epoch: [003][300/500]   Time 0.410 (0.435)   Data 0.001 (0.061)   Loss 1.5503 (1.7050)   Prec@1 38.000 (36.086)   Prec@5 91.000 (88.003)   [2025-10-25 02:12:03]
  Epoch: [003][400/500]   Time 0.408 (0.420)   Data 0.002 (0.046)   Loss 1.6657 (1.6967)   Prec@1 39.000 (36.441)   Prec@5 89.000 (88.282)   [2025-10-25 02:12:40]
  **Train** Prec@1 36.816 Prec@5 88.424 Error@1 63.184
  **Test** Prec@1 43.960 Prec@5 91.890 Error@1 56.040
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 02:13:37] [Epoch=004/040] [Need: 02:15:15] [LR=0.0100] [Best : Accuracy=43.96, Error=56.04]
  Epoch: [004][000/500]   Time 18.479 (18.479)   Data 17.924 (17.924)   Loss 1.6265 (1.6265)   Prec@1 38.000 (38.000)   Prec@5 89.000 (89.000)   [2025-10-25 02:13:56]
  Epoch: [004][100/500]   Time 0.335 (0.550)   Data 0.000 (0.179)   Loss 1.5646 (1.6573)   Prec@1 39.000 (38.158)   Prec@5 94.000 (88.832)   [2025-10-25 02:14:33]
  Epoch: [004][200/500]   Time 0.329 (0.465)   Data 0.001 (0.090)   Loss 1.7008 (1.6468)   Prec@1 26.000 (38.378)   Prec@5 89.000 (89.358)   [2025-10-25 02:15:10]
  Epoch: [004][300/500]   Time 0.365 (0.437)   Data 0.001 (0.061)   Loss 1.7778 (1.6410)   Prec@1 31.000 (38.698)   Prec@5 84.000 (89.402)   [2025-10-25 02:15:49]
  Epoch: [004][400/500]   Time 0.399 (0.423)   Data 0.001 (0.046)   Loss 1.6353 (1.6322)   Prec@1 38.000 (39.222)   Prec@5 86.000 (89.571)   [2025-10-25 02:16:27]
  **Train** Prec@1 39.480 Prec@5 89.704 Error@1 60.520
  **Test** Prec@1 47.430 Prec@5 93.500 Error@1 52.570
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 02:17:23] [Epoch=005/040] [Need: 02:11:33] [LR=0.0100] [Best : Accuracy=47.43, Error=52.57]
  Epoch: [005][000/500]   Time 18.160 (18.160)   Data 17.622 (17.622)   Loss 1.6682 (1.6682)   Prec@1 39.000 (39.000)   Prec@5 87.000 (87.000)   [2025-10-25 02:17:41]
  Epoch: [005][100/500]   Time 0.399 (0.547)   Data 0.001 (0.176)   Loss 1.6461 (1.6127)   Prec@1 41.000 (39.822)   Prec@5 85.000 (89.535)   [2025-10-25 02:18:18]
  Epoch: [005][200/500]   Time 0.327 (0.463)   Data 0.000 (0.089)   Loss 1.7005 (1.5936)   Prec@1 36.000 (40.766)   Prec@5 90.000 (89.955)   [2025-10-25 02:18:56]
  Epoch: [005][300/500]   Time 0.352 (0.433)   Data 0.002 (0.060)   Loss 1.4673 (1.5899)   Prec@1 51.000 (41.047)   Prec@5 95.000 (89.990)   [2025-10-25 02:19:33]
  Epoch: [005][400/500]   Time 0.333 (0.420)   Data 0.001 (0.045)   Loss 1.7771 (1.5827)   Prec@1 34.000 (41.357)   Prec@5 88.000 (90.070)   [2025-10-25 02:20:11]
  **Train** Prec@1 41.828 Prec@5 90.312 Error@1 58.172
  **Test** Prec@1 49.370 Prec@5 93.910 Error@1 50.630
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 02:21:08] [Epoch=006/040] [Need: 02:07:43] [LR=0.0100] [Best : Accuracy=49.37, Error=50.63]
  Epoch: [006][000/500]   Time 18.419 (18.419)   Data 17.882 (17.882)   Loss 1.6170 (1.6170)   Prec@1 36.000 (36.000)   Prec@5 90.000 (90.000)   [2025-10-25 02:21:26]
  Epoch: [006][100/500]   Time 0.358 (0.552)   Data 0.001 (0.178)   Loss 1.6618 (1.5396)   Prec@1 37.000 (43.634)   Prec@5 92.000 (91.099)   [2025-10-25 02:22:04]
  Epoch: [006][200/500]   Time 0.334 (0.467)   Data 0.000 (0.090)   Loss 1.4157 (1.5291)   Prec@1 42.000 (43.821)   Prec@5 94.000 (91.313)   [2025-10-25 02:22:42]
  Epoch: [006][300/500]   Time 0.408 (0.436)   Data 0.002 (0.061)   Loss 1.3690 (1.5216)   Prec@1 42.000 (44.080)   Prec@5 97.000 (91.475)   [2025-10-25 02:23:19]
  Epoch: [006][400/500]   Time 0.355 (0.421)   Data 0.002 (0.046)   Loss 1.3884 (1.5159)   Prec@1 49.000 (44.299)   Prec@5 94.000 (91.561)   [2025-10-25 02:23:57]
  **Train** Prec@1 44.536 Prec@5 91.542 Error@1 55.464
  **Test** Prec@1 52.210 Prec@5 94.220 Error@1 47.790
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 02:24:54] [Epoch=007/040] [Need: 02:04:01] [LR=0.0100] [Best : Accuracy=52.21, Error=47.79]
  Epoch: [007][000/500]   Time 18.529 (18.529)   Data 17.987 (17.987)   Loss 1.3720 (1.3720)   Prec@1 54.000 (54.000)   Prec@5 91.000 (91.000)   [2025-10-25 02:25:12]
  Epoch: [007][100/500]   Time 0.382 (0.562)   Data 0.004 (0.179)   Loss 1.3701 (1.4921)   Prec@1 52.000 (45.594)   Prec@5 92.000 (91.287)   [2025-10-25 02:25:51]
  Epoch: [007][200/500]   Time 0.334 (0.467)   Data 0.000 (0.091)   Loss 1.4691 (1.4826)   Prec@1 44.000 (45.876)   Prec@5 95.000 (91.527)   [2025-10-25 02:26:28]
  Epoch: [007][300/500]   Time 0.402 (0.435)   Data 0.001 (0.061)   Loss 1.2993 (1.4724)   Prec@1 53.000 (46.249)   Prec@5 95.000 (91.827)   [2025-10-25 02:27:05]
  Epoch: [007][400/500]   Time 0.339 (0.421)   Data 0.000 (0.046)   Loss 1.3419 (1.4644)   Prec@1 54.000 (46.551)   Prec@5 94.000 (91.970)   [2025-10-25 02:27:43]
  **Train** Prec@1 46.818 Prec@5 92.120 Error@1 53.182
  **Test** Prec@1 52.750 Prec@5 94.660 Error@1 47.250
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 02:28:40] [Epoch=008/040] [Need: 02:00:18] [LR=0.0100] [Best : Accuracy=52.75, Error=47.25]
  Epoch: [008][000/500]   Time 18.373 (18.373)   Data 17.808 (17.808)   Loss 1.4727 (1.4727)   Prec@1 49.000 (49.000)   Prec@5 93.000 (93.000)   [2025-10-25 02:28:58]
  Epoch: [008][100/500]   Time 0.353 (0.554)   Data 0.001 (0.178)   Loss 1.3372 (1.4133)   Prec@1 51.000 (48.950)   Prec@5 99.000 (93.089)   [2025-10-25 02:29:36]
  Epoch: [008][200/500]   Time 0.329 (0.467)   Data 0.000 (0.090)   Loss 1.3634 (1.4103)   Prec@1 47.000 (48.448)   Prec@5 96.000 (93.139)   [2025-10-25 02:30:14]
  Epoch: [008][300/500]   Time 0.356 (0.437)   Data 0.001 (0.060)   Loss 1.3583 (1.4039)   Prec@1 50.000 (48.462)   Prec@5 92.000 (93.100)   [2025-10-25 02:30:52]
  Epoch: [008][400/500]   Time 0.409 (0.422)   Data 0.002 (0.046)   Loss 1.3675 (1.3995)   Prec@1 53.000 (48.820)   Prec@5 91.000 (93.095)   [2025-10-25 02:31:29]
  **Train** Prec@1 48.984 Prec@5 93.072 Error@1 51.016
  **Test** Prec@1 57.920 Prec@5 96.200 Error@1 42.080
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 02:32:26] [Epoch=009/040] [Need: 01:56:33] [LR=0.0100] [Best : Accuracy=57.92, Error=42.08]
  Epoch: [009][000/500]   Time 18.954 (18.954)   Data 18.380 (18.380)   Loss 1.2339 (1.2339)   Prec@1 54.000 (54.000)   Prec@5 94.000 (94.000)   [2025-10-25 02:32:45]
  Epoch: [009][100/500]   Time 0.370 (0.560)   Data 0.001 (0.183)   Loss 1.3843 (1.3715)   Prec@1 49.000 (50.762)   Prec@5 94.000 (92.921)   [2025-10-25 02:33:22]
  Epoch: [009][200/500]   Time 0.403 (0.470)   Data 0.001 (0.093)   Loss 1.3202 (1.3632)   Prec@1 47.000 (50.811)   Prec@5 91.000 (93.418)   [2025-10-25 02:34:00]
  Epoch: [009][300/500]   Time 0.398 (0.439)   Data 0.002 (0.062)   Loss 1.4141 (1.3514)   Prec@1 38.000 (50.924)   Prec@5 94.000 (93.542)   [2025-10-25 02:34:38]
  Epoch: [009][400/500]   Time 0.350 (0.426)   Data 0.000 (0.047)   Loss 1.3777 (1.3470)   Prec@1 54.000 (51.145)   Prec@5 89.000 (93.566)   [2025-10-25 02:35:17]
  **Train** Prec@1 51.362 Prec@5 93.540 Error@1 48.638
  **Test** Prec@1 59.440 Prec@5 95.690 Error@1 40.560
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 02:36:13] [Epoch=010/040] [Need: 01:52:53] [LR=0.0100] [Best : Accuracy=59.44, Error=40.56]
  Epoch: [010][000/500]   Time 18.448 (18.448)   Data 17.897 (17.897)   Loss 1.1748 (1.1748)   Prec@1 60.000 (60.000)   Prec@5 95.000 (95.000)   [2025-10-25 02:36:32]
  Epoch: [010][100/500]   Time 0.440 (0.559)   Data 0.004 (0.179)   Loss 1.2359 (1.2947)   Prec@1 60.000 (53.931)   Prec@5 94.000 (93.871)   [2025-10-25 02:37:10]
  Epoch: [010][200/500]   Time 0.402 (0.470)   Data 0.001 (0.090)   Loss 1.1576 (1.2932)   Prec@1 58.000 (53.776)   Prec@5 94.000 (93.975)   [2025-10-25 02:37:48]
  Epoch: [010][300/500]   Time 0.420 (0.440)   Data 0.002 (0.061)   Loss 1.1119 (1.2913)   Prec@1 65.000 (53.684)   Prec@5 95.000 (94.090)   [2025-10-25 02:38:26]
  Epoch: [010][400/500]   Time 0.335 (0.424)   Data 0.001 (0.046)   Loss 1.3160 (1.2876)   Prec@1 50.000 (53.574)   Prec@5 94.000 (94.102)   [2025-10-25 02:39:03]
  **Train** Prec@1 53.472 Prec@5 94.058 Error@1 46.528
  **Test** Prec@1 61.750 Prec@5 96.090 Error@1 38.250
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 02:40:02] [Epoch=011/040] [Need: 01:49:15] [LR=0.0100] [Best : Accuracy=61.75, Error=38.25]
  Epoch: [011][000/500]   Time 19.656 (19.656)   Data 19.103 (19.103)   Loss 1.1811 (1.1811)   Prec@1 55.000 (55.000)   Prec@5 97.000 (97.000)   [2025-10-25 02:40:22]
  Epoch: [011][100/500]   Time 0.357 (0.573)   Data 0.001 (0.190)   Loss 1.3103 (1.2570)   Prec@1 50.000 (55.020)   Prec@5 95.000 (94.683)   [2025-10-25 02:41:00]
  Epoch: [011][200/500]   Time 0.408 (0.476)   Data 0.002 (0.096)   Loss 1.1510 (1.2437)   Prec@1 57.000 (55.204)   Prec@5 95.000 (94.731)   [2025-10-25 02:41:38]
  Epoch: [011][300/500]   Time 0.391 (0.444)   Data 0.000 (0.065)   Loss 1.2755 (1.2447)   Prec@1 59.000 (55.286)   Prec@5 94.000 (94.551)   [2025-10-25 02:42:16]
  Epoch: [011][400/500]   Time 0.347 (0.427)   Data 0.000 (0.049)   Loss 1.0812 (1.2455)   Prec@1 64.000 (55.324)   Prec@5 95.000 (94.646)   [2025-10-25 02:42:53]
  **Train** Prec@1 55.386 Prec@5 94.644 Error@1 44.614
  **Test** Prec@1 64.690 Prec@5 96.990 Error@1 35.310
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 02:43:51] [Epoch=012/040] [Need: 01:45:35] [LR=0.0100] [Best : Accuracy=64.69, Error=35.31]
  Epoch: [012][000/500]   Time 19.767 (19.767)   Data 19.216 (19.216)   Loss 1.1475 (1.1475)   Prec@1 56.000 (56.000)   Prec@5 98.000 (98.000)   [2025-10-25 02:44:11]
  Epoch: [012][100/500]   Time 0.432 (0.578)   Data 0.001 (0.192)   Loss 1.1591 (1.2212)   Prec@1 65.000 (56.129)   Prec@5 93.000 (94.743)   [2025-10-25 02:44:49]
  Epoch: [012][200/500]   Time 0.347 (0.479)   Data 0.002 (0.097)   Loss 1.3102 (1.2203)   Prec@1 52.000 (56.428)   Prec@5 97.000 (94.692)   [2025-10-25 02:45:27]
  Epoch: [012][300/500]   Time 0.430 (0.444)   Data 0.002 (0.065)   Loss 1.1764 (1.2143)   Prec@1 56.000 (56.608)   Prec@5 95.000 (94.771)   [2025-10-25 02:46:04]
  Epoch: [012][400/500]   Time 0.393 (0.427)   Data 0.002 (0.049)   Loss 1.1782 (1.2109)   Prec@1 68.000 (56.796)   Prec@5 94.000 (94.766)   [2025-10-25 02:46:42]
  **Train** Prec@1 56.940 Prec@5 94.888 Error@1 43.060
  **Test** Prec@1 65.650 Prec@5 96.900 Error@1 34.350
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 02:47:40] [Epoch=013/040] [Need: 01:41:55] [LR=0.0100] [Best : Accuracy=65.65, Error=34.35]
  Epoch: [013][000/500]   Time 18.764 (18.764)   Data 18.214 (18.214)   Loss 1.2354 (1.2354)   Prec@1 55.000 (55.000)   Prec@5 97.000 (97.000)   [2025-10-25 02:47:58]
  Epoch: [013][100/500]   Time 0.399 (0.560)   Data 0.001 (0.182)   Loss 1.2292 (1.1869)   Prec@1 57.000 (56.881)   Prec@5 94.000 (95.347)   [2025-10-25 02:48:36]
  Epoch: [013][200/500]   Time 0.398 (0.468)   Data 0.000 (0.092)   Loss 1.2434 (1.1904)   Prec@1 50.000 (57.209)   Prec@5 97.000 (95.134)   [2025-10-25 02:49:14]
  Epoch: [013][300/500]   Time 0.407 (0.439)   Data 0.002 (0.062)   Loss 1.3834 (1.1835)   Prec@1 60.000 (57.565)   Prec@5 93.000 (95.123)   [2025-10-25 02:49:52]
  Epoch: [013][400/500]   Time 0.407 (0.424)   Data 0.002 (0.047)   Loss 0.9992 (1.1748)   Prec@1 67.000 (57.983)   Prec@5 98.000 (95.170)   [2025-10-25 02:50:30]
  **Train** Prec@1 58.110 Prec@5 95.180 Error@1 41.890
  **Test** Prec@1 66.120 Prec@5 97.300 Error@1 33.880
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 02:51:32] [Epoch=014/040] [Need: 01:38:19] [LR=0.0100] [Best : Accuracy=66.12, Error=33.88]
  Epoch: [014][000/500]   Time 18.733 (18.733)   Data 18.182 (18.182)   Loss 1.1053 (1.1053)   Prec@1 67.000 (67.000)   Prec@5 94.000 (94.000)   [2025-10-25 02:51:51]
  Epoch: [014][100/500]   Time 0.345 (0.563)   Data 0.000 (0.181)   Loss 1.1408 (1.1494)   Prec@1 59.000 (59.535)   Prec@5 97.000 (95.446)   [2025-10-25 02:52:29]
  Epoch: [014][200/500]   Time 0.341 (0.471)   Data 0.001 (0.092)   Loss 1.2834 (1.1478)   Prec@1 54.000 (59.706)   Prec@5 96.000 (95.502)   [2025-10-25 02:53:07]
  Epoch: [014][300/500]   Time 0.383 (0.440)   Data 0.002 (0.062)   Loss 1.1249 (1.1459)   Prec@1 68.000 (59.708)   Prec@5 93.000 (95.512)   [2025-10-25 02:53:44]
  Epoch: [014][400/500]   Time 0.414 (0.425)   Data 0.002 (0.047)   Loss 1.0484 (1.1379)   Prec@1 69.000 (59.975)   Prec@5 95.000 (95.601)   [2025-10-25 02:54:22]
  **Train** Prec@1 59.982 Prec@5 95.586 Error@1 40.018
  **Test** Prec@1 66.900 Prec@5 97.370 Error@1 33.100
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 02:55:20] [Epoch=015/040] [Need: 01:34:34] [LR=0.0100] [Best : Accuracy=66.90, Error=33.10]
  Epoch: [015][000/500]   Time 19.574 (19.574)   Data 19.007 (19.007)   Loss 1.1084 (1.1084)   Prec@1 60.000 (60.000)   Prec@5 94.000 (94.000)   [2025-10-25 02:55:40]
  Epoch: [015][100/500]   Time 0.342 (0.567)   Data 0.002 (0.189)   Loss 1.1975 (1.1133)   Prec@1 57.000 (60.653)   Prec@5 93.000 (95.960)   [2025-10-25 02:56:18]
  Epoch: [015][200/500]   Time 0.401 (0.476)   Data 0.002 (0.096)   Loss 1.3042 (1.1173)   Prec@1 54.000 (60.796)   Prec@5 92.000 (95.866)   [2025-10-25 02:56:56]
  Epoch: [015][300/500]   Time 0.409 (0.444)   Data 0.002 (0.064)   Loss 1.1798 (1.1218)   Prec@1 58.000 (60.635)   Prec@5 95.000 (95.767)   [2025-10-25 02:57:34]
  Epoch: [015][400/500]   Time 0.392 (0.428)   Data 0.002 (0.049)   Loss 1.0688 (1.1145)   Prec@1 62.000 (60.855)   Prec@5 95.000 (95.858)   [2025-10-25 02:58:12]
  **Train** Prec@1 60.894 Prec@5 95.902 Error@1 39.106
  **Test** Prec@1 67.820 Prec@5 97.510 Error@1 32.180
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 02:59:08] [Epoch=016/040] [Need: 01:30:49] [LR=0.0100] [Best : Accuracy=67.82, Error=32.18]
  Epoch: [016][000/500]   Time 18.763 (18.763)   Data 18.205 (18.205)   Loss 1.1392 (1.1392)   Prec@1 61.000 (61.000)   Prec@5 97.000 (97.000)   [2025-10-25 02:59:27]
  Epoch: [016][100/500]   Time 0.333 (0.565)   Data 0.001 (0.182)   Loss 1.2350 (1.0829)   Prec@1 57.000 (62.158)   Prec@5 95.000 (95.980)   [2025-10-25 03:00:06]
  Epoch: [016][200/500]   Time 0.339 (0.472)   Data 0.001 (0.092)   Loss 1.0037 (1.0795)   Prec@1 62.000 (61.821)   Prec@5 97.000 (96.000)   [2025-10-25 03:00:43]
  Epoch: [016][300/500]   Time 0.402 (0.443)   Data 0.002 (0.062)   Loss 0.9224 (1.0784)   Prec@1 72.000 (61.977)   Prec@5 97.000 (95.904)   [2025-10-25 03:01:22]
  Epoch: [016][400/500]   Time 0.412 (0.427)   Data 0.001 (0.047)   Loss 1.0689 (1.0771)   Prec@1 59.000 (62.037)   Prec@5 95.000 (95.900)   [2025-10-25 03:02:00]
  **Train** Prec@1 62.118 Prec@5 95.916 Error@1 37.882
  **Test** Prec@1 70.460 Prec@5 97.790 Error@1 29.540
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 03:02:57] [Epoch=017/040] [Need: 01:27:04] [LR=0.0100] [Best : Accuracy=70.46, Error=29.54]
  Epoch: [017][000/500]   Time 18.941 (18.941)   Data 18.390 (18.390)   Loss 1.1595 (1.1595)   Prec@1 59.000 (59.000)   Prec@5 92.000 (92.000)   [2025-10-25 03:03:16]
  Epoch: [017][100/500]   Time 0.401 (0.557)   Data 0.002 (0.183)   Loss 1.2126 (1.0734)   Prec@1 53.000 (62.257)   Prec@5 93.000 (96.069)   [2025-10-25 03:03:53]
  Epoch: [017][200/500]   Time 0.414 (0.466)   Data 0.002 (0.093)   Loss 1.0937 (1.0686)   Prec@1 63.000 (62.682)   Prec@5 95.000 (95.990)   [2025-10-25 03:04:31]
  Epoch: [017][300/500]   Time 0.403 (0.438)   Data 0.002 (0.062)   Loss 1.0831 (1.0702)   Prec@1 67.000 (62.691)   Prec@5 98.000 (95.967)   [2025-10-25 03:05:09]
  Epoch: [017][400/500]   Time 0.355 (0.423)   Data 0.002 (0.047)   Loss 0.9784 (1.0641)   Prec@1 68.000 (62.913)   Prec@5 99.000 (96.042)   [2025-10-25 03:05:47]
  **Train** Prec@1 63.040 Prec@5 96.040 Error@1 36.960
  **Test** Prec@1 71.620 Prec@5 97.850 Error@1 28.380
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 03:06:42] [Epoch=018/040] [Need: 01:23:14] [LR=0.0100] [Best : Accuracy=71.62, Error=28.38]
  Epoch: [018][000/500]   Time 19.159 (19.159)   Data 18.589 (18.589)   Loss 0.9565 (0.9565)   Prec@1 64.000 (64.000)   Prec@5 96.000 (96.000)   [2025-10-25 03:07:01]
  Epoch: [018][100/500]   Time 0.347 (0.564)   Data 0.000 (0.185)   Loss 0.9285 (1.0462)   Prec@1 67.000 (63.683)   Prec@5 99.000 (96.396)   [2025-10-25 03:07:39]
  Epoch: [018][200/500]   Time 0.360 (0.472)   Data 0.002 (0.094)   Loss 1.0043 (1.0441)   Prec@1 66.000 (63.632)   Prec@5 98.000 (96.279)   [2025-10-25 03:08:17]
  Epoch: [018][300/500]   Time 0.358 (0.445)   Data 0.001 (0.063)   Loss 0.9085 (1.0417)   Prec@1 66.000 (63.708)   Prec@5 97.000 (96.259)   [2025-10-25 03:08:56]
  Epoch: [018][400/500]   Time 0.406 (0.429)   Data 0.001 (0.048)   Loss 1.0932 (1.0416)   Prec@1 67.000 (63.596)   Prec@5 95.000 (96.292)   [2025-10-25 03:09:34]
  **Train** Prec@1 63.744 Prec@5 96.264 Error@1 36.256
  **Test** Prec@1 72.190 Prec@5 97.870 Error@1 27.810
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 03:10:32] [Epoch=019/040] [Need: 01:19:30] [LR=0.0100] [Best : Accuracy=72.19, Error=27.81]
  Epoch: [019][000/500]   Time 19.827 (19.827)   Data 19.283 (19.283)   Loss 0.9502 (0.9502)   Prec@1 65.000 (65.000)   Prec@5 99.000 (99.000)   [2025-10-25 03:10:52]
  Epoch: [019][100/500]   Time 0.409 (0.565)   Data 0.003 (0.192)   Loss 1.0082 (1.0307)   Prec@1 68.000 (63.802)   Prec@5 96.000 (96.238)   [2025-10-25 03:11:29]
  Epoch: [019][200/500]   Time 0.337 (0.472)   Data 0.000 (0.097)   Loss 0.9638 (1.0291)   Prec@1 71.000 (63.667)   Prec@5 97.000 (96.129)   [2025-10-25 03:12:07]
  Epoch: [019][300/500]   Time 0.419 (0.440)   Data 0.002 (0.065)   Loss 1.0542 (1.0189)   Prec@1 65.000 (64.249)   Prec@5 96.000 (96.286)   [2025-10-25 03:12:45]
  Epoch: [019][400/500]   Time 0.376 (0.425)   Data 0.001 (0.049)   Loss 0.9882 (1.0190)   Prec@1 64.000 (64.332)   Prec@5 100.000 (96.292)   [2025-10-25 03:13:22]
  **Train** Prec@1 64.492 Prec@5 96.354 Error@1 35.508
  **Test** Prec@1 72.720 Prec@5 97.750 Error@1 27.280
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 03:14:20] [Epoch=020/040] [Need: 01:15:44] [LR=0.0100] [Best : Accuracy=72.72, Error=27.28]
  Epoch: [020][000/500]   Time 18.278 (18.278)   Data 17.726 (17.726)   Loss 1.1297 (1.1297)   Prec@1 65.000 (65.000)   Prec@5 95.000 (95.000)   [2025-10-25 03:14:39]
  Epoch: [020][100/500]   Time 0.404 (0.560)   Data 0.002 (0.177)   Loss 1.1621 (0.9928)   Prec@1 57.000 (65.733)   Prec@5 95.000 (96.723)   [2025-10-25 03:15:17]
  Epoch: [020][200/500]   Time 0.420 (0.492)   Data 0.002 (0.090)   Loss 1.3785 (0.9967)   Prec@1 56.000 (65.836)   Prec@5 94.000 (96.647)   [2025-10-25 03:15:59]
  Epoch: [020][300/500]   Time 0.414 (0.457)   Data 0.002 (0.060)   Loss 0.9616 (0.9979)   Prec@1 66.000 (65.485)   Prec@5 98.000 (96.681)   [2025-10-25 03:16:38]
  Epoch: [020][400/500]   Time 0.352 (0.438)   Data 0.001 (0.046)   Loss 1.2723 (0.9986)   Prec@1 60.000 (65.531)   Prec@5 98.000 (96.603)   [2025-10-25 03:17:16]
  **Train** Prec@1 65.574 Prec@5 96.682 Error@1 34.426
  **Test** Prec@1 72.930 Prec@5 98.170 Error@1 27.070
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 03:18:12] [Epoch=021/040] [Need: 01:12:01] [LR=0.0100] [Best : Accuracy=72.93, Error=27.07]
  Epoch: [021][000/500]   Time 18.378 (18.378)   Data 17.836 (17.836)   Loss 1.0562 (1.0562)   Prec@1 66.000 (66.000)   Prec@5 97.000 (97.000)   [2025-10-25 03:18:30]
  Epoch: [021][100/500]   Time 0.331 (0.549)   Data 0.001 (0.178)   Loss 0.8019 (0.9781)   Prec@1 72.000 (65.644)   Prec@5 97.000 (96.762)   [2025-10-25 03:19:07]
  Epoch: [021][200/500]   Time 0.333 (0.464)   Data 0.001 (0.090)   Loss 0.8992 (0.9798)   Prec@1 70.000 (65.841)   Prec@5 97.000 (96.826)   [2025-10-25 03:19:45]
  Epoch: [021][300/500]   Time 0.417 (0.436)   Data 0.001 (0.061)   Loss 0.9411 (0.9758)   Prec@1 64.000 (65.831)   Prec@5 100.000 (96.860)   [2025-10-25 03:20:23]
  Epoch: [021][400/500]   Time 0.360 (0.423)   Data 0.001 (0.046)   Loss 0.8625 (0.9775)   Prec@1 76.000 (65.863)   Prec@5 98.000 (96.838)   [2025-10-25 03:21:01]
  **Train** Prec@1 65.970 Prec@5 96.756 Error@1 34.030
  **Test** Prec@1 74.270 Prec@5 98.420 Error@1 25.730
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 03:21:58] [Epoch=022/040] [Need: 01:08:12] [LR=0.0100] [Best : Accuracy=74.27, Error=25.73]
  Epoch: [022][000/500]   Time 18.231 (18.231)   Data 17.669 (17.669)   Loss 0.8965 (0.8965)   Prec@1 73.000 (73.000)   Prec@5 96.000 (96.000)   [2025-10-25 03:22:16]
  Epoch: [022][100/500]   Time 0.401 (0.559)   Data 0.002 (0.176)   Loss 0.7750 (0.9635)   Prec@1 75.000 (66.604)   Prec@5 100.000 (96.941)   [2025-10-25 03:22:54]
  Epoch: [022][200/500]   Time 0.340 (0.468)   Data 0.000 (0.089)   Loss 0.9036 (0.9612)   Prec@1 63.000 (66.478)   Prec@5 97.000 (96.896)   [2025-10-25 03:23:32]
  Epoch: [022][300/500]   Time 0.330 (0.437)   Data 0.001 (0.060)   Loss 0.9354 (0.9586)   Prec@1 66.000 (66.671)   Prec@5 97.000 (96.880)   [2025-10-25 03:24:09]
  Epoch: [022][400/500]   Time 0.327 (0.422)   Data 0.001 (0.045)   Loss 0.9422 (0.9615)   Prec@1 63.000 (66.551)   Prec@5 98.000 (96.825)   [2025-10-25 03:24:47]
  **Train** Prec@1 66.760 Prec@5 96.854 Error@1 33.240
  **Test** Prec@1 74.260 Prec@5 98.080 Error@1 25.740

==>>[2025-10-25 03:25:44] [Epoch=023/040] [Need: 01:04:24] [LR=0.0100] [Best : Accuracy=74.27, Error=25.73]
  Epoch: [023][000/500]   Time 18.422 (18.422)   Data 17.877 (17.877)   Loss 0.8706 (0.8706)   Prec@1 71.000 (71.000)   Prec@5 98.000 (98.000)   [2025-10-25 03:26:03]
  Epoch: [023][100/500]   Time 0.397 (0.554)   Data 0.001 (0.178)   Loss 0.7184 (0.9372)   Prec@1 74.000 (67.554)   Prec@5 97.000 (96.861)   [2025-10-25 03:26:40]
  Epoch: [023][200/500]   Time 0.338 (0.464)   Data 0.000 (0.090)   Loss 0.9185 (0.9444)   Prec@1 70.000 (67.159)   Prec@5 98.000 (96.831)   [2025-10-25 03:27:18]
  Epoch: [023][300/500]   Time 0.352 (0.436)   Data 0.002 (0.061)   Loss 0.8645 (0.9437)   Prec@1 70.000 (67.243)   Prec@5 100.000 (96.880)   [2025-10-25 03:27:56]
  Epoch: [023][400/500]   Time 0.417 (0.421)   Data 0.002 (0.046)   Loss 0.8694 (0.9402)   Prec@1 72.000 (67.449)   Prec@5 98.000 (96.903)   [2025-10-25 03:28:33]
  **Train** Prec@1 67.436 Prec@5 96.922 Error@1 32.564
  **Test** Prec@1 73.010 Prec@5 98.090 Error@1 26.990

==>>[2025-10-25 03:29:30] [Epoch=024/040] [Need: 01:00:36] [LR=0.0100] [Best : Accuracy=74.27, Error=25.73]
  Epoch: [024][000/500]   Time 19.740 (19.740)   Data 19.181 (19.181)   Loss 0.8500 (0.8500)   Prec@1 74.000 (74.000)   Prec@5 99.000 (99.000)   [2025-10-25 03:29:50]
  Epoch: [024][100/500]   Time 0.354 (0.571)   Data 0.002 (0.191)   Loss 0.8426 (0.9255)   Prec@1 72.000 (68.020)   Prec@5 100.000 (97.089)   [2025-10-25 03:30:28]
  Epoch: [024][200/500]   Time 0.326 (0.473)   Data 0.001 (0.097)   Loss 0.9369 (0.9375)   Prec@1 65.000 (67.642)   Prec@5 99.000 (96.975)   [2025-10-25 03:31:05]
  Epoch: [024][300/500]   Time 0.410 (0.454)   Data 0.002 (0.065)   Loss 1.0286 (0.9353)   Prec@1 65.000 (67.601)   Prec@5 96.000 (97.050)   [2025-10-25 03:31:47]
  Epoch: [024][400/500]   Time 0.419 (0.438)   Data 0.002 (0.049)   Loss 0.8326 (0.9300)   Prec@1 73.000 (67.853)   Prec@5 98.000 (97.067)   [2025-10-25 03:32:26]
  **Train** Prec@1 67.924 Prec@5 97.070 Error@1 32.076
  **Test** Prec@1 74.430 Prec@5 98.180 Error@1 25.570
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 03:33:24] [Epoch=025/040] [Need: 00:56:53] [LR=0.0010] [Best : Accuracy=74.43, Error=25.57]
  Epoch: [025][000/500]   Time 18.169 (18.169)   Data 17.620 (17.620)   Loss 0.8273 (0.8273)   Prec@1 71.000 (71.000)   Prec@5 98.000 (98.000)   [2025-10-25 03:33:43]
  Epoch: [025][100/500]   Time 0.333 (0.549)   Data 0.000 (0.176)   Loss 0.8111 (0.8675)   Prec@1 73.000 (69.723)   Prec@5 97.000 (97.257)   [2025-10-25 03:34:20]
  Epoch: [025][200/500]   Time 0.335 (0.465)   Data 0.000 (0.089)   Loss 0.7950 (0.8677)   Prec@1 72.000 (69.975)   Prec@5 98.000 (97.313)   [2025-10-25 03:34:58]
  Epoch: [025][300/500]   Time 0.417 (0.435)   Data 0.002 (0.060)   Loss 0.9628 (0.8615)   Prec@1 67.000 (70.282)   Prec@5 97.000 (97.342)   [2025-10-25 03:35:36]
  Epoch: [025][400/500]   Time 0.333 (0.421)   Data 0.001 (0.045)   Loss 0.8695 (0.8584)   Prec@1 69.000 (70.414)   Prec@5 96.000 (97.399)   [2025-10-25 03:36:13]
  **Train** Prec@1 70.480 Prec@5 97.360 Error@1 29.520
  **Test** Prec@1 76.990 Prec@5 98.570 Error@1 23.010
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 03:37:10] [Epoch=026/040] [Need: 00:53:04] [LR=0.0010] [Best : Accuracy=76.99, Error=23.01]
  Epoch: [026][000/500]   Time 18.295 (18.295)   Data 17.731 (17.731)   Loss 0.7844 (0.7844)   Prec@1 72.000 (72.000)   Prec@5 96.000 (96.000)   [2025-10-25 03:37:29]
  Epoch: [026][100/500]   Time 0.330 (0.555)   Data 0.000 (0.177)   Loss 0.8481 (0.8416)   Prec@1 72.000 (70.465)   Prec@5 97.000 (97.465)   [2025-10-25 03:38:06]
  Epoch: [026][200/500]   Time 0.392 (0.468)   Data 0.002 (0.090)   Loss 0.8163 (0.8392)   Prec@1 71.000 (70.791)   Prec@5 97.000 (97.448)   [2025-10-25 03:38:44]
  Epoch: [026][300/500]   Time 0.397 (0.436)   Data 0.002 (0.060)   Loss 0.7638 (0.8377)   Prec@1 71.000 (71.086)   Prec@5 99.000 (97.492)   [2025-10-25 03:39:22]
  Epoch: [026][400/500]   Time 0.394 (0.422)   Data 0.002 (0.046)   Loss 0.7112 (0.8369)   Prec@1 82.000 (71.214)   Prec@5 99.000 (97.504)   [2025-10-25 03:39:59]
  **Train** Prec@1 71.102 Prec@5 97.466 Error@1 28.898
  **Test** Prec@1 77.330 Prec@5 98.620 Error@1 22.670
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 03:40:56] [Epoch=027/040] [Need: 00:49:16] [LR=0.0010] [Best : Accuracy=77.33, Error=22.67]
  Epoch: [027][000/500]   Time 18.540 (18.540)   Data 18.002 (18.002)   Loss 0.7854 (0.7854)   Prec@1 74.000 (74.000)   Prec@5 99.000 (99.000)   [2025-10-25 03:41:15]
  Epoch: [027][100/500]   Time 0.395 (0.559)   Data 0.002 (0.180)   Loss 0.8782 (0.8281)   Prec@1 65.000 (71.386)   Prec@5 96.000 (97.614)   [2025-10-25 03:41:53]
  Epoch: [027][200/500]   Time 0.400 (0.468)   Data 0.002 (0.091)   Loss 1.0113 (0.8259)   Prec@1 63.000 (71.507)   Prec@5 98.000 (97.711)   [2025-10-25 03:42:30]
  Epoch: [027][300/500]   Time 0.357 (0.437)   Data 0.001 (0.061)   Loss 0.9029 (0.8276)   Prec@1 64.000 (71.558)   Prec@5 97.000 (97.661)   [2025-10-25 03:43:08]
  Epoch: [027][400/500]   Time 0.486 (0.423)   Data 0.002 (0.046)   Loss 0.7754 (0.8302)   Prec@1 75.000 (71.551)   Prec@5 98.000 (97.601)   [2025-10-25 03:43:46]
  **Train** Prec@1 71.654 Prec@5 97.654 Error@1 28.346
  **Test** Prec@1 77.580 Prec@5 98.650 Error@1 22.420
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 03:44:43] [Epoch=028/040] [Need: 00:45:29] [LR=0.0010] [Best : Accuracy=77.58, Error=22.42]
  Epoch: [028][000/500]   Time 18.746 (18.746)   Data 18.199 (18.199)   Loss 0.7934 (0.7934)   Prec@1 72.000 (72.000)   Prec@5 98.000 (98.000)   [2025-10-25 03:45:02]
  Epoch: [028][100/500]   Time 0.330 (0.557)   Data 0.001 (0.182)   Loss 0.8620 (0.8345)   Prec@1 72.000 (71.832)   Prec@5 99.000 (97.574)   [2025-10-25 03:45:39]
  Epoch: [028][200/500]   Time 0.376 (0.468)   Data 0.000 (0.092)   Loss 0.8416 (0.8295)   Prec@1 73.000 (71.567)   Prec@5 97.000 (97.701)   [2025-10-25 03:46:17]
  Epoch: [028][300/500]   Time 0.353 (0.437)   Data 0.002 (0.062)   Loss 0.8183 (0.8319)   Prec@1 72.000 (71.429)   Prec@5 100.000 (97.631)   [2025-10-25 03:46:55]
  Epoch: [028][400/500]   Time 0.419 (0.423)   Data 0.002 (0.047)   Loss 0.8062 (0.8319)   Prec@1 68.000 (71.312)   Prec@5 99.000 (97.638)   [2025-10-25 03:47:33]
  **Train** Prec@1 71.506 Prec@5 97.660 Error@1 28.494
  **Test** Prec@1 77.670 Prec@5 98.750 Error@1 22.330
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 03:48:29] [Epoch=029/040] [Need: 00:41:40] [LR=0.0010] [Best : Accuracy=77.67, Error=22.33]
  Epoch: [029][000/500]   Time 18.372 (18.372)   Data 17.832 (17.832)   Loss 0.8580 (0.8580)   Prec@1 74.000 (74.000)   Prec@5 98.000 (98.000)   [2025-10-25 03:48:47]
  Epoch: [029][100/500]   Time 0.363 (0.554)   Data 0.001 (0.178)   Loss 0.8060 (0.8067)   Prec@1 79.000 (72.040)   Prec@5 96.000 (97.683)   [2025-10-25 03:49:25]
  Epoch: [029][200/500]   Time 0.353 (0.466)   Data 0.000 (0.090)   Loss 0.7624 (0.8179)   Prec@1 75.000 (72.010)   Prec@5 99.000 (97.692)   [2025-10-25 03:50:03]
  Epoch: [029][300/500]   Time 0.353 (0.437)   Data 0.001 (0.061)   Loss 0.9602 (0.8140)   Prec@1 62.000 (72.203)   Prec@5 98.000 (97.678)   [2025-10-25 03:50:40]
  Epoch: [029][400/500]   Time 0.402 (0.422)   Data 0.002 (0.046)   Loss 0.7641 (0.8115)   Prec@1 78.000 (72.137)   Prec@5 96.000 (97.676)   [2025-10-25 03:51:18]
  **Train** Prec@1 71.938 Prec@5 97.660 Error@1 28.062
  **Test** Prec@1 77.670 Prec@5 98.760 Error@1 22.330
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 03:52:15] [Epoch=030/040] [Need: 00:37:53] [LR=0.0010] [Best : Accuracy=77.67, Error=22.33]
  Epoch: [030][000/500]   Time 18.279 (18.279)   Data 17.717 (17.717)   Loss 0.7477 (0.7477)   Prec@1 69.000 (69.000)   Prec@5 99.000 (99.000)   [2025-10-25 03:52:33]
  Epoch: [030][100/500]   Time 0.327 (0.555)   Data 0.000 (0.177)   Loss 0.8042 (0.8119)   Prec@1 72.000 (71.673)   Prec@5 98.000 (97.782)   [2025-10-25 03:53:11]
  Epoch: [030][200/500]   Time 0.395 (0.465)   Data 0.002 (0.089)   Loss 0.7029 (0.8160)   Prec@1 72.000 (71.771)   Prec@5 100.000 (97.811)   [2025-10-25 03:53:48]
  Epoch: [030][300/500]   Time 0.362 (0.435)   Data 0.000 (0.060)   Loss 1.0166 (0.8154)   Prec@1 65.000 (71.811)   Prec@5 95.000 (97.774)   [2025-10-25 03:54:26]
  Epoch: [030][400/500]   Time 0.328 (0.421)   Data 0.001 (0.046)   Loss 0.7749 (0.8119)   Prec@1 73.000 (71.863)   Prec@5 99.000 (97.798)   [2025-10-25 03:55:03]
  **Train** Prec@1 71.858 Prec@5 97.784 Error@1 28.142
  **Test** Prec@1 77.870 Prec@5 98.730 Error@1 22.130
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 03:56:01] [Epoch=031/040] [Need: 00:34:05] [LR=0.0010] [Best : Accuracy=77.87, Error=22.13]
  Epoch: [031][000/500]   Time 19.313 (19.313)   Data 18.766 (18.766)   Loss 0.7962 (0.7962)   Prec@1 65.000 (65.000)   Prec@5 98.000 (98.000)   [2025-10-25 03:56:20]
  Epoch: [031][100/500]   Time 0.423 (0.563)   Data 0.002 (0.187)   Loss 0.7659 (0.8118)   Prec@1 71.000 (72.238)   Prec@5 99.000 (97.762)   [2025-10-25 03:56:57]
  Epoch: [031][200/500]   Time 0.384 (0.468)   Data 0.001 (0.095)   Loss 0.9575 (0.8110)   Prec@1 67.000 (72.015)   Prec@5 96.000 (97.766)   [2025-10-25 03:57:35]
  Epoch: [031][300/500]   Time 0.425 (0.437)   Data 0.003 (0.064)   Loss 0.7902 (0.8084)   Prec@1 76.000 (72.136)   Prec@5 98.000 (97.777)   [2025-10-25 03:58:12]
  Epoch: [031][400/500]   Time 0.370 (0.421)   Data 0.001 (0.048)   Loss 0.8269 (0.8087)   Prec@1 67.000 (72.252)   Prec@5 97.000 (97.736)   [2025-10-25 03:58:49]
  **Train** Prec@1 72.296 Prec@5 97.732 Error@1 27.704
  **Test** Prec@1 78.380 Prec@5 98.770 Error@1 21.620
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 03:59:46] [Epoch=032/040] [Need: 00:30:17] [LR=0.0010] [Best : Accuracy=78.38, Error=21.62]
  Epoch: [032][000/500]   Time 19.000 (19.000)   Data 18.450 (18.450)   Loss 0.8341 (0.8341)   Prec@1 69.000 (69.000)   Prec@5 100.000 (100.000)   [2025-10-25 04:00:05]
  Epoch: [032][100/500]   Time 0.354 (0.559)   Data 0.000 (0.184)   Loss 0.9461 (0.8127)   Prec@1 61.000 (72.040)   Prec@5 98.000 (97.673)   [2025-10-25 04:00:43]
  Epoch: [032][200/500]   Time 0.404 (0.469)   Data 0.002 (0.093)   Loss 0.9089 (0.8124)   Prec@1 71.000 (72.000)   Prec@5 97.000 (97.786)   [2025-10-25 04:01:21]
  Epoch: [032][300/500]   Time 0.401 (0.438)   Data 0.002 (0.063)   Loss 0.8476 (0.8082)   Prec@1 71.000 (72.110)   Prec@5 97.000 (97.738)   [2025-10-25 04:01:58]
  Epoch: [032][400/500]   Time 0.331 (0.423)   Data 0.000 (0.047)   Loss 0.7885 (0.8060)   Prec@1 69.000 (72.175)   Prec@5 100.000 (97.736)   [2025-10-25 04:02:36]
  **Train** Prec@1 72.234 Prec@5 97.738 Error@1 27.766
  **Test** Prec@1 78.100 Prec@5 98.710 Error@1 21.900

==>>[2025-10-25 04:03:32] [Epoch=033/040] [Need: 00:26:30] [LR=0.0010] [Best : Accuracy=78.38, Error=21.62]
  Epoch: [033][000/500]   Time 18.147 (18.147)   Data 17.604 (17.604)   Loss 0.6608 (0.6608)   Prec@1 77.000 (77.000)   Prec@5 99.000 (99.000)   [2025-10-25 04:03:50]
  Epoch: [033][100/500]   Time 0.385 (0.553)   Data 0.001 (0.176)   Loss 0.6980 (0.8049)   Prec@1 82.000 (72.099)   Prec@5 99.000 (97.733)   [2025-10-25 04:04:28]
  Epoch: [033][200/500]   Time 0.334 (0.466)   Data 0.001 (0.089)   Loss 0.7621 (0.8032)   Prec@1 75.000 (72.328)   Prec@5 98.000 (97.677)   [2025-10-25 04:05:06]
  Epoch: [033][300/500]   Time 0.350 (0.436)   Data 0.000 (0.060)   Loss 0.8136 (0.8040)   Prec@1 70.000 (72.259)   Prec@5 99.000 (97.704)   [2025-10-25 04:05:44]
  Epoch: [033][400/500]   Time 0.399 (0.420)   Data 0.001 (0.045)   Loss 0.7950 (0.8059)   Prec@1 70.000 (72.122)   Prec@5 99.000 (97.706)   [2025-10-25 04:06:21]
  **Train** Prec@1 72.150 Prec@5 97.710 Error@1 27.850
  **Test** Prec@1 78.360 Prec@5 98.690 Error@1 21.640

==>>[2025-10-25 04:07:17] [Epoch=034/040] [Need: 00:22:42] [LR=0.0010] [Best : Accuracy=78.38, Error=21.62]
  Epoch: [034][000/500]   Time 18.111 (18.111)   Data 17.564 (17.564)   Loss 0.6813 (0.6813)   Prec@1 74.000 (74.000)   Prec@5 100.000 (100.000)   [2025-10-25 04:07:35]
  Epoch: [034][100/500]   Time 0.403 (0.547)   Data 0.002 (0.175)   Loss 0.7259 (0.7920)   Prec@1 74.000 (72.743)   Prec@5 99.000 (97.713)   [2025-10-25 04:08:12]
  Epoch: [034][200/500]   Time 0.348 (0.465)   Data 0.000 (0.089)   Loss 0.8465 (0.7903)   Prec@1 65.000 (73.080)   Prec@5 98.000 (97.687)   [2025-10-25 04:08:50]
  Epoch: [034][300/500]   Time 0.329 (0.435)   Data 0.001 (0.060)   Loss 0.8159 (0.7928)   Prec@1 72.000 (72.804)   Prec@5 100.000 (97.784)   [2025-10-25 04:09:28]
  Epoch: [034][400/500]   Time 0.333 (0.420)   Data 0.001 (0.045)   Loss 0.7757 (0.7913)   Prec@1 72.000 (72.913)   Prec@5 98.000 (97.793)   [2025-10-25 04:10:05]
  **Train** Prec@1 72.618 Prec@5 97.788 Error@1 27.382
  **Test** Prec@1 78.400 Prec@5 98.710 Error@1 21.600
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 04:11:01] [Epoch=035/040] [Need: 00:18:55] [LR=0.0010] [Best : Accuracy=78.40, Error=21.60]
  Epoch: [035][000/500]   Time 18.326 (18.326)   Data 17.763 (17.763)   Loss 0.7912 (0.7912)   Prec@1 74.000 (74.000)   Prec@5 99.000 (99.000)   [2025-10-25 04:11:20]
  Epoch: [035][100/500]   Time 0.398 (0.551)   Data 0.001 (0.177)   Loss 0.7974 (0.7823)   Prec@1 72.000 (73.257)   Prec@5 96.000 (98.010)   [2025-10-25 04:11:57]
  Epoch: [035][200/500]   Time 0.346 (0.463)   Data 0.000 (0.090)   Loss 0.6359 (0.7875)   Prec@1 82.000 (72.886)   Prec@5 97.000 (97.935)   [2025-10-25 04:12:34]
  Epoch: [035][300/500]   Time 0.404 (0.434)   Data 0.002 (0.060)   Loss 0.6893 (0.7877)   Prec@1 77.000 (72.857)   Prec@5 99.000 (97.890)   [2025-10-25 04:13:12]
  Epoch: [035][400/500]   Time 0.411 (0.420)   Data 0.002 (0.046)   Loss 0.6355 (0.7908)   Prec@1 76.000 (72.908)   Prec@5 99.000 (97.828)   [2025-10-25 04:13:50]
  **Train** Prec@1 72.776 Prec@5 97.818 Error@1 27.224
  **Test** Prec@1 78.540 Prec@5 98.740 Error@1 21.460
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 04:14:46] [Epoch=036/040] [Need: 00:15:07] [LR=0.0010] [Best : Accuracy=78.54, Error=21.46]
  Epoch: [036][000/500]   Time 18.483 (18.483)   Data 17.952 (17.952)   Loss 0.8246 (0.8246)   Prec@1 71.000 (71.000)   Prec@5 100.000 (100.000)   [2025-10-25 04:15:04]
  Epoch: [036][100/500]   Time 0.336 (0.557)   Data 0.000 (0.179)   Loss 0.7111 (0.7964)   Prec@1 80.000 (72.149)   Prec@5 99.000 (97.743)   [2025-10-25 04:15:42]
  Epoch: [036][200/500]   Time 0.334 (0.467)   Data 0.001 (0.091)   Loss 0.8821 (0.7936)   Prec@1 69.000 (72.438)   Prec@5 96.000 (97.821)   [2025-10-25 04:16:20]
  Epoch: [036][300/500]   Time 0.363 (0.439)   Data 0.000 (0.061)   Loss 0.7219 (0.7872)   Prec@1 72.000 (72.791)   Prec@5 99.000 (97.824)   [2025-10-25 04:16:58]
  Epoch: [036][400/500]   Time 0.359 (0.423)   Data 0.001 (0.046)   Loss 0.7887 (0.7886)   Prec@1 77.000 (72.771)   Prec@5 98.000 (97.853)   [2025-10-25 04:17:36]
  **Train** Prec@1 72.816 Prec@5 97.812 Error@1 27.184
  **Test** Prec@1 78.560 Prec@5 98.770 Error@1 21.440
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 04:18:32] [Epoch=037/040] [Need: 00:11:20] [LR=0.0010] [Best : Accuracy=78.56, Error=21.44]
  Epoch: [037][000/500]   Time 18.302 (18.302)   Data 17.758 (17.758)   Loss 0.7938 (0.7938)   Prec@1 73.000 (73.000)   Prec@5 99.000 (99.000)   [2025-10-25 04:18:50]
  Epoch: [037][100/500]   Time 0.387 (0.551)   Data 0.001 (0.177)   Loss 0.6711 (0.7823)   Prec@1 78.000 (73.495)   Prec@5 99.000 (97.782)   [2025-10-25 04:19:28]
  Epoch: [037][200/500]   Time 0.349 (0.467)   Data 0.002 (0.090)   Loss 0.8113 (0.7892)   Prec@1 73.000 (73.035)   Prec@5 98.000 (97.697)   [2025-10-25 04:20:06]
  Epoch: [037][300/500]   Time 0.409 (0.438)   Data 0.002 (0.060)   Loss 0.6923 (0.7854)   Prec@1 74.000 (73.056)   Prec@5 97.000 (97.691)   [2025-10-25 04:20:44]
  Epoch: [037][400/500]   Time 0.398 (0.424)   Data 0.001 (0.046)   Loss 0.6190 (0.7856)   Prec@1 84.000 (73.077)   Prec@5 96.000 (97.706)   [2025-10-25 04:21:22]
  **Train** Prec@1 73.036 Prec@5 97.728 Error@1 26.964
  **Test** Prec@1 78.720 Prec@5 98.760 Error@1 21.280
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 04:22:18] [Epoch=038/040] [Need: 00:07:33] [LR=0.0010] [Best : Accuracy=78.72, Error=21.28]
  Epoch: [038][000/500]   Time 18.261 (18.261)   Data 17.696 (17.696)   Loss 0.7773 (0.7773)   Prec@1 75.000 (75.000)   Prec@5 98.000 (98.000)   [2025-10-25 04:22:36]
  Epoch: [038][100/500]   Time 0.406 (0.555)   Data 0.002 (0.177)   Loss 0.7093 (0.7765)   Prec@1 77.000 (73.010)   Prec@5 99.000 (97.960)   [2025-10-25 04:23:14]
  Epoch: [038][200/500]   Time 0.338 (0.466)   Data 0.000 (0.089)   Loss 0.5436 (0.7852)   Prec@1 80.000 (72.706)   Prec@5 100.000 (97.935)   [2025-10-25 04:23:51]
  Epoch: [038][300/500]   Time 0.405 (0.437)   Data 0.002 (0.060)   Loss 0.9216 (0.7886)   Prec@1 68.000 (72.714)   Prec@5 95.000 (97.904)   [2025-10-25 04:24:29]
  Epoch: [038][400/500]   Time 0.330 (0.422)   Data 0.000 (0.045)   Loss 0.8736 (0.7874)   Prec@1 68.000 (72.818)   Prec@5 99.000 (97.905)   [2025-10-25 04:25:07]
  **Train** Prec@1 72.854 Prec@5 97.946 Error@1 27.146
  **Test** Prec@1 78.840 Prec@5 98.750 Error@1 21.160
=> Obtain best accuracy, and update the best model

==>>[2025-10-25 04:26:03] [Epoch=039/040] [Need: 00:03:46] [LR=0.0010] [Best : Accuracy=78.84, Error=21.16]
  Epoch: [039][000/500]   Time 18.271 (18.271)   Data 17.707 (17.707)   Loss 0.8359 (0.8359)   Prec@1 73.000 (73.000)   Prec@5 97.000 (97.000)   [2025-10-25 04:26:21]
  Epoch: [039][100/500]   Time 0.409 (0.548)   Data 0.002 (0.177)   Loss 0.9095 (0.7854)   Prec@1 73.000 (72.851)   Prec@5 96.000 (97.980)   [2025-10-25 04:26:58]
  Epoch: [039][200/500]   Time 0.331 (0.463)   Data 0.001 (0.089)   Loss 0.8408 (0.7870)   Prec@1 74.000 (72.990)   Prec@5 95.000 (97.905)   [2025-10-25 04:27:36]
  Epoch: [039][300/500]   Time 0.334 (0.432)   Data 0.000 (0.060)   Loss 0.7133 (0.7860)   Prec@1 75.000 (72.937)   Prec@5 97.000 (97.890)   [2025-10-25 04:28:13]
  Epoch: [039][400/500]   Time 0.404 (0.419)   Data 0.000 (0.045)   Loss 0.7803 (0.7855)   Prec@1 72.000 (72.890)   Prec@5 98.000 (97.933)   [2025-10-25 04:28:50]
  **Train** Prec@1 73.182 Prec@5 97.890 Error@1 26.818
  **Test** Prec@1 78.730 Prec@5 98.790 Error@1 21.270
