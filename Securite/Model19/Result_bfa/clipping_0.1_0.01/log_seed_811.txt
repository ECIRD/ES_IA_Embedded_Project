save path : ./save/resnet9_quan/clipping_0.1_0.01
{'data_path': './dataset', 'arch': 'resnet9_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': False, 'clipping_coeff': 0.1, 'learning_rate': 0.01, 'manualSeed': 811, 'save_path': './save/resnet9_quan/clipping_0.1_0.01', 'enable_bfa': False, 'resume': '', 'quan_bitwidth': None, 'reset_weight': False, 'evaluate': False, 'n_iter': 30, 'model_only': False, 'random_bfa': False, 'use_cuda': True}
Random Seed: 811
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'resnet9_quan'
=> network :
 Sequential(
  (0): Sequential(
    (0): quan_Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (1): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (2): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (4): Dropout2d(p=0.2, inplace=False)
  (5): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (skip): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (6): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Dropout2d(p=0.3, inplace=False)
  (9): Sequential(
    (0): quan_Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (10): Sequential(
    (0): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (12): Dropout2d(p=0.3, inplace=False)
  (13): Sequential(
    (0): quan_Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (14): AdaptiveAvgPool2d(output_size=(1, 1))
  (15): Flatten()
  (16): quan_Linear(in_features=64, out_features=128, bias=False)
  (17): ReLU(inplace=True)
  (18): Dropout(p=0.35, inplace=False)
  (19): quan_Linear(in_features=128, out_features=10, bias=False)
  (20): Softmax(dim=1)
)
=> do not use any checkpoint for resnet9_quan model

==>>[2025-10-28 10:52:22] [Epoch=000/040] [Need: 00:00:00] [LR=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/500]   Time 24.112 (24.112)   Data 22.396 (22.396)   Loss 2.2999 (2.2999)   Prec@1 11.000 (11.000)   Prec@5 63.000 (63.000)   [2025-10-28 10:52:46]
  Epoch: [000][100/500]   Time 0.068 (0.301)   Data 0.001 (0.222)   Loss 2.2987 (2.3013)   Prec@1 12.000 (11.099)   Prec@5 51.000 (51.020)   [2025-10-28 10:52:52]
  Epoch: [000][200/500]   Time 0.069 (0.183)   Data 0.001 (0.112)   Loss 2.3026 (2.2959)   Prec@1 11.000 (11.955)   Prec@5 52.000 (53.020)   [2025-10-28 10:52:59]
  Epoch: [000][300/500]   Time 0.060 (0.144)   Data 0.000 (0.075)   Loss 2.2962 (2.2908)   Prec@1 14.000 (12.249)   Prec@5 65.000 (55.432)   [2025-10-28 10:53:05]
  Epoch: [000][400/500]   Time 0.062 (0.124)   Data 0.001 (0.056)   Loss 2.2643 (2.2867)   Prec@1 18.000 (12.985)   Prec@5 72.000 (57.681)   [2025-10-28 10:53:12]
  **Train** Prec@1 13.690 Prec@5 59.742 Error@1 86.310
  **Test** Prec@1 17.400 Prec@5 65.600 Error@1 82.600
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 10:53:41] [Epoch=001/040] [Need: 00:51:13] [LR=0.0100] [Best : Accuracy=17.40, Error=82.60]
  Epoch: [001][000/500]   Time 26.182 (26.182)   Data 25.926 (25.926)   Loss 2.2553 (2.2553)   Prec@1 17.000 (17.000)   Prec@5 70.000 (70.000)   [2025-10-28 10:54:07]
  Epoch: [001][100/500]   Time 0.053 (0.312)   Data 0.000 (0.257)   Loss 2.2541 (2.2645)   Prec@1 19.000 (17.079)   Prec@5 71.000 (68.614)   [2025-10-28 10:54:12]
  Epoch: [001][200/500]   Time 0.056 (0.184)   Data 0.000 (0.129)   Loss 2.2551 (2.2621)   Prec@1 17.000 (17.169)   Prec@5 61.000 (68.920)   [2025-10-28 10:54:18]
  Epoch: [001][300/500]   Time 0.054 (0.141)   Data 0.000 (0.086)   Loss 2.2575 (2.2596)   Prec@1 18.000 (17.223)   Prec@5 79.000 (69.372)   [2025-10-28 10:54:23]
  Epoch: [001][400/500]   Time 0.052 (0.120)   Data 0.000 (0.065)   Loss 2.2363 (2.2575)   Prec@1 20.000 (17.274)   Prec@5 79.000 (69.631)   [2025-10-28 10:54:29]
  **Train** Prec@1 17.344 Prec@5 69.872 Error@1 82.656
  **Test** Prec@1 16.630 Prec@5 63.060 Error@1 83.370

==>>[2025-10-28 10:55:02] [Epoch=002/040] [Need: 00:50:19] [LR=0.0100] [Best : Accuracy=17.40, Error=82.60]
  Epoch: [002][000/500]   Time 49.668 (49.668)   Data 49.019 (49.019)   Loss 2.2121 (2.2121)   Prec@1 23.000 (23.000)   Prec@5 80.000 (80.000)   [2025-10-28 10:55:51]
  Epoch: [002][100/500]   Time 0.053 (0.549)   Data 0.000 (0.486)   Loss 2.2257 (2.2433)   Prec@1 26.000 (17.782)   Prec@5 75.000 (71.891)   [2025-10-28 10:55:57]
  Epoch: [002][200/500]   Time 0.051 (0.303)   Data 0.000 (0.244)   Loss 2.2003 (2.2409)   Prec@1 24.000 (17.910)   Prec@5 80.000 (72.274)   [2025-10-28 10:56:03]
  Epoch: [002][300/500]   Time 0.055 (0.220)   Data 0.000 (0.163)   Loss 2.2332 (2.2396)   Prec@1 17.000 (17.831)   Prec@5 74.000 (72.532)   [2025-10-28 10:56:08]
  Epoch: [002][400/500]   Time 0.052 (0.179)   Data 0.000 (0.122)   Loss 2.2223 (2.2379)   Prec@1 17.000 (17.768)   Prec@5 76.000 (72.943)   [2025-10-28 10:56:13]
  **Train** Prec@1 17.856 Prec@5 73.052 Error@1 82.144
  **Test** Prec@1 19.400 Prec@5 71.200 Error@1 80.600
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 10:56:44] [Epoch=003/040] [Need: 00:53:46] [LR=0.0100] [Best : Accuracy=19.40, Error=80.60]
  Epoch: [003][000/500]   Time 19.978 (19.978)   Data 19.732 (19.732)   Loss 2.2197 (2.2197)   Prec@1 16.000 (16.000)   Prec@5 75.000 (75.000)   [2025-10-28 10:57:04]
  Epoch: [003][100/500]   Time 0.055 (0.250)   Data 0.000 (0.196)   Loss 2.2325 (2.2296)   Prec@1 21.000 (18.327)   Prec@5 69.000 (74.653)   [2025-10-28 10:57:09]
  Epoch: [003][200/500]   Time 0.054 (0.152)   Data 0.000 (0.098)   Loss 2.2445 (2.2291)   Prec@1 19.000 (18.403)   Prec@5 75.000 (74.637)   [2025-10-28 10:57:14]
  Epoch: [003][300/500]   Time 0.055 (0.120)   Data 0.000 (0.066)   Loss 2.2546 (2.2281)   Prec@1 19.000 (18.512)   Prec@5 74.000 (74.910)   [2025-10-28 10:57:20]
  Epoch: [003][400/500]   Time 0.053 (0.104)   Data 0.000 (0.049)   Loss 2.2282 (2.2277)   Prec@1 16.000 (18.638)   Prec@5 77.000 (75.095)   [2025-10-28 10:57:25]
  **Train** Prec@1 18.858 Prec@5 75.008 Error@1 81.142
  **Test** Prec@1 17.980 Prec@5 65.460 Error@1 82.020

==>>[2025-10-28 10:57:52] [Epoch=004/040] [Need: 00:49:34] [LR=0.0100] [Best : Accuracy=19.40, Error=80.60]
  Epoch: [004][000/500]   Time 19.145 (19.145)   Data 18.866 (18.866)   Loss 2.2345 (2.2345)   Prec@1 18.000 (18.000)   Prec@5 76.000 (76.000)   [2025-10-28 10:58:12]
  Epoch: [004][100/500]   Time 0.052 (0.242)   Data 0.000 (0.187)   Loss 2.2137 (2.2221)   Prec@1 17.000 (21.267)   Prec@5 74.000 (74.842)   [2025-10-28 10:58:17]
  Epoch: [004][200/500]   Time 0.055 (0.148)   Data 0.000 (0.094)   Loss 2.2095 (2.2176)   Prec@1 25.000 (21.806)   Prec@5 82.000 (75.746)   [2025-10-28 10:58:22]
  Epoch: [004][300/500]   Time 0.051 (0.117)   Data 0.000 (0.063)   Loss 2.1910 (2.2150)   Prec@1 24.000 (22.156)   Prec@5 74.000 (75.854)   [2025-10-28 10:58:28]
  Epoch: [004][400/500]   Time 0.051 (0.101)   Data 0.000 (0.047)   Loss 2.2098 (2.2128)   Prec@1 16.000 (22.272)   Prec@5 79.000 (76.167)   [2025-10-28 10:58:33]
  **Train** Prec@1 22.518 Prec@5 76.212 Error@1 77.482
  **Test** Prec@1 17.990 Prec@5 68.150 Error@1 82.010

==>>[2025-10-28 10:58:59] [Epoch=005/040] [Need: 00:46:17] [LR=0.0100] [Best : Accuracy=19.40, Error=80.60]
  Epoch: [005][000/500]   Time 18.806 (18.806)   Data 18.539 (18.539)   Loss 2.2432 (2.2432)   Prec@1 20.000 (20.000)   Prec@5 69.000 (69.000)   [2025-10-28 10:59:17]
  Epoch: [005][100/500]   Time 0.059 (0.239)   Data 0.000 (0.184)   Loss 2.1616 (2.1939)   Prec@1 30.000 (24.446)   Prec@5 76.000 (76.970)   [2025-10-28 10:59:23]
  Epoch: [005][200/500]   Time 0.057 (0.148)   Data 0.000 (0.092)   Loss 2.1521 (2.1935)   Prec@1 34.000 (24.473)   Prec@5 75.000 (77.119)   [2025-10-28 10:59:28]
  Epoch: [005][300/500]   Time 0.059 (0.117)   Data 0.000 (0.062)   Loss 2.1960 (2.1923)   Prec@1 25.000 (24.498)   Prec@5 71.000 (77.372)   [2025-10-28 10:59:34]
  Epoch: [005][400/500]   Time 0.056 (0.102)   Data 0.000 (0.046)   Loss 2.1975 (2.1908)   Prec@1 24.000 (24.693)   Prec@5 71.000 (77.344)   [2025-10-28 10:59:40]
  **Train** Prec@1 24.790 Prec@5 77.632 Error@1 75.210
  **Test** Prec@1 23.510 Prec@5 72.810 Error@1 76.490
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 11:00:08] [Epoch=006/040] [Need: 00:43:59] [LR=0.0100] [Best : Accuracy=23.51, Error=76.49]
  Epoch: [006][000/500]   Time 19.068 (19.068)   Data 18.817 (18.817)   Loss 2.1614 (2.1614)   Prec@1 27.000 (27.000)   Prec@5 83.000 (83.000)   [2025-10-28 11:00:27]
  Epoch: [006][100/500]   Time 0.053 (0.241)   Data 0.000 (0.186)   Loss 2.1871 (2.1779)   Prec@1 25.000 (26.248)   Prec@5 79.000 (78.663)   [2025-10-28 11:00:32]
  Epoch: [006][200/500]   Time 0.055 (0.148)   Data 0.000 (0.094)   Loss 2.1983 (2.1798)   Prec@1 24.000 (25.831)   Prec@5 81.000 (77.920)   [2025-10-28 11:00:38]
  Epoch: [006][300/500]   Time 0.055 (0.117)   Data 0.000 (0.063)   Loss 2.1931 (2.1766)   Prec@1 26.000 (26.179)   Prec@5 85.000 (78.266)   [2025-10-28 11:00:43]
  Epoch: [006][400/500]   Time 0.052 (0.102)   Data 0.000 (0.047)   Loss 2.1730 (2.1735)   Prec@1 30.000 (26.466)   Prec@5 69.000 (78.389)   [2025-10-28 11:00:49]
  **Train** Prec@1 26.528 Prec@5 78.504 Error@1 73.472
  **Test** Prec@1 27.920 Prec@5 78.850 Error@1 72.080
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 11:01:15] [Epoch=007/040] [Need: 00:41:51] [LR=0.0100] [Best : Accuracy=27.92, Error=72.08]
  Epoch: [007][000/500]   Time 18.227 (18.227)   Data 17.993 (17.993)   Loss 2.1312 (2.1312)   Prec@1 35.000 (35.000)   Prec@5 80.000 (80.000)   [2025-10-28 11:01:33]
  Epoch: [007][100/500]   Time 0.052 (0.233)   Data 0.001 (0.178)   Loss 2.1619 (2.1587)   Prec@1 26.000 (28.099)   Prec@5 77.000 (79.139)   [2025-10-28 11:01:38]
  Epoch: [007][200/500]   Time 0.054 (0.144)   Data 0.000 (0.090)   Loss 2.1104 (2.1569)   Prec@1 33.000 (28.080)   Prec@5 83.000 (79.114)   [2025-10-28 11:01:43]
  Epoch: [007][300/500]   Time 0.054 (0.114)   Data 0.000 (0.060)   Loss 2.1625 (2.1585)   Prec@1 23.000 (27.814)   Prec@5 76.000 (78.897)   [2025-10-28 11:01:49]
  Epoch: [007][400/500]   Time 0.055 (0.099)   Data 0.000 (0.045)   Loss 2.1554 (2.1565)   Prec@1 29.000 (28.192)   Prec@5 71.000 (79.027)   [2025-10-28 11:01:54]
  **Train** Prec@1 28.256 Prec@5 79.064 Error@1 71.744
  **Test** Prec@1 29.130 Prec@5 77.900 Error@1 70.870
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 11:02:21] [Epoch=008/040] [Need: 00:39:54] [LR=0.0100] [Best : Accuracy=29.13, Error=70.87]
  Epoch: [008][000/500]   Time 18.523 (18.523)   Data 18.300 (18.300)   Loss 2.1684 (2.1684)   Prec@1 26.000 (26.000)   Prec@5 76.000 (76.000)   [2025-10-28 11:02:39]
  Epoch: [008][100/500]   Time 0.051 (0.236)   Data 0.000 (0.181)   Loss 2.1138 (2.1429)   Prec@1 32.000 (30.485)   Prec@5 82.000 (78.713)   [2025-10-28 11:02:44]
  Epoch: [008][200/500]   Time 0.054 (0.145)   Data 0.000 (0.091)   Loss 2.1234 (2.1431)   Prec@1 33.000 (30.493)   Prec@5 82.000 (79.244)   [2025-10-28 11:02:50]
  Epoch: [008][300/500]   Time 0.053 (0.115)   Data 0.000 (0.061)   Loss 2.0949 (2.1417)   Prec@1 41.000 (30.731)   Prec@5 79.000 (79.581)   [2025-10-28 11:02:55]
  Epoch: [008][400/500]   Time 0.055 (0.100)   Data 0.000 (0.046)   Loss 2.1750 (2.1413)   Prec@1 25.000 (30.840)   Prec@5 76.000 (79.835)   [2025-10-28 11:03:01]
  **Train** Prec@1 30.934 Prec@5 79.810 Error@1 69.066
  **Test** Prec@1 33.000 Prec@5 79.820 Error@1 67.000
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 11:03:28] [Epoch=009/040] [Need: 00:38:14] [LR=0.0100] [Best : Accuracy=33.00, Error=67.00]
  Epoch: [009][000/500]   Time 19.637 (19.637)   Data 19.423 (19.423)   Loss 2.1161 (2.1161)   Prec@1 34.000 (34.000)   Prec@5 75.000 (75.000)   [2025-10-28 11:03:48]
  Epoch: [009][100/500]   Time 0.053 (0.247)   Data 0.000 (0.192)   Loss 2.1375 (2.1354)   Prec@1 28.000 (31.851)   Prec@5 76.000 (79.238)   [2025-10-28 11:03:53]
  Epoch: [009][200/500]   Time 0.053 (0.151)   Data 0.000 (0.097)   Loss 2.1565 (2.1319)   Prec@1 28.000 (32.443)   Prec@5 73.000 (79.786)   [2025-10-28 11:03:59]
  Epoch: [009][300/500]   Time 0.056 (0.119)   Data 0.000 (0.065)   Loss 2.1596 (2.1312)   Prec@1 28.000 (32.658)   Prec@5 77.000 (79.997)   [2025-10-28 11:04:04]
  Epoch: [009][400/500]   Time 0.055 (0.103)   Data 0.000 (0.049)   Loss 2.0433 (2.1271)   Prec@1 40.000 (33.115)   Prec@5 84.000 (80.294)   [2025-10-28 11:04:10]
  **Train** Prec@1 33.082 Prec@5 80.380 Error@1 66.918
  **Test** Prec@1 27.220 Prec@5 73.800 Error@1 72.780

==>>[2025-10-28 11:04:39] [Epoch=010/040] [Need: 00:36:50] [LR=0.0100] [Best : Accuracy=33.00, Error=67.00]
  Epoch: [010][000/500]   Time 19.445 (19.445)   Data 19.240 (19.240)   Loss 2.1092 (2.1092)   Prec@1 33.000 (33.000)   Prec@5 85.000 (85.000)   [2025-10-28 11:04:58]
  Epoch: [010][100/500]   Time 0.054 (0.245)   Data 0.000 (0.191)   Loss 2.1156 (2.1194)   Prec@1 34.000 (33.891)   Prec@5 77.000 (80.762)   [2025-10-28 11:05:04]
  Epoch: [010][200/500]   Time 0.053 (0.150)   Data 0.000 (0.096)   Loss 2.1337 (2.1201)   Prec@1 31.000 (33.741)   Prec@5 76.000 (80.866)   [2025-10-28 11:05:09]
  Epoch: [010][300/500]   Time 0.056 (0.118)   Data 0.000 (0.064)   Loss 2.0560 (2.1173)   Prec@1 39.000 (34.103)   Prec@5 81.000 (81.146)   [2025-10-28 11:05:15]
  Epoch: [010][400/500]   Time 0.056 (0.103)   Data 0.001 (0.048)   Loss 2.0919 (2.1142)   Prec@1 38.000 (34.401)   Prec@5 84.000 (81.137)   [2025-10-28 11:05:20]
  **Train** Prec@1 34.394 Prec@5 81.198 Error@1 65.606
  **Test** Prec@1 27.700 Prec@5 78.010 Error@1 72.300

==>>[2025-10-28 11:05:50] [Epoch=011/040] [Need: 00:35:30] [LR=0.0100] [Best : Accuracy=33.00, Error=67.00]
  Epoch: [011][000/500]   Time 35.067 (35.067)   Data 34.467 (34.467)   Loss 2.1762 (2.1762)   Prec@1 25.000 (25.000)   Prec@5 72.000 (72.000)   [2025-10-28 11:06:25]
  Epoch: [011][100/500]   Time 0.068 (0.419)   Data 0.001 (0.342)   Loss 2.1296 (2.1057)   Prec@1 35.000 (35.020)   Prec@5 82.000 (82.663)   [2025-10-28 11:06:33]
  Epoch: [011][200/500]   Time 0.071 (0.247)   Data 0.000 (0.172)   Loss 2.1566 (2.1069)   Prec@1 27.000 (35.045)   Prec@5 82.000 (82.935)   [2025-10-28 11:06:40]
  Epoch: [011][300/500]   Time 0.068 (0.188)   Data 0.001 (0.115)   Loss 2.1439 (2.1059)   Prec@1 34.000 (35.186)   Prec@5 79.000 (82.718)   [2025-10-28 11:06:47]
  Epoch: [011][400/500]   Time 0.072 (0.159)   Data 0.001 (0.086)   Loss 2.1025 (2.1051)   Prec@1 36.000 (35.332)   Prec@5 80.000 (82.830)   [2025-10-28 11:06:54]
  **Train** Prec@1 35.510 Prec@5 82.966 Error@1 64.490
  **Test** Prec@1 31.560 Prec@5 81.750 Error@1 68.440

==>>[2025-10-28 11:07:33] [Epoch=012/040] [Need: 00:35:24] [LR=0.0100] [Best : Accuracy=33.00, Error=67.00]
  Epoch: [012][000/500]   Time 25.164 (25.164)   Data 24.867 (24.867)   Loss 2.0855 (2.0855)   Prec@1 40.000 (40.000)   Prec@5 90.000 (90.000)   [2025-10-28 11:07:58]
  Epoch: [012][100/500]   Time 0.065 (0.315)   Data 0.000 (0.247)   Loss 2.0852 (2.0988)   Prec@1 36.000 (35.792)   Prec@5 76.000 (84.050)   [2025-10-28 11:08:05]
  Epoch: [012][200/500]   Time 0.063 (0.192)   Data 0.000 (0.124)   Loss 2.0692 (2.0966)   Prec@1 42.000 (36.448)   Prec@5 82.000 (84.095)   [2025-10-28 11:08:11]
  Epoch: [012][300/500]   Time 0.067 (0.151)   Data 0.000 (0.083)   Loss 2.1237 (2.0970)   Prec@1 33.000 (36.352)   Prec@5 83.000 (83.900)   [2025-10-28 11:08:18]
  Epoch: [012][400/500]   Time 0.067 (0.130)   Data 0.001 (0.062)   Loss 2.0917 (2.0967)   Prec@1 37.000 (36.287)   Prec@5 85.000 (83.930)   [2025-10-28 11:08:25]
  **Train** Prec@1 36.306 Prec@5 84.080 Error@1 63.694
  **Test** Prec@1 30.390 Prec@5 81.740 Error@1 69.610

==>>[2025-10-28 11:08:59] [Epoch=013/040] [Need: 00:34:31] [LR=0.0100] [Best : Accuracy=33.00, Error=67.00]
  Epoch: [013][000/500]   Time 26.737 (26.737)   Data 26.532 (26.532)   Loss 2.1817 (2.1817)   Prec@1 25.000 (25.000)   Prec@5 77.000 (77.000)   [2025-10-28 11:09:26]
  Epoch: [013][100/500]   Time 0.066 (0.331)   Data 0.001 (0.263)   Loss 2.1026 (2.0958)   Prec@1 34.000 (36.228)   Prec@5 83.000 (85.287)   [2025-10-28 11:09:33]
  Epoch: [013][200/500]   Time 0.070 (0.200)   Data 0.001 (0.132)   Loss 2.0375 (2.0894)   Prec@1 42.000 (36.955)   Prec@5 91.000 (85.632)   [2025-10-28 11:09:40]
  Epoch: [013][300/500]   Time 0.065 (0.156)   Data 0.000 (0.089)   Loss 2.0488 (2.0880)   Prec@1 40.000 (37.030)   Prec@5 84.000 (85.638)   [2025-10-28 11:09:46]
  Epoch: [013][400/500]   Time 0.068 (0.135)   Data 0.001 (0.067)   Loss 2.0275 (2.0856)   Prec@1 46.000 (37.416)   Prec@5 84.000 (85.781)   [2025-10-28 11:09:53]
  **Train** Prec@1 37.404 Prec@5 85.774 Error@1 62.596
  **Test** Prec@1 36.350 Prec@5 86.770 Error@1 63.650
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 11:10:22] [Epoch=014/040] [Need: 00:33:26] [LR=0.0100] [Best : Accuracy=36.35, Error=63.65]
  Epoch: [014][000/500]   Time 20.566 (20.566)   Data 20.310 (20.310)   Loss 2.1185 (2.1185)   Prec@1 33.000 (33.000)   Prec@5 79.000 (79.000)   [2025-10-28 11:10:43]
  Epoch: [014][100/500]   Time 0.068 (0.268)   Data 0.001 (0.202)   Loss 2.1459 (2.0844)   Prec@1 31.000 (37.416)   Prec@5 84.000 (86.079)   [2025-10-28 11:10:49]
  Epoch: [014][200/500]   Time 0.067 (0.168)   Data 0.000 (0.101)   Loss 2.0676 (2.0806)   Prec@1 38.000 (37.846)   Prec@5 80.000 (86.328)   [2025-10-28 11:10:56]
  Epoch: [014][300/500]   Time 0.065 (0.134)   Data 0.000 (0.068)   Loss 2.1215 (2.0785)   Prec@1 36.000 (38.007)   Prec@5 88.000 (86.615)   [2025-10-28 11:11:03]
  Epoch: [014][400/500]   Time 0.066 (0.117)   Data 0.000 (0.051)   Loss 2.0876 (2.0784)   Prec@1 37.000 (38.017)   Prec@5 89.000 (86.706)   [2025-10-28 11:11:09]
  **Train** Prec@1 38.294 Prec@5 86.718 Error@1 61.706
  **Test** Prec@1 33.500 Prec@5 84.940 Error@1 66.500

==>>[2025-10-28 11:11:44] [Epoch=015/040] [Need: 00:32:15] [LR=0.0100] [Best : Accuracy=36.35, Error=63.65]
  Epoch: [015][000/500]   Time 25.017 (25.017)   Data 24.748 (24.748)   Loss 2.0187 (2.0187)   Prec@1 46.000 (46.000)   Prec@5 92.000 (92.000)   [2025-10-28 11:12:09]
  Epoch: [015][100/500]   Time 0.065 (0.314)   Data 0.000 (0.245)   Loss 2.0074 (2.0710)   Prec@1 46.000 (38.713)   Prec@5 90.000 (87.614)   [2025-10-28 11:12:15]
  Epoch: [015][200/500]   Time 0.063 (0.191)   Data 0.000 (0.123)   Loss 2.0684 (2.0673)   Prec@1 40.000 (39.403)   Prec@5 91.000 (87.438)   [2025-10-28 11:12:22]
  Epoch: [015][300/500]   Time 0.064 (0.150)   Data 0.000 (0.083)   Loss 2.0768 (2.0668)   Prec@1 42.000 (39.442)   Prec@5 87.000 (87.515)   [2025-10-28 11:12:29]
  Epoch: [015][400/500]   Time 0.068 (0.130)   Data 0.001 (0.062)   Loss 1.9632 (2.0668)   Prec@1 55.000 (39.454)   Prec@5 90.000 (87.544)   [2025-10-28 11:12:36]
  **Train** Prec@1 39.460 Prec@5 87.378 Error@1 60.540
  **Test** Prec@1 36.000 Prec@5 86.860 Error@1 64.000

==>>[2025-10-28 11:13:07] [Epoch=016/040] [Need: 00:31:07] [LR=0.0100] [Best : Accuracy=36.35, Error=63.65]
  Epoch: [016][000/500]   Time 24.111 (24.111)   Data 23.821 (23.821)   Loss 2.0266 (2.0266)   Prec@1 44.000 (44.000)   Prec@5 83.000 (83.000)   [2025-10-28 11:13:31]
  Epoch: [016][100/500]   Time 0.063 (0.303)   Data 0.001 (0.236)   Loss 2.0045 (2.0642)   Prec@1 47.000 (39.653)   Prec@5 90.000 (87.535)   [2025-10-28 11:13:38]
  Epoch: [016][200/500]   Time 0.061 (0.185)   Data 0.000 (0.119)   Loss 2.0944 (2.0646)   Prec@1 33.000 (39.662)   Prec@5 86.000 (87.388)   [2025-10-28 11:13:45]
  Epoch: [016][300/500]   Time 0.064 (0.146)   Data 0.000 (0.079)   Loss 2.1008 (2.0631)   Prec@1 34.000 (39.934)   Prec@5 86.000 (87.449)   [2025-10-28 11:13:51]
  Epoch: [016][400/500]   Time 0.066 (0.126)   Data 0.002 (0.060)   Loss 2.0967 (2.0617)   Prec@1 37.000 (40.052)   Prec@5 88.000 (87.416)   [2025-10-28 11:13:58]
  **Train** Prec@1 40.212 Prec@5 87.418 Error@1 59.788
  **Test** Prec@1 32.870 Prec@5 84.630 Error@1 67.130

==>>[2025-10-28 11:14:29] [Epoch=017/040] [Need: 00:29:54] [LR=0.0100] [Best : Accuracy=36.35, Error=63.65]
  Epoch: [017][000/500]   Time 25.688 (25.688)   Data 25.382 (25.382)   Loss 2.0962 (2.0962)   Prec@1 36.000 (36.000)   Prec@5 87.000 (87.000)   [2025-10-28 11:14:54]
  Epoch: [017][100/500]   Time 0.068 (0.321)   Data 0.000 (0.252)   Loss 2.0020 (2.0594)   Prec@1 48.000 (40.188)   Prec@5 85.000 (87.584)   [2025-10-28 11:15:01]
  Epoch: [017][200/500]   Time 0.067 (0.194)   Data 0.000 (0.127)   Loss 2.0644 (2.0537)   Prec@1 43.000 (41.055)   Prec@5 91.000 (87.846)   [2025-10-28 11:15:08]
  Epoch: [017][300/500]   Time 0.064 (0.151)   Data 0.000 (0.085)   Loss 1.9791 (2.0504)   Prec@1 50.000 (41.505)   Prec@5 93.000 (87.977)   [2025-10-28 11:15:14]
  Epoch: [017][400/500]   Time 0.065 (0.130)   Data 0.000 (0.064)   Loss 2.0159 (2.0502)   Prec@1 46.000 (41.501)   Prec@5 81.000 (87.898)   [2025-10-28 11:15:21]
  **Train** Prec@1 41.426 Prec@5 87.664 Error@1 58.574
  **Test** Prec@1 40.120 Prec@5 87.960 Error@1 59.880
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 11:15:52] [Epoch=018/040] [Need: 00:28:43] [LR=0.0100] [Best : Accuracy=40.12, Error=59.88]
  Epoch: [018][000/500]   Time 25.358 (25.358)   Data 25.055 (25.055)   Loss 2.0844 (2.0844)   Prec@1 39.000 (39.000)   Prec@5 87.000 (87.000)   [2025-10-28 11:16:17]
  Epoch: [018][100/500]   Time 0.064 (0.316)   Data 0.000 (0.248)   Loss 2.0570 (2.0450)   Prec@1 39.000 (41.723)   Prec@5 85.000 (87.426)   [2025-10-28 11:16:24]
  Epoch: [018][200/500]   Time 0.066 (0.191)   Data 0.001 (0.125)   Loss 2.1183 (2.0429)   Prec@1 36.000 (42.045)   Prec@5 83.000 (87.607)   [2025-10-28 11:16:30]
  Epoch: [018][300/500]   Time 0.068 (0.150)   Data 0.000 (0.084)   Loss 2.0446 (2.0436)   Prec@1 41.000 (41.977)   Prec@5 88.000 (87.518)   [2025-10-28 11:16:37]
  Epoch: [018][400/500]   Time 0.066 (0.129)   Data 0.001 (0.063)   Loss 2.0728 (2.0423)   Prec@1 42.000 (42.127)   Prec@5 83.000 (87.561)   [2025-10-28 11:16:44]
  **Train** Prec@1 41.798 Prec@5 87.422 Error@1 58.202
  **Test** Prec@1 36.690 Prec@5 84.320 Error@1 63.310

==>>[2025-10-28 11:17:30] [Epoch=019/040] [Need: 00:27:46] [LR=0.0100] [Best : Accuracy=40.12, Error=59.88]
  Epoch: [019][000/500]   Time 31.289 (31.289)   Data 30.859 (30.859)   Loss 2.0836 (2.0836)   Prec@1 40.000 (40.000)   Prec@5 86.000 (86.000)   [2025-10-28 11:18:01]
  Epoch: [019][100/500]   Time 0.065 (0.376)   Data 0.001 (0.306)   Loss 2.0039 (2.0393)   Prec@1 45.000 (42.416)   Prec@5 86.000 (87.822)   [2025-10-28 11:18:08]
  Epoch: [019][200/500]   Time 0.064 (0.224)   Data 0.000 (0.154)   Loss 2.1041 (2.0431)   Prec@1 37.000 (42.030)   Prec@5 82.000 (87.299)   [2025-10-28 11:18:15]
  Epoch: [019][300/500]   Time 0.068 (0.172)   Data 0.001 (0.103)   Loss 2.0539 (2.0430)   Prec@1 43.000 (41.963)   Prec@5 87.000 (87.465)   [2025-10-28 11:18:22]
  Epoch: [019][400/500]   Time 0.066 (0.145)   Data 0.000 (0.077)   Loss 2.0721 (2.0417)   Prec@1 40.000 (42.132)   Prec@5 86.000 (87.484)   [2025-10-28 11:18:28]
  **Train** Prec@1 42.482 Prec@5 87.654 Error@1 57.518
  **Test** Prec@1 38.840 Prec@5 87.580 Error@1 61.160

==>>[2025-10-28 11:18:58] [Epoch=020/040] [Need: 00:26:36] [LR=0.0100] [Best : Accuracy=40.12, Error=59.88]
  Epoch: [020][000/500]   Time 22.477 (22.477)   Data 22.225 (22.225)   Loss 1.9930 (1.9930)   Prec@1 47.000 (47.000)   Prec@5 90.000 (90.000)   [2025-10-28 11:19:21]
  Epoch: [020][100/500]   Time 0.062 (0.288)   Data 0.001 (0.220)   Loss 1.9965 (2.0281)   Prec@1 47.000 (43.465)   Prec@5 91.000 (87.416)   [2025-10-28 11:19:28]
  Epoch: [020][200/500]   Time 0.069 (0.177)   Data 0.002 (0.111)   Loss 1.9658 (2.0319)   Prec@1 51.000 (43.050)   Prec@5 92.000 (87.443)   [2025-10-28 11:19:34]
  Epoch: [020][300/500]   Time 0.069 (0.141)   Data 0.000 (0.074)   Loss 2.0418 (2.0325)   Prec@1 42.000 (42.927)   Prec@5 83.000 (87.449)   [2025-10-28 11:19:41]
  Epoch: [020][400/500]   Time 0.070 (0.123)   Data 0.000 (0.056)   Loss 2.0167 (2.0307)   Prec@1 45.000 (43.132)   Prec@5 87.000 (87.601)   [2025-10-28 11:19:48]
  **Train** Prec@1 43.166 Prec@5 87.700 Error@1 56.834
  **Test** Prec@1 40.300 Prec@5 90.080 Error@1 59.700
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 11:20:18] [Epoch=021/040] [Need: 00:25:15] [LR=0.0100] [Best : Accuracy=40.30, Error=59.70]
  Epoch: [021][000/500]   Time 23.152 (23.152)   Data 22.847 (22.847)   Loss 2.0303 (2.0303)   Prec@1 43.000 (43.000)   Prec@5 87.000 (87.000)   [2025-10-28 11:20:41]
  Epoch: [021][100/500]   Time 0.068 (0.294)   Data 0.001 (0.227)   Loss 2.0757 (2.0325)   Prec@1 38.000 (42.802)   Prec@5 86.000 (87.050)   [2025-10-28 11:20:47]
  Epoch: [021][200/500]   Time 0.067 (0.180)   Data 0.000 (0.114)   Loss 2.0256 (2.0310)   Prec@1 44.000 (43.020)   Prec@5 88.000 (87.502)   [2025-10-28 11:20:54]
  Epoch: [021][300/500]   Time 0.065 (0.142)   Data 0.000 (0.076)   Loss 2.0155 (2.0269)   Prec@1 43.000 (43.472)   Prec@5 88.000 (87.674)   [2025-10-28 11:21:01]
  Epoch: [021][400/500]   Time 0.068 (0.123)   Data 0.002 (0.057)   Loss 2.0054 (2.0257)   Prec@1 46.000 (43.628)   Prec@5 89.000 (87.613)   [2025-10-28 11:21:07]
  **Train** Prec@1 43.640 Prec@5 87.660 Error@1 56.360
  **Test** Prec@1 39.190 Prec@5 88.320 Error@1 60.810

==>>[2025-10-28 11:21:37] [Epoch=022/040] [Need: 00:23:55] [LR=0.0100] [Best : Accuracy=40.30, Error=59.70]
  Epoch: [022][000/500]   Time 22.404 (22.404)   Data 22.104 (22.104)   Loss 2.0139 (2.0139)   Prec@1 45.000 (45.000)   Prec@5 91.000 (91.000)   [2025-10-28 11:21:59]
  Epoch: [022][100/500]   Time 0.063 (0.287)   Data 0.001 (0.219)   Loss 2.0306 (2.0200)   Prec@1 42.000 (44.109)   Prec@5 93.000 (88.287)   [2025-10-28 11:22:06]
  Epoch: [022][200/500]   Time 0.057 (0.176)   Data 0.001 (0.110)   Loss 2.0385 (2.0173)   Prec@1 42.000 (44.378)   Prec@5 86.000 (88.602)   [2025-10-28 11:22:12]
  Epoch: [022][300/500]   Time 0.066 (0.140)   Data 0.000 (0.074)   Loss 2.0151 (2.0182)   Prec@1 47.000 (44.279)   Prec@5 86.000 (88.246)   [2025-10-28 11:22:19]
  Epoch: [022][400/500]   Time 0.066 (0.121)   Data 0.001 (0.056)   Loss 2.0156 (2.0197)   Prec@1 43.000 (44.157)   Prec@5 92.000 (88.115)   [2025-10-28 11:22:25]
  **Train** Prec@1 44.138 Prec@5 88.042 Error@1 55.862
  **Test** Prec@1 39.160 Prec@5 86.680 Error@1 60.840

==>>[2025-10-28 11:22:55] [Epoch=023/040] [Need: 00:22:34] [LR=0.0100] [Best : Accuracy=40.30, Error=59.70]
  Epoch: [023][000/500]   Time 21.934 (21.934)   Data 21.625 (21.625)   Loss 2.0420 (2.0420)   Prec@1 41.000 (41.000)   Prec@5 84.000 (84.000)   [2025-10-28 11:23:16]
  Epoch: [023][100/500]   Time 0.065 (0.281)   Data 0.000 (0.214)   Loss 2.0063 (2.0161)   Prec@1 46.000 (44.426)   Prec@5 91.000 (88.713)   [2025-10-28 11:23:23]
  Epoch: [023][200/500]   Time 0.062 (0.174)   Data 0.000 (0.108)   Loss 2.0315 (2.0127)   Prec@1 40.000 (44.905)   Prec@5 89.000 (88.637)   [2025-10-28 11:23:29]
  Epoch: [023][300/500]   Time 0.066 (0.138)   Data 0.000 (0.072)   Loss 2.0102 (2.0141)   Prec@1 43.000 (44.837)   Prec@5 89.000 (88.422)   [2025-10-28 11:23:36]
  Epoch: [023][400/500]   Time 0.068 (0.120)   Data 0.001 (0.054)   Loss 2.0145 (2.0135)   Prec@1 44.000 (44.893)   Prec@5 91.000 (88.444)   [2025-10-28 11:23:43]
  **Train** Prec@1 44.838 Prec@5 88.372 Error@1 55.162
  **Test** Prec@1 28.000 Prec@5 77.580 Error@1 72.000

==>>[2025-10-28 11:24:12] [Epoch=024/040] [Need: 00:21:13] [LR=0.0100] [Best : Accuracy=40.30, Error=59.70]
  Epoch: [024][000/500]   Time 23.305 (23.305)   Data 23.005 (23.005)   Loss 1.9948 (1.9948)   Prec@1 45.000 (45.000)   Prec@5 89.000 (89.000)   [2025-10-28 11:24:36]
  Epoch: [024][100/500]   Time 0.065 (0.295)   Data 0.000 (0.228)   Loss 2.0951 (2.0187)   Prec@1 37.000 (43.851)   Prec@5 84.000 (88.554)   [2025-10-28 11:24:42]
  Epoch: [024][200/500]   Time 0.070 (0.181)   Data 0.001 (0.115)   Loss 1.9463 (2.0152)   Prec@1 54.000 (44.363)   Prec@5 90.000 (88.692)   [2025-10-28 11:24:49]
  Epoch: [024][300/500]   Time 0.065 (0.143)   Data 0.000 (0.077)   Loss 2.0037 (2.0117)   Prec@1 45.000 (44.877)   Prec@5 83.000 (88.608)   [2025-10-28 11:24:55]
  Epoch: [024][400/500]   Time 0.063 (0.125)   Data 0.000 (0.058)   Loss 2.0491 (2.0100)   Prec@1 40.000 (45.140)   Prec@5 86.000 (88.696)   [2025-10-28 11:25:03]
  **Train** Prec@1 45.084 Prec@5 88.662 Error@1 54.916
  **Test** Prec@1 33.760 Prec@5 83.740 Error@1 66.240

==>>[2025-10-28 11:25:34] [Epoch=025/040] [Need: 00:19:55] [LR=0.0010] [Best : Accuracy=40.30, Error=59.70]
  Epoch: [025][000/500]   Time 24.397 (24.397)   Data 24.128 (24.128)   Loss 2.0467 (2.0467)   Prec@1 42.000 (42.000)   Prec@5 86.000 (86.000)   [2025-10-28 11:25:59]
  Epoch: [025][100/500]   Time 0.064 (0.307)   Data 0.000 (0.239)   Loss 1.9665 (1.9915)   Prec@1 52.000 (47.079)   Prec@5 90.000 (89.941)   [2025-10-28 11:26:05]
  Epoch: [025][200/500]   Time 0.065 (0.187)   Data 0.000 (0.120)   Loss 1.9693 (1.9870)   Prec@1 48.000 (47.711)   Prec@5 90.000 (90.085)   [2025-10-28 11:26:12]
  Epoch: [025][300/500]   Time 0.063 (0.147)   Data 0.001 (0.080)   Loss 1.9675 (1.9846)   Prec@1 51.000 (47.831)   Prec@5 91.000 (90.113)   [2025-10-28 11:26:19]
  Epoch: [025][400/500]   Time 0.065 (0.127)   Data 0.000 (0.061)   Loss 2.0087 (1.9820)   Prec@1 44.000 (48.182)   Prec@5 87.000 (90.142)   [2025-10-28 11:26:25]
  **Train** Prec@1 48.432 Prec@5 90.140 Error@1 51.568
  **Test** Prec@1 47.510 Prec@5 91.020 Error@1 52.490
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 11:26:56] [Epoch=026/040] [Need: 00:18:36] [LR=0.0010] [Best : Accuracy=47.51, Error=52.49]
  Epoch: [026][000/500]   Time 23.615 (23.615)   Data 23.403 (23.403)   Loss 1.9609 (1.9609)   Prec@1 51.000 (51.000)   Prec@5 89.000 (89.000)   [2025-10-28 11:27:20]
  Epoch: [026][100/500]   Time 0.067 (0.299)   Data 0.000 (0.232)   Loss 1.9482 (1.9655)   Prec@1 53.000 (50.089)   Prec@5 88.000 (90.653)   [2025-10-28 11:27:26]
  Epoch: [026][200/500]   Time 0.070 (0.185)   Data 0.000 (0.117)   Loss 1.9228 (1.9649)   Prec@1 54.000 (50.109)   Prec@5 92.000 (90.512)   [2025-10-28 11:27:33]
  Epoch: [026][300/500]   Time 0.075 (0.149)   Data 0.001 (0.078)   Loss 1.9235 (1.9649)   Prec@1 54.000 (50.030)   Prec@5 86.000 (90.452)   [2025-10-28 11:27:41]
  Epoch: [026][400/500]   Time 0.066 (0.129)   Data 0.000 (0.059)   Loss 1.9762 (1.9651)   Prec@1 48.000 (50.002)   Prec@5 94.000 (90.591)   [2025-10-28 11:27:48]
  **Train** Prec@1 49.998 Prec@5 90.576 Error@1 50.002
  **Test** Prec@1 47.470 Prec@5 91.130 Error@1 52.530

==>>[2025-10-28 11:28:24] [Epoch=027/040] [Need: 00:17:20] [LR=0.0010] [Best : Accuracy=47.51, Error=52.49]
  Epoch: [027][000/500]   Time 22.940 (22.940)   Data 22.618 (22.618)   Loss 1.9630 (1.9630)   Prec@1 50.000 (50.000)   Prec@5 89.000 (89.000)   [2025-10-28 11:28:47]
  Epoch: [027][100/500]   Time 0.053 (0.279)   Data 0.000 (0.224)   Loss 1.9098 (1.9557)   Prec@1 56.000 (50.891)   Prec@5 94.000 (89.891)   [2025-10-28 11:28:52]
  Epoch: [027][200/500]   Time 0.055 (0.167)   Data 0.000 (0.113)   Loss 1.9312 (1.9577)   Prec@1 54.000 (50.761)   Prec@5 97.000 (90.234)   [2025-10-28 11:28:58]
  Epoch: [027][300/500]   Time 0.056 (0.130)   Data 0.000 (0.075)   Loss 1.8725 (1.9559)   Prec@1 61.000 (50.860)   Prec@5 97.000 (90.545)   [2025-10-28 11:29:03]
  Epoch: [027][400/500]   Time 0.051 (0.111)   Data 0.000 (0.057)   Loss 1.9604 (1.9583)   Prec@1 50.000 (50.691)   Prec@5 84.000 (90.501)   [2025-10-28 11:29:08]
  **Train** Prec@1 50.572 Prec@5 90.544 Error@1 49.428
  **Test** Prec@1 48.760 Prec@5 91.330 Error@1 51.240
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 11:29:36] [Epoch=028/040] [Need: 00:15:57] [LR=0.0010] [Best : Accuracy=48.76, Error=51.24]
  Epoch: [028][000/500]   Time 19.081 (19.081)   Data 18.795 (18.795)   Loss 1.9247 (1.9247)   Prec@1 54.000 (54.000)   Prec@5 89.000 (89.000)   [2025-10-28 11:29:55]
  Epoch: [028][100/500]   Time 0.053 (0.242)   Data 0.000 (0.186)   Loss 1.8646 (1.9547)   Prec@1 63.000 (51.238)   Prec@5 95.000 (90.515)   [2025-10-28 11:30:00]
  Epoch: [028][200/500]   Time 0.051 (0.148)   Data 0.000 (0.094)   Loss 1.8692 (1.9552)   Prec@1 61.000 (51.070)   Prec@5 93.000 (90.706)   [2025-10-28 11:30:05]
  Epoch: [028][300/500]   Time 0.052 (0.117)   Data 0.001 (0.063)   Loss 1.9486 (1.9550)   Prec@1 52.000 (51.023)   Prec@5 88.000 (90.870)   [2025-10-28 11:30:11]
  Epoch: [028][400/500]   Time 0.056 (0.102)   Data 0.001 (0.047)   Loss 1.9645 (1.9550)   Prec@1 49.000 (51.170)   Prec@5 92.000 (90.798)   [2025-10-28 11:30:16]
  **Train** Prec@1 51.022 Prec@5 90.730 Error@1 48.978
  **Test** Prec@1 46.640 Prec@5 90.170 Error@1 53.360

==>>[2025-10-28 11:30:43] [Epoch=029/040] [Need: 00:14:32] [LR=0.0010] [Best : Accuracy=48.76, Error=51.24]
  Epoch: [029][000/500]   Time 18.994 (18.994)   Data 18.740 (18.740)   Loss 1.9040 (1.9040)   Prec@1 56.000 (56.000)   Prec@5 92.000 (92.000)   [2025-10-28 11:31:02]
  Epoch: [029][100/500]   Time 0.060 (0.241)   Data 0.000 (0.186)   Loss 1.9142 (1.9552)   Prec@1 54.000 (50.980)   Prec@5 91.000 (91.307)   [2025-10-28 11:31:07]
  Epoch: [029][200/500]   Time 0.055 (0.148)   Data 0.000 (0.093)   Loss 1.9357 (1.9546)   Prec@1 53.000 (51.010)   Prec@5 90.000 (90.945)   [2025-10-28 11:31:13]
  Epoch: [029][300/500]   Time 0.059 (0.117)   Data 0.000 (0.062)   Loss 1.9761 (1.9544)   Prec@1 47.000 (51.123)   Prec@5 93.000 (91.080)   [2025-10-28 11:31:18]
  Epoch: [029][400/500]   Time 0.055 (0.101)   Data 0.000 (0.047)   Loss 1.9991 (1.9555)   Prec@1 49.000 (50.988)   Prec@5 92.000 (90.955)   [2025-10-28 11:31:24]
  **Train** Prec@1 51.166 Prec@5 90.888 Error@1 48.834
  **Test** Prec@1 45.770 Prec@5 89.750 Error@1 54.230

==>>[2025-10-28 11:31:50] [Epoch=030/040] [Need: 00:13:09] [LR=0.0010] [Best : Accuracy=48.76, Error=51.24]
  Epoch: [030][000/500]   Time 17.932 (17.932)   Data 17.661 (17.661)   Loss 1.8848 (1.8848)   Prec@1 59.000 (59.000)   Prec@5 91.000 (91.000)   [2025-10-28 11:32:08]
  Epoch: [030][100/500]   Time 0.056 (0.229)   Data 0.000 (0.175)   Loss 1.9570 (1.9534)   Prec@1 49.000 (51.248)   Prec@5 94.000 (91.149)   [2025-10-28 11:32:14]
  Epoch: [030][200/500]   Time 0.053 (0.142)   Data 0.000 (0.088)   Loss 1.9528 (1.9526)   Prec@1 50.000 (51.209)   Prec@5 88.000 (90.881)   [2025-10-28 11:32:19]
  Epoch: [030][300/500]   Time 0.055 (0.113)   Data 0.000 (0.059)   Loss 1.9339 (1.9536)   Prec@1 54.000 (51.090)   Prec@5 95.000 (90.824)   [2025-10-28 11:32:24]
  Epoch: [030][400/500]   Time 0.053 (0.098)   Data 0.000 (0.044)   Loss 1.9630 (1.9533)   Prec@1 49.000 (51.085)   Prec@5 87.000 (90.736)   [2025-10-28 11:32:30]
  **Train** Prec@1 50.998 Prec@5 90.648 Error@1 49.002
  **Test** Prec@1 45.720 Prec@5 89.710 Error@1 54.280

==>>[2025-10-28 11:32:55] [Epoch=031/040] [Need: 00:11:46] [LR=0.0010] [Best : Accuracy=48.76, Error=51.24]
  Epoch: [031][000/500]   Time 18.350 (18.350)   Data 18.038 (18.038)   Loss 2.0785 (2.0785)   Prec@1 36.000 (36.000)   Prec@5 79.000 (79.000)   [2025-10-28 11:33:14]
  Epoch: [031][100/500]   Time 0.067 (0.248)   Data 0.001 (0.179)   Loss 1.9111 (1.9485)   Prec@1 56.000 (51.564)   Prec@5 91.000 (90.653)   [2025-10-28 11:33:20]
  Epoch: [031][200/500]   Time 0.056 (0.152)   Data 0.000 (0.090)   Loss 1.9353 (1.9506)   Prec@1 55.000 (51.438)   Prec@5 92.000 (90.836)   [2025-10-28 11:33:26]
  Epoch: [031][300/500]   Time 0.058 (0.120)   Data 0.000 (0.060)   Loss 1.9501 (1.9503)   Prec@1 51.000 (51.488)   Prec@5 91.000 (90.993)   [2025-10-28 11:33:31]
  Epoch: [031][400/500]   Time 0.056 (0.103)   Data 0.000 (0.045)   Loss 2.0834 (1.9494)   Prec@1 39.000 (51.564)   Prec@5 90.000 (90.873)   [2025-10-28 11:33:37]
  **Train** Prec@1 51.590 Prec@5 90.766 Error@1 48.410
  **Test** Prec@1 48.340 Prec@5 90.940 Error@1 51.660

==>>[2025-10-28 11:34:03] [Epoch=032/040] [Need: 00:10:25] [LR=0.0010] [Best : Accuracy=48.76, Error=51.24]
  Epoch: [032][000/500]   Time 20.213 (20.213)   Data 19.984 (19.984)   Loss 1.9497 (1.9497)   Prec@1 53.000 (53.000)   Prec@5 91.000 (91.000)   [2025-10-28 11:34:23]
  Epoch: [032][100/500]   Time 0.056 (0.254)   Data 0.000 (0.198)   Loss 2.1015 (1.9539)   Prec@1 37.000 (51.050)   Prec@5 84.000 (90.990)   [2025-10-28 11:34:28]
  Epoch: [032][200/500]   Time 0.056 (0.155)   Data 0.000 (0.100)   Loss 1.9915 (1.9526)   Prec@1 49.000 (51.194)   Prec@5 91.000 (90.985)   [2025-10-28 11:34:34]
  Epoch: [032][300/500]   Time 0.056 (0.122)   Data 0.000 (0.067)   Loss 2.0034 (1.9522)   Prec@1 47.000 (51.322)   Prec@5 89.000 (90.993)   [2025-10-28 11:34:40]
  Epoch: [032][400/500]   Time 0.056 (0.106)   Data 0.001 (0.050)   Loss 1.9143 (1.9506)   Prec@1 56.000 (51.509)   Prec@5 93.000 (90.975)   [2025-10-28 11:34:45]
  **Train** Prec@1 51.788 Prec@5 90.976 Error@1 48.212
  **Test** Prec@1 48.740 Prec@5 91.140 Error@1 51.260

==>>[2025-10-28 11:35:14] [Epoch=033/040] [Need: 00:09:05] [LR=0.0010] [Best : Accuracy=48.76, Error=51.24]
  Epoch: [033][000/500]   Time 19.572 (19.572)   Data 19.293 (19.293)   Loss 1.9668 (1.9668)   Prec@1 51.000 (51.000)   Prec@5 90.000 (90.000)   [2025-10-28 11:35:34]
  Epoch: [033][100/500]   Time 0.055 (0.247)   Data 0.000 (0.191)   Loss 2.0110 (1.9479)   Prec@1 44.000 (51.663)   Prec@5 91.000 (90.931)   [2025-10-28 11:35:39]
  Epoch: [033][200/500]   Time 0.059 (0.151)   Data 0.000 (0.096)   Loss 1.9138 (1.9485)   Prec@1 53.000 (51.453)   Prec@5 94.000 (90.687)   [2025-10-28 11:35:45]
  Epoch: [033][300/500]   Time 0.055 (0.119)   Data 0.000 (0.064)   Loss 1.9115 (1.9474)   Prec@1 56.000 (51.658)   Prec@5 89.000 (90.767)   [2025-10-28 11:35:50]
  Epoch: [033][400/500]   Time 0.053 (0.103)   Data 0.000 (0.048)   Loss 1.9749 (1.9473)   Prec@1 48.000 (51.753)   Prec@5 89.000 (90.796)   [2025-10-28 11:35:56]
  **Train** Prec@1 51.648 Prec@5 90.800 Error@1 48.352
  **Test** Prec@1 46.680 Prec@5 90.310 Error@1 53.320

==>>[2025-10-28 11:36:22] [Epoch=034/040] [Need: 00:07:45] [LR=0.0010] [Best : Accuracy=48.76, Error=51.24]
  Epoch: [034][000/500]   Time 19.267 (19.267)   Data 19.021 (19.021)   Loss 1.8198 (1.8198)   Prec@1 67.000 (67.000)   Prec@5 97.000 (97.000)   [2025-10-28 11:36:42]
  Epoch: [034][100/500]   Time 0.053 (0.243)   Data 0.000 (0.188)   Loss 1.9586 (1.9479)   Prec@1 49.000 (51.723)   Prec@5 96.000 (90.515)   [2025-10-28 11:36:47]
  Epoch: [034][200/500]   Time 0.055 (0.149)   Data 0.000 (0.095)   Loss 1.9218 (1.9495)   Prec@1 54.000 (51.582)   Prec@5 92.000 (90.463)   [2025-10-28 11:36:52]
  Epoch: [034][300/500]   Time 0.055 (0.118)   Data 0.000 (0.063)   Loss 1.9255 (1.9486)   Prec@1 54.000 (51.598)   Prec@5 91.000 (90.558)   [2025-10-28 11:36:58]
  Epoch: [034][400/500]   Time 0.054 (0.102)   Data 0.000 (0.048)   Loss 1.9128 (1.9482)   Prec@1 54.000 (51.668)   Prec@5 97.000 (90.673)   [2025-10-28 11:37:03]
  **Train** Prec@1 51.940 Prec@5 90.740 Error@1 48.060
  **Test** Prec@1 47.770 Prec@5 89.850 Error@1 52.230

==>>[2025-10-28 11:37:30] [Epoch=035/040] [Need: 00:06:26] [LR=0.0010] [Best : Accuracy=48.76, Error=51.24]
  Epoch: [035][000/500]   Time 19.008 (19.008)   Data 18.813 (18.813)   Loss 1.9164 (1.9164)   Prec@1 54.000 (54.000)   Prec@5 92.000 (92.000)   [2025-10-28 11:37:49]
  Epoch: [035][100/500]   Time 0.055 (0.241)   Data 0.000 (0.186)   Loss 1.9302 (1.9381)   Prec@1 54.000 (52.822)   Prec@5 91.000 (90.941)   [2025-10-28 11:37:54]
  Epoch: [035][200/500]   Time 0.055 (0.148)   Data 0.000 (0.094)   Loss 1.9454 (1.9406)   Prec@1 52.000 (52.388)   Prec@5 83.000 (90.796)   [2025-10-28 11:38:00]
  Epoch: [035][300/500]   Time 0.056 (0.117)   Data 0.000 (0.063)   Loss 1.8901 (1.9429)   Prec@1 59.000 (52.223)   Prec@5 97.000 (90.890)   [2025-10-28 11:38:05]
  Epoch: [035][400/500]   Time 0.055 (0.102)   Data 0.000 (0.047)   Loss 1.8961 (1.9439)   Prec@1 57.000 (52.155)   Prec@5 90.000 (90.880)   [2025-10-28 11:38:11]
  **Train** Prec@1 52.144 Prec@5 90.946 Error@1 47.856
  **Test** Prec@1 49.540 Prec@5 91.650 Error@1 50.460
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 11:38:37] [Epoch=036/040] [Need: 00:05:08] [LR=0.0010] [Best : Accuracy=49.54, Error=50.46]
  Epoch: [036][000/500]   Time 23.525 (23.525)   Data 23.047 (23.047)   Loss 1.9148 (1.9148)   Prec@1 56.000 (56.000)   Prec@5 88.000 (88.000)   [2025-10-28 11:39:01]
  Epoch: [036][100/500]   Time 0.054 (0.289)   Data 0.000 (0.228)   Loss 1.8997 (1.9483)   Prec@1 55.000 (51.475)   Prec@5 92.000 (91.069)   [2025-10-28 11:39:06]
  Epoch: [036][200/500]   Time 0.056 (0.173)   Data 0.000 (0.115)   Loss 1.9265 (1.9466)   Prec@1 52.000 (51.682)   Prec@5 89.000 (91.109)   [2025-10-28 11:39:12]
  Epoch: [036][300/500]   Time 0.060 (0.135)   Data 0.000 (0.077)   Loss 1.8952 (1.9451)   Prec@1 57.000 (51.811)   Prec@5 96.000 (91.156)   [2025-10-28 11:39:18]
  Epoch: [036][400/500]   Time 0.058 (0.116)   Data 0.000 (0.058)   Loss 1.9796 (1.9454)   Prec@1 50.000 (51.833)   Prec@5 95.000 (91.132)   [2025-10-28 11:39:24]
  **Train** Prec@1 52.008 Prec@5 91.062 Error@1 47.992
  **Test** Prec@1 45.660 Prec@5 89.230 Error@1 54.340

==>>[2025-10-28 11:39:58] [Epoch=037/040] [Need: 00:03:51] [LR=0.0010] [Best : Accuracy=49.54, Error=50.46]
  Epoch: [037][000/500]   Time 19.863 (19.863)   Data 19.587 (19.587)   Loss 1.9460 (1.9460)   Prec@1 50.000 (50.000)   Prec@5 95.000 (95.000)   [2025-10-28 11:40:18]
  Epoch: [037][100/500]   Time 0.052 (0.250)   Data 0.000 (0.194)   Loss 1.9701 (1.9472)   Prec@1 50.000 (51.782)   Prec@5 91.000 (91.059)   [2025-10-28 11:40:23]
  Epoch: [037][200/500]   Time 0.055 (0.153)   Data 0.000 (0.098)   Loss 1.8984 (1.9407)   Prec@1 56.000 (52.697)   Prec@5 92.000 (91.328)   [2025-10-28 11:40:29]
  Epoch: [037][300/500]   Time 0.054 (0.120)   Data 0.000 (0.065)   Loss 1.8979 (1.9404)   Prec@1 58.000 (52.691)   Prec@5 95.000 (91.282)   [2025-10-28 11:40:34]
  Epoch: [037][400/500]   Time 0.055 (0.104)   Data 0.000 (0.049)   Loss 1.9160 (1.9397)   Prec@1 56.000 (52.743)   Prec@5 88.000 (91.165)   [2025-10-28 11:40:40]
  **Train** Prec@1 52.776 Prec@5 91.210 Error@1 47.224
  **Test** Prec@1 45.970 Prec@5 89.400 Error@1 54.030

==>>[2025-10-28 11:41:07] [Epoch=038/040] [Need: 00:02:33] [LR=0.0010] [Best : Accuracy=49.54, Error=50.46]
  Epoch: [038][000/500]   Time 17.954 (17.954)   Data 17.675 (17.675)   Loss 1.9649 (1.9649)   Prec@1 47.000 (47.000)   Prec@5 94.000 (94.000)   [2025-10-28 11:41:25]
  Epoch: [038][100/500]   Time 0.054 (0.231)   Data 0.000 (0.175)   Loss 1.9391 (1.9406)   Prec@1 51.000 (52.723)   Prec@5 95.000 (91.218)   [2025-10-28 11:41:30]
  Epoch: [038][200/500]   Time 0.057 (0.143)   Data 0.001 (0.088)   Loss 1.8650 (1.9398)   Prec@1 62.000 (52.726)   Prec@5 93.000 (91.100)   [2025-10-28 11:41:36]
  Epoch: [038][300/500]   Time 0.055 (0.113)   Data 0.000 (0.059)   Loss 1.9202 (1.9403)   Prec@1 53.000 (52.515)   Prec@5 89.000 (91.076)   [2025-10-28 11:41:41]
  Epoch: [038][400/500]   Time 0.054 (0.099)   Data 0.000 (0.044)   Loss 1.9820 (1.9400)   Prec@1 52.000 (52.571)   Prec@5 86.000 (91.037)   [2025-10-28 11:41:47]
  **Train** Prec@1 52.502 Prec@5 91.042 Error@1 47.498
  **Test** Prec@1 49.000 Prec@5 91.760 Error@1 51.000

==>>[2025-10-28 11:42:12] [Epoch=039/040] [Need: 00:01:16] [LR=0.0010] [Best : Accuracy=49.54, Error=50.46]
  Epoch: [039][000/500]   Time 17.822 (17.822)   Data 17.542 (17.542)   Loss 1.9833 (1.9833)   Prec@1 46.000 (46.000)   Prec@5 89.000 (89.000)   [2025-10-28 11:42:30]
  Epoch: [039][100/500]   Time 0.053 (0.229)   Data 0.000 (0.174)   Loss 2.0340 (1.9376)   Prec@1 43.000 (52.911)   Prec@5 94.000 (91.079)   [2025-10-28 11:42:35]
  Epoch: [039][200/500]   Time 0.054 (0.142)   Data 0.000 (0.087)   Loss 1.9383 (1.9372)   Prec@1 50.000 (52.950)   Prec@5 91.000 (91.229)   [2025-10-28 11:42:41]
  Epoch: [039][300/500]   Time 0.055 (0.113)   Data 0.000 (0.058)   Loss 1.9704 (1.9349)   Prec@1 48.000 (53.169)   Prec@5 93.000 (91.219)   [2025-10-28 11:42:46]
  Epoch: [039][400/500]   Time 0.054 (0.099)   Data 0.000 (0.044)   Loss 1.9104 (1.9356)   Prec@1 58.000 (53.107)   Prec@5 95.000 (91.162)   [2025-10-28 11:42:52]
  **Train** Prec@1 53.124 Prec@5 91.178 Error@1 46.876
  **Test** Prec@1 48.260 Prec@5 91.010 Error@1 51.740
