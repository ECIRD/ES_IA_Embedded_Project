save path : ./save/resnet9_quan/clipping_0.1_0.01
{'data_path': './dataset', 'arch': 'resnet9_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': False, 'clipping_coeff': 0.1, 'learning_rate': 0.01, 'manualSeed': 1235, 'save_path': './save/resnet9_quan/clipping_0.1_0.01', 'enable_bfa': False, 'resume': '', 'quan_bitwidth': None, 'reset_weight': False, 'evaluate': False, 'n_iter': 30, 'model_only': False, 'random_bfa': False, 'use_cuda': True}
Random Seed: 1235
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'resnet9_quan'
=> network :
 Sequential(
  (0): Sequential(
    (0): quan_Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (1): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (2): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (4): Dropout2d(p=0.2, inplace=False)
  (5): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (skip): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (6): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Dropout2d(p=0.3, inplace=False)
  (9): Sequential(
    (0): quan_Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (10): Sequential(
    (0): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (12): Dropout2d(p=0.3, inplace=False)
  (13): Sequential(
    (0): quan_Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (14): AdaptiveAvgPool2d(output_size=(1, 1))
  (15): Flatten()
  (16): quan_Linear(in_features=64, out_features=128, bias=False)
  (17): ReLU(inplace=True)
  (18): Dropout(p=0.35, inplace=False)
  (19): quan_Linear(in_features=128, out_features=10, bias=False)
  (20): Softmax(dim=1)
)
=> do not use any checkpoint for resnet9_quan model

==>>[2025-10-28 11:43:26] [Epoch=000/040] [Need: 00:00:00] [LR=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/500]   Time 18.758 (18.758)   Data 17.554 (17.554)   Loss 2.3012 (2.3012)   Prec@1 8.000 (8.000)   Prec@5 58.000 (58.000)   [2025-10-28 11:43:45]
  Epoch: [000][100/500]   Time 0.054 (0.238)   Data 0.000 (0.174)   Loss 2.3011 (2.3020)   Prec@1 6.000 (10.881)   Prec@5 54.000 (50.881)   [2025-10-28 11:43:50]
  Epoch: [000][200/500]   Time 0.054 (0.146)   Data 0.000 (0.087)   Loss 2.3005 (2.3011)   Prec@1 11.000 (11.398)   Prec@5 56.000 (52.960)   [2025-10-28 11:43:56]
  Epoch: [000][300/500]   Time 0.053 (0.115)   Data 0.000 (0.058)   Loss 2.2907 (2.2990)   Prec@1 9.000 (12.246)   Prec@5 62.000 (55.528)   [2025-10-28 11:44:01]
  Epoch: [000][400/500]   Time 0.058 (0.100)   Data 0.001 (0.044)   Loss 2.2773 (2.2957)   Prec@1 17.000 (13.342)   Prec@5 67.000 (58.050)   [2025-10-28 11:44:07]
  **Train** Prec@1 13.962 Prec@5 60.466 Error@1 86.038
  **Test** Prec@1 15.040 Prec@5 64.310 Error@1 84.960
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 11:44:48] [Epoch=001/040] [Need: 00:53:06] [LR=0.0100] [Best : Accuracy=15.04, Error=84.96]
  Epoch: [001][000/500]   Time 37.294 (37.294)   Data 36.981 (36.981)   Loss 2.2697 (2.2697)   Prec@1 17.000 (17.000)   Prec@5 71.000 (71.000)   [2025-10-28 11:45:26]
  Epoch: [001][100/500]   Time 0.070 (0.435)   Data 0.000 (0.366)   Loss 2.2209 (2.2674)   Prec@1 28.000 (16.792)   Prec@5 74.000 (72.079)   [2025-10-28 11:45:32]
  Epoch: [001][200/500]   Time 0.071 (0.252)   Data 0.001 (0.184)   Loss 2.2559 (2.2630)   Prec@1 17.000 (17.100)   Prec@5 80.000 (72.100)   [2025-10-28 11:45:39]
  Epoch: [001][300/500]   Time 0.066 (0.191)   Data 0.001 (0.123)   Loss 2.2458 (2.2611)   Prec@1 16.000 (16.947)   Prec@5 79.000 (72.259)   [2025-10-28 11:45:46]
  Epoch: [001][400/500]   Time 0.077 (0.161)   Data 0.000 (0.093)   Loss 2.2246 (2.2576)   Prec@1 20.000 (17.135)   Prec@5 74.000 (72.476)   [2025-10-28 11:45:53]
  **Train** Prec@1 17.240 Prec@5 72.662 Error@1 82.760
  **Test** Prec@1 15.990 Prec@5 62.780 Error@1 84.010
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 11:46:24] [Epoch=002/040] [Need: 00:56:13] [LR=0.0100] [Best : Accuracy=15.99, Error=84.01]
  Epoch: [002][000/500]   Time 22.484 (22.484)   Data 22.189 (22.189)   Loss 2.2044 (2.2044)   Prec@1 25.000 (25.000)   Prec@5 74.000 (74.000)   [2025-10-28 11:46:47]
  Epoch: [002][100/500]   Time 0.068 (0.288)   Data 0.001 (0.220)   Loss 2.1880 (2.2355)   Prec@1 27.000 (18.376)   Prec@5 82.000 (75.069)   [2025-10-28 11:46:53]
  Epoch: [002][200/500]   Time 0.067 (0.177)   Data 0.001 (0.111)   Loss 2.2394 (2.2362)   Prec@1 18.000 (17.970)   Prec@5 77.000 (74.433)   [2025-10-28 11:47:00]
  Epoch: [002][300/500]   Time 0.067 (0.141)   Data 0.000 (0.074)   Loss 2.2475 (2.2349)   Prec@1 17.000 (18.120)   Prec@5 70.000 (74.488)   [2025-10-28 11:47:07]
  Epoch: [002][400/500]   Time 0.067 (0.122)   Data 0.000 (0.056)   Loss 2.2277 (2.2350)   Prec@1 20.000 (18.012)   Prec@5 81.000 (74.673)   [2025-10-28 11:47:13]
  **Train** Prec@1 17.982 Prec@5 74.916 Error@1 82.018
  **Test** Prec@1 18.220 Prec@5 69.150 Error@1 81.780
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 11:47:43] [Epoch=003/040] [Need: 00:52:45] [LR=0.0100] [Best : Accuracy=18.22, Error=81.78]
  Epoch: [003][000/500]   Time 23.206 (23.206)   Data 22.910 (22.910)   Loss 2.2569 (2.2569)   Prec@1 14.000 (14.000)   Prec@5 73.000 (73.000)   [2025-10-28 11:48:06]
  Epoch: [003][100/500]   Time 0.061 (0.297)   Data 0.000 (0.227)   Loss 2.2552 (2.2306)   Prec@1 16.000 (18.109)   Prec@5 70.000 (75.515)   [2025-10-28 11:48:13]
  Epoch: [003][200/500]   Time 0.063 (0.183)   Data 0.000 (0.114)   Loss 2.2531 (2.2276)   Prec@1 16.000 (18.876)   Prec@5 61.000 (75.498)   [2025-10-28 11:48:20]
  Epoch: [003][300/500]   Time 0.067 (0.144)   Data 0.000 (0.076)   Loss 2.2347 (2.2274)   Prec@1 20.000 (18.718)   Prec@5 75.000 (75.757)   [2025-10-28 11:48:27]
  Epoch: [003][400/500]   Time 0.069 (0.125)   Data 0.001 (0.058)   Loss 2.2309 (2.2260)   Prec@1 22.000 (19.055)   Prec@5 72.000 (75.848)   [2025-10-28 11:48:33]
  **Train** Prec@1 19.530 Prec@5 75.908 Error@1 80.470
  **Test** Prec@1 20.480 Prec@5 74.220 Error@1 79.520
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 11:49:04] [Epoch=004/040] [Need: 00:50:36] [LR=0.0100] [Best : Accuracy=20.48, Error=79.52]
  Epoch: [004][000/500]   Time 24.681 (24.681)   Data 24.385 (24.385)   Loss 2.2233 (2.2233)   Prec@1 26.000 (26.000)   Prec@5 73.000 (73.000)   [2025-10-28 11:49:29]
  Epoch: [004][100/500]   Time 0.063 (0.311)   Data 0.001 (0.242)   Loss 2.2335 (2.2182)   Prec@1 16.000 (21.911)   Prec@5 74.000 (76.050)   [2025-10-28 11:49:35]
  Epoch: [004][200/500]   Time 0.071 (0.190)   Data 0.001 (0.122)   Loss 2.2067 (2.2138)   Prec@1 20.000 (22.502)   Prec@5 71.000 (76.154)   [2025-10-28 11:49:42]
  Epoch: [004][300/500]   Time 0.069 (0.150)   Data 0.001 (0.081)   Loss 2.2435 (2.2097)   Prec@1 17.000 (22.957)   Prec@5 75.000 (76.565)   [2025-10-28 11:49:49]
  Epoch: [004][400/500]   Time 0.069 (0.130)   Data 0.000 (0.061)   Loss 2.1641 (2.2074)   Prec@1 28.000 (23.185)   Prec@5 81.000 (76.748)   [2025-10-28 11:49:56]
  **Train** Prec@1 23.418 Prec@5 76.988 Error@1 76.582
  **Test** Prec@1 25.410 Prec@5 78.940 Error@1 74.590
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 11:50:44] [Epoch=005/040] [Need: 00:51:01] [LR=0.0100] [Best : Accuracy=25.41, Error=74.59]
  Epoch: [005][000/500]   Time 23.994 (23.994)   Data 23.617 (23.617)   Loss 2.1844 (2.1844)   Prec@1 27.000 (27.000)   Prec@5 86.000 (86.000)   [2025-10-28 11:51:08]
  Epoch: [005][100/500]   Time 0.067 (0.305)   Data 0.000 (0.234)   Loss 2.2215 (2.1905)   Prec@1 20.000 (24.653)   Prec@5 84.000 (77.861)   [2025-10-28 11:51:15]
  Epoch: [005][200/500]   Time 0.080 (0.186)   Data 0.000 (0.118)   Loss 2.2179 (2.1902)   Prec@1 21.000 (24.637)   Prec@5 75.000 (78.025)   [2025-10-28 11:51:22]
  Epoch: [005][300/500]   Time 0.065 (0.147)   Data 0.000 (0.079)   Loss 2.1078 (2.1887)   Prec@1 38.000 (24.844)   Prec@5 85.000 (78.389)   [2025-10-28 11:51:28]
  Epoch: [005][400/500]   Time 0.075 (0.127)   Data 0.000 (0.059)   Loss 2.1712 (2.1879)   Prec@1 29.000 (24.950)   Prec@5 83.000 (78.429)   [2025-10-28 11:51:35]
  **Train** Prec@1 25.030 Prec@5 78.566 Error@1 74.970
  **Test** Prec@1 22.780 Prec@5 74.200 Error@1 77.220

==>>[2025-10-28 11:52:06] [Epoch=006/040] [Need: 00:49:04] [LR=0.0100] [Best : Accuracy=25.41, Error=74.59]
  Epoch: [006][000/500]   Time 26.552 (26.552)   Data 26.253 (26.253)   Loss 2.1880 (2.1880)   Prec@1 29.000 (29.000)   Prec@5 78.000 (78.000)   [2025-10-28 11:52:33]
  Epoch: [006][100/500]   Time 0.062 (0.329)   Data 0.001 (0.260)   Loss 2.1783 (2.1835)   Prec@1 27.000 (25.317)   Prec@5 74.000 (79.376)   [2025-10-28 11:52:39]
  Epoch: [006][200/500]   Time 0.062 (0.198)   Data 0.000 (0.131)   Loss 2.1725 (2.1790)   Prec@1 31.000 (25.965)   Prec@5 79.000 (79.299)   [2025-10-28 11:52:46]
  Epoch: [006][300/500]   Time 0.068 (0.154)   Data 0.000 (0.088)   Loss 2.1661 (2.1797)   Prec@1 28.000 (25.761)   Prec@5 80.000 (79.465)   [2025-10-28 11:52:53]
  Epoch: [006][400/500]   Time 0.076 (0.133)   Data 0.000 (0.066)   Loss 2.2031 (2.1789)   Prec@1 20.000 (25.753)   Prec@5 83.000 (79.599)   [2025-10-28 11:53:00]
  **Train** Prec@1 25.930 Prec@5 79.702 Error@1 74.070
  **Test** Prec@1 26.720 Prec@5 79.990 Error@1 73.280
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 11:53:31] [Epoch=007/040] [Need: 00:47:27] [LR=0.0100] [Best : Accuracy=26.72, Error=73.28]
  Epoch: [007][000/500]   Time 24.465 (24.465)   Data 24.213 (24.213)   Loss 2.1732 (2.1732)   Prec@1 26.000 (26.000)   Prec@5 75.000 (75.000)   [2025-10-28 11:53:55]
  Epoch: [007][100/500]   Time 0.064 (0.309)   Data 0.000 (0.240)   Loss 2.1261 (2.1691)   Prec@1 33.000 (26.891)   Prec@5 85.000 (80.000)   [2025-10-28 11:54:02]
  Epoch: [007][200/500]   Time 0.062 (0.189)   Data 0.000 (0.121)   Loss 2.1708 (2.1673)   Prec@1 28.000 (27.199)   Prec@5 83.000 (79.905)   [2025-10-28 11:54:09]
  Epoch: [007][300/500]   Time 0.065 (0.148)   Data 0.001 (0.081)   Loss 2.1720 (2.1683)   Prec@1 24.000 (27.093)   Prec@5 78.000 (79.990)   [2025-10-28 11:54:15]
  Epoch: [007][400/500]   Time 0.066 (0.128)   Data 0.000 (0.061)   Loss 2.1779 (2.1670)   Prec@1 28.000 (27.284)   Prec@5 79.000 (79.880)   [2025-10-28 11:54:22]
  **Train** Prec@1 27.280 Prec@5 79.982 Error@1 72.720
  **Test** Prec@1 28.080 Prec@5 80.870 Error@1 71.920
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 11:54:53] [Epoch=008/040] [Need: 00:45:45] [LR=0.0100] [Best : Accuracy=28.08, Error=71.92]
  Epoch: [008][000/500]   Time 22.934 (22.934)   Data 22.651 (22.651)   Loss 2.1388 (2.1388)   Prec@1 30.000 (30.000)   Prec@5 78.000 (78.000)   [2025-10-28 11:55:16]
  Epoch: [008][100/500]   Time 0.067 (0.292)   Data 0.001 (0.224)   Loss 2.1515 (2.1680)   Prec@1 28.000 (27.663)   Prec@5 84.000 (79.554)   [2025-10-28 11:55:22]
  Epoch: [008][200/500]   Time 0.070 (0.179)   Data 0.000 (0.113)   Loss 2.1624 (2.1648)   Prec@1 25.000 (27.796)   Prec@5 86.000 (79.781)   [2025-10-28 11:55:29]
  Epoch: [008][300/500]   Time 0.068 (0.142)   Data 0.001 (0.076)   Loss 2.1637 (2.1629)   Prec@1 28.000 (28.033)   Prec@5 79.000 (79.870)   [2025-10-28 11:55:36]
  Epoch: [008][400/500]   Time 0.070 (0.123)   Data 0.000 (0.057)   Loss 2.1814 (2.1611)   Prec@1 28.000 (28.269)   Prec@5 81.000 (79.965)   [2025-10-28 11:55:42]
  **Train** Prec@1 28.452 Prec@5 79.832 Error@1 71.548
  **Test** Prec@1 25.360 Prec@5 74.360 Error@1 74.640

==>>[2025-10-28 11:56:13] [Epoch=009/040] [Need: 00:43:58] [LR=0.0100] [Best : Accuracy=28.08, Error=71.92]
  Epoch: [009][000/500]   Time 23.286 (23.286)   Data 22.988 (22.988)   Loss 2.1044 (2.1044)   Prec@1 38.000 (38.000)   Prec@5 79.000 (79.000)   [2025-10-28 11:56:36]
  Epoch: [009][100/500]   Time 0.065 (0.295)   Data 0.001 (0.228)   Loss 2.1653 (2.1567)   Prec@1 25.000 (29.267)   Prec@5 81.000 (80.406)   [2025-10-28 11:56:42]
  Epoch: [009][200/500]   Time 0.064 (0.181)   Data 0.000 (0.115)   Loss 2.1199 (2.1547)   Prec@1 35.000 (29.706)   Prec@5 83.000 (80.378)   [2025-10-28 11:56:49]
  Epoch: [009][300/500]   Time 0.063 (0.142)   Data 0.000 (0.077)   Loss 2.1386 (2.1523)   Prec@1 32.000 (29.947)   Prec@5 82.000 (80.528)   [2025-10-28 11:56:55]
  Epoch: [009][400/500]   Time 0.065 (0.123)   Data 0.000 (0.058)   Loss 2.1732 (2.1488)   Prec@1 26.000 (30.514)   Prec@5 71.000 (80.564)   [2025-10-28 11:57:02]
  **Train** Prec@1 30.690 Prec@5 80.766 Error@1 69.310
  **Test** Prec@1 29.760 Prec@5 78.200 Error@1 70.240
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 11:57:32] [Epoch=010/040] [Need: 00:42:15] [LR=0.0100] [Best : Accuracy=29.76, Error=70.24]
  Epoch: [010][000/500]   Time 22.841 (22.841)   Data 22.536 (22.536)   Loss 2.0824 (2.0824)   Prec@1 42.000 (42.000)   Prec@5 86.000 (86.000)   [2025-10-28 11:57:55]
  Epoch: [010][100/500]   Time 0.068 (0.292)   Data 0.001 (0.224)   Loss 2.1441 (2.1422)   Prec@1 33.000 (31.584)   Prec@5 83.000 (80.653)   [2025-10-28 11:58:01]
  Epoch: [010][200/500]   Time 0.064 (0.180)   Data 0.000 (0.112)   Loss 2.1272 (2.1411)   Prec@1 31.000 (31.443)   Prec@5 84.000 (80.393)   [2025-10-28 11:58:08]
  Epoch: [010][300/500]   Time 0.065 (0.143)   Data 0.002 (0.075)   Loss 2.1382 (2.1397)   Prec@1 29.000 (31.535)   Prec@5 79.000 (81.199)   [2025-10-28 11:58:15]
  Epoch: [010][400/500]   Time 0.064 (0.124)   Data 0.001 (0.057)   Loss 2.1197 (2.1379)   Prec@1 33.000 (31.596)   Prec@5 78.000 (81.377)   [2025-10-28 11:58:22]
  **Train** Prec@1 31.876 Prec@5 81.688 Error@1 68.124
  **Test** Prec@1 26.890 Prec@5 76.550 Error@1 73.110

==>>[2025-10-28 11:58:52] [Epoch=011/040] [Need: 00:40:38] [LR=0.0100] [Best : Accuracy=29.76, Error=70.24]
  Epoch: [011][000/500]   Time 22.844 (22.844)   Data 22.526 (22.526)   Loss 2.0763 (2.0763)   Prec@1 40.000 (40.000)   Prec@5 90.000 (90.000)   [2025-10-28 11:59:14]
  Epoch: [011][100/500]   Time 0.061 (0.290)   Data 0.000 (0.223)   Loss 2.1226 (2.1272)   Prec@1 36.000 (32.921)   Prec@5 90.000 (82.772)   [2025-10-28 11:59:21]
  Epoch: [011][200/500]   Time 0.064 (0.179)   Data 0.000 (0.112)   Loss 2.0929 (2.1240)   Prec@1 39.000 (33.318)   Prec@5 82.000 (82.711)   [2025-10-28 11:59:27]
  Epoch: [011][300/500]   Time 0.068 (0.141)   Data 0.000 (0.075)   Loss 2.1166 (2.1240)   Prec@1 36.000 (33.100)   Prec@5 85.000 (82.731)   [2025-10-28 11:59:34]
  Epoch: [011][400/500]   Time 0.066 (0.123)   Data 0.001 (0.057)   Loss 2.1268 (2.1252)   Prec@1 31.000 (32.973)   Prec@5 79.000 (82.808)   [2025-10-28 11:59:41]
  **Train** Prec@1 33.190 Prec@5 83.068 Error@1 66.810
  **Test** Prec@1 30.020 Prec@5 79.340 Error@1 69.980
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 12:00:11] [Epoch=012/040] [Need: 00:39:03] [LR=0.0100] [Best : Accuracy=30.02, Error=69.98]
  Epoch: [012][000/500]   Time 23.038 (23.038)   Data 22.744 (22.744)   Loss 2.0799 (2.0799)   Prec@1 40.000 (40.000)   Prec@5 83.000 (83.000)   [2025-10-28 12:00:34]
  Epoch: [012][100/500]   Time 0.063 (0.292)   Data 0.000 (0.226)   Loss 2.0897 (2.1176)   Prec@1 38.000 (33.663)   Prec@5 87.000 (84.040)   [2025-10-28 12:00:41]
  Epoch: [012][200/500]   Time 0.068 (0.180)   Data 0.000 (0.114)   Loss 2.0387 (2.1167)   Prec@1 45.000 (33.935)   Prec@5 85.000 (84.055)   [2025-10-28 12:00:47]
  Epoch: [012][300/500]   Time 0.070 (0.142)   Data 0.001 (0.076)   Loss 2.1257 (2.1137)   Prec@1 31.000 (34.203)   Prec@5 86.000 (84.110)   [2025-10-28 12:00:54]
  Epoch: [012][400/500]   Time 0.066 (0.123)   Data 0.000 (0.057)   Loss 2.1103 (2.1125)   Prec@1 36.000 (34.342)   Prec@5 86.000 (84.337)   [2025-10-28 12:01:01]
  **Train** Prec@1 34.350 Prec@5 84.536 Error@1 65.650
  **Test** Prec@1 30.940 Prec@5 82.280 Error@1 69.060
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 12:01:30] [Epoch=013/040] [Need: 00:37:30] [LR=0.0100] [Best : Accuracy=30.94, Error=69.06]
  Epoch: [013][000/500]   Time 22.320 (22.320)   Data 22.030 (22.030)   Loss 2.1110 (2.1110)   Prec@1 32.000 (32.000)   Prec@5 85.000 (85.000)   [2025-10-28 12:01:53]
  Epoch: [013][100/500]   Time 0.062 (0.287)   Data 0.000 (0.218)   Loss 2.0772 (2.1019)   Prec@1 39.000 (35.624)   Prec@5 83.000 (84.911)   [2025-10-28 12:01:59]
  Epoch: [013][200/500]   Time 0.063 (0.177)   Data 0.000 (0.110)   Loss 2.0436 (2.1010)   Prec@1 40.000 (35.662)   Prec@5 89.000 (85.488)   [2025-10-28 12:02:06]
  Epoch: [013][300/500]   Time 0.070 (0.140)   Data 0.000 (0.074)   Loss 2.0750 (2.1012)   Prec@1 35.000 (35.621)   Prec@5 85.000 (85.748)   [2025-10-28 12:02:13]
  Epoch: [013][400/500]   Time 0.068 (0.122)   Data 0.000 (0.055)   Loss 2.0757 (2.0994)   Prec@1 37.000 (35.858)   Prec@5 93.000 (85.905)   [2025-10-28 12:02:19]
  **Train** Prec@1 35.932 Prec@5 85.870 Error@1 64.068
  **Test** Prec@1 36.040 Prec@5 86.700 Error@1 63.960
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 12:02:49] [Epoch=014/040] [Need: 00:35:58] [LR=0.0100] [Best : Accuracy=36.04, Error=63.96]
  Epoch: [014][000/500]   Time 22.711 (22.711)   Data 22.403 (22.403)   Loss 2.0841 (2.0841)   Prec@1 37.000 (37.000)   Prec@5 90.000 (90.000)   [2025-10-28 12:03:12]
  Epoch: [014][100/500]   Time 0.062 (0.290)   Data 0.000 (0.222)   Loss 2.0959 (2.0889)   Prec@1 37.000 (37.050)   Prec@5 91.000 (86.040)   [2025-10-28 12:03:18]
  Epoch: [014][200/500]   Time 0.068 (0.178)   Data 0.000 (0.112)   Loss 2.0734 (2.0896)   Prec@1 41.000 (37.025)   Prec@5 83.000 (85.945)   [2025-10-28 12:03:25]
  Epoch: [014][300/500]   Time 0.066 (0.141)   Data 0.001 (0.075)   Loss 2.0895 (2.0895)   Prec@1 34.000 (37.120)   Prec@5 83.000 (85.970)   [2025-10-28 12:03:31]
  Epoch: [014][400/500]   Time 0.064 (0.122)   Data 0.000 (0.056)   Loss 2.1197 (2.0890)   Prec@1 31.000 (37.214)   Prec@5 82.000 (86.150)   [2025-10-28 12:03:38]
  **Train** Prec@1 37.434 Prec@5 86.228 Error@1 62.566
  **Test** Prec@1 30.270 Prec@5 80.800 Error@1 69.730

==>>[2025-10-28 12:04:08] [Epoch=015/040] [Need: 00:34:28] [LR=0.0100] [Best : Accuracy=36.04, Error=63.96]
  Epoch: [015][000/500]   Time 22.991 (22.991)   Data 22.718 (22.718)   Loss 2.0778 (2.0778)   Prec@1 40.000 (40.000)   Prec@5 83.000 (83.000)   [2025-10-28 12:04:31]
  Epoch: [015][100/500]   Time 0.063 (0.292)   Data 0.000 (0.225)   Loss 2.0562 (2.0842)   Prec@1 44.000 (37.861)   Prec@5 91.000 (87.040)   [2025-10-28 12:04:37]
  Epoch: [015][200/500]   Time 0.064 (0.179)   Data 0.000 (0.113)   Loss 2.0602 (2.0811)   Prec@1 39.000 (38.194)   Prec@5 84.000 (86.672)   [2025-10-28 12:04:44]
  Epoch: [015][300/500]   Time 0.065 (0.141)   Data 0.000 (0.076)   Loss 2.0550 (2.0790)   Prec@1 40.000 (38.276)   Prec@5 92.000 (86.821)   [2025-10-28 12:04:50]
  Epoch: [015][400/500]   Time 0.068 (0.123)   Data 0.000 (0.057)   Loss 2.0923 (2.0777)   Prec@1 38.000 (38.401)   Prec@5 87.000 (86.805)   [2025-10-28 12:04:57]
  **Train** Prec@1 38.488 Prec@5 86.834 Error@1 61.512
  **Test** Prec@1 39.850 Prec@5 90.020 Error@1 60.150
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 12:05:27] [Epoch=016/040] [Need: 00:33:00] [LR=0.0100] [Best : Accuracy=39.85, Error=60.15]
  Epoch: [016][000/500]   Time 22.787 (22.787)   Data 22.474 (22.474)   Loss 2.1197 (2.1197)   Prec@1 35.000 (35.000)   Prec@5 80.000 (80.000)   [2025-10-28 12:05:50]
  Epoch: [016][100/500]   Time 0.064 (0.290)   Data 0.000 (0.223)   Loss 2.0951 (2.0702)   Prec@1 33.000 (39.158)   Prec@5 88.000 (86.792)   [2025-10-28 12:05:56]
  Epoch: [016][200/500]   Time 0.065 (0.178)   Data 0.000 (0.112)   Loss 2.0792 (2.0703)   Prec@1 37.000 (39.234)   Prec@5 91.000 (87.159)   [2025-10-28 12:06:03]
  Epoch: [016][300/500]   Time 0.078 (0.141)   Data 0.001 (0.075)   Loss 2.1075 (2.0675)   Prec@1 34.000 (39.542)   Prec@5 87.000 (87.216)   [2025-10-28 12:06:09]
  Epoch: [016][400/500]   Time 0.071 (0.123)   Data 0.000 (0.056)   Loss 2.0349 (2.0657)   Prec@1 45.000 (39.736)   Prec@5 90.000 (87.387)   [2025-10-28 12:06:16]
  **Train** Prec@1 39.884 Prec@5 87.472 Error@1 60.116
  **Test** Prec@1 34.750 Prec@5 82.100 Error@1 65.250

==>>[2025-10-28 12:06:46] [Epoch=017/040] [Need: 00:31:33] [LR=0.0100] [Best : Accuracy=39.85, Error=60.15]
  Epoch: [017][000/500]   Time 23.106 (23.106)   Data 22.800 (22.800)   Loss 2.0434 (2.0434)   Prec@1 45.000 (45.000)   Prec@5 88.000 (88.000)   [2025-10-28 12:07:09]
  Epoch: [017][100/500]   Time 0.062 (0.294)   Data 0.000 (0.226)   Loss 2.0811 (2.0555)   Prec@1 37.000 (41.000)   Prec@5 86.000 (87.752)   [2025-10-28 12:07:16]
  Epoch: [017][200/500]   Time 0.066 (0.180)   Data 0.000 (0.114)   Loss 2.1223 (2.0601)   Prec@1 32.000 (40.308)   Prec@5 86.000 (87.786)   [2025-10-28 12:07:22]
  Epoch: [017][300/500]   Time 0.069 (0.142)   Data 0.000 (0.076)   Loss 2.0572 (2.0557)   Prec@1 40.000 (40.920)   Prec@5 85.000 (87.934)   [2025-10-28 12:07:29]
  Epoch: [017][400/500]   Time 0.068 (0.123)   Data 0.000 (0.057)   Loss 2.0334 (2.0547)   Prec@1 44.000 (40.988)   Prec@5 87.000 (88.060)   [2025-10-28 12:07:35]
  **Train** Prec@1 40.904 Prec@5 87.946 Error@1 59.096
  **Test** Prec@1 42.020 Prec@5 90.270 Error@1 57.980
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 12:08:06] [Epoch=018/040] [Need: 00:30:06] [LR=0.0100] [Best : Accuracy=42.02, Error=57.98]
  Epoch: [018][000/500]   Time 23.408 (23.408)   Data 23.149 (23.149)   Loss 2.0264 (2.0264)   Prec@1 44.000 (44.000)   Prec@5 87.000 (87.000)   [2025-10-28 12:08:29]
  Epoch: [018][100/500]   Time 0.065 (0.296)   Data 0.000 (0.230)   Loss 2.0185 (2.0503)   Prec@1 45.000 (41.079)   Prec@5 87.000 (88.257)   [2025-10-28 12:08:36]
  Epoch: [018][200/500]   Time 0.068 (0.182)   Data 0.000 (0.116)   Loss 2.0026 (2.0494)   Prec@1 48.000 (41.264)   Prec@5 92.000 (88.274)   [2025-10-28 12:08:42]
  Epoch: [018][300/500]   Time 0.066 (0.144)   Data 0.001 (0.077)   Loss 2.0506 (2.0507)   Prec@1 43.000 (41.130)   Prec@5 90.000 (88.306)   [2025-10-28 12:08:49]
  Epoch: [018][400/500]   Time 0.068 (0.125)   Data 0.001 (0.058)   Loss 2.0561 (2.0498)   Prec@1 39.000 (41.369)   Prec@5 91.000 (88.212)   [2025-10-28 12:08:56]
  **Train** Prec@1 41.536 Prec@5 88.318 Error@1 58.464
  **Test** Prec@1 42.670 Prec@5 89.460 Error@1 57.330
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 12:09:27] [Epoch=019/040] [Need: 00:28:44] [LR=0.0100] [Best : Accuracy=42.67, Error=57.33]
  Epoch: [019][000/500]   Time 24.258 (24.258)   Data 23.960 (23.960)   Loss 2.0911 (2.0911)   Prec@1 36.000 (36.000)   Prec@5 87.000 (87.000)   [2025-10-28 12:09:51]
  Epoch: [019][100/500]   Time 0.063 (0.306)   Data 0.000 (0.238)   Loss 2.0395 (2.0447)   Prec@1 41.000 (41.812)   Prec@5 87.000 (88.020)   [2025-10-28 12:09:58]
  Epoch: [019][200/500]   Time 0.069 (0.187)   Data 0.001 (0.120)   Loss 2.0795 (2.0438)   Prec@1 38.000 (41.856)   Prec@5 80.000 (87.731)   [2025-10-28 12:10:05]
  Epoch: [019][300/500]   Time 0.068 (0.148)   Data 0.001 (0.080)   Loss 1.9667 (2.0416)   Prec@1 50.000 (42.100)   Prec@5 84.000 (88.080)   [2025-10-28 12:10:11]
  Epoch: [019][400/500]   Time 0.067 (0.128)   Data 0.001 (0.060)   Loss 2.0468 (2.0400)   Prec@1 42.000 (42.289)   Prec@5 89.000 (88.252)   [2025-10-28 12:10:18]
  **Train** Prec@1 42.474 Prec@5 88.198 Error@1 57.526
  **Test** Prec@1 43.430 Prec@5 90.580 Error@1 56.570
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 12:10:48] [Epoch=020/040] [Need: 00:27:21] [LR=0.0100] [Best : Accuracy=43.43, Error=56.57]
  Epoch: [020][000/500]   Time 22.271 (22.271)   Data 21.975 (21.975)   Loss 2.0292 (2.0292)   Prec@1 45.000 (45.000)   Prec@5 87.000 (87.000)   [2025-10-28 12:11:10]
  Epoch: [020][100/500]   Time 0.066 (0.285)   Data 0.000 (0.218)   Loss 2.1143 (2.0388)   Prec@1 36.000 (42.347)   Prec@5 87.000 (88.149)   [2025-10-28 12:11:17]
  Epoch: [020][200/500]   Time 0.069 (0.176)   Data 0.000 (0.110)   Loss 2.0921 (2.0383)   Prec@1 36.000 (42.269)   Prec@5 87.000 (88.572)   [2025-10-28 12:11:23]
  Epoch: [020][300/500]   Time 0.065 (0.139)   Data 0.000 (0.073)   Loss 1.9789 (2.0353)   Prec@1 50.000 (42.611)   Prec@5 94.000 (88.668)   [2025-10-28 12:11:30]
  Epoch: [020][400/500]   Time 0.064 (0.122)   Data 0.000 (0.055)   Loss 2.0499 (2.0338)   Prec@1 41.000 (42.823)   Prec@5 87.000 (88.728)   [2025-10-28 12:11:37]
  **Train** Prec@1 42.956 Prec@5 88.682 Error@1 57.044
  **Test** Prec@1 42.900 Prec@5 89.930 Error@1 57.100

==>>[2025-10-28 12:12:08] [Epoch=021/040] [Need: 00:25:57] [LR=0.0100] [Best : Accuracy=43.43, Error=56.57]
  Epoch: [021][000/500]   Time 22.918 (22.918)   Data 22.675 (22.675)   Loss 2.0097 (2.0097)   Prec@1 47.000 (47.000)   Prec@5 89.000 (89.000)   [2025-10-28 12:12:31]
  Epoch: [021][100/500]   Time 0.070 (0.291)   Data 0.001 (0.225)   Loss 1.9872 (2.0281)   Prec@1 51.000 (43.475)   Prec@5 92.000 (88.228)   [2025-10-28 12:12:37]
  Epoch: [021][200/500]   Time 0.067 (0.180)   Data 0.000 (0.113)   Loss 2.0616 (2.0240)   Prec@1 40.000 (43.910)   Prec@5 89.000 (88.542)   [2025-10-28 12:12:44]
  Epoch: [021][300/500]   Time 0.063 (0.142)   Data 0.000 (0.076)   Loss 1.9674 (2.0240)   Prec@1 52.000 (43.854)   Prec@5 86.000 (88.508)   [2025-10-28 12:12:51]
  Epoch: [021][400/500]   Time 0.068 (0.123)   Data 0.000 (0.057)   Loss 2.0061 (2.0240)   Prec@1 43.000 (43.835)   Prec@5 93.000 (88.631)   [2025-10-28 12:12:57]
  **Train** Prec@1 43.464 Prec@5 88.544 Error@1 56.536
  **Test** Prec@1 38.550 Prec@5 86.370 Error@1 61.450

==>>[2025-10-28 12:13:26] [Epoch=022/040] [Need: 00:24:32] [LR=0.0100] [Best : Accuracy=43.43, Error=56.57]
  Epoch: [022][000/500]   Time 22.824 (22.824)   Data 22.541 (22.541)   Loss 2.0124 (2.0124)   Prec@1 44.000 (44.000)   Prec@5 90.000 (90.000)   [2025-10-28 12:13:49]
  Epoch: [022][100/500]   Time 0.073 (0.291)   Data 0.000 (0.223)   Loss 2.0649 (2.0259)   Prec@1 39.000 (43.277)   Prec@5 85.000 (88.307)   [2025-10-28 12:13:56]
  Epoch: [022][200/500]   Time 0.063 (0.179)   Data 0.001 (0.112)   Loss 2.0176 (2.0250)   Prec@1 44.000 (43.597)   Prec@5 87.000 (88.204)   [2025-10-28 12:14:02]
  Epoch: [022][300/500]   Time 0.076 (0.142)   Data 0.000 (0.075)   Loss 2.0294 (2.0219)   Prec@1 43.000 (43.987)   Prec@5 91.000 (88.532)   [2025-10-28 12:14:09]
  Epoch: [022][400/500]   Time 0.066 (0.124)   Data 0.001 (0.057)   Loss 1.9706 (2.0222)   Prec@1 50.000 (43.920)   Prec@5 86.000 (88.564)   [2025-10-28 12:14:16]
  **Train** Prec@1 43.776 Prec@5 88.644 Error@1 56.224
  **Test** Prec@1 47.910 Prec@5 91.260 Error@1 52.090
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 12:14:46] [Epoch=023/040] [Need: 00:23:09] [LR=0.0100] [Best : Accuracy=47.91, Error=52.09]
  Epoch: [023][000/500]   Time 22.901 (22.901)   Data 22.605 (22.605)   Loss 2.0676 (2.0676)   Prec@1 37.000 (37.000)   Prec@5 85.000 (85.000)   [2025-10-28 12:15:09]
  Epoch: [023][100/500]   Time 0.062 (0.292)   Data 0.001 (0.224)   Loss 2.0682 (2.0190)   Prec@1 39.000 (44.406)   Prec@5 89.000 (88.990)   [2025-10-28 12:15:16]
  Epoch: [023][200/500]   Time 0.067 (0.180)   Data 0.000 (0.113)   Loss 2.0102 (2.0152)   Prec@1 45.000 (44.766)   Prec@5 86.000 (88.940)   [2025-10-28 12:15:22]
  Epoch: [023][300/500]   Time 0.063 (0.142)   Data 0.001 (0.075)   Loss 1.9722 (2.0161)   Prec@1 50.000 (44.528)   Prec@5 89.000 (88.927)   [2025-10-28 12:15:29]
  Epoch: [023][400/500]   Time 0.072 (0.123)   Data 0.000 (0.057)   Loss 2.0507 (2.0160)   Prec@1 42.000 (44.601)   Prec@5 87.000 (88.988)   [2025-10-28 12:15:36]
  **Train** Prec@1 44.600 Prec@5 88.980 Error@1 55.400
  **Test** Prec@1 43.660 Prec@5 88.930 Error@1 56.340

==>>[2025-10-28 12:16:05] [Epoch=024/040] [Need: 00:21:45] [LR=0.0100] [Best : Accuracy=47.91, Error=52.09]
  Epoch: [024][000/500]   Time 22.537 (22.537)   Data 22.295 (22.295)   Loss 1.9956 (1.9956)   Prec@1 48.000 (48.000)   Prec@5 88.000 (88.000)   [2025-10-28 12:16:28]
  Epoch: [024][100/500]   Time 0.066 (0.288)   Data 0.000 (0.221)   Loss 1.9826 (2.0053)   Prec@1 48.000 (45.812)   Prec@5 93.000 (89.228)   [2025-10-28 12:16:34]
  Epoch: [024][200/500]   Time 0.066 (0.177)   Data 0.000 (0.111)   Loss 2.0236 (2.0119)   Prec@1 44.000 (45.010)   Prec@5 88.000 (89.040)   [2025-10-28 12:16:41]
  Epoch: [024][300/500]   Time 0.068 (0.141)   Data 0.000 (0.074)   Loss 1.9490 (2.0153)   Prec@1 54.000 (44.628)   Prec@5 90.000 (88.864)   [2025-10-28 12:16:48]
  Epoch: [024][400/500]   Time 0.068 (0.122)   Data 0.000 (0.056)   Loss 1.9260 (2.0141)   Prec@1 55.000 (44.788)   Prec@5 91.000 (88.938)   [2025-10-28 12:16:54]
  **Train** Prec@1 44.802 Prec@5 88.880 Error@1 55.198
  **Test** Prec@1 42.840 Prec@5 88.650 Error@1 57.160

==>>[2025-10-28 12:17:24] [Epoch=025/040] [Need: 00:20:22] [LR=0.0010] [Best : Accuracy=47.91, Error=52.09]
  Epoch: [025][000/500]   Time 22.726 (22.726)   Data 22.475 (22.475)   Loss 2.0128 (2.0128)   Prec@1 44.000 (44.000)   Prec@5 88.000 (88.000)   [2025-10-28 12:17:47]
  Epoch: [025][100/500]   Time 0.066 (0.290)   Data 0.000 (0.223)   Loss 1.9409 (1.9950)   Prec@1 53.000 (46.475)   Prec@5 88.000 (89.525)   [2025-10-28 12:17:53]
  Epoch: [025][200/500]   Time 0.065 (0.179)   Data 0.000 (0.112)   Loss 2.0301 (1.9896)   Prec@1 44.000 (47.199)   Prec@5 85.000 (89.682)   [2025-10-28 12:18:00]
  Epoch: [025][300/500]   Time 0.067 (0.142)   Data 0.001 (0.075)   Loss 1.9564 (1.9855)   Prec@1 50.000 (47.698)   Prec@5 88.000 (90.053)   [2025-10-28 12:18:07]
  Epoch: [025][400/500]   Time 0.068 (0.123)   Data 0.001 (0.056)   Loss 1.9881 (1.9834)   Prec@1 47.000 (47.860)   Prec@5 93.000 (90.132)   [2025-10-28 12:18:14]
  **Train** Prec@1 48.018 Prec@5 90.250 Error@1 51.982
  **Test** Prec@1 49.400 Prec@5 91.570 Error@1 50.600
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 12:18:43] [Epoch=026/040] [Need: 00:18:59] [LR=0.0010] [Best : Accuracy=49.40, Error=50.60]
  Epoch: [026][000/500]   Time 22.466 (22.466)   Data 22.182 (22.182)   Loss 1.9158 (1.9158)   Prec@1 55.000 (55.000)   Prec@5 90.000 (90.000)   [2025-10-28 12:19:06]
  Epoch: [026][100/500]   Time 0.065 (0.287)   Data 0.000 (0.220)   Loss 1.9555 (1.9700)   Prec@1 52.000 (49.317)   Prec@5 93.000 (90.802)   [2025-10-28 12:19:12]
  Epoch: [026][200/500]   Time 0.066 (0.177)   Data 0.000 (0.111)   Loss 1.9945 (1.9719)   Prec@1 47.000 (49.085)   Prec@5 93.000 (90.582)   [2025-10-28 12:19:19]
  Epoch: [026][300/500]   Time 0.063 (0.140)   Data 0.000 (0.074)   Loss 2.0105 (1.9738)   Prec@1 47.000 (48.854)   Prec@5 91.000 (90.635)   [2025-10-28 12:19:25]
  Epoch: [026][400/500]   Time 0.069 (0.122)   Data 0.001 (0.056)   Loss 1.9824 (1.9740)   Prec@1 46.000 (48.893)   Prec@5 93.000 (90.621)   [2025-10-28 12:19:32]
  **Train** Prec@1 48.910 Prec@5 90.658 Error@1 51.090
  **Test** Prec@1 49.540 Prec@5 91.760 Error@1 50.460
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 12:20:02] [Epoch=027/040] [Need: 00:17:36] [LR=0.0010] [Best : Accuracy=49.54, Error=50.46]
  Epoch: [027][000/500]   Time 23.480 (23.480)   Data 23.261 (23.261)   Loss 1.9482 (1.9482)   Prec@1 51.000 (51.000)   Prec@5 95.000 (95.000)   [2025-10-28 12:20:25]
  Epoch: [027][100/500]   Time 0.066 (0.297)   Data 0.001 (0.231)   Loss 2.0429 (1.9698)   Prec@1 41.000 (49.356)   Prec@5 88.000 (90.644)   [2025-10-28 12:20:32]
  Epoch: [027][200/500]   Time 0.065 (0.182)   Data 0.000 (0.116)   Loss 1.9051 (1.9671)   Prec@1 56.000 (49.706)   Prec@5 92.000 (90.826)   [2025-10-28 12:20:39]
  Epoch: [027][300/500]   Time 0.068 (0.144)   Data 0.000 (0.078)   Loss 1.9807 (1.9684)   Prec@1 49.000 (49.535)   Prec@5 90.000 (90.860)   [2025-10-28 12:20:45]
  Epoch: [027][400/500]   Time 0.067 (0.125)   Data 0.000 (0.058)   Loss 1.9439 (1.9686)   Prec@1 52.000 (49.516)   Prec@5 93.000 (90.855)   [2025-10-28 12:20:52]
  **Train** Prec@1 49.640 Prec@5 90.924 Error@1 50.360
  **Test** Prec@1 48.920 Prec@5 91.030 Error@1 51.080

==>>[2025-10-28 12:21:22] [Epoch=028/040] [Need: 00:16:15] [LR=0.0010] [Best : Accuracy=49.54, Error=50.46]
  Epoch: [028][000/500]   Time 22.983 (22.983)   Data 22.778 (22.778)   Loss 2.0452 (2.0452)   Prec@1 39.000 (39.000)   Prec@5 86.000 (86.000)   [2025-10-28 12:21:45]
  Epoch: [028][100/500]   Time 0.067 (0.293)   Data 0.000 (0.226)   Loss 2.0063 (1.9681)   Prec@1 45.000 (49.040)   Prec@5 89.000 (91.109)   [2025-10-28 12:21:51]
  Epoch: [028][200/500]   Time 0.065 (0.180)   Data 0.000 (0.114)   Loss 2.0453 (1.9712)   Prec@1 40.000 (48.851)   Prec@5 83.000 (91.035)   [2025-10-28 12:21:58]
  Epoch: [028][300/500]   Time 0.064 (0.143)   Data 0.000 (0.076)   Loss 1.9488 (1.9670)   Prec@1 51.000 (49.435)   Prec@5 95.000 (91.020)   [2025-10-28 12:22:05]
  Epoch: [028][400/500]   Time 0.069 (0.124)   Data 0.001 (0.057)   Loss 1.9430 (1.9651)   Prec@1 54.000 (49.686)   Prec@5 96.000 (91.202)   [2025-10-28 12:22:11]
  **Train** Prec@1 49.694 Prec@5 91.258 Error@1 50.306
  **Test** Prec@1 49.090 Prec@5 91.210 Error@1 50.910

==>>[2025-10-28 12:22:41] [Epoch=029/040] [Need: 00:14:52] [LR=0.0010] [Best : Accuracy=49.54, Error=50.46]
  Epoch: [029][000/500]   Time 22.910 (22.910)   Data 22.602 (22.602)   Loss 1.9987 (1.9987)   Prec@1 46.000 (46.000)   Prec@5 88.000 (88.000)   [2025-10-28 12:23:04]
  Epoch: [029][100/500]   Time 0.054 (0.291)   Data 0.000 (0.224)   Loss 1.9623 (1.9573)   Prec@1 51.000 (50.624)   Prec@5 92.000 (91.119)   [2025-10-28 12:23:10]
  Epoch: [029][200/500]   Time 0.066 (0.179)   Data 0.001 (0.113)   Loss 2.0107 (1.9580)   Prec@1 45.000 (50.622)   Prec@5 92.000 (91.030)   [2025-10-28 12:23:17]
  Epoch: [029][300/500]   Time 0.067 (0.142)   Data 0.000 (0.076)   Loss 1.8422 (1.9620)   Prec@1 65.000 (50.083)   Prec@5 96.000 (91.040)   [2025-10-28 12:23:24]
  Epoch: [029][400/500]   Time 0.065 (0.123)   Data 0.000 (0.057)   Loss 1.9969 (1.9610)   Prec@1 47.000 (50.232)   Prec@5 88.000 (91.075)   [2025-10-28 12:23:30]
  **Train** Prec@1 50.200 Prec@5 91.152 Error@1 49.800
  **Test** Prec@1 47.910 Prec@5 90.570 Error@1 52.090

==>>[2025-10-28 12:24:00] [Epoch=030/040] [Need: 00:13:31] [LR=0.0010] [Best : Accuracy=49.54, Error=50.46]
  Epoch: [030][000/500]   Time 23.128 (23.128)   Data 22.836 (22.836)   Loss 1.9544 (1.9544)   Prec@1 52.000 (52.000)   Prec@5 88.000 (88.000)   [2025-10-28 12:24:23]
  Epoch: [030][100/500]   Time 0.065 (0.293)   Data 0.001 (0.226)   Loss 1.9523 (1.9602)   Prec@1 49.000 (50.248)   Prec@5 90.000 (90.871)   [2025-10-28 12:24:30]
  Epoch: [030][200/500]   Time 0.069 (0.180)   Data 0.000 (0.114)   Loss 1.9250 (1.9618)   Prec@1 54.000 (49.920)   Prec@5 95.000 (91.025)   [2025-10-28 12:24:36]
  Epoch: [030][300/500]   Time 0.063 (0.143)   Data 0.000 (0.076)   Loss 1.9073 (1.9614)   Prec@1 57.000 (50.093)   Prec@5 92.000 (91.027)   [2025-10-28 12:24:43]
  Epoch: [030][400/500]   Time 0.082 (0.124)   Data 0.000 (0.057)   Loss 1.9089 (1.9588)   Prec@1 54.000 (50.352)   Prec@5 92.000 (91.057)   [2025-10-28 12:24:50]
  **Train** Prec@1 50.330 Prec@5 91.098 Error@1 49.670
  **Test** Prec@1 48.210 Prec@5 90.320 Error@1 51.790

==>>[2025-10-28 12:25:19] [Epoch=031/040] [Need: 00:12:09] [LR=0.0010] [Best : Accuracy=49.54, Error=50.46]
  Epoch: [031][000/500]   Time 22.528 (22.528)   Data 22.304 (22.304)   Loss 2.0352 (2.0352)   Prec@1 40.000 (40.000)   Prec@5 97.000 (97.000)   [2025-10-28 12:25:42]
  Epoch: [031][100/500]   Time 0.066 (0.288)   Data 0.000 (0.221)   Loss 1.9197 (1.9606)   Prec@1 55.000 (50.515)   Prec@5 95.000 (91.257)   [2025-10-28 12:25:48]
  Epoch: [031][200/500]   Time 0.067 (0.177)   Data 0.001 (0.111)   Loss 2.0128 (1.9610)   Prec@1 45.000 (50.308)   Prec@5 87.000 (91.104)   [2025-10-28 12:25:55]
  Epoch: [031][300/500]   Time 0.066 (0.141)   Data 0.000 (0.074)   Loss 1.9588 (1.9623)   Prec@1 49.000 (50.013)   Prec@5 90.000 (91.123)   [2025-10-28 12:26:02]
  Epoch: [031][400/500]   Time 0.064 (0.123)   Data 0.001 (0.056)   Loss 1.9321 (1.9612)   Prec@1 52.000 (50.112)   Prec@5 92.000 (91.122)   [2025-10-28 12:26:08]
  **Train** Prec@1 50.274 Prec@5 91.136 Error@1 49.726
  **Test** Prec@1 50.520 Prec@5 91.500 Error@1 49.480
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 12:26:38] [Epoch=032/040] [Need: 00:10:47] [LR=0.0010] [Best : Accuracy=50.52, Error=49.48]
  Epoch: [032][000/500]   Time 22.418 (22.418)   Data 22.113 (22.113)   Loss 2.0217 (2.0217)   Prec@1 41.000 (41.000)   Prec@5 87.000 (87.000)   [2025-10-28 12:27:00]
  Epoch: [032][100/500]   Time 0.067 (0.286)   Data 0.001 (0.219)   Loss 1.9755 (1.9608)   Prec@1 48.000 (50.248)   Prec@5 94.000 (90.673)   [2025-10-28 12:27:06]
  Epoch: [032][200/500]   Time 0.066 (0.177)   Data 0.000 (0.110)   Loss 1.9429 (1.9594)   Prec@1 52.000 (50.289)   Prec@5 89.000 (90.831)   [2025-10-28 12:27:13]
  Epoch: [032][300/500]   Time 0.066 (0.140)   Data 0.001 (0.074)   Loss 1.9264 (1.9564)   Prec@1 55.000 (50.721)   Prec@5 92.000 (91.010)   [2025-10-28 12:27:20]
  Epoch: [032][400/500]   Time 0.069 (0.122)   Data 0.001 (0.055)   Loss 2.0025 (1.9578)   Prec@1 45.000 (50.544)   Prec@5 90.000 (91.022)   [2025-10-28 12:27:27]
  **Train** Prec@1 50.594 Prec@5 91.182 Error@1 49.406
  **Test** Prec@1 48.210 Prec@5 90.520 Error@1 51.790

==>>[2025-10-28 12:27:56] [Epoch=033/040] [Need: 00:09:26] [LR=0.0010] [Best : Accuracy=50.52, Error=49.48]
  Epoch: [033][000/500]   Time 22.665 (22.665)   Data 22.363 (22.363)   Loss 1.9460 (1.9460)   Prec@1 54.000 (54.000)   Prec@5 90.000 (90.000)   [2025-10-28 12:28:19]
  Epoch: [033][100/500]   Time 0.064 (0.289)   Data 0.000 (0.222)   Loss 1.9394 (1.9530)   Prec@1 51.000 (51.208)   Prec@5 93.000 (90.752)   [2025-10-28 12:28:25]
  Epoch: [033][200/500]   Time 0.065 (0.177)   Data 0.000 (0.112)   Loss 1.9403 (1.9560)   Prec@1 53.000 (50.776)   Prec@5 85.000 (90.861)   [2025-10-28 12:28:32]
  Epoch: [033][300/500]   Time 0.066 (0.141)   Data 0.000 (0.075)   Loss 1.9645 (1.9563)   Prec@1 49.000 (50.714)   Prec@5 87.000 (90.870)   [2025-10-28 12:28:39]
  Epoch: [033][400/500]   Time 0.066 (0.122)   Data 0.001 (0.056)   Loss 2.0184 (1.9573)   Prec@1 42.000 (50.599)   Prec@5 93.000 (90.985)   [2025-10-28 12:28:45]
  **Train** Prec@1 50.780 Prec@5 91.086 Error@1 49.220
  **Test** Prec@1 48.270 Prec@5 90.600 Error@1 51.730

==>>[2025-10-28 12:29:15] [Epoch=034/040] [Need: 00:08:04] [LR=0.0010] [Best : Accuracy=50.52, Error=49.48]
  Epoch: [034][000/500]   Time 26.414 (26.414)   Data 26.158 (26.158)   Loss 2.0327 (2.0327)   Prec@1 44.000 (44.000)   Prec@5 90.000 (90.000)   [2025-10-28 12:29:41]
  Epoch: [034][100/500]   Time 0.051 (0.320)   Data 0.000 (0.259)   Loss 1.8741 (1.9513)   Prec@1 61.000 (51.396)   Prec@5 91.000 (91.505)   [2025-10-28 12:29:47]
  Epoch: [034][200/500]   Time 0.050 (0.187)   Data 0.000 (0.130)   Loss 1.9246 (1.9537)   Prec@1 56.000 (51.065)   Prec@5 97.000 (91.478)   [2025-10-28 12:29:52]
  Epoch: [034][300/500]   Time 0.054 (0.143)   Data 0.000 (0.087)   Loss 1.9856 (1.9553)   Prec@1 47.000 (50.860)   Prec@5 87.000 (91.375)   [2025-10-28 12:29:58]
  Epoch: [034][400/500]   Time 0.055 (0.121)   Data 0.000 (0.065)   Loss 2.0008 (1.9544)   Prec@1 45.000 (50.953)   Prec@5 88.000 (91.367)   [2025-10-28 12:30:03]
  **Train** Prec@1 50.976 Prec@5 91.272 Error@1 49.024
  **Test** Prec@1 49.910 Prec@5 91.390 Error@1 50.090

==>>[2025-10-28 12:30:28] [Epoch=035/040] [Need: 00:06:43] [LR=0.0010] [Best : Accuracy=50.52, Error=49.48]
  Epoch: [035][000/500]   Time 17.741 (17.741)   Data 17.454 (17.454)   Loss 2.0296 (2.0296)   Prec@1 41.000 (41.000)   Prec@5 87.000 (87.000)   [2025-10-28 12:30:46]
  Epoch: [035][100/500]   Time 0.054 (0.228)   Data 0.000 (0.173)   Loss 1.9994 (1.9545)   Prec@1 45.000 (50.931)   Prec@5 88.000 (91.099)   [2025-10-28 12:30:51]
  Epoch: [035][200/500]   Time 0.057 (0.142)   Data 0.000 (0.087)   Loss 1.9131 (1.9529)   Prec@1 55.000 (51.010)   Prec@5 93.000 (91.368)   [2025-10-28 12:30:57]
  Epoch: [035][300/500]   Time 0.053 (0.113)   Data 0.000 (0.058)   Loss 1.9134 (1.9521)   Prec@1 56.000 (51.186)   Prec@5 90.000 (91.286)   [2025-10-28 12:31:02]
  Epoch: [035][400/500]   Time 0.052 (0.098)   Data 0.000 (0.044)   Loss 1.9725 (1.9523)   Prec@1 48.000 (51.160)   Prec@5 87.000 (91.389)   [2025-10-28 12:31:08]
  **Train** Prec@1 51.190 Prec@5 91.298 Error@1 48.810
  **Test** Prec@1 50.440 Prec@5 91.650 Error@1 49.560

==>>[2025-10-28 12:31:33] [Epoch=036/040] [Need: 00:05:20] [LR=0.0010] [Best : Accuracy=50.52, Error=49.48]
  Epoch: [036][000/500]   Time 17.740 (17.740)   Data 17.551 (17.551)   Loss 1.8838 (1.8838)   Prec@1 59.000 (59.000)   Prec@5 93.000 (93.000)   [2025-10-28 12:31:51]
  Epoch: [036][100/500]   Time 0.055 (0.228)   Data 0.001 (0.174)   Loss 1.9409 (1.9475)   Prec@1 53.000 (51.535)   Prec@5 91.000 (91.554)   [2025-10-28 12:31:56]
  Epoch: [036][200/500]   Time 0.055 (0.141)   Data 0.000 (0.087)   Loss 1.9482 (1.9525)   Prec@1 51.000 (50.915)   Prec@5 91.000 (91.448)   [2025-10-28 12:32:01]
  Epoch: [036][300/500]   Time 0.059 (0.113)   Data 0.001 (0.058)   Loss 2.0062 (1.9514)   Prec@1 45.000 (51.007)   Prec@5 90.000 (91.382)   [2025-10-28 12:32:07]
  Epoch: [036][400/500]   Time 0.055 (0.099)   Data 0.001 (0.044)   Loss 1.9169 (1.9506)   Prec@1 53.000 (51.100)   Prec@5 94.000 (91.466)   [2025-10-28 12:32:12]
  **Train** Prec@1 51.220 Prec@5 91.412 Error@1 48.780
  **Test** Prec@1 51.180 Prec@5 92.070 Error@1 48.820
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 12:32:38] [Epoch=037/040] [Need: 00:03:59] [LR=0.0010] [Best : Accuracy=51.18, Error=48.82]
  Epoch: [037][000/500]   Time 17.887 (17.887)   Data 17.704 (17.704)   Loss 2.0223 (2.0223)   Prec@1 45.000 (45.000)   Prec@5 85.000 (85.000)   [2025-10-28 12:32:56]
  Epoch: [037][100/500]   Time 0.055 (0.230)   Data 0.000 (0.175)   Loss 1.9010 (1.9543)   Prec@1 59.000 (51.069)   Prec@5 92.000 (91.386)   [2025-10-28 12:33:01]
  Epoch: [037][200/500]   Time 0.052 (0.143)   Data 0.000 (0.088)   Loss 1.9750 (1.9494)   Prec@1 47.000 (51.463)   Prec@5 90.000 (91.587)   [2025-10-28 12:33:07]
  Epoch: [037][300/500]   Time 0.057 (0.113)   Data 0.000 (0.059)   Loss 1.9188 (1.9496)   Prec@1 55.000 (51.355)   Prec@5 92.000 (91.495)   [2025-10-28 12:33:12]
  Epoch: [037][400/500]   Time 0.054 (0.099)   Data 0.000 (0.044)   Loss 1.9709 (1.9490)   Prec@1 47.000 (51.484)   Prec@5 88.000 (91.359)   [2025-10-28 12:33:18]
  **Train** Prec@1 51.422 Prec@5 91.272 Error@1 48.578
  **Test** Prec@1 49.390 Prec@5 91.050 Error@1 50.610

==>>[2025-10-28 12:33:43] [Epoch=038/040] [Need: 00:02:38] [LR=0.0010] [Best : Accuracy=51.18, Error=48.82]
  Epoch: [038][000/500]   Time 18.466 (18.466)   Data 18.179 (18.179)   Loss 2.0242 (2.0242)   Prec@1 42.000 (42.000)   Prec@5 93.000 (93.000)   [2025-10-28 12:34:02]
  Epoch: [038][100/500]   Time 0.055 (0.236)   Data 0.000 (0.180)   Loss 1.9198 (1.9513)   Prec@1 54.000 (51.099)   Prec@5 90.000 (91.257)   [2025-10-28 12:34:07]
  Epoch: [038][200/500]   Time 0.057 (0.146)   Data 0.000 (0.091)   Loss 1.9537 (1.9491)   Prec@1 51.000 (51.502)   Prec@5 95.000 (91.333)   [2025-10-28 12:34:13]
  Epoch: [038][300/500]   Time 0.052 (0.116)   Data 0.000 (0.061)   Loss 1.8941 (1.9490)   Prec@1 59.000 (51.508)   Prec@5 92.000 (91.369)   [2025-10-28 12:34:18]
  Epoch: [038][400/500]   Time 0.055 (0.101)   Data 0.000 (0.046)   Loss 1.9953 (1.9505)   Prec@1 44.000 (51.334)   Prec@5 91.000 (91.364)   [2025-10-28 12:34:24]
  **Train** Prec@1 51.400 Prec@5 91.382 Error@1 48.600
  **Test** Prec@1 44.910 Prec@5 87.650 Error@1 55.090

==>>[2025-10-28 12:34:50] [Epoch=039/040] [Need: 00:01:19] [LR=0.0010] [Best : Accuracy=51.18, Error=48.82]
  Epoch: [039][000/500]   Time 17.796 (17.796)   Data 17.518 (17.518)   Loss 1.9475 (1.9475)   Prec@1 54.000 (54.000)   Prec@5 93.000 (93.000)   [2025-10-28 12:35:08]
  Epoch: [039][100/500]   Time 0.053 (0.230)   Data 0.000 (0.174)   Loss 1.9152 (1.9475)   Prec@1 55.000 (51.594)   Prec@5 93.000 (91.436)   [2025-10-28 12:35:13]
  Epoch: [039][200/500]   Time 0.057 (0.143)   Data 0.001 (0.087)   Loss 1.9611 (1.9445)   Prec@1 48.000 (51.995)   Prec@5 87.000 (91.473)   [2025-10-28 12:35:19]
  Epoch: [039][300/500]   Time 0.053 (0.114)   Data 0.000 (0.058)   Loss 1.9391 (1.9451)   Prec@1 52.000 (51.874)   Prec@5 93.000 (91.385)   [2025-10-28 12:35:24]
  Epoch: [039][400/500]   Time 0.056 (0.099)   Data 0.000 (0.044)   Loss 1.8765 (1.9459)   Prec@1 59.000 (51.751)   Prec@5 92.000 (91.469)   [2025-10-28 12:35:30]
  **Train** Prec@1 51.692 Prec@5 91.526 Error@1 48.308
  **Test** Prec@1 49.460 Prec@5 90.640 Error@1 50.540
