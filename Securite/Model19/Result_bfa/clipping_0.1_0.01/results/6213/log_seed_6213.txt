save path : ./save/resnet9_quan/clipping_0.1_0.01/results/6213
{'data_path': './dataset', 'arch': 'resnet9_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': False, 'clipping_coeff': 0.1, 'learning_rate': 0.01, 'manualSeed': 6213, 'save_path': './save/resnet9_quan/clipping_0.1_0.01/results/6213', 'enable_bfa': True, 'resume': './save/resnet9_quan/clipping_0.1_0.01/model_best.pth.tar', 'quan_bitwidth': None, 'reset_weight': True, 'evaluate': True, 'n_iter': 30, 'fine_tune': True, 'model_only': False, 'random_bfa': False, 'use_cuda': True}
Random Seed: 6213
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'resnet9_quan'
=> network :
 Sequential(
  (0): Sequential(
    (0): quan_Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (1): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (2): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (4): Dropout2d(p=0.2, inplace=False)
  (5): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (skip): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (6): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Dropout2d(p=0.3, inplace=False)
  (9): Sequential(
    (0): quan_Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (10): Sequential(
    (0): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (12): Dropout2d(p=0.3, inplace=False)
  (13): Sequential(
    (0): quan_Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (14): AdaptiveAvgPool2d(output_size=(1, 1))
  (15): Flatten()
  (16): quan_Linear(in_features=64, out_features=128, bias=False)
  (17): ReLU(inplace=True)
  (18): Dropout(p=0.35, inplace=False)
  (19): quan_Linear(in_features=128, out_features=10, bias=False)
  (20): Softmax(dim=1)
)
=> loading checkpoint './save/resnet9_quan/clipping_0.1_0.01/model_best.pth.tar'
=> loaded checkpoint './save/resnet9_quan/clipping_0.1_0.01/model_best.pth.tar' (epoch 0)
  **Test** Prec@1 51.180 Prec@5 92.070 Error@1 48.820
k_top=100
Attack_sample=100
************** ATTACK iteration *****************
Iteration: [001/030]   Attack Time 0.930 (0.930)  [2025-10-29 12:31:29]
loss before attack: 1.6177
loss after attack: 1.8347
bit flips: 1
hamming_dist: 1
  **Test** Prec@1 40.020 Prec@5 85.780 Error@1 59.980
iteration Time 18.878 (18.878)
************** ATTACK iteration *****************
Iteration: [002/030]   Attack Time 0.312 (0.621)  [2025-10-29 12:31:48]
loss before attack: 1.8347
loss after attack: 2.0365
bit flips: 2
hamming_dist: 2
  **Test** Prec@1 28.850 Prec@5 77.400 Error@1 71.150
iteration Time 18.453 (18.666)
************** ATTACK iteration *****************
Iteration: [003/030]   Attack Time 0.253 (0.498)  [2025-10-29 12:32:07]
loss before attack: 2.0365
loss after attack: 2.1481
bit flips: 3
hamming_dist: 3
  **Test** Prec@1 22.900 Prec@5 70.800 Error@1 77.100
iteration Time 18.419 (18.584)
************** ATTACK iteration *****************
Iteration: [004/030]   Attack Time 0.254 (0.437)  [2025-10-29 12:32:26]
loss before attack: 2.1481
loss after attack: 2.1741
bit flips: 4
hamming_dist: 4
  **Test** Prec@1 20.220 Prec@5 67.380 Error@1 79.780
iteration Time 18.870 (18.655)
************** ATTACK iteration *****************
Iteration: [005/030]   Attack Time 0.259 (0.401)  [2025-10-29 12:32:45]
loss before attack: 2.1741
loss after attack: 2.1887
bit flips: 5
hamming_dist: 5
  **Test** Prec@1 16.140 Prec@5 61.680 Error@1 83.860
iteration Time 18.894 (18.703)
************** ATTACK iteration *****************
Iteration: [006/030]   Attack Time 0.247 (0.376)  [2025-10-29 12:33:04]
loss before attack: 2.1887
loss after attack: 2.2208
bit flips: 6
hamming_dist: 6
  **Test** Prec@1 13.770 Prec@5 57.740 Error@1 86.230
iteration Time 18.560 (18.679)
************** ATTACK iteration *****************
Iteration: [007/030]   Attack Time 0.254 (0.358)  [2025-10-29 12:33:23]
loss before attack: 2.2208
loss after attack: 2.2372
bit flips: 7
hamming_dist: 7
  **Test** Prec@1 12.730 Prec@5 56.740 Error@1 87.270
iteration Time 19.080 (18.736)
************** ATTACK iteration *****************
Iteration: [008/030]   Attack Time 0.249 (0.345)  [2025-10-29 12:33:42]
loss before attack: 2.2372
loss after attack: 2.2395
bit flips: 8
hamming_dist: 8
  **Test** Prec@1 11.850 Prec@5 54.830 Error@1 88.150
iteration Time 18.748 (18.738)
************** ATTACK iteration *****************
Iteration: [009/030]   Attack Time 0.250 (0.334)  [2025-10-29 12:34:01]
loss before attack: 2.2395
loss after attack: 2.2403
bit flips: 9
hamming_dist: 9
  **Test** Prec@1 11.750 Prec@5 54.510 Error@1 88.250
iteration Time 18.720 (18.736)
************** ATTACK iteration *****************
Iteration: [010/030]   Attack Time 0.253 (0.326)  [2025-10-29 12:34:20]
loss before attack: 2.2403
loss after attack: 2.2406
bit flips: 10
hamming_dist: 10
  **Test** Prec@1 11.580 Prec@5 54.090 Error@1 88.420
iteration Time 18.310 (18.693)
************** ATTACK iteration *****************
Iteration: [011/030]   Attack Time 0.208 (0.315)  [2025-10-29 12:34:39]
loss before attack: 2.2406
loss after attack: 2.2406
bit flips: 11
hamming_dist: 11
  **Test** Prec@1 11.680 Prec@5 54.180 Error@1 88.320
iteration Time 18.771 (18.700)
************** ATTACK iteration *****************
Iteration: [012/030]   Attack Time 0.220 (0.307)  [2025-10-29 12:34:58]
loss before attack: 2.2406
loss after attack: 2.2407
bit flips: 12
hamming_dist: 12
  **Test** Prec@1 11.820 Prec@5 54.490 Error@1 88.180
iteration Time 18.478 (18.682)
************** ATTACK iteration *****************
Iteration: [013/030]   Attack Time 0.204 (0.299)  [2025-10-29 12:35:16]
loss before attack: 2.2407
loss after attack: 2.2407
bit flips: 13
hamming_dist: 13
  **Test** Prec@1 11.830 Prec@5 54.580 Error@1 88.170
iteration Time 18.006 (18.630)
************** ATTACK iteration *****************
Iteration: [014/030]   Attack Time 0.204 (0.293)  [2025-10-29 12:35:35]
loss before attack: 2.2407
loss after attack: 2.2408
bit flips: 14
hamming_dist: 14
  **Test** Prec@1 11.500 Prec@5 53.950 Error@1 88.500
iteration Time 18.657 (18.632)
************** ATTACK iteration *****************
Iteration: [015/030]   Attack Time 0.211 (0.287)  [2025-10-29 12:35:53]
loss before attack: 2.2408
loss after attack: 2.2408
bit flips: 15
hamming_dist: 15
  **Test** Prec@1 11.450 Prec@5 53.870 Error@1 88.550
iteration Time 18.239 (18.606)
************** ATTACK iteration *****************
Iteration: [016/030]   Attack Time 0.207 (0.282)  [2025-10-29 12:36:12]
loss before attack: 2.2408
loss after attack: 2.2408
bit flips: 16
hamming_dist: 16
  **Test** Prec@1 11.450 Prec@5 53.890 Error@1 88.550
iteration Time 18.319 (18.588)
************** ATTACK iteration *****************
Iteration: [017/030]   Attack Time 0.207 (0.278)  [2025-10-29 12:36:30]
loss before attack: 2.2408
loss after attack: 2.2409
bit flips: 17
hamming_dist: 17
  **Test** Prec@1 11.420 Prec@5 53.980 Error@1 88.580
iteration Time 18.191 (18.564)
************** ATTACK iteration *****************
Iteration: [018/030]   Attack Time 0.205 (0.274)  [2025-10-29 12:36:49]
loss before attack: 2.2409
loss after attack: 2.2409
bit flips: 18
hamming_dist: 18
  **Test** Prec@1 11.430 Prec@5 54.090 Error@1 88.570
iteration Time 18.184 (18.543)
************** ATTACK iteration *****************
Iteration: [019/030]   Attack Time 0.211 (0.270)  [2025-10-29 12:37:07]
loss before attack: 2.2409
loss after attack: 2.2409
bit flips: 19
hamming_dist: 19
  **Test** Prec@1 11.180 Prec@5 53.490 Error@1 88.820
iteration Time 18.055 (18.518)
************** ATTACK iteration *****************
Iteration: [020/030]   Attack Time 0.142 (0.264)  [2025-10-29 12:37:25]
loss before attack: 2.2409
loss after attack: 2.2409
bit flips: 20
hamming_dist: 20
  **Test** Prec@1 11.190 Prec@5 53.560 Error@1 88.810
iteration Time 17.873 (18.485)
************** ATTACK iteration *****************
Iteration: [021/030]   Attack Time 0.144 (0.258)  [2025-10-29 12:37:43]
loss before attack: 2.2409
loss after attack: 2.2410
bit flips: 21
hamming_dist: 21
  **Test** Prec@1 11.210 Prec@5 53.640 Error@1 88.790
iteration Time 18.044 (18.464)
************** ATTACK iteration *****************
Iteration: [022/030]   Attack Time 0.149 (0.253)  [2025-10-29 12:38:02]
loss before attack: 2.2410
loss after attack: 2.2410
bit flips: 22
hamming_dist: 22
  **Test** Prec@1 11.190 Prec@5 53.590 Error@1 88.810
iteration Time 18.061 (18.446)
************** ATTACK iteration *****************
Iteration: [023/030]   Attack Time 1.407 (0.303)  [2025-10-29 12:38:21]
loss before attack: 2.2410
loss after attack: 2.2410
bit flips: 23
hamming_dist: 23
  **Test** Prec@1 11.180 Prec@5 53.570 Error@1 88.820
iteration Time 22.719 (18.632)
************** ATTACK iteration *****************
Iteration: [024/030]   Attack Time 0.183 (0.298)  [2025-10-29 12:38:44]
loss before attack: 2.2410
loss after attack: 2.2410
bit flips: 24
hamming_dist: 24
  **Test** Prec@1 11.180 Prec@5 53.570 Error@1 88.820
iteration Time 20.059 (18.691)
************** ATTACK iteration *****************
Iteration: [025/030]   Attack Time 0.178 (0.294)  [2025-10-29 12:39:04]
loss before attack: 2.2410
loss after attack: 2.2411
bit flips: 25
hamming_dist: 25
  **Test** Prec@1 11.160 Prec@5 53.530 Error@1 88.840
iteration Time 23.369 (18.878)
************** ATTACK iteration *****************
Iteration: [026/030]   Attack Time 0.244 (0.292)  [2025-10-29 12:39:28]
loss before attack: 2.2411
loss after attack: 2.2414
bit flips: 26
hamming_dist: 26
  **Test** Prec@1 11.150 Prec@5 53.560 Error@1 88.850
iteration Time 33.693 (19.448)
************** ATTACK iteration *****************
Iteration: [027/030]   Attack Time 0.359 (0.294)  [2025-10-29 12:40:02]
loss before attack: 2.2414
loss after attack: 2.2464
bit flips: 27
hamming_dist: 27
  **Test** Prec@1 10.670 Prec@5 52.750 Error@1 89.330
iteration Time 26.194 (19.698)
