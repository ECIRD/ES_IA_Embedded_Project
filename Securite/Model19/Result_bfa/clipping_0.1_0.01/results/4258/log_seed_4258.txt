save path : ./save/resnet9_quan/clipping_0.1_0.01/results/4258
{'data_path': './dataset', 'arch': 'resnet9_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': False, 'clipping_coeff': 0.1, 'learning_rate': 0.01, 'manualSeed': 4258, 'save_path': './save/resnet9_quan/clipping_0.1_0.01/results/4258', 'enable_bfa': True, 'resume': './save/resnet9_quan/clipping_0.1_0.01/model_best.pth.tar', 'quan_bitwidth': None, 'reset_weight': True, 'evaluate': True, 'n_iter': 30, 'fine_tune': True, 'model_only': False, 'random_bfa': False, 'use_cuda': True}
Random Seed: 4258
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'resnet9_quan'
=> network :
 Sequential(
  (0): Sequential(
    (0): quan_Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (1): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (2): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (4): Dropout2d(p=0.2, inplace=False)
  (5): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (skip): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (6): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Dropout2d(p=0.3, inplace=False)
  (9): Sequential(
    (0): quan_Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (10): Sequential(
    (0): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (12): Dropout2d(p=0.3, inplace=False)
  (13): Sequential(
    (0): quan_Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (14): AdaptiveAvgPool2d(output_size=(1, 1))
  (15): Flatten()
  (16): quan_Linear(in_features=64, out_features=128, bias=False)
  (17): ReLU(inplace=True)
  (18): Dropout(p=0.35, inplace=False)
  (19): quan_Linear(in_features=128, out_features=10, bias=False)
  (20): Softmax(dim=1)
)
=> loading checkpoint './save/resnet9_quan/clipping_0.1_0.01/model_best.pth.tar'
=> loaded checkpoint './save/resnet9_quan/clipping_0.1_0.01/model_best.pth.tar' (epoch 0)
  **Test** Prec@1 51.180 Prec@5 92.070 Error@1 48.820
k_top=100
Attack_sample=100
************** ATTACK iteration *****************
Iteration: [001/030]   Attack Time 0.526 (0.526)  [2025-10-29 12:15:07]
loss before attack: 1.5860
loss after attack: 1.6981
bit flips: 1
hamming_dist: 1
  **Test** Prec@1 47.180 Prec@5 89.740 Error@1 52.820
iteration Time 18.648 (18.648)
************** ATTACK iteration *****************
Iteration: [002/030]   Attack Time 0.252 (0.389)  [2025-10-29 12:15:26]
loss before attack: 1.6981
loss after attack: 1.8317
bit flips: 2
hamming_dist: 2
  **Test** Prec@1 46.170 Prec@5 89.330 Error@1 53.830
iteration Time 18.762 (18.705)
************** ATTACK iteration *****************
Iteration: [003/030]   Attack Time 0.254 (0.344)  [2025-10-29 12:15:45]
loss before attack: 1.8317
loss after attack: 1.9132
bit flips: 3
hamming_dist: 3
  **Test** Prec@1 39.870 Prec@5 84.600 Error@1 60.130
iteration Time 18.476 (18.629)
************** ATTACK iteration *****************
Iteration: [004/030]   Attack Time 0.255 (0.321)  [2025-10-29 12:16:04]
loss before attack: 1.9132
loss after attack: 1.9842
bit flips: 4
hamming_dist: 4
  **Test** Prec@1 33.990 Prec@5 79.410 Error@1 66.010
iteration Time 18.320 (18.552)
************** ATTACK iteration *****************
Iteration: [005/030]   Attack Time 0.251 (0.307)  [2025-10-29 12:16:22]
loss before attack: 1.9842
loss after attack: 2.0291
bit flips: 5
hamming_dist: 5
  **Test** Prec@1 28.570 Prec@5 74.250 Error@1 71.430
iteration Time 18.229 (18.487)
************** ATTACK iteration *****************
Iteration: [006/030]   Attack Time 0.255 (0.299)  [2025-10-29 12:16:41]
loss before attack: 2.0291
loss after attack: 2.0585
bit flips: 6
hamming_dist: 6
  **Test** Prec@1 25.480 Prec@5 70.820 Error@1 74.520
iteration Time 18.500 (18.489)
************** ATTACK iteration *****************
Iteration: [007/030]   Attack Time 0.257 (0.293)  [2025-10-29 12:16:59]
loss before attack: 2.0585
loss after attack: 2.0865
bit flips: 7
hamming_dist: 7
  **Test** Prec@1 23.090 Prec@5 67.840 Error@1 76.910
iteration Time 18.328 (18.466)
************** ATTACK iteration *****************
Iteration: [008/030]   Attack Time 0.255 (0.288)  [2025-10-29 12:17:18]
loss before attack: 2.0865
loss after attack: 2.1162
bit flips: 8
hamming_dist: 8
  **Test** Prec@1 22.000 Prec@5 66.350 Error@1 78.000
iteration Time 18.586 (18.481)
************** ATTACK iteration *****************
Iteration: [009/030]   Attack Time 0.251 (0.284)  [2025-10-29 12:17:37]
loss before attack: 2.1162
loss after attack: 2.1611
bit flips: 9
hamming_dist: 9
  **Test** Prec@1 18.600 Prec@5 63.170 Error@1 81.400
iteration Time 18.530 (18.487)
************** ATTACK iteration *****************
Iteration: [010/030]   Attack Time 0.253 (0.281)  [2025-10-29 12:17:56]
loss before attack: 2.1611
loss after attack: 2.1836
bit flips: 10
hamming_dist: 10
  **Test** Prec@1 16.010 Prec@5 59.230 Error@1 83.990
iteration Time 18.613 (18.499)
************** ATTACK iteration *****************
Iteration: [011/030]   Attack Time 0.250 (0.278)  [2025-10-29 12:18:15]
loss before attack: 2.1836
loss after attack: 2.2004
bit flips: 11
hamming_dist: 11
  **Test** Prec@1 14.630 Prec@5 56.650 Error@1 85.370
iteration Time 18.441 (18.494)
************** ATTACK iteration *****************
Iteration: [012/030]   Attack Time 0.253 (0.276)  [2025-10-29 12:18:33]
loss before attack: 2.2004
loss after attack: 2.2286
bit flips: 12
hamming_dist: 12
  **Test** Prec@1 13.650 Prec@5 55.680 Error@1 86.350
iteration Time 18.224 (18.471)
************** ATTACK iteration *****************
Iteration: [013/030]   Attack Time 0.197 (0.270)  [2025-10-29 12:18:52]
loss before attack: 2.2286
loss after attack: 2.2358
bit flips: 13
hamming_dist: 13
  **Test** Prec@1 12.670 Prec@5 54.640 Error@1 87.330
iteration Time 18.208 (18.451)
************** ATTACK iteration *****************
Iteration: [014/030]   Attack Time 0.205 (0.265)  [2025-10-29 12:19:10]
loss before attack: 2.2358
loss after attack: 2.2436
bit flips: 14
hamming_dist: 14
  **Test** Prec@1 12.190 Prec@5 53.930 Error@1 87.810
iteration Time 18.162 (18.431)
************** ATTACK iteration *****************
Iteration: [015/030]   Attack Time 0.204 (0.261)  [2025-10-29 12:19:28]
loss before attack: 2.2436
loss after attack: 2.2494
bit flips: 15
hamming_dist: 15
  **Test** Prec@1 11.660 Prec@5 53.330 Error@1 88.340
iteration Time 27.698 (19.048)
************** ATTACK iteration *****************
Iteration: [016/030]   Attack Time 0.304 (0.264)  [2025-10-29 12:19:56]
loss before attack: 2.2494
loss after attack: 2.2571
bit flips: 16
hamming_dist: 16
  **Test** Prec@1 11.390 Prec@5 53.200 Error@1 88.610
iteration Time 27.047 (19.548)
************** ATTACK iteration *****************
Iteration: [017/030]   Attack Time 0.198 (0.260)  [2025-10-29 12:20:24]
loss before attack: 2.2571
loss after attack: 2.2581
bit flips: 17
hamming_dist: 17
  **Test** Prec@1 11.180 Prec@5 52.800 Error@1 88.820
iteration Time 18.368 (19.479)
************** ATTACK iteration *****************
Iteration: [018/030]   Attack Time 0.171 (0.255)  [2025-10-29 12:20:42]
loss before attack: 2.2581
loss after attack: 2.2588
bit flips: 18
hamming_dist: 18
  **Test** Prec@1 10.940 Prec@5 52.460 Error@1 89.060
iteration Time 18.062 (19.400)
