save path : ./save/resnet9_quan/nominal_0.01
{'data_path': './dataset', 'arch': 'resnet9_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': False, 'clipping_coeff': 0.0, 'learning_rate': 0.01, 'manualSeed': 7430, 'save_path': './save/resnet9_quan/nominal_0.01', 'enable_bfa': False, 'resume': '', 'quan_bitwidth': None, 'reset_weight': False, 'evaluate': False, 'n_iter': 30, 'model_only': False, 'random_bfa': False, 'use_cuda': True}
Random Seed: 7430
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'resnet9_quan'
=> network :
 Sequential(
  (0): Sequential(
    (0): quan_Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (1): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (2): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (4): Dropout2d(p=0.2, inplace=False)
  (5): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (skip): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (6): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Dropout2d(p=0.3, inplace=False)
  (9): Sequential(
    (0): quan_Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (10): Sequential(
    (0): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (12): Dropout2d(p=0.3, inplace=False)
  (13): Sequential(
    (0): quan_Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (14): AdaptiveAvgPool2d(output_size=(1, 1))
  (15): Flatten()
  (16): quan_Linear(in_features=64, out_features=128, bias=False)
  (17): ReLU(inplace=True)
  (18): Dropout(p=0.35, inplace=False)
  (19): quan_Linear(in_features=128, out_features=10, bias=False)
  (20): Softmax(dim=1)
)
=> do not use any checkpoint for resnet9_quan model

==>>[2025-10-26 11:20:02] [Epoch=000/040] [Need: 00:00:00] [LR=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/500]   Time 19.510 (19.510)   Data 18.231 (18.231)   Loss 2.3016 (2.3016)   Prec@1 9.000 (9.000)   Prec@5 52.000 (52.000)   [2025-10-26 11:20:21]
  Epoch: [000][100/500]   Time 0.051 (0.244)   Data 0.000 (0.181)   Loss 2.3022 (2.3018)   Prec@1 7.000 (10.743)   Prec@5 50.000 (51.584)   [2025-10-26 11:20:26]
  Epoch: [000][200/500]   Time 0.052 (0.148)   Data 0.000 (0.091)   Loss 2.3038 (2.2997)   Prec@1 8.000 (10.861)   Prec@5 59.000 (52.552)   [2025-10-26 11:20:31]
  Epoch: [000][300/500]   Time 0.047 (0.116)   Data 0.000 (0.061)   Loss 2.2951 (2.2948)   Prec@1 13.000 (11.060)   Prec@5 61.000 (53.548)   [2025-10-26 11:20:36]
  Epoch: [000][400/500]   Time 0.051 (0.100)   Data 0.000 (0.046)   Loss 2.2954 (2.2907)   Prec@1 12.000 (11.950)   Prec@5 60.000 (54.968)   [2025-10-26 11:20:42]
  **Train** Prec@1 12.844 Prec@5 56.704 Error@1 87.156
  **Test** Prec@1 17.730 Prec@5 67.810 Error@1 82.270
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 11:21:06] [Epoch=001/040] [Need: 00:42:03] [LR=0.0100] [Best : Accuracy=17.73, Error=82.27]
  Epoch: [001][000/500]   Time 17.662 (17.662)   Data 17.382 (17.382)   Loss 2.2835 (2.2835)   Prec@1 17.000 (17.000)   Prec@5 69.000 (69.000)   [2025-10-26 11:21:24]
  Epoch: [001][100/500]   Time 0.053 (0.226)   Data 0.000 (0.172)   Loss 2.2592 (2.2650)   Prec@1 19.000 (17.158)   Prec@5 67.000 (67.416)   [2025-10-26 11:21:29]
  Epoch: [001][200/500]   Time 0.050 (0.139)   Data 0.000 (0.087)   Loss 2.2513 (2.2644)   Prec@1 20.000 (17.159)   Prec@5 65.000 (68.577)   [2025-10-26 11:21:34]
  Epoch: [001][300/500]   Time 0.053 (0.111)   Data 0.000 (0.058)   Loss 2.2604 (2.2624)   Prec@1 13.000 (17.336)   Prec@5 70.000 (69.372)   [2025-10-26 11:21:40]
  Epoch: [001][400/500]   Time 0.052 (0.096)   Data 0.000 (0.044)   Loss 2.2140 (2.2597)   Prec@1 27.000 (17.446)   Prec@5 81.000 (70.055)   [2025-10-26 11:21:45]
  **Train** Prec@1 17.670 Prec@5 70.478 Error@1 82.330
  **Test** Prec@1 17.850 Prec@5 69.200 Error@1 82.150
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 11:22:10] [Epoch=002/040] [Need: 00:40:34] [LR=0.0100] [Best : Accuracy=17.85, Error=82.15]
  Epoch: [002][000/500]   Time 17.932 (17.932)   Data 17.662 (17.662)   Loss 2.2462 (2.2462)   Prec@1 13.000 (13.000)   Prec@5 69.000 (69.000)   [2025-10-26 11:22:28]
  Epoch: [002][100/500]   Time 0.052 (0.228)   Data 0.000 (0.175)   Loss 2.2482 (2.2402)   Prec@1 16.000 (18.257)   Prec@5 72.000 (72.564)   [2025-10-26 11:22:33]
  Epoch: [002][200/500]   Time 0.051 (0.141)   Data 0.000 (0.088)   Loss 2.2268 (2.2375)   Prec@1 19.000 (18.368)   Prec@5 76.000 (73.279)   [2025-10-26 11:22:38]
  Epoch: [002][300/500]   Time 0.054 (0.112)   Data 0.000 (0.059)   Loss 2.2170 (2.2347)   Prec@1 24.000 (18.824)   Prec@5 77.000 (73.877)   [2025-10-26 11:22:44]
  Epoch: [002][400/500]   Time 0.053 (0.097)   Data 0.000 (0.044)   Loss 2.1746 (2.2307)   Prec@1 30.000 (19.511)   Prec@5 78.000 (74.162)   [2025-10-26 11:22:49]
  **Train** Prec@1 20.150 Prec@5 74.334 Error@1 79.850
  **Test** Prec@1 22.340 Prec@5 73.820 Error@1 77.660
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 11:23:15] [Epoch=003/040] [Need: 00:39:41] [LR=0.0100] [Best : Accuracy=22.34, Error=77.66]
  Epoch: [003][000/500]   Time 36.095 (36.095)   Data 35.845 (35.845)   Loss 2.1634 (2.1634)   Prec@1 27.000 (27.000)   Prec@5 84.000 (84.000)   [2025-10-26 11:23:51]
  Epoch: [003][100/500]   Time 0.054 (0.409)   Data 0.000 (0.355)   Loss 2.2058 (2.2057)   Prec@1 23.000 (23.485)   Prec@5 70.000 (75.564)   [2025-10-26 11:23:56]
  Epoch: [003][200/500]   Time 0.053 (0.231)   Data 0.001 (0.178)   Loss 2.1644 (2.2021)   Prec@1 29.000 (23.761)   Prec@5 79.000 (75.716)   [2025-10-26 11:24:01]
  Epoch: [003][300/500]   Time 0.055 (0.172)   Data 0.000 (0.119)   Loss 2.2082 (2.2002)   Prec@1 24.000 (23.834)   Prec@5 82.000 (76.040)   [2025-10-26 11:24:07]
  Epoch: [003][400/500]   Time 0.052 (0.142)   Data 0.000 (0.090)   Loss 2.1733 (2.1966)   Prec@1 30.000 (24.190)   Prec@5 77.000 (76.397)   [2025-10-26 11:24:12]
  **Train** Prec@1 24.364 Prec@5 76.412 Error@1 75.636
  **Test** Prec@1 23.140 Prec@5 72.590 Error@1 76.860
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 11:24:38] [Epoch=004/040] [Need: 00:41:23] [LR=0.0100] [Best : Accuracy=23.14, Error=76.86]
  Epoch: [004][000/500]   Time 18.353 (18.353)   Data 18.070 (18.070)   Loss 2.2172 (2.2172)   Prec@1 21.000 (21.000)   Prec@5 69.000 (69.000)   [2025-10-26 11:24:56]
  Epoch: [004][100/500]   Time 0.051 (0.233)   Data 0.000 (0.179)   Loss 2.0927 (2.1821)   Prec@1 37.000 (25.376)   Prec@5 85.000 (77.149)   [2025-10-26 11:25:01]
  Epoch: [004][200/500]   Time 0.054 (0.144)   Data 0.000 (0.090)   Loss 2.1950 (2.1776)   Prec@1 27.000 (26.045)   Prec@5 76.000 (77.617)   [2025-10-26 11:25:07]
  Epoch: [004][300/500]   Time 0.053 (0.114)   Data 0.000 (0.060)   Loss 2.2072 (2.1770)   Prec@1 20.000 (26.056)   Prec@5 75.000 (77.518)   [2025-10-26 11:25:12]
  Epoch: [004][400/500]   Time 0.054 (0.099)   Data 0.000 (0.045)   Loss 2.1717 (2.1741)   Prec@1 25.000 (26.322)   Prec@5 74.000 (77.848)   [2025-10-26 11:25:17]
  **Train** Prec@1 26.252 Prec@5 77.748 Error@1 73.748
  **Test** Prec@1 26.620 Prec@5 80.870 Error@1 73.380
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 11:25:44] [Epoch=005/040] [Need: 00:39:53] [LR=0.0100] [Best : Accuracy=26.62, Error=73.38]
  Epoch: [005][000/500]   Time 19.839 (19.839)   Data 19.569 (19.569)   Loss 2.1614 (2.1614)   Prec@1 27.000 (27.000)   Prec@5 75.000 (75.000)   [2025-10-26 11:26:04]
  Epoch: [005][100/500]   Time 0.051 (0.249)   Data 0.000 (0.194)   Loss 2.1194 (2.1696)   Prec@1 31.000 (26.505)   Prec@5 84.000 (77.693)   [2025-10-26 11:26:09]
  Epoch: [005][200/500]   Time 0.052 (0.152)   Data 0.000 (0.097)   Loss 2.1207 (2.1653)   Prec@1 31.000 (27.154)   Prec@5 83.000 (78.244)   [2025-10-26 11:26:14]
  Epoch: [005][300/500]   Time 0.055 (0.119)   Data 0.000 (0.065)   Loss 2.1261 (2.1626)   Prec@1 29.000 (27.575)   Prec@5 82.000 (78.571)   [2025-10-26 11:26:20]
  Epoch: [005][400/500]   Time 0.053 (0.103)   Data 0.000 (0.049)   Loss 2.1291 (2.1624)   Prec@1 32.000 (27.596)   Prec@5 84.000 (78.786)   [2025-10-26 11:26:25]
  **Train** Prec@1 27.890 Prec@5 79.136 Error@1 72.110
  **Test** Prec@1 29.260 Prec@5 80.090 Error@1 70.740
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 11:26:51] [Epoch=006/040] [Need: 00:38:41] [LR=0.0100] [Best : Accuracy=29.26, Error=70.74]
  Epoch: [006][000/500]   Time 18.579 (18.579)   Data 18.306 (18.306)   Loss 2.1095 (2.1095)   Prec@1 34.000 (34.000)   Prec@5 85.000 (85.000)   [2025-10-26 11:27:10]
  Epoch: [006][100/500]   Time 0.052 (0.236)   Data 0.000 (0.181)   Loss 2.1147 (2.1504)   Prec@1 33.000 (29.554)   Prec@5 84.000 (79.901)   [2025-10-26 11:27:15]
  Epoch: [006][200/500]   Time 0.051 (0.145)   Data 0.000 (0.091)   Loss 2.1879 (2.1494)   Prec@1 25.000 (29.657)   Prec@5 73.000 (80.129)   [2025-10-26 11:27:21]
  Epoch: [006][300/500]   Time 0.053 (0.115)   Data 0.001 (0.061)   Loss 2.1113 (2.1506)   Prec@1 35.000 (29.512)   Prec@5 81.000 (79.814)   [2025-10-26 11:27:26]
  Epoch: [006][400/500]   Time 0.057 (0.100)   Data 0.000 (0.046)   Loss 2.2008 (2.1498)   Prec@1 22.000 (29.606)   Prec@5 76.000 (79.728)   [2025-10-26 11:27:31]
  **Train** Prec@1 30.014 Prec@5 79.842 Error@1 69.986
  **Test** Prec@1 29.880 Prec@5 81.600 Error@1 70.120
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 11:27:57] [Epoch=007/040] [Need: 00:37:20] [LR=0.0100] [Best : Accuracy=29.88, Error=70.12]
  Epoch: [007][000/500]   Time 18.459 (18.459)   Data 18.183 (18.183)   Loss 2.1044 (2.1044)   Prec@1 36.000 (36.000)   Prec@5 91.000 (91.000)   [2025-10-26 11:28:16]
  Epoch: [007][100/500]   Time 0.054 (0.235)   Data 0.000 (0.180)   Loss 2.1552 (2.1407)   Prec@1 25.000 (30.941)   Prec@5 86.000 (79.693)   [2025-10-26 11:28:21]
  Epoch: [007][200/500]   Time 0.056 (0.145)   Data 0.001 (0.091)   Loss 2.1127 (2.1360)   Prec@1 29.000 (31.393)   Prec@5 77.000 (79.781)   [2025-10-26 11:28:26]
  Epoch: [007][300/500]   Time 0.055 (0.115)   Data 0.000 (0.061)   Loss 2.0755 (2.1353)   Prec@1 34.000 (31.445)   Prec@5 86.000 (80.007)   [2025-10-26 11:28:32]
  Epoch: [007][400/500]   Time 0.054 (0.100)   Data 0.000 (0.045)   Loss 2.1278 (2.1341)   Prec@1 34.000 (31.603)   Prec@5 81.000 (80.142)   [2025-10-26 11:28:37]
  **Train** Prec@1 31.724 Prec@5 80.070 Error@1 68.276
  **Test** Prec@1 33.520 Prec@5 80.330 Error@1 66.480
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 11:29:03] [Epoch=008/040] [Need: 00:36:03] [LR=0.0100] [Best : Accuracy=33.52, Error=66.48]
  Epoch: [008][000/500]   Time 18.095 (18.095)   Data 17.833 (17.833)   Loss 2.1860 (2.1860)   Prec@1 23.000 (23.000)   Prec@5 77.000 (77.000)   [2025-10-26 11:29:21]
  Epoch: [008][100/500]   Time 0.054 (0.231)   Data 0.000 (0.177)   Loss 2.1047 (2.1235)   Prec@1 32.000 (33.099)   Prec@5 87.000 (81.059)   [2025-10-26 11:29:26]
  Epoch: [008][200/500]   Time 0.056 (0.143)   Data 0.000 (0.089)   Loss 2.1531 (2.1195)   Prec@1 31.000 (33.408)   Prec@5 79.000 (80.995)   [2025-10-26 11:29:31]
  Epoch: [008][300/500]   Time 0.056 (0.114)   Data 0.000 (0.059)   Loss 2.0776 (2.1184)   Prec@1 37.000 (33.458)   Prec@5 87.000 (81.259)   [2025-10-26 11:29:37]
  Epoch: [008][400/500]   Time 0.080 (0.101)   Data 0.001 (0.045)   Loss 2.0529 (2.1179)   Prec@1 41.000 (33.519)   Prec@5 83.000 (81.254)   [2025-10-26 11:29:43]
  **Train** Prec@1 33.610 Prec@5 81.300 Error@1 66.390
  **Test** Prec@1 36.920 Prec@5 83.130 Error@1 63.080
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 11:30:20] [Epoch=009/040] [Need: 00:35:29] [LR=0.0100] [Best : Accuracy=36.92, Error=63.08]
  Epoch: [009][000/500]   Time 28.231 (28.231)   Data 27.879 (27.879)   Loss 2.1193 (2.1193)   Prec@1 32.000 (32.000)   Prec@5 83.000 (83.000)   [2025-10-26 11:30:48]
  Epoch: [009][100/500]   Time 0.052 (0.331)   Data 0.000 (0.276)   Loss 2.1376 (2.1077)   Prec@1 30.000 (34.594)   Prec@5 86.000 (81.525)   [2025-10-26 11:30:54]
  Epoch: [009][200/500]   Time 0.054 (0.193)   Data 0.000 (0.139)   Loss 2.1232 (2.1105)   Prec@1 32.000 (34.239)   Prec@5 79.000 (81.756)   [2025-10-26 11:30:59]
  Epoch: [009][300/500]   Time 0.059 (0.147)   Data 0.000 (0.093)   Loss 2.0431 (2.1099)   Prec@1 45.000 (34.455)   Prec@5 82.000 (81.947)   [2025-10-26 11:31:04]
  Epoch: [009][400/500]   Time 0.053 (0.124)   Data 0.000 (0.070)   Loss 2.1043 (2.1070)   Prec@1 34.000 (34.726)   Prec@5 87.000 (82.190)   [2025-10-26 11:31:10]
  **Train** Prec@1 34.942 Prec@5 82.210 Error@1 65.058
  **Test** Prec@1 37.030 Prec@5 82.370 Error@1 62.970
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 11:31:36] [Epoch=010/040] [Need: 00:34:42] [LR=0.0100] [Best : Accuracy=37.03, Error=62.97]
  Epoch: [010][000/500]   Time 18.370 (18.370)   Data 18.097 (18.097)   Loss 2.1734 (2.1734)   Prec@1 26.000 (26.000)   Prec@5 84.000 (84.000)   [2025-10-26 11:31:54]
  Epoch: [010][100/500]   Time 0.051 (0.235)   Data 0.001 (0.179)   Loss 2.1649 (2.0922)   Prec@1 29.000 (36.297)   Prec@5 78.000 (83.376)   [2025-10-26 11:32:00]
  Epoch: [010][200/500]   Time 0.053 (0.145)   Data 0.000 (0.090)   Loss 2.1048 (2.0929)   Prec@1 36.000 (36.204)   Prec@5 84.000 (83.338)   [2025-10-26 11:32:05]
  Epoch: [010][300/500]   Time 0.052 (0.115)   Data 0.000 (0.060)   Loss 2.1146 (2.0920)   Prec@1 35.000 (36.312)   Prec@5 79.000 (83.581)   [2025-10-26 11:32:11]
  Epoch: [010][400/500]   Time 0.057 (0.100)   Data 0.001 (0.045)   Loss 2.1317 (2.0924)   Prec@1 30.000 (36.202)   Prec@5 88.000 (83.796)   [2025-10-26 11:32:16]
  **Train** Prec@1 36.472 Prec@5 84.144 Error@1 63.528
  **Test** Prec@1 38.570 Prec@5 87.850 Error@1 61.430
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 11:32:42] [Epoch=011/040] [Need: 00:33:24] [LR=0.0100] [Best : Accuracy=38.57, Error=61.43]
  Epoch: [011][000/500]   Time 18.240 (18.240)   Data 18.022 (18.022)   Loss 2.0392 (2.0392)   Prec@1 46.000 (46.000)   Prec@5 86.000 (86.000)   [2025-10-26 11:33:00]
  Epoch: [011][100/500]   Time 0.052 (0.233)   Data 0.000 (0.179)   Loss 2.0266 (2.0754)   Prec@1 43.000 (37.950)   Prec@5 87.000 (84.673)   [2025-10-26 11:33:05]
  Epoch: [011][200/500]   Time 0.055 (0.144)   Data 0.000 (0.090)   Loss 2.0413 (2.0759)   Prec@1 41.000 (37.925)   Prec@5 87.000 (84.910)   [2025-10-26 11:33:11]
  Epoch: [011][300/500]   Time 0.053 (0.115)   Data 0.000 (0.060)   Loss 2.0594 (2.0778)   Prec@1 40.000 (37.551)   Prec@5 87.000 (85.073)   [2025-10-26 11:33:16]
  Epoch: [011][400/500]   Time 0.054 (0.100)   Data 0.000 (0.045)   Loss 2.1135 (2.0776)   Prec@1 33.000 (37.591)   Prec@5 84.000 (85.187)   [2025-10-26 11:33:22]
  **Train** Prec@1 37.654 Prec@5 85.326 Error@1 62.346
  **Test** Prec@1 39.400 Prec@5 88.100 Error@1 60.600
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 11:33:48] [Epoch=012/040] [Need: 00:32:07] [LR=0.0100] [Best : Accuracy=39.40, Error=60.60]
  Epoch: [012][000/500]   Time 18.236 (18.236)   Data 17.953 (17.953)   Loss 2.0217 (2.0217)   Prec@1 43.000 (43.000)   Prec@5 90.000 (90.000)   [2025-10-26 11:34:06]
  Epoch: [012][100/500]   Time 0.051 (0.233)   Data 0.000 (0.178)   Loss 2.0489 (2.0672)   Prec@1 43.000 (38.703)   Prec@5 87.000 (86.693)   [2025-10-26 11:34:11]
  Epoch: [012][200/500]   Time 0.054 (0.144)   Data 0.000 (0.089)   Loss 2.0814 (2.0694)   Prec@1 36.000 (38.413)   Prec@5 84.000 (86.711)   [2025-10-26 11:34:17]
  Epoch: [012][300/500]   Time 0.052 (0.114)   Data 0.000 (0.060)   Loss 2.0525 (2.0710)   Prec@1 41.000 (38.223)   Prec@5 87.000 (86.631)   [2025-10-26 11:34:22]
  Epoch: [012][400/500]   Time 0.054 (0.099)   Data 0.001 (0.045)   Loss 2.0910 (2.0678)   Prec@1 36.000 (38.576)   Prec@5 90.000 (86.763)   [2025-10-26 11:34:28]
  **Train** Prec@1 38.598 Prec@5 86.826 Error@1 61.402
  **Test** Prec@1 44.240 Prec@5 90.910 Error@1 55.760
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 11:34:55] [Epoch=013/040] [Need: 00:30:54] [LR=0.0100] [Best : Accuracy=44.24, Error=55.76]
  Epoch: [013][000/500]   Time 18.416 (18.416)   Data 18.130 (18.130)   Loss 2.1054 (2.1054)   Prec@1 35.000 (35.000)   Prec@5 85.000 (85.000)   [2025-10-26 11:35:13]
  Epoch: [013][100/500]   Time 0.058 (0.235)   Data 0.000 (0.180)   Loss 2.0509 (2.0639)   Prec@1 43.000 (39.149)   Prec@5 86.000 (86.485)   [2025-10-26 11:35:19]
  Epoch: [013][200/500]   Time 0.056 (0.145)   Data 0.000 (0.090)   Loss 2.0296 (2.0593)   Prec@1 42.000 (39.572)   Prec@5 90.000 (86.925)   [2025-10-26 11:35:24]
  Epoch: [013][300/500]   Time 0.058 (0.115)   Data 0.001 (0.060)   Loss 2.0284 (2.0595)   Prec@1 44.000 (39.522)   Prec@5 91.000 (87.076)   [2025-10-26 11:35:30]
  Epoch: [013][400/500]   Time 0.055 (0.100)   Data 0.000 (0.045)   Loss 2.0314 (2.0581)   Prec@1 43.000 (39.591)   Prec@5 88.000 (87.272)   [2025-10-26 11:35:35]
  **Train** Prec@1 39.720 Prec@5 87.420 Error@1 60.280
  **Test** Prec@1 38.240 Prec@5 86.000 Error@1 61.760

==>>[2025-10-26 11:36:01] [Epoch=014/040] [Need: 00:29:41] [LR=0.0100] [Best : Accuracy=44.24, Error=55.76]
  Epoch: [014][000/500]   Time 18.313 (18.313)   Data 18.105 (18.105)   Loss 2.1203 (2.1203)   Prec@1 29.000 (29.000)   Prec@5 86.000 (86.000)   [2025-10-26 11:36:19]
  Epoch: [014][100/500]   Time 0.054 (0.234)   Data 0.000 (0.179)   Loss 2.0112 (2.0587)   Prec@1 47.000 (39.426)   Prec@5 85.000 (87.248)   [2025-10-26 11:36:25]
  Epoch: [014][200/500]   Time 0.055 (0.145)   Data 0.000 (0.090)   Loss 2.0842 (2.0595)   Prec@1 35.000 (39.368)   Prec@5 86.000 (87.015)   [2025-10-26 11:36:30]
  Epoch: [014][300/500]   Time 0.057 (0.115)   Data 0.000 (0.060)   Loss 2.0074 (2.0586)   Prec@1 46.000 (39.515)   Prec@5 89.000 (86.960)   [2025-10-26 11:36:36]
  Epoch: [014][400/500]   Time 0.059 (0.100)   Data 0.000 (0.045)   Loss 2.0371 (2.0542)   Prec@1 44.000 (40.037)   Prec@5 86.000 (87.162)   [2025-10-26 11:36:41]
  **Train** Prec@1 40.292 Prec@5 87.318 Error@1 59.708
  **Test** Prec@1 40.550 Prec@5 87.690 Error@1 59.450

==>>[2025-10-26 11:37:07] [Epoch=015/040] [Need: 00:28:28] [LR=0.0100] [Best : Accuracy=44.24, Error=55.76]
  Epoch: [015][000/500]   Time 17.892 (17.892)   Data 17.666 (17.666)   Loss 2.0684 (2.0684)   Prec@1 36.000 (36.000)   Prec@5 88.000 (88.000)   [2025-10-26 11:37:25]
  Epoch: [015][100/500]   Time 0.054 (0.230)   Data 0.000 (0.175)   Loss 1.9209 (2.0458)   Prec@1 55.000 (41.099)   Prec@5 90.000 (87.525)   [2025-10-26 11:37:30]
  Epoch: [015][200/500]   Time 0.053 (0.142)   Data 0.000 (0.088)   Loss 1.9754 (2.0443)   Prec@1 51.000 (41.159)   Prec@5 85.000 (87.672)   [2025-10-26 11:37:36]
  Epoch: [015][300/500]   Time 0.056 (0.113)   Data 0.000 (0.059)   Loss 2.0692 (2.0442)   Prec@1 38.000 (41.193)   Prec@5 89.000 (87.618)   [2025-10-26 11:37:41]
  Epoch: [015][400/500]   Time 0.055 (0.099)   Data 0.001 (0.044)   Loss 2.0634 (2.0429)   Prec@1 38.000 (41.274)   Prec@5 89.000 (87.758)   [2025-10-26 11:37:47]
  **Train** Prec@1 41.508 Prec@5 87.840 Error@1 58.492
  **Test** Prec@1 41.740 Prec@5 88.610 Error@1 58.260

==>>[2025-10-26 11:38:12] [Epoch=016/040] [Need: 00:27:15] [LR=0.0100] [Best : Accuracy=44.24, Error=55.76]
  Epoch: [016][000/500]   Time 18.097 (18.097)   Data 17.809 (17.809)   Loss 2.0366 (2.0366)   Prec@1 42.000 (42.000)   Prec@5 93.000 (93.000)   [2025-10-26 11:38:30]
  Epoch: [016][100/500]   Time 0.054 (0.231)   Data 0.001 (0.176)   Loss 2.0244 (2.0356)   Prec@1 46.000 (42.139)   Prec@5 88.000 (88.158)   [2025-10-26 11:38:36]
  Epoch: [016][200/500]   Time 0.052 (0.143)   Data 0.000 (0.089)   Loss 2.0496 (2.0357)   Prec@1 41.000 (42.129)   Prec@5 88.000 (88.294)   [2025-10-26 11:38:41]
  Epoch: [016][300/500]   Time 0.058 (0.114)   Data 0.001 (0.059)   Loss 2.0200 (2.0340)   Prec@1 43.000 (42.246)   Prec@5 95.000 (88.458)   [2025-10-26 11:38:47]
  Epoch: [016][400/500]   Time 0.059 (0.099)   Data 0.000 (0.045)   Loss 2.0094 (2.0332)   Prec@1 43.000 (42.332)   Prec@5 83.000 (88.401)   [2025-10-26 11:38:52]
  **Train** Prec@1 42.278 Prec@5 88.268 Error@1 57.722
  **Test** Prec@1 46.540 Prec@5 91.530 Error@1 53.460
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 11:39:18] [Epoch=017/040] [Need: 00:26:04] [LR=0.0100] [Best : Accuracy=46.54, Error=53.46]
  Epoch: [017][000/500]   Time 18.861 (18.861)   Data 18.586 (18.586)   Loss 2.0549 (2.0549)   Prec@1 41.000 (41.000)   Prec@5 86.000 (86.000)   [2025-10-26 11:39:37]
  Epoch: [017][100/500]   Time 0.055 (0.239)   Data 0.001 (0.184)   Loss 2.0406 (2.0203)   Prec@1 43.000 (43.752)   Prec@5 90.000 (88.188)   [2025-10-26 11:39:42]
  Epoch: [017][200/500]   Time 0.052 (0.148)   Data 0.000 (0.093)   Loss 1.9968 (2.0278)   Prec@1 48.000 (42.896)   Prec@5 92.000 (88.114)   [2025-10-26 11:39:48]
  Epoch: [017][300/500]   Time 0.057 (0.117)   Data 0.000 (0.062)   Loss 2.0374 (2.0273)   Prec@1 42.000 (42.960)   Prec@5 91.000 (88.299)   [2025-10-26 11:39:53]
  Epoch: [017][400/500]   Time 0.058 (0.102)   Data 0.000 (0.046)   Loss 2.0000 (2.0280)   Prec@1 49.000 (42.945)   Prec@5 86.000 (88.411)   [2025-10-26 11:39:59]
  **Train** Prec@1 43.078 Prec@5 88.606 Error@1 56.922
  **Test** Prec@1 44.130 Prec@5 89.490 Error@1 55.870

==>>[2025-10-26 11:40:25] [Epoch=018/040] [Need: 00:24:54] [LR=0.0100] [Best : Accuracy=46.54, Error=53.46]
  Epoch: [018][000/500]   Time 18.871 (18.871)   Data 18.592 (18.592)   Loss 1.9792 (1.9792)   Prec@1 47.000 (47.000)   Prec@5 89.000 (89.000)   [2025-10-26 11:40:44]
  Epoch: [018][100/500]   Time 0.049 (0.240)   Data 0.000 (0.184)   Loss 2.0623 (2.0211)   Prec@1 40.000 (43.802)   Prec@5 89.000 (88.574)   [2025-10-26 11:40:49]
  Epoch: [018][200/500]   Time 0.053 (0.147)   Data 0.000 (0.093)   Loss 1.9484 (2.0253)   Prec@1 54.000 (43.333)   Prec@5 94.000 (88.602)   [2025-10-26 11:40:54]
  Epoch: [018][300/500]   Time 0.057 (0.117)   Data 0.000 (0.062)   Loss 2.0185 (2.0248)   Prec@1 41.000 (43.249)   Prec@5 89.000 (88.615)   [2025-10-26 11:41:00]
  Epoch: [018][400/500]   Time 0.058 (0.102)   Data 0.000 (0.047)   Loss 2.0667 (2.0217)   Prec@1 37.000 (43.519)   Prec@5 87.000 (88.758)   [2025-10-26 11:41:05]
  **Train** Prec@1 43.742 Prec@5 88.740 Error@1 56.258
  **Test** Prec@1 48.320 Prec@5 92.010 Error@1 51.680
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 11:41:31] [Epoch=019/040] [Need: 00:23:45] [LR=0.0100] [Best : Accuracy=48.32, Error=51.68]
  Epoch: [019][000/500]   Time 18.077 (18.077)   Data 17.860 (17.860)   Loss 1.9894 (1.9894)   Prec@1 48.000 (48.000)   Prec@5 91.000 (91.000)   [2025-10-26 11:41:49]
  Epoch: [019][100/500]   Time 0.053 (0.231)   Data 0.000 (0.177)   Loss 1.9704 (2.0110)   Prec@1 48.000 (44.554)   Prec@5 89.000 (89.059)   [2025-10-26 11:41:54]
  Epoch: [019][200/500]   Time 0.054 (0.143)   Data 0.000 (0.089)   Loss 2.0602 (2.0145)   Prec@1 39.000 (44.204)   Prec@5 83.000 (88.856)   [2025-10-26 11:42:00]
  Epoch: [019][300/500]   Time 0.057 (0.114)   Data 0.000 (0.059)   Loss 1.9476 (2.0130)   Prec@1 51.000 (44.306)   Prec@5 90.000 (88.870)   [2025-10-26 11:42:05]
  Epoch: [019][400/500]   Time 0.058 (0.099)   Data 0.000 (0.045)   Loss 2.0010 (2.0135)   Prec@1 43.000 (44.287)   Prec@5 87.000 (88.781)   [2025-10-26 11:42:11]
  **Train** Prec@1 44.122 Prec@5 88.906 Error@1 55.878
  **Test** Prec@1 44.970 Prec@5 90.180 Error@1 55.030

==>>[2025-10-26 11:42:36] [Epoch=020/040] [Need: 00:22:34] [LR=0.0100] [Best : Accuracy=48.32, Error=51.68]
  Epoch: [020][000/500]   Time 18.286 (18.286)   Data 18.018 (18.018)   Loss 2.0073 (2.0073)   Prec@1 45.000 (45.000)   Prec@5 92.000 (92.000)   [2025-10-26 11:42:55]
  Epoch: [020][100/500]   Time 0.053 (0.233)   Data 0.000 (0.179)   Loss 2.0621 (2.0110)   Prec@1 39.000 (44.782)   Prec@5 89.000 (89.455)   [2025-10-26 11:43:00]
  Epoch: [020][200/500]   Time 0.055 (0.144)   Data 0.000 (0.090)   Loss 1.9928 (2.0097)   Prec@1 45.000 (44.806)   Prec@5 91.000 (89.090)   [2025-10-26 11:43:05]
  Epoch: [020][300/500]   Time 0.054 (0.114)   Data 0.000 (0.060)   Loss 1.9523 (2.0078)   Prec@1 50.000 (44.917)   Prec@5 89.000 (89.236)   [2025-10-26 11:43:11]
  Epoch: [020][400/500]   Time 0.056 (0.100)   Data 0.000 (0.045)   Loss 2.0819 (2.0069)   Prec@1 40.000 (45.065)   Prec@5 90.000 (89.072)   [2025-10-26 11:43:16]
  **Train** Prec@1 44.910 Prec@5 88.956 Error@1 55.090
  **Test** Prec@1 47.390 Prec@5 91.670 Error@1 52.610

==>>[2025-10-26 11:43:42] [Epoch=021/040] [Need: 00:21:25] [LR=0.0100] [Best : Accuracy=48.32, Error=51.68]
  Epoch: [021][000/500]   Time 18.088 (18.088)   Data 17.818 (17.818)   Loss 2.0689 (2.0689)   Prec@1 37.000 (37.000)   Prec@5 89.000 (89.000)   [2025-10-26 11:44:01]
  Epoch: [021][100/500]   Time 0.054 (0.232)   Data 0.001 (0.177)   Loss 2.0481 (1.9996)   Prec@1 41.000 (45.931)   Prec@5 86.000 (89.554)   [2025-10-26 11:44:06]
  Epoch: [021][200/500]   Time 0.055 (0.144)   Data 0.000 (0.089)   Loss 1.9908 (2.0009)   Prec@1 49.000 (45.816)   Prec@5 91.000 (89.741)   [2025-10-26 11:44:11]
  Epoch: [021][300/500]   Time 0.057 (0.114)   Data 0.000 (0.059)   Loss 2.0184 (1.9993)   Prec@1 41.000 (45.944)   Prec@5 97.000 (89.668)   [2025-10-26 11:44:17]
  Epoch: [021][400/500]   Time 0.056 (0.100)   Data 0.000 (0.045)   Loss 2.0072 (1.9996)   Prec@1 43.000 (45.850)   Prec@5 88.000 (89.461)   [2025-10-26 11:44:22]
  **Train** Prec@1 45.742 Prec@5 89.414 Error@1 54.258
  **Test** Prec@1 43.750 Prec@5 89.060 Error@1 56.250

==>>[2025-10-26 11:44:49] [Epoch=022/040] [Need: 00:20:16] [LR=0.0100] [Best : Accuracy=48.32, Error=51.68]
  Epoch: [022][000/500]   Time 18.135 (18.135)   Data 17.862 (17.862)   Loss 2.0036 (2.0036)   Prec@1 45.000 (45.000)   Prec@5 93.000 (93.000)   [2025-10-26 11:45:07]
  Epoch: [022][100/500]   Time 0.054 (0.233)   Data 0.000 (0.177)   Loss 2.0117 (2.0088)   Prec@1 43.000 (44.931)   Prec@5 93.000 (89.624)   [2025-10-26 11:45:12]
  Epoch: [022][200/500]   Time 0.060 (0.144)   Data 0.001 (0.089)   Loss 1.9337 (2.0020)   Prec@1 55.000 (45.592)   Prec@5 90.000 (89.448)   [2025-10-26 11:45:18]
  Epoch: [022][300/500]   Time 0.057 (0.115)   Data 0.000 (0.060)   Loss 1.9878 (1.9997)   Prec@1 47.000 (45.688)   Prec@5 89.000 (89.492)   [2025-10-26 11:45:23]
  Epoch: [022][400/500]   Time 0.056 (0.100)   Data 0.000 (0.045)   Loss 1.9446 (1.9991)   Prec@1 53.000 (45.718)   Prec@5 93.000 (89.429)   [2025-10-26 11:45:29]
  **Train** Prec@1 45.554 Prec@5 89.390 Error@1 54.446
  **Test** Prec@1 48.400 Prec@5 91.550 Error@1 51.600
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 11:45:55] [Epoch=023/040] [Need: 00:19:08] [LR=0.0100] [Best : Accuracy=48.40, Error=51.60]
  Epoch: [023][000/500]   Time 18.225 (18.225)   Data 17.947 (17.947)   Loss 1.9495 (1.9495)   Prec@1 51.000 (51.000)   Prec@5 89.000 (89.000)   [2025-10-26 11:46:13]
  Epoch: [023][100/500]   Time 0.054 (0.233)   Data 0.000 (0.178)   Loss 2.0110 (1.9977)   Prec@1 44.000 (46.198)   Prec@5 86.000 (89.495)   [2025-10-26 11:46:19]
  Epoch: [023][200/500]   Time 0.053 (0.144)   Data 0.000 (0.089)   Loss 1.9345 (1.9925)   Prec@1 53.000 (46.572)   Prec@5 88.000 (89.692)   [2025-10-26 11:46:24]
  Epoch: [023][300/500]   Time 0.054 (0.114)   Data 0.001 (0.060)   Loss 1.9258 (1.9945)   Prec@1 54.000 (46.312)   Prec@5 93.000 (89.502)   [2025-10-26 11:46:30]
  Epoch: [023][400/500]   Time 0.058 (0.100)   Data 0.000 (0.045)   Loss 2.0447 (1.9936)   Prec@1 42.000 (46.349)   Prec@5 86.000 (89.444)   [2025-10-26 11:46:35]
  **Train** Prec@1 46.544 Prec@5 89.504 Error@1 53.456
  **Test** Prec@1 49.850 Prec@5 91.810 Error@1 50.150
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 11:47:01] [Epoch=024/040] [Need: 00:17:59] [LR=0.0100] [Best : Accuracy=49.85, Error=50.15]
  Epoch: [024][000/500]   Time 18.160 (18.160)   Data 17.887 (17.887)   Loss 1.9879 (1.9879)   Prec@1 50.000 (50.000)   Prec@5 92.000 (92.000)   [2025-10-26 11:47:19]
  Epoch: [024][100/500]   Time 0.054 (0.233)   Data 0.001 (0.177)   Loss 2.0538 (1.9973)   Prec@1 38.000 (45.871)   Prec@5 91.000 (89.267)   [2025-10-26 11:47:25]
  Epoch: [024][200/500]   Time 0.055 (0.144)   Data 0.000 (0.089)   Loss 1.9820 (1.9920)   Prec@1 46.000 (46.507)   Prec@5 94.000 (89.667)   [2025-10-26 11:47:30]
  Epoch: [024][300/500]   Time 0.052 (0.114)   Data 0.000 (0.060)   Loss 1.9502 (1.9902)   Prec@1 52.000 (46.691)   Prec@5 94.000 (89.947)   [2025-10-26 11:47:36]
  Epoch: [024][400/500]   Time 0.055 (0.100)   Data 0.001 (0.045)   Loss 1.9899 (1.9885)   Prec@1 46.000 (46.898)   Prec@5 95.000 (90.032)   [2025-10-26 11:47:41]
  **Train** Prec@1 46.896 Prec@5 89.998 Error@1 53.104
  **Test** Prec@1 49.580 Prec@5 91.100 Error@1 50.420

==>>[2025-10-26 11:48:07] [Epoch=025/040] [Need: 00:16:51] [LR=0.0010] [Best : Accuracy=49.85, Error=50.15]
  Epoch: [025][000/500]   Time 18.089 (18.089)   Data 17.800 (17.800)   Loss 1.9340 (1.9340)   Prec@1 52.000 (52.000)   Prec@5 93.000 (93.000)   [2025-10-26 11:48:25]
  Epoch: [025][100/500]   Time 0.056 (0.233)   Data 0.000 (0.177)   Loss 1.9882 (1.9798)   Prec@1 46.000 (47.901)   Prec@5 91.000 (90.198)   [2025-10-26 11:48:31]
  Epoch: [025][200/500]   Time 0.055 (0.144)   Data 0.001 (0.089)   Loss 1.9711 (1.9718)   Prec@1 48.000 (48.736)   Prec@5 90.000 (90.507)   [2025-10-26 11:48:36]
  Epoch: [025][300/500]   Time 0.054 (0.114)   Data 0.000 (0.059)   Loss 1.9745 (1.9670)   Prec@1 48.000 (49.269)   Prec@5 93.000 (90.545)   [2025-10-26 11:48:42]
  Epoch: [025][400/500]   Time 0.053 (0.100)   Data 0.000 (0.045)   Loss 1.9748 (1.9650)   Prec@1 49.000 (49.464)   Prec@5 89.000 (90.698)   [2025-10-26 11:48:47]
  **Train** Prec@1 49.630 Prec@5 90.730 Error@1 50.370
  **Test** Prec@1 50.760 Prec@5 91.820 Error@1 49.240
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 11:49:13] [Epoch=026/040] [Need: 00:15:43] [LR=0.0010] [Best : Accuracy=50.76, Error=49.24]
  Epoch: [026][000/500]   Time 17.947 (17.947)   Data 17.676 (17.676)   Loss 2.0190 (2.0190)   Prec@1 45.000 (45.000)   Prec@5 88.000 (88.000)   [2025-10-26 11:49:31]
  Epoch: [026][100/500]   Time 0.052 (0.231)   Data 0.000 (0.175)   Loss 1.9408 (1.9543)   Prec@1 50.000 (50.634)   Prec@5 92.000 (90.921)   [2025-10-26 11:49:37]
  Epoch: [026][200/500]   Time 0.056 (0.143)   Data 0.000 (0.088)   Loss 2.0019 (1.9547)   Prec@1 45.000 (50.522)   Prec@5 90.000 (91.090)   [2025-10-26 11:49:42]
  Epoch: [026][300/500]   Time 0.059 (0.115)   Data 0.000 (0.059)   Loss 1.9492 (1.9522)   Prec@1 52.000 (50.771)   Prec@5 90.000 (91.269)   [2025-10-26 11:49:48]
  Epoch: [026][400/500]   Time 0.056 (0.100)   Data 0.000 (0.044)   Loss 1.9777 (1.9537)   Prec@1 49.000 (50.549)   Prec@5 88.000 (91.155)   [2025-10-26 11:49:53]
  **Train** Prec@1 50.662 Prec@5 91.172 Error@1 49.338
  **Test** Prec@1 50.960 Prec@5 91.950 Error@1 49.040
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 11:50:19] [Epoch=027/040] [Need: 00:14:34] [LR=0.0010] [Best : Accuracy=50.96, Error=49.04]
  Epoch: [027][000/500]   Time 18.381 (18.381)   Data 18.108 (18.108)   Loss 1.9181 (1.9181)   Prec@1 53.000 (53.000)   Prec@5 92.000 (92.000)   [2025-10-26 11:50:37]
  Epoch: [027][100/500]   Time 0.059 (0.236)   Data 0.000 (0.179)   Loss 1.9259 (1.9545)   Prec@1 57.000 (50.436)   Prec@5 89.000 (91.030)   [2025-10-26 11:50:43]
  Epoch: [027][200/500]   Time 0.056 (0.146)   Data 0.000 (0.090)   Loss 1.9206 (1.9527)   Prec@1 55.000 (50.662)   Prec@5 92.000 (91.025)   [2025-10-26 11:50:48]
  Epoch: [027][300/500]   Time 0.056 (0.116)   Data 0.000 (0.060)   Loss 2.0185 (1.9539)   Prec@1 42.000 (50.571)   Prec@5 88.000 (91.056)   [2025-10-26 11:50:54]
  Epoch: [027][400/500]   Time 0.056 (0.101)   Data 0.000 (0.045)   Loss 1.8489 (1.9517)   Prec@1 62.000 (50.833)   Prec@5 95.000 (91.122)   [2025-10-26 11:50:59]
  **Train** Prec@1 50.910 Prec@5 91.260 Error@1 49.090
  **Test** Prec@1 51.040 Prec@5 91.690 Error@1 48.960
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 11:51:25] [Epoch=028/040] [Need: 00:13:27] [LR=0.0010] [Best : Accuracy=51.04, Error=48.96]
  Epoch: [028][000/500]   Time 18.042 (18.042)   Data 17.838 (17.838)   Loss 1.9835 (1.9835)   Prec@1 46.000 (46.000)   Prec@5 89.000 (89.000)   [2025-10-26 11:51:43]
  Epoch: [028][100/500]   Time 0.055 (0.231)   Data 0.000 (0.177)   Loss 1.9529 (1.9547)   Prec@1 51.000 (50.475)   Prec@5 90.000 (91.515)   [2025-10-26 11:51:48]
  Epoch: [028][200/500]   Time 0.057 (0.143)   Data 0.000 (0.089)   Loss 1.9189 (1.9523)   Prec@1 56.000 (50.706)   Prec@5 90.000 (91.597)   [2025-10-26 11:51:54]
  Epoch: [028][300/500]   Time 0.057 (0.114)   Data 0.000 (0.059)   Loss 1.8815 (1.9506)   Prec@1 59.000 (50.950)   Prec@5 92.000 (91.568)   [2025-10-26 11:52:00]
  Epoch: [028][400/500]   Time 0.059 (0.100)   Data 0.000 (0.045)   Loss 1.9498 (1.9510)   Prec@1 51.000 (50.915)   Prec@5 89.000 (91.511)   [2025-10-26 11:52:05]
  **Train** Prec@1 51.004 Prec@5 91.512 Error@1 48.996
  **Test** Prec@1 51.500 Prec@5 92.230 Error@1 48.500
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 11:52:31] [Epoch=029/040] [Need: 00:12:19] [LR=0.0010] [Best : Accuracy=51.50, Error=48.50]
  Epoch: [029][000/500]   Time 18.052 (18.052)   Data 17.767 (17.767)   Loss 1.9810 (1.9810)   Prec@1 48.000 (48.000)   Prec@5 92.000 (92.000)   [2025-10-26 11:52:49]
  Epoch: [029][100/500]   Time 0.056 (0.231)   Data 0.000 (0.176)   Loss 1.9761 (1.9458)   Prec@1 49.000 (51.614)   Prec@5 91.000 (91.653)   [2025-10-26 11:52:54]
  Epoch: [029][200/500]   Time 0.060 (0.144)   Data 0.000 (0.089)   Loss 1.9443 (1.9436)   Prec@1 51.000 (51.796)   Prec@5 95.000 (91.652)   [2025-10-26 11:53:00]
  Epoch: [029][300/500]   Time 0.056 (0.114)   Data 0.000 (0.059)   Loss 1.9972 (1.9453)   Prec@1 45.000 (51.621)   Prec@5 91.000 (91.462)   [2025-10-26 11:53:05]
  Epoch: [029][400/500]   Time 0.056 (0.100)   Data 0.000 (0.044)   Loss 1.9086 (1.9453)   Prec@1 55.000 (51.633)   Prec@5 93.000 (91.474)   [2025-10-26 11:53:11]
  **Train** Prec@1 51.480 Prec@5 91.440 Error@1 48.520
  **Test** Prec@1 49.890 Prec@5 91.110 Error@1 50.110

==>>[2025-10-26 11:53:37] [Epoch=030/040] [Need: 00:11:11] [LR=0.0010] [Best : Accuracy=51.50, Error=48.50]
  Epoch: [030][000/500]   Time 18.246 (18.246)   Data 17.967 (17.967)   Loss 1.8928 (1.8928)   Prec@1 58.000 (58.000)   Prec@5 93.000 (93.000)   [2025-10-26 11:53:55]
  Epoch: [030][100/500]   Time 0.053 (0.233)   Data 0.000 (0.178)   Loss 1.9335 (1.9407)   Prec@1 54.000 (52.020)   Prec@5 92.000 (91.554)   [2025-10-26 11:54:00]
  Epoch: [030][200/500]   Time 0.058 (0.144)   Data 0.001 (0.090)   Loss 1.9496 (1.9423)   Prec@1 52.000 (51.761)   Prec@5 92.000 (91.682)   [2025-10-26 11:54:06]
  Epoch: [030][300/500]   Time 0.054 (0.114)   Data 0.000 (0.060)   Loss 1.9832 (1.9424)   Prec@1 49.000 (51.777)   Prec@5 92.000 (91.654)   [2025-10-26 11:54:11]
  Epoch: [030][400/500]   Time 0.055 (0.100)   Data 0.000 (0.045)   Loss 1.9171 (1.9418)   Prec@1 56.000 (51.853)   Prec@5 87.000 (91.601)   [2025-10-26 11:54:17]
  **Train** Prec@1 51.690 Prec@5 91.574 Error@1 48.310
  **Test** Prec@1 51.240 Prec@5 92.240 Error@1 48.760

==>>[2025-10-26 11:54:43] [Epoch=031/040] [Need: 00:10:04] [LR=0.0010] [Best : Accuracy=51.50, Error=48.50]
  Epoch: [031][000/500]   Time 18.621 (18.621)   Data 18.346 (18.346)   Loss 1.8533 (1.8533)   Prec@1 59.000 (59.000)   Prec@5 91.000 (91.000)   [2025-10-26 11:55:01]
  Epoch: [031][100/500]   Time 0.053 (0.237)   Data 0.001 (0.182)   Loss 1.8959 (1.9471)   Prec@1 59.000 (51.366)   Prec@5 95.000 (91.020)   [2025-10-26 11:55:07]
  Epoch: [031][200/500]   Time 0.053 (0.146)   Data 0.000 (0.091)   Loss 1.9379 (1.9441)   Prec@1 53.000 (51.697)   Prec@5 87.000 (91.358)   [2025-10-26 11:55:12]
  Epoch: [031][300/500]   Time 0.054 (0.116)   Data 0.000 (0.061)   Loss 1.9389 (1.9410)   Prec@1 51.000 (51.990)   Prec@5 94.000 (91.429)   [2025-10-26 11:55:18]
  Epoch: [031][400/500]   Time 0.057 (0.101)   Data 0.001 (0.046)   Loss 1.9142 (1.9398)   Prec@1 56.000 (52.107)   Prec@5 89.000 (91.501)   [2025-10-26 11:55:23]
  **Train** Prec@1 51.926 Prec@5 91.450 Error@1 48.074
  **Test** Prec@1 51.610 Prec@5 91.760 Error@1 48.390
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 11:55:50] [Epoch=032/040] [Need: 00:08:57] [LR=0.0010] [Best : Accuracy=51.61, Error=48.39]
  Epoch: [032][000/500]   Time 18.089 (18.089)   Data 17.803 (17.803)   Loss 1.9174 (1.9174)   Prec@1 51.000 (51.000)   Prec@5 95.000 (95.000)   [2025-10-26 11:56:08]
  Epoch: [032][100/500]   Time 0.055 (0.232)   Data 0.000 (0.176)   Loss 1.9706 (1.9439)   Prec@1 49.000 (51.386)   Prec@5 93.000 (91.356)   [2025-10-26 11:56:13]
  Epoch: [032][200/500]   Time 0.055 (0.144)   Data 0.000 (0.089)   Loss 1.8862 (1.9425)   Prec@1 58.000 (51.617)   Prec@5 87.000 (91.682)   [2025-10-26 11:56:19]
  Epoch: [032][300/500]   Time 0.058 (0.114)   Data 0.000 (0.059)   Loss 1.8832 (1.9407)   Prec@1 60.000 (51.894)   Prec@5 94.000 (91.767)   [2025-10-26 11:56:24]
  Epoch: [032][400/500]   Time 0.057 (0.100)   Data 0.000 (0.045)   Loss 1.9307 (1.9398)   Prec@1 53.000 (52.037)   Prec@5 89.000 (91.726)   [2025-10-26 11:56:30]
  **Train** Prec@1 52.010 Prec@5 91.676 Error@1 47.990
  **Test** Prec@1 51.410 Prec@5 91.700 Error@1 48.590

==>>[2025-10-26 11:56:56] [Epoch=033/040] [Need: 00:07:49] [LR=0.0010] [Best : Accuracy=51.61, Error=48.39]
  Epoch: [033][000/500]   Time 18.002 (18.002)   Data 17.716 (17.716)   Loss 1.8666 (1.8666)   Prec@1 61.000 (61.000)   Prec@5 89.000 (89.000)   [2025-10-26 11:57:14]
  Epoch: [033][100/500]   Time 0.054 (0.230)   Data 0.000 (0.176)   Loss 1.9091 (1.9384)   Prec@1 52.000 (52.208)   Prec@5 94.000 (90.950)   [2025-10-26 11:57:19]
  Epoch: [033][200/500]   Time 0.054 (0.143)   Data 0.000 (0.088)   Loss 1.9261 (1.9379)   Prec@1 53.000 (52.229)   Prec@5 93.000 (91.194)   [2025-10-26 11:57:24]
  Epoch: [033][300/500]   Time 0.056 (0.114)   Data 0.000 (0.059)   Loss 2.0235 (1.9381)   Prec@1 45.000 (52.216)   Prec@5 85.000 (91.382)   [2025-10-26 11:57:30]
  Epoch: [033][400/500]   Time 0.056 (0.099)   Data 0.000 (0.044)   Loss 1.8924 (1.9397)   Prec@1 59.000 (52.030)   Prec@5 94.000 (91.364)   [2025-10-26 11:57:35]
  **Train** Prec@1 51.974 Prec@5 91.478 Error@1 48.026
  **Test** Prec@1 50.810 Prec@5 91.730 Error@1 49.190

==>>[2025-10-26 11:58:01] [Epoch=034/040] [Need: 00:06:42] [LR=0.0010] [Best : Accuracy=51.61, Error=48.39]
  Epoch: [034][000/500]   Time 18.057 (18.057)   Data 17.788 (17.788)   Loss 1.8798 (1.8798)   Prec@1 59.000 (59.000)   Prec@5 93.000 (93.000)   [2025-10-26 11:58:19]
  Epoch: [034][100/500]   Time 0.053 (0.231)   Data 0.000 (0.176)   Loss 1.9306 (1.9480)   Prec@1 55.000 (51.030)   Prec@5 93.000 (91.337)   [2025-10-26 11:58:25]
  Epoch: [034][200/500]   Time 0.052 (0.143)   Data 0.000 (0.089)   Loss 1.9503 (1.9391)   Prec@1 52.000 (52.124)   Prec@5 86.000 (91.373)   [2025-10-26 11:58:30]
  Epoch: [034][300/500]   Time 0.059 (0.114)   Data 0.000 (0.059)   Loss 2.0552 (1.9369)   Prec@1 41.000 (52.322)   Prec@5 89.000 (91.568)   [2025-10-26 11:58:35]
  Epoch: [034][400/500]   Time 0.055 (0.099)   Data 0.000 (0.045)   Loss 1.9259 (1.9371)   Prec@1 54.000 (52.307)   Prec@5 95.000 (91.638)   [2025-10-26 11:58:41]
  **Train** Prec@1 52.282 Prec@5 91.682 Error@1 47.718
  **Test** Prec@1 52.260 Prec@5 92.270 Error@1 47.740
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 11:59:07] [Epoch=035/040] [Need: 00:05:35] [LR=0.0010] [Best : Accuracy=52.26, Error=47.74]
  Epoch: [035][000/500]   Time 18.229 (18.229)   Data 17.943 (17.943)   Loss 1.8771 (1.8771)   Prec@1 60.000 (60.000)   Prec@5 92.000 (92.000)   [2025-10-26 11:59:25]
  Epoch: [035][100/500]   Time 0.053 (0.233)   Data 0.000 (0.178)   Loss 1.9339 (1.9376)   Prec@1 48.000 (52.149)   Prec@5 92.000 (91.822)   [2025-10-26 11:59:31]
  Epoch: [035][200/500]   Time 0.055 (0.144)   Data 0.000 (0.089)   Loss 1.9125 (1.9348)   Prec@1 55.000 (52.398)   Prec@5 93.000 (91.627)   [2025-10-26 11:59:36]
  Epoch: [035][300/500]   Time 0.056 (0.115)   Data 0.000 (0.060)   Loss 1.9127 (1.9381)   Prec@1 52.000 (52.126)   Prec@5 94.000 (91.668)   [2025-10-26 11:59:42]
  Epoch: [035][400/500]   Time 0.055 (0.100)   Data 0.000 (0.045)   Loss 1.8784 (1.9365)   Prec@1 58.000 (52.334)   Prec@5 92.000 (91.706)   [2025-10-26 11:59:47]
  **Train** Prec@1 52.342 Prec@5 91.684 Error@1 47.658
  **Test** Prec@1 52.520 Prec@5 91.950 Error@1 47.480
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 12:00:13] [Epoch=036/040] [Need: 00:04:27] [LR=0.0010] [Best : Accuracy=52.52, Error=47.48]
  Epoch: [036][000/500]   Time 18.119 (18.119)   Data 17.847 (17.847)   Loss 1.9118 (1.9118)   Prec@1 54.000 (54.000)   Prec@5 90.000 (90.000)   [2025-10-26 12:00:31]
  Epoch: [036][100/500]   Time 0.054 (0.232)   Data 0.000 (0.177)   Loss 1.8748 (1.9278)   Prec@1 61.000 (53.337)   Prec@5 96.000 (91.861)   [2025-10-26 12:00:37]
  Epoch: [036][200/500]   Time 0.055 (0.144)   Data 0.001 (0.089)   Loss 1.9373 (1.9370)   Prec@1 52.000 (52.318)   Prec@5 89.000 (91.627)   [2025-10-26 12:00:42]
  Epoch: [036][300/500]   Time 0.059 (0.115)   Data 0.000 (0.059)   Loss 1.9514 (1.9384)   Prec@1 50.000 (52.186)   Prec@5 91.000 (91.495)   [2025-10-26 12:00:48]
  Epoch: [036][400/500]   Time 0.055 (0.100)   Data 0.001 (0.045)   Loss 1.9696 (1.9396)   Prec@1 49.000 (52.017)   Prec@5 92.000 (91.449)   [2025-10-26 12:00:53]
  **Train** Prec@1 52.178 Prec@5 91.530 Error@1 47.822
  **Test** Prec@1 53.460 Prec@5 92.500 Error@1 46.540
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 12:01:19] [Epoch=037/040] [Need: 00:03:20] [LR=0.0010] [Best : Accuracy=53.46, Error=46.54]
  Epoch: [037][000/500]   Time 17.864 (17.864)   Data 17.666 (17.666)   Loss 1.9142 (1.9142)   Prec@1 56.000 (56.000)   Prec@5 95.000 (95.000)   [2025-10-26 12:01:37]
  Epoch: [037][100/500]   Time 0.055 (0.229)   Data 0.000 (0.175)   Loss 1.9392 (1.9346)   Prec@1 54.000 (52.475)   Prec@5 93.000 (91.604)   [2025-10-26 12:01:42]
  Epoch: [037][200/500]   Time 0.057 (0.142)   Data 0.000 (0.088)   Loss 1.9153 (1.9356)   Prec@1 53.000 (52.473)   Prec@5 91.000 (91.761)   [2025-10-26 12:01:48]
  Epoch: [037][300/500]   Time 0.057 (0.113)   Data 0.000 (0.059)   Loss 1.9609 (1.9341)   Prec@1 47.000 (52.688)   Prec@5 91.000 (91.664)   [2025-10-26 12:01:53]
  Epoch: [037][400/500]   Time 0.058 (0.099)   Data 0.001 (0.044)   Loss 1.9220 (1.9350)   Prec@1 56.000 (52.579)   Prec@5 96.000 (91.601)   [2025-10-26 12:01:59]
  **Train** Prec@1 52.726 Prec@5 91.716 Error@1 47.274
  **Test** Prec@1 50.870 Prec@5 91.240 Error@1 49.130

==>>[2025-10-26 12:02:24] [Epoch=038/040] [Need: 00:02:13] [LR=0.0010] [Best : Accuracy=53.46, Error=46.54]
  Epoch: [038][000/500]   Time 18.136 (18.136)   Data 17.862 (17.862)   Loss 1.8727 (1.8727)   Prec@1 58.000 (58.000)   Prec@5 92.000 (92.000)   [2025-10-26 12:02:42]
  Epoch: [038][100/500]   Time 0.052 (0.232)   Data 0.000 (0.177)   Loss 1.9162 (1.9420)   Prec@1 56.000 (51.772)   Prec@5 93.000 (91.634)   [2025-10-26 12:02:48]
  Epoch: [038][200/500]   Time 0.056 (0.144)   Data 0.000 (0.089)   Loss 1.9364 (1.9409)   Prec@1 51.000 (51.945)   Prec@5 91.000 (91.428)   [2025-10-26 12:02:53]
  Epoch: [038][300/500]   Time 0.055 (0.114)   Data 0.000 (0.060)   Loss 1.8645 (1.9383)   Prec@1 62.000 (52.229)   Prec@5 90.000 (91.578)   [2025-10-26 12:02:59]
  Epoch: [038][400/500]   Time 0.055 (0.100)   Data 0.000 (0.045)   Loss 1.9378 (1.9356)   Prec@1 52.000 (52.594)   Prec@5 95.000 (91.586)   [2025-10-26 12:03:04]
  **Train** Prec@1 52.726 Prec@5 91.638 Error@1 47.274
  **Test** Prec@1 53.360 Prec@5 92.320 Error@1 46.640

==>>[2025-10-26 12:03:30] [Epoch=039/040] [Need: 00:01:06] [LR=0.0010] [Best : Accuracy=53.46, Error=46.54]
  Epoch: [039][000/500]   Time 18.030 (18.030)   Data 17.772 (17.772)   Loss 1.8851 (1.8851)   Prec@1 60.000 (60.000)   Prec@5 94.000 (94.000)   [2025-10-26 12:03:48]
  Epoch: [039][100/500]   Time 0.054 (0.231)   Data 0.000 (0.176)   Loss 1.8397 (1.9321)   Prec@1 65.000 (53.139)   Prec@5 94.000 (91.614)   [2025-10-26 12:03:53]
  Epoch: [039][200/500]   Time 0.054 (0.143)   Data 0.000 (0.089)   Loss 1.9355 (1.9298)   Prec@1 51.000 (53.109)   Prec@5 88.000 (91.746)   [2025-10-26 12:03:59]
  Epoch: [039][300/500]   Time 0.057 (0.114)   Data 0.001 (0.059)   Loss 1.9051 (1.9303)   Prec@1 57.000 (53.000)   Prec@5 95.000 (91.804)   [2025-10-26 12:04:05]
  Epoch: [039][400/500]   Time 0.059 (0.100)   Data 0.000 (0.044)   Loss 1.9321 (1.9311)   Prec@1 53.000 (52.945)   Prec@5 93.000 (91.853)   [2025-10-26 12:04:10]
  **Train** Prec@1 52.918 Prec@5 91.850 Error@1 47.082
  **Test** Prec@1 51.720 Prec@5 91.830 Error@1 48.280
