save path : ./save/resnet9_quan/nominal_0.01/results/5555
{'data_path': './dataset', 'arch': 'resnet9_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': False, 'clipping_coeff': 0.0, 'learning_rate': 0.01, 'manualSeed': 5555, 'save_path': './save/resnet9_quan/nominal_0.01/results/5555', 'enable_bfa': True, 'resume': './save/resnet9_quan/nominal_0.01/model_best.pth.tar', 'quan_bitwidth': None, 'reset_weight': True, 'evaluate': True, 'n_iter': 30, 'fine_tune': True, 'model_only': False, 'random_bfa': False, 'use_cuda': True}
Random Seed: 5555
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'resnet9_quan'
=> network :
 Sequential(
  (0): Sequential(
    (0): quan_Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (1): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (2): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (4): Dropout2d(p=0.2, inplace=False)
  (5): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (skip): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (6): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Dropout2d(p=0.3, inplace=False)
  (9): Sequential(
    (0): quan_Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (10): Sequential(
    (0): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (12): Dropout2d(p=0.3, inplace=False)
  (13): Sequential(
    (0): quan_Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (14): AdaptiveAvgPool2d(output_size=(1, 1))
  (15): Flatten()
  (16): quan_Linear(in_features=64, out_features=128, bias=False)
  (17): ReLU(inplace=True)
  (18): Dropout(p=0.35, inplace=False)
  (19): quan_Linear(in_features=128, out_features=10, bias=False)
  (20): Softmax(dim=1)
)
=> loading checkpoint './save/resnet9_quan/nominal_0.01/model_best.pth.tar'
=> loaded checkpoint './save/resnet9_quan/nominal_0.01/model_best.pth.tar' (epoch 0)
  **Test** Prec@1 51.140 Prec@5 89.640 Error@1 48.860
k_top=100
Attack_sample=100
************** ATTACK iteration *****************
Iteration: [001/030]   Attack Time 1.870 (1.870)  [2025-10-29 10:46:09]
loss before attack: 1.6055
loss after attack: 1.8768
bit flips: 1
hamming_dist: 1
  **Test** Prec@1 39.680 Prec@5 80.280 Error@1 60.320
iteration Time 26.199 (26.199)
************** ATTACK iteration *****************
Iteration: [002/030]   Attack Time 0.486 (1.178)  [2025-10-29 10:46:35]
loss before attack: 1.8768
loss after attack: 2.0772
bit flips: 2
hamming_dist: 2
  **Test** Prec@1 26.630 Prec@5 69.030 Error@1 73.370
iteration Time 25.845 (26.022)
************** ATTACK iteration *****************
Iteration: [003/030]   Attack Time 0.285 (0.880)  [2025-10-29 10:47:01]
loss before attack: 2.0772
loss after attack: 2.1850
bit flips: 3
hamming_dist: 3
  **Test** Prec@1 20.130 Prec@5 61.540 Error@1 79.870
iteration Time 23.857 (25.300)
************** ATTACK iteration *****************
Iteration: [004/030]   Attack Time 0.285 (0.732)  [2025-10-29 10:47:26]
loss before attack: 2.1850
loss after attack: 2.3147
bit flips: 4
hamming_dist: 4
  **Test** Prec@1 14.420 Prec@5 55.680 Error@1 85.580
iteration Time 24.486 (25.097)
************** ATTACK iteration *****************
Iteration: [005/030]   Attack Time 0.276 (0.640)  [2025-10-29 10:47:50]
loss before attack: 2.3147
loss after attack: 2.3507
bit flips: 5
hamming_dist: 5
  **Test** Prec@1 13.020 Prec@5 52.600 Error@1 86.980
iteration Time 30.671 (26.211)
************** ATTACK iteration *****************
Iteration: [006/030]   Attack Time 0.281 (0.581)  [2025-10-29 10:48:21]
loss before attack: 2.3507
loss after attack: 2.3636
bit flips: 6
hamming_dist: 6
  **Test** Prec@1 12.790 Prec@5 52.030 Error@1 87.210
iteration Time 26.075 (26.189)
************** ATTACK iteration *****************
Iteration: [007/030]   Attack Time 0.311 (0.542)  [2025-10-29 10:48:48]
loss before attack: 2.3636
loss after attack: 2.3812
bit flips: 7
hamming_dist: 7
  **Test** Prec@1 15.180 Prec@5 54.680 Error@1 84.820
iteration Time 25.668 (26.114)
************** ATTACK iteration *****************
Iteration: [008/030]   Attack Time 0.258 (0.507)  [2025-10-29 10:49:14]
loss before attack: 2.3812
loss after attack: 2.3954
bit flips: 8
hamming_dist: 8
  **Test** Prec@1 13.820 Prec@5 51.700 Error@1 86.180
iteration Time 23.884 (25.835)
************** ATTACK iteration *****************
Iteration: [009/030]   Attack Time 0.212 (0.474)  [2025-10-29 10:49:38]
loss before attack: 2.3954
loss after attack: 2.4034
bit flips: 9
hamming_dist: 9
  **Test** Prec@1 13.140 Prec@5 50.960 Error@1 86.860
iteration Time 25.318 (25.778)
************** ATTACK iteration *****************
Iteration: [010/030]   Attack Time 0.225 (0.449)  [2025-10-29 10:50:03]
loss before attack: 2.4034
loss after attack: 2.4132
bit flips: 10
hamming_dist: 10
  **Test** Prec@1 11.950 Prec@5 49.570 Error@1 88.050
iteration Time 26.441 (25.844)
************** ATTACK iteration *****************
Iteration: [011/030]   Attack Time 0.224 (0.428)  [2025-10-29 10:50:30]
loss before attack: 2.4132
loss after attack: 2.4212
bit flips: 11
hamming_dist: 11
  **Test** Prec@1 11.530 Prec@5 48.900 Error@1 88.470
iteration Time 30.979 (26.311)
************** ATTACK iteration *****************
Iteration: [012/030]   Attack Time 1.133 (0.487)  [2025-10-29 10:51:02]
loss before attack: 2.4212
loss after attack: 2.4234
bit flips: 12
hamming_dist: 12
  **Test** Prec@1 11.500 Prec@5 49.170 Error@1 88.500
iteration Time 31.316 (26.728)
************** ATTACK iteration *****************
