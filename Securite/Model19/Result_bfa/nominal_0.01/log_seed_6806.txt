save path : ./save/resnet9_quan/nominal_0.01
{'data_path': './dataset', 'arch': 'resnet9_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': False, 'clipping_coeff': 0.0, 'learning_rate': 0.01, 'manualSeed': 6806, 'save_path': './save/resnet9_quan/nominal_0.01', 'enable_bfa': False, 'resume': '', 'quan_bitwidth': None, 'reset_weight': False, 'evaluate': False, 'n_iter': 30, 'model_only': False, 'random_bfa': False, 'use_cuda': True}
Random Seed: 6806
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'resnet9_quan'
=> network :
 Sequential(
  (0): Sequential(
    (0): quan_Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (1): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (2): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (4): Dropout2d(p=0.2, inplace=False)
  (5): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (skip): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (6): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Dropout2d(p=0.3, inplace=False)
  (9): Sequential(
    (0): quan_Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (10): Sequential(
    (0): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (12): Dropout2d(p=0.3, inplace=False)
  (13): Sequential(
    (0): quan_Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (14): AdaptiveAvgPool2d(output_size=(1, 1))
  (15): Flatten()
  (16): quan_Linear(in_features=64, out_features=128, bias=False)
  (17): ReLU(inplace=True)
  (18): Dropout(p=0.35, inplace=False)
  (19): quan_Linear(in_features=128, out_features=10, bias=False)
  (20): Softmax(dim=1)
)
=> do not use any checkpoint for resnet9_quan model

==>>[2025-10-26 12:04:44] [Epoch=000/040] [Need: 00:00:00] [LR=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/500]   Time 19.193 (19.193)   Data 18.084 (18.084)   Loss 2.3021 (2.3021)   Prec@1 10.000 (10.000)   Prec@5 54.000 (54.000)   [2025-10-26 12:05:04]
  Epoch: [000][100/500]   Time 0.056 (0.243)   Data 0.000 (0.179)   Loss 2.3026 (2.3022)   Prec@1 7.000 (10.634)   Prec@5 52.000 (51.594)   [2025-10-26 12:05:09]
  Epoch: [000][200/500]   Time 0.057 (0.149)   Data 0.000 (0.090)   Loss 2.3007 (2.3017)   Prec@1 11.000 (10.607)   Prec@5 54.000 (52.259)   [2025-10-26 12:05:14]
  Epoch: [000][300/500]   Time 0.057 (0.118)   Data 0.000 (0.060)   Loss 2.2977 (2.3010)   Prec@1 12.000 (10.814)   Prec@5 59.000 (53.020)   [2025-10-26 12:05:20]
  Epoch: [000][400/500]   Time 0.057 (0.102)   Data 0.000 (0.045)   Loss 2.2886 (2.2996)   Prec@1 17.000 (11.314)   Prec@5 52.000 (53.788)   [2025-10-26 12:05:25]
  **Train** Prec@1 12.114 Prec@5 54.912 Error@1 87.886
  **Test** Prec@1 13.320 Prec@5 55.360 Error@1 86.680
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 12:05:52] [Epoch=001/040] [Need: 00:43:44] [LR=0.0100] [Best : Accuracy=13.32, Error=86.68]
  Epoch: [001][000/500]   Time 18.085 (18.085)   Data 17.898 (17.898)   Loss 2.2991 (2.2991)   Prec@1 13.000 (13.000)   Prec@5 60.000 (60.000)   [2025-10-26 12:06:10]
  Epoch: [001][100/500]   Time 0.052 (0.232)   Data 0.000 (0.177)   Loss 2.2695 (2.2694)   Prec@1 14.000 (16.891)   Prec@5 61.000 (64.851)   [2025-10-26 12:06:15]
  Epoch: [001][200/500]   Time 0.055 (0.143)   Data 0.000 (0.089)   Loss 2.2795 (2.2648)   Prec@1 20.000 (17.388)   Prec@5 70.000 (67.726)   [2025-10-26 12:06:21]
  Epoch: [001][300/500]   Time 0.055 (0.114)   Data 0.000 (0.060)   Loss 2.2417 (2.2598)   Prec@1 19.000 (17.548)   Prec@5 76.000 (69.867)   [2025-10-26 12:06:26]
  Epoch: [001][400/500]   Time 0.056 (0.100)   Data 0.001 (0.045)   Loss 2.2320 (2.2555)   Prec@1 21.000 (17.733)   Prec@5 78.000 (70.995)   [2025-10-26 12:06:32]
  **Train** Prec@1 17.752 Prec@5 71.646 Error@1 82.248
  **Test** Prec@1 15.400 Prec@5 60.640 Error@1 84.600
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 12:06:58] [Epoch=002/040] [Need: 00:42:09] [LR=0.0100] [Best : Accuracy=15.40, Error=84.60]
  Epoch: [002][000/500]   Time 18.134 (18.134)   Data 17.886 (17.886)   Loss 2.1734 (2.1734)   Prec@1 28.000 (28.000)   Prec@5 84.000 (84.000)   [2025-10-26 12:07:16]
  Epoch: [002][100/500]   Time 0.053 (0.232)   Data 0.000 (0.177)   Loss 2.2148 (2.2315)   Prec@1 24.000 (18.802)   Prec@5 80.000 (75.426)   [2025-10-26 12:07:21]
  Epoch: [002][200/500]   Time 0.057 (0.144)   Data 0.000 (0.089)   Loss 2.2463 (2.2320)   Prec@1 14.000 (18.378)   Prec@5 73.000 (75.527)   [2025-10-26 12:07:27]
  Epoch: [002][300/500]   Time 0.054 (0.114)   Data 0.000 (0.060)   Loss 2.2389 (2.2316)   Prec@1 17.000 (18.375)   Prec@5 70.000 (75.478)   [2025-10-26 12:07:32]
  Epoch: [002][400/500]   Time 0.056 (0.100)   Data 0.000 (0.045)   Loss 2.2446 (2.2309)   Prec@1 14.000 (18.444)   Prec@5 83.000 (75.596)   [2025-10-26 12:07:38]
  **Train** Prec@1 18.486 Prec@5 75.654 Error@1 81.514
  **Test** Prec@1 15.680 Prec@5 59.940 Error@1 84.320
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 12:08:04] [Epoch=003/040] [Need: 00:40:56] [LR=0.0100] [Best : Accuracy=15.68, Error=84.32]
  Epoch: [003][000/500]   Time 18.981 (18.981)   Data 18.695 (18.695)   Loss 2.2573 (2.2573)   Prec@1 17.000 (17.000)   Prec@5 76.000 (76.000)   [2025-10-26 12:08:23]
  Epoch: [003][100/500]   Time 0.052 (0.241)   Data 0.000 (0.185)   Loss 2.2408 (2.2248)   Prec@1 17.000 (19.238)   Prec@5 76.000 (76.010)   [2025-10-26 12:08:28]
  Epoch: [003][200/500]   Time 0.055 (0.148)   Data 0.000 (0.093)   Loss 2.2042 (2.2237)   Prec@1 20.000 (19.179)   Prec@5 81.000 (75.950)   [2025-10-26 12:08:34]
  Epoch: [003][300/500]   Time 0.058 (0.118)   Data 0.000 (0.062)   Loss 2.2235 (2.2235)   Prec@1 17.000 (19.193)   Prec@5 72.000 (76.226)   [2025-10-26 12:08:39]
  Epoch: [003][400/500]   Time 0.055 (0.103)   Data 0.000 (0.047)   Loss 2.1728 (2.2231)   Prec@1 30.000 (19.187)   Prec@5 73.000 (76.424)   [2025-10-26 12:08:45]
  **Train** Prec@1 19.286 Prec@5 76.774 Error@1 80.714
  **Test** Prec@1 11.120 Prec@5 53.700 Error@1 88.880

==>>[2025-10-26 12:09:12] [Epoch=004/040] [Need: 00:40:10] [LR=0.0100] [Best : Accuracy=15.68, Error=84.32]
  Epoch: [004][000/500]   Time 19.621 (19.621)   Data 19.334 (19.334)   Loss 2.2100 (2.2100)   Prec@1 17.000 (17.000)   Prec@5 76.000 (76.000)   [2025-10-26 12:09:32]
  Epoch: [004][100/500]   Time 0.052 (0.247)   Data 0.000 (0.192)   Loss 2.2265 (2.2173)   Prec@1 16.000 (19.812)   Prec@5 76.000 (77.822)   [2025-10-26 12:09:37]
  Epoch: [004][200/500]   Time 0.056 (0.152)   Data 0.000 (0.096)   Loss 2.1908 (2.2170)   Prec@1 27.000 (20.050)   Prec@5 81.000 (78.050)   [2025-10-26 12:09:43]
  Epoch: [004][300/500]   Time 0.056 (0.120)   Data 0.000 (0.064)   Loss 2.2079 (2.2154)   Prec@1 18.000 (20.522)   Prec@5 80.000 (77.870)   [2025-10-26 12:09:49]
  Epoch: [004][400/500]   Time 0.056 (0.104)   Data 0.000 (0.048)   Loss 2.2392 (2.2140)   Prec@1 20.000 (20.970)   Prec@5 71.000 (77.938)   [2025-10-26 12:09:54]
  **Train** Prec@1 21.268 Prec@5 78.016 Error@1 78.732
  **Test** Prec@1 21.550 Prec@5 75.460 Error@1 78.450
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 12:10:20] [Epoch=005/040] [Need: 00:39:08] [LR=0.0100] [Best : Accuracy=21.55, Error=78.45]
  Epoch: [005][000/500]   Time 18.337 (18.337)   Data 18.063 (18.063)   Loss 2.1954 (2.1954)   Prec@1 26.000 (26.000)   Prec@5 83.000 (83.000)   [2025-10-26 12:10:38]
  Epoch: [005][100/500]   Time 0.053 (0.236)   Data 0.000 (0.179)   Loss 2.1833 (2.1931)   Prec@1 25.000 (23.871)   Prec@5 81.000 (78.802)   [2025-10-26 12:10:44]
  Epoch: [005][200/500]   Time 0.058 (0.146)   Data 0.000 (0.090)   Loss 2.2422 (2.1953)   Prec@1 16.000 (23.512)   Prec@5 80.000 (78.791)   [2025-10-26 12:10:49]
  Epoch: [005][300/500]   Time 0.056 (0.116)   Data 0.000 (0.060)   Loss 2.1720 (2.1930)   Prec@1 22.000 (23.734)   Prec@5 80.000 (78.870)   [2025-10-26 12:10:55]
  Epoch: [005][400/500]   Time 0.052 (0.101)   Data 0.000 (0.045)   Loss 2.1890 (2.1910)   Prec@1 25.000 (23.950)   Prec@5 75.000 (79.050)   [2025-10-26 12:11:01]
  **Train** Prec@1 24.174 Prec@5 79.054 Error@1 75.826
  **Test** Prec@1 22.710 Prec@5 74.650 Error@1 77.290
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 12:11:27] [Epoch=006/040] [Need: 00:37:58] [LR=0.0100] [Best : Accuracy=22.71, Error=77.29]
  Epoch: [006][000/500]   Time 18.167 (18.167)   Data 17.960 (17.960)   Loss 2.1735 (2.1735)   Prec@1 24.000 (24.000)   Prec@5 77.000 (77.000)   [2025-10-26 12:11:45]
  Epoch: [006][100/500]   Time 0.054 (0.232)   Data 0.000 (0.178)   Loss 2.2089 (2.1819)   Prec@1 23.000 (25.178)   Prec@5 77.000 (79.871)   [2025-10-26 12:11:50]
  Epoch: [006][200/500]   Time 0.052 (0.144)   Data 0.000 (0.089)   Loss 2.1450 (2.1815)   Prec@1 35.000 (25.259)   Prec@5 84.000 (79.607)   [2025-10-26 12:11:56]
  Epoch: [006][300/500]   Time 0.056 (0.114)   Data 0.000 (0.060)   Loss 2.2349 (2.1807)   Prec@1 17.000 (25.462)   Prec@5 81.000 (79.678)   [2025-10-26 12:12:01]
  Epoch: [006][400/500]   Time 0.057 (0.100)   Data 0.000 (0.045)   Loss 2.1591 (2.1785)   Prec@1 26.000 (25.726)   Prec@5 79.000 (79.671)   [2025-10-26 12:12:07]
  **Train** Prec@1 26.048 Prec@5 79.690 Error@1 73.952
  **Test** Prec@1 27.860 Prec@5 80.800 Error@1 72.140
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 12:12:32] [Epoch=007/040] [Need: 00:36:45] [LR=0.0100] [Best : Accuracy=27.86, Error=72.14]
  Epoch: [007][000/500]   Time 17.920 (17.920)   Data 17.632 (17.632)   Loss 2.1540 (2.1540)   Prec@1 30.000 (30.000)   Prec@5 81.000 (81.000)   [2025-10-26 12:12:50]
  Epoch: [007][100/500]   Time 0.056 (0.230)   Data 0.000 (0.175)   Loss 2.1852 (2.1683)   Prec@1 25.000 (27.030)   Prec@5 78.000 (79.683)   [2025-10-26 12:12:56]
  Epoch: [007][200/500]   Time 0.054 (0.143)   Data 0.000 (0.088)   Loss 2.1555 (2.1656)   Prec@1 30.000 (27.577)   Prec@5 81.000 (79.881)   [2025-10-26 12:13:01]
  Epoch: [007][300/500]   Time 0.055 (0.114)   Data 0.000 (0.059)   Loss 2.1702 (2.1630)   Prec@1 28.000 (27.860)   Prec@5 77.000 (79.904)   [2025-10-26 12:13:07]
  Epoch: [007][400/500]   Time 0.059 (0.100)   Data 0.000 (0.044)   Loss 2.1971 (2.1624)   Prec@1 22.000 (28.000)   Prec@5 70.000 (79.743)   [2025-10-26 12:13:12]
  **Train** Prec@1 28.296 Prec@5 79.732 Error@1 71.704
  **Test** Prec@1 23.050 Prec@5 73.340 Error@1 76.950

==>>[2025-10-26 12:13:38] [Epoch=008/040] [Need: 00:35:33] [LR=0.0100] [Best : Accuracy=27.86, Error=72.14]
  Epoch: [008][000/500]   Time 18.271 (18.271)   Data 18.017 (18.017)   Loss 2.1714 (2.1714)   Prec@1 29.000 (29.000)   Prec@5 79.000 (79.000)   [2025-10-26 12:13:56]
  Epoch: [008][100/500]   Time 0.052 (0.233)   Data 0.000 (0.179)   Loss 2.1587 (2.1478)   Prec@1 30.000 (29.782)   Prec@5 75.000 (79.218)   [2025-10-26 12:14:01]
  Epoch: [008][200/500]   Time 0.057 (0.145)   Data 0.000 (0.090)   Loss 2.1387 (2.1453)   Prec@1 29.000 (30.085)   Prec@5 82.000 (79.587)   [2025-10-26 12:14:07]
  Epoch: [008][300/500]   Time 0.058 (0.115)   Data 0.000 (0.060)   Loss 2.1654 (2.1430)   Prec@1 26.000 (30.405)   Prec@5 76.000 (80.003)   [2025-10-26 12:14:12]
  Epoch: [008][400/500]   Time 0.057 (0.100)   Data 0.000 (0.045)   Loss 2.1398 (2.1399)   Prec@1 32.000 (30.731)   Prec@5 86.000 (80.237)   [2025-10-26 12:14:18]
  **Train** Prec@1 30.802 Prec@5 80.146 Error@1 69.198
  **Test** Prec@1 33.670 Prec@5 81.970 Error@1 66.330
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 12:14:44] [Epoch=009/040] [Need: 00:34:25] [LR=0.0100] [Best : Accuracy=33.67, Error=66.33]
  Epoch: [009][000/500]   Time 18.491 (18.491)   Data 18.236 (18.236)   Loss 2.1250 (2.1250)   Prec@1 32.000 (32.000)   Prec@5 84.000 (84.000)   [2025-10-26 12:15:03]
  Epoch: [009][100/500]   Time 0.053 (0.235)   Data 0.001 (0.181)   Loss 2.1124 (2.1247)   Prec@1 33.000 (32.475)   Prec@5 82.000 (80.564)   [2025-10-26 12:15:08]
  Epoch: [009][200/500]   Time 0.058 (0.145)   Data 0.000 (0.091)   Loss 2.1149 (2.1260)   Prec@1 32.000 (32.323)   Prec@5 83.000 (80.433)   [2025-10-26 12:15:13]
  Epoch: [009][300/500]   Time 0.056 (0.115)   Data 0.000 (0.061)   Loss 2.1572 (2.1240)   Prec@1 27.000 (32.738)   Prec@5 74.000 (80.548)   [2025-10-26 12:15:19]
  Epoch: [009][400/500]   Time 0.058 (0.101)   Data 0.000 (0.046)   Loss 2.1277 (2.1228)   Prec@1 39.000 (32.953)   Prec@5 76.000 (80.761)   [2025-10-26 12:15:24]
  **Train** Prec@1 33.148 Prec@5 80.790 Error@1 66.852
  **Test** Prec@1 34.420 Prec@5 82.480 Error@1 65.580
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 12:15:51] [Epoch=010/040] [Need: 00:33:19] [LR=0.0100] [Best : Accuracy=34.42, Error=65.58]
  Epoch: [010][000/500]   Time 18.140 (18.140)   Data 17.864 (17.864)   Loss 2.1469 (2.1469)   Prec@1 31.000 (31.000)   Prec@5 76.000 (76.000)   [2025-10-26 12:16:09]
  Epoch: [010][100/500]   Time 0.052 (0.232)   Data 0.000 (0.177)   Loss 2.1462 (2.1098)   Prec@1 31.000 (34.624)   Prec@5 77.000 (80.455)   [2025-10-26 12:16:14]
  Epoch: [010][200/500]   Time 0.054 (0.144)   Data 0.000 (0.089)   Loss 2.1138 (2.1027)   Prec@1 37.000 (35.502)   Prec@5 75.000 (80.866)   [2025-10-26 12:16:20]
  Epoch: [010][300/500]   Time 0.057 (0.114)   Data 0.000 (0.059)   Loss 2.1013 (2.1028)   Prec@1 39.000 (35.359)   Prec@5 83.000 (80.997)   [2025-10-26 12:16:25]
  Epoch: [010][400/500]   Time 0.056 (0.100)   Data 0.000 (0.045)   Loss 2.1342 (2.0999)   Prec@1 31.000 (35.676)   Prec@5 81.000 (81.307)   [2025-10-26 12:16:31]
  **Train** Prec@1 35.608 Prec@5 81.410 Error@1 64.392
  **Test** Prec@1 31.980 Prec@5 79.250 Error@1 68.020

==>>[2025-10-26 12:16:57] [Epoch=011/040] [Need: 00:32:10] [LR=0.0100] [Best : Accuracy=34.42, Error=65.58]
  Epoch: [011][000/500]   Time 18.055 (18.055)   Data 17.776 (17.776)   Loss 2.0969 (2.0969)   Prec@1 35.000 (35.000)   Prec@5 78.000 (78.000)   [2025-10-26 12:17:15]
  Epoch: [011][100/500]   Time 0.055 (0.231)   Data 0.000 (0.176)   Loss 2.1022 (2.0854)   Prec@1 35.000 (37.208)   Prec@5 84.000 (82.941)   [2025-10-26 12:17:20]
  Epoch: [011][200/500]   Time 0.055 (0.143)   Data 0.000 (0.089)   Loss 2.0317 (2.0857)   Prec@1 42.000 (37.035)   Prec@5 85.000 (82.692)   [2025-10-26 12:17:26]
  Epoch: [011][300/500]   Time 0.058 (0.114)   Data 0.001 (0.059)   Loss 2.0062 (2.0828)   Prec@1 47.000 (37.419)   Prec@5 86.000 (82.807)   [2025-10-26 12:17:31]
  Epoch: [011][400/500]   Time 0.053 (0.100)   Data 0.000 (0.044)   Loss 2.1009 (2.0819)   Prec@1 34.000 (37.439)   Prec@5 82.000 (82.845)   [2025-10-26 12:17:37]
  **Train** Prec@1 37.666 Prec@5 82.926 Error@1 62.334
  **Test** Prec@1 28.720 Prec@5 80.630 Error@1 71.280

==>>[2025-10-26 12:18:02] [Epoch=012/040] [Need: 00:31:01] [LR=0.0100] [Best : Accuracy=34.42, Error=65.58]
  Epoch: [012][000/500]   Time 18.030 (18.030)   Data 17.809 (17.809)   Loss 2.0486 (2.0486)   Prec@1 41.000 (41.000)   Prec@5 91.000 (91.000)   [2025-10-26 12:18:21]
  Epoch: [012][100/500]   Time 0.054 (0.231)   Data 0.000 (0.176)   Loss 2.0173 (2.0665)   Prec@1 45.000 (39.000)   Prec@5 90.000 (83.723)   [2025-10-26 12:18:26]
  Epoch: [012][200/500]   Time 0.053 (0.143)   Data 0.000 (0.089)   Loss 2.0149 (2.0698)   Prec@1 47.000 (38.567)   Prec@5 83.000 (83.721)   [2025-10-26 12:18:31]
  Epoch: [012][300/500]   Time 0.055 (0.114)   Data 0.000 (0.059)   Loss 2.0214 (2.0683)   Prec@1 45.000 (38.724)   Prec@5 83.000 (83.748)   [2025-10-26 12:18:37]
  Epoch: [012][400/500]   Time 0.056 (0.100)   Data 0.000 (0.045)   Loss 2.0381 (2.0667)   Prec@1 41.000 (38.848)   Prec@5 88.000 (83.830)   [2025-10-26 12:18:42]
  **Train** Prec@1 38.940 Prec@5 84.020 Error@1 61.060
  **Test** Prec@1 36.840 Prec@5 85.650 Error@1 63.160
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 12:19:08] [Epoch=013/040] [Need: 00:29:53] [LR=0.0100] [Best : Accuracy=36.84, Error=63.16]
  Epoch: [013][000/500]   Time 18.192 (18.192)   Data 17.919 (17.919)   Loss 2.0479 (2.0479)   Prec@1 38.000 (38.000)   Prec@5 87.000 (87.000)   [2025-10-26 12:19:26]
  Epoch: [013][100/500]   Time 0.057 (0.233)   Data 0.001 (0.178)   Loss 2.0992 (2.0592)   Prec@1 33.000 (39.307)   Prec@5 80.000 (84.931)   [2025-10-26 12:19:32]
  Epoch: [013][200/500]   Time 0.056 (0.144)   Data 0.000 (0.089)   Loss 1.9932 (2.0513)   Prec@1 45.000 (40.323)   Prec@5 84.000 (85.015)   [2025-10-26 12:19:37]
  Epoch: [013][300/500]   Time 0.059 (0.115)   Data 0.000 (0.060)   Loss 2.0216 (2.0485)   Prec@1 45.000 (40.654)   Prec@5 88.000 (85.193)   [2025-10-26 12:19:43]
  Epoch: [013][400/500]   Time 0.058 (0.100)   Data 0.000 (0.045)   Loss 2.0404 (2.0482)   Prec@1 42.000 (40.651)   Prec@5 89.000 (85.377)   [2025-10-26 12:19:48]
  **Train** Prec@1 40.694 Prec@5 85.386 Error@1 59.306
  **Test** Prec@1 40.390 Prec@5 87.800 Error@1 59.610
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 12:20:14] [Epoch=014/040] [Need: 00:28:46] [LR=0.0100] [Best : Accuracy=40.39, Error=59.61]
  Epoch: [014][000/500]   Time 18.139 (18.139)   Data 17.852 (17.852)   Loss 2.0304 (2.0304)   Prec@1 44.000 (44.000)   Prec@5 82.000 (82.000)   [2025-10-26 12:20:32]
  Epoch: [014][100/500]   Time 0.056 (0.233)   Data 0.000 (0.177)   Loss 2.0172 (2.0382)   Prec@1 46.000 (41.554)   Prec@5 90.000 (86.079)   [2025-10-26 12:20:38]
  Epoch: [014][200/500]   Time 0.057 (0.145)   Data 0.000 (0.089)   Loss 2.0714 (2.0387)   Prec@1 40.000 (41.572)   Prec@5 89.000 (86.279)   [2025-10-26 12:20:43]
  Epoch: [014][300/500]   Time 0.057 (0.115)   Data 0.001 (0.059)   Loss 2.0418 (2.0378)   Prec@1 42.000 (41.668)   Prec@5 86.000 (86.153)   [2025-10-26 12:20:49]
  Epoch: [014][400/500]   Time 0.054 (0.100)   Data 0.000 (0.045)   Loss 2.0203 (2.0369)   Prec@1 42.000 (41.815)   Prec@5 88.000 (86.160)   [2025-10-26 12:20:54]
  **Train** Prec@1 41.946 Prec@5 86.240 Error@1 58.054
  **Test** Prec@1 37.810 Prec@5 85.020 Error@1 62.190

==>>[2025-10-26 12:21:20] [Epoch=015/040] [Need: 00:27:39] [LR=0.0100] [Best : Accuracy=40.39, Error=59.61]
  Epoch: [015][000/500]   Time 18.133 (18.133)   Data 17.898 (17.898)   Loss 2.0477 (2.0477)   Prec@1 41.000 (41.000)   Prec@5 89.000 (89.000)   [2025-10-26 12:21:38]
  Epoch: [015][100/500]   Time 0.054 (0.232)   Data 0.000 (0.177)   Loss 2.0519 (2.0345)   Prec@1 39.000 (42.119)   Prec@5 86.000 (85.950)   [2025-10-26 12:21:44]
  Epoch: [015][200/500]   Time 0.055 (0.144)   Data 0.000 (0.089)   Loss 2.0361 (2.0287)   Prec@1 44.000 (42.711)   Prec@5 82.000 (86.209)   [2025-10-26 12:21:49]
  Epoch: [015][300/500]   Time 0.055 (0.115)   Data 0.000 (0.060)   Loss 2.0464 (2.0287)   Prec@1 41.000 (42.721)   Prec@5 78.000 (86.319)   [2025-10-26 12:21:55]
  Epoch: [015][400/500]   Time 0.055 (0.100)   Data 0.000 (0.045)   Loss 2.0585 (2.0286)   Prec@1 37.000 (42.738)   Prec@5 86.000 (86.501)   [2025-10-26 12:22:00]
  **Train** Prec@1 42.656 Prec@5 86.546 Error@1 57.344
  **Test** Prec@1 42.500 Prec@5 87.790 Error@1 57.500
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 12:22:26] [Epoch=016/040] [Need: 00:26:32] [LR=0.0100] [Best : Accuracy=42.50, Error=57.50]
  Epoch: [016][000/500]   Time 18.226 (18.226)   Data 17.951 (17.951)   Loss 2.0708 (2.0708)   Prec@1 37.000 (37.000)   Prec@5 84.000 (84.000)   [2025-10-26 12:22:44]
  Epoch: [016][100/500]   Time 0.055 (0.233)   Data 0.001 (0.178)   Loss 1.9525 (2.0127)   Prec@1 50.000 (44.495)   Prec@5 90.000 (87.663)   [2025-10-26 12:22:50]
  Epoch: [016][200/500]   Time 0.058 (0.144)   Data 0.000 (0.089)   Loss 2.0740 (2.0131)   Prec@1 38.000 (44.443)   Prec@5 83.000 (87.065)   [2025-10-26 12:22:55]
  Epoch: [016][300/500]   Time 0.054 (0.115)   Data 0.000 (0.060)   Loss 2.0056 (2.0160)   Prec@1 46.000 (44.110)   Prec@5 85.000 (87.003)   [2025-10-26 12:23:01]
  Epoch: [016][400/500]   Time 0.059 (0.100)   Data 0.000 (0.045)   Loss 2.0293 (2.0179)   Prec@1 40.000 (43.858)   Prec@5 91.000 (86.893)   [2025-10-26 12:23:06]
  **Train** Prec@1 43.798 Prec@5 86.906 Error@1 56.202
  **Test** Prec@1 39.540 Prec@5 84.390 Error@1 60.460

==>>[2025-10-26 12:23:32] [Epoch=017/040] [Need: 00:25:25] [LR=0.0100] [Best : Accuracy=42.50, Error=57.50]
  Epoch: [017][000/500]   Time 18.124 (18.124)   Data 17.937 (17.937)   Loss 1.9686 (1.9686)   Prec@1 49.000 (49.000)   Prec@5 90.000 (90.000)   [2025-10-26 12:23:50]
  Epoch: [017][100/500]   Time 0.053 (0.232)   Data 0.000 (0.178)   Loss 2.0232 (2.0170)   Prec@1 42.000 (43.703)   Prec@5 85.000 (87.663)   [2025-10-26 12:23:56]
  Epoch: [017][200/500]   Time 0.059 (0.144)   Data 0.000 (0.089)   Loss 2.0105 (2.0147)   Prec@1 47.000 (43.990)   Prec@5 88.000 (87.622)   [2025-10-26 12:24:01]
  Epoch: [017][300/500]   Time 0.052 (0.114)   Data 0.000 (0.060)   Loss 1.9782 (2.0133)   Prec@1 45.000 (44.176)   Prec@5 88.000 (87.415)   [2025-10-26 12:24:07]
  Epoch: [017][400/500]   Time 0.054 (0.100)   Data 0.000 (0.045)   Loss 2.0533 (2.0133)   Prec@1 37.000 (44.214)   Prec@5 89.000 (87.476)   [2025-10-26 12:24:12]
  **Train** Prec@1 44.472 Prec@5 87.522 Error@1 55.528
  **Test** Prec@1 36.800 Prec@5 83.500 Error@1 63.200

==>>[2025-10-26 12:24:38] [Epoch=018/040] [Need: 00:24:18] [LR=0.0100] [Best : Accuracy=42.50, Error=57.50]
  Epoch: [018][000/500]   Time 18.727 (18.727)   Data 18.447 (18.447)   Loss 1.9440 (1.9440)   Prec@1 51.000 (51.000)   Prec@5 87.000 (87.000)   [2025-10-26 12:24:57]
  Epoch: [018][100/500]   Time 0.053 (0.238)   Data 0.000 (0.183)   Loss 2.0470 (2.0120)   Prec@1 39.000 (44.505)   Prec@5 86.000 (87.248)   [2025-10-26 12:25:02]
  Epoch: [018][200/500]   Time 0.058 (0.147)   Data 0.000 (0.092)   Loss 1.9623 (2.0127)   Prec@1 53.000 (44.368)   Prec@5 93.000 (87.428)   [2025-10-26 12:25:07]
  Epoch: [018][300/500]   Time 0.053 (0.117)   Data 0.000 (0.061)   Loss 2.0513 (2.0111)   Prec@1 36.000 (44.512)   Prec@5 85.000 (87.455)   [2025-10-26 12:25:13]
  Epoch: [018][400/500]   Time 0.054 (0.102)   Data 0.000 (0.046)   Loss 2.0415 (2.0095)   Prec@1 43.000 (44.736)   Prec@5 86.000 (87.621)   [2025-10-26 12:25:19]
  **Train** Prec@1 44.970 Prec@5 87.780 Error@1 55.030
  **Test** Prec@1 41.450 Prec@5 85.620 Error@1 58.550

==>>[2025-10-26 12:25:45] [Epoch=019/040] [Need: 00:23:13] [LR=0.0100] [Best : Accuracy=42.50, Error=57.50]
  Epoch: [019][000/500]   Time 18.225 (18.225)   Data 17.941 (17.941)   Loss 1.9528 (1.9528)   Prec@1 51.000 (51.000)   Prec@5 90.000 (90.000)   [2025-10-26 12:26:03]
  Epoch: [019][100/500]   Time 0.056 (0.233)   Data 0.001 (0.178)   Loss 2.0085 (2.0004)   Prec@1 45.000 (45.614)   Prec@5 85.000 (88.040)   [2025-10-26 12:26:08]
  Epoch: [019][200/500]   Time 0.056 (0.144)   Data 0.000 (0.089)   Loss 1.9683 (2.0024)   Prec@1 49.000 (45.413)   Prec@5 97.000 (88.234)   [2025-10-26 12:26:14]
  Epoch: [019][300/500]   Time 0.057 (0.115)   Data 0.000 (0.060)   Loss 2.0661 (2.0021)   Prec@1 38.000 (45.542)   Prec@5 84.000 (88.083)   [2025-10-26 12:26:20]
  Epoch: [019][400/500]   Time 0.056 (0.100)   Data 0.000 (0.045)   Loss 1.9863 (2.0017)   Prec@1 47.000 (45.506)   Prec@5 89.000 (88.100)   [2025-10-26 12:26:25]
  **Train** Prec@1 45.436 Prec@5 88.118 Error@1 54.564
  **Test** Prec@1 48.770 Prec@5 90.310 Error@1 51.230
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 12:26:52] [Epoch=020/040] [Need: 00:22:07] [LR=0.0100] [Best : Accuracy=48.77, Error=51.23]
  Epoch: [020][000/500]   Time 18.411 (18.411)   Data 18.137 (18.137)   Loss 2.0134 (2.0134)   Prec@1 45.000 (45.000)   Prec@5 87.000 (87.000)   [2025-10-26 12:27:10]
  Epoch: [020][100/500]   Time 0.054 (0.236)   Data 0.000 (0.180)   Loss 2.0139 (1.9955)   Prec@1 43.000 (46.545)   Prec@5 87.000 (88.168)   [2025-10-26 12:27:15]
  Epoch: [020][200/500]   Time 0.058 (0.146)   Data 0.000 (0.090)   Loss 1.9121 (1.9950)   Prec@1 57.000 (46.562)   Prec@5 88.000 (88.313)   [2025-10-26 12:27:21]
  Epoch: [020][300/500]   Time 0.052 (0.116)   Data 0.000 (0.060)   Loss 2.0781 (1.9962)   Prec@1 37.000 (46.262)   Prec@5 85.000 (88.346)   [2025-10-26 12:27:26]
  Epoch: [020][400/500]   Time 0.056 (0.101)   Data 0.000 (0.045)   Loss 1.9895 (1.9950)   Prec@1 48.000 (46.394)   Prec@5 86.000 (88.354)   [2025-10-26 12:27:32]
  **Train** Prec@1 46.464 Prec@5 88.442 Error@1 53.536
  **Test** Prec@1 40.780 Prec@5 85.680 Error@1 59.220

==>>[2025-10-26 12:27:58] [Epoch=021/040] [Need: 00:21:00] [LR=0.0100] [Best : Accuracy=48.77, Error=51.23]
  Epoch: [021][000/500]   Time 18.291 (18.291)   Data 18.024 (18.024)   Loss 1.9503 (1.9503)   Prec@1 51.000 (51.000)   Prec@5 89.000 (89.000)   [2025-10-26 12:28:16]
  Epoch: [021][100/500]   Time 0.052 (0.233)   Data 0.000 (0.179)   Loss 2.0222 (1.9963)   Prec@1 45.000 (46.109)   Prec@5 87.000 (87.743)   [2025-10-26 12:28:21]
  Epoch: [021][200/500]   Time 0.054 (0.145)   Data 0.001 (0.090)   Loss 1.9276 (1.9921)   Prec@1 52.000 (46.577)   Prec@5 91.000 (88.090)   [2025-10-26 12:28:27]
  Epoch: [021][300/500]   Time 0.054 (0.115)   Data 0.001 (0.060)   Loss 2.0331 (1.9909)   Prec@1 41.000 (46.674)   Prec@5 83.000 (88.027)   [2025-10-26 12:28:32]
  Epoch: [021][400/500]   Time 0.055 (0.100)   Data 0.000 (0.045)   Loss 1.9118 (1.9899)   Prec@1 54.000 (46.743)   Prec@5 92.000 (88.002)   [2025-10-26 12:28:38]
  **Train** Prec@1 46.790 Prec@5 88.038 Error@1 53.210
  **Test** Prec@1 44.440 Prec@5 87.340 Error@1 55.560

==>>[2025-10-26 12:29:04] [Epoch=022/040] [Need: 00:19:53] [LR=0.0100] [Best : Accuracy=48.77, Error=51.23]
  Epoch: [022][000/500]   Time 17.807 (17.807)   Data 17.527 (17.527)   Loss 1.9390 (1.9390)   Prec@1 51.000 (51.000)   Prec@5 88.000 (88.000)   [2025-10-26 12:29:22]
  Epoch: [022][100/500]   Time 0.055 (0.230)   Data 0.001 (0.174)   Loss 1.9403 (1.9971)   Prec@1 51.000 (45.990)   Prec@5 92.000 (88.059)   [2025-10-26 12:29:27]
  Epoch: [022][200/500]   Time 0.055 (0.143)   Data 0.000 (0.087)   Loss 1.9552 (1.9901)   Prec@1 50.000 (46.731)   Prec@5 87.000 (88.284)   [2025-10-26 12:29:32]
  Epoch: [022][300/500]   Time 0.057 (0.114)   Data 0.001 (0.058)   Loss 1.9579 (1.9908)   Prec@1 49.000 (46.561)   Prec@5 88.000 (88.133)   [2025-10-26 12:29:38]
  Epoch: [022][400/500]   Time 0.061 (0.099)   Data 0.000 (0.044)   Loss 1.9475 (1.9887)   Prec@1 52.000 (46.805)   Prec@5 88.000 (88.319)   [2025-10-26 12:29:44]
  **Train** Prec@1 46.992 Prec@5 88.314 Error@1 53.008
  **Test** Prec@1 46.590 Prec@5 88.290 Error@1 53.410

==>>[2025-10-26 12:30:09] [Epoch=023/040] [Need: 00:18:47] [LR=0.0100] [Best : Accuracy=48.77, Error=51.23]
  Epoch: [023][000/500]   Time 17.935 (17.935)   Data 17.719 (17.719)   Loss 1.9724 (1.9724)   Prec@1 50.000 (50.000)   Prec@5 90.000 (90.000)   [2025-10-26 12:30:27]
  Epoch: [023][100/500]   Time 0.056 (0.230)   Data 0.000 (0.176)   Loss 1.9527 (1.9882)   Prec@1 52.000 (46.901)   Prec@5 89.000 (88.139)   [2025-10-26 12:30:33]
  Epoch: [023][200/500]   Time 0.053 (0.143)   Data 0.000 (0.088)   Loss 1.9601 (1.9847)   Prec@1 50.000 (47.269)   Prec@5 83.000 (88.139)   [2025-10-26 12:30:38]
  Epoch: [023][300/500]   Time 0.055 (0.114)   Data 0.000 (0.059)   Loss 1.9780 (1.9819)   Prec@1 48.000 (47.621)   Prec@5 88.000 (88.100)   [2025-10-26 12:30:44]
  Epoch: [023][400/500]   Time 0.058 (0.100)   Data 0.000 (0.044)   Loss 1.9683 (1.9815)   Prec@1 50.000 (47.698)   Prec@5 90.000 (88.224)   [2025-10-26 12:30:49]
  **Train** Prec@1 47.640 Prec@5 88.264 Error@1 52.360
  **Test** Prec@1 43.960 Prec@5 87.500 Error@1 56.040

==>>[2025-10-26 12:31:15] [Epoch=024/040] [Need: 00:17:40] [LR=0.0100] [Best : Accuracy=48.77, Error=51.23]
  Epoch: [024][000/500]   Time 18.231 (18.231)   Data 17.955 (17.955)   Loss 1.9337 (1.9337)   Prec@1 55.000 (55.000)   Prec@5 95.000 (95.000)   [2025-10-26 12:31:33]
  Epoch: [024][100/500]   Time 0.057 (0.233)   Data 0.000 (0.178)   Loss 2.0082 (1.9840)   Prec@1 47.000 (47.277)   Prec@5 93.000 (88.010)   [2025-10-26 12:31:39]
  Epoch: [024][200/500]   Time 0.054 (0.145)   Data 0.000 (0.089)   Loss 2.0574 (1.9839)   Prec@1 38.000 (47.348)   Prec@5 83.000 (88.030)   [2025-10-26 12:31:44]
  Epoch: [024][300/500]   Time 0.054 (0.115)   Data 0.000 (0.060)   Loss 2.0031 (1.9814)   Prec@1 44.000 (47.578)   Prec@5 85.000 (88.478)   [2025-10-26 12:31:50]
  Epoch: [024][400/500]   Time 0.056 (0.100)   Data 0.000 (0.045)   Loss 1.9460 (1.9778)   Prec@1 50.000 (48.010)   Prec@5 91.000 (88.461)   [2025-10-26 12:31:55]
  **Train** Prec@1 48.152 Prec@5 88.454 Error@1 51.848
  **Test** Prec@1 48.870 Prec@5 88.280 Error@1 51.130
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 12:32:21] [Epoch=025/040] [Need: 00:16:33] [LR=0.0010] [Best : Accuracy=48.87, Error=51.13]
  Epoch: [025][000/500]   Time 17.881 (17.881)   Data 17.608 (17.608)   Loss 1.9173 (1.9173)   Prec@1 58.000 (58.000)   Prec@5 85.000 (85.000)   [2025-10-26 12:32:39]
  Epoch: [025][100/500]   Time 0.056 (0.230)   Data 0.000 (0.175)   Loss 1.9232 (1.9585)   Prec@1 54.000 (50.168)   Prec@5 88.000 (88.614)   [2025-10-26 12:32:44]
  Epoch: [025][200/500]   Time 0.057 (0.143)   Data 0.000 (0.088)   Loss 1.9055 (1.9572)   Prec@1 58.000 (50.318)   Prec@5 89.000 (88.975)   [2025-10-26 12:32:50]
  Epoch: [025][300/500]   Time 0.057 (0.114)   Data 0.000 (0.059)   Loss 1.9552 (1.9593)   Prec@1 49.000 (49.973)   Prec@5 91.000 (88.774)   [2025-10-26 12:32:55]
  Epoch: [025][400/500]   Time 0.056 (0.100)   Data 0.000 (0.044)   Loss 1.8980 (1.9567)   Prec@1 58.000 (50.214)   Prec@5 92.000 (88.990)   [2025-10-26 12:33:01]
  **Train** Prec@1 50.372 Prec@5 89.044 Error@1 49.628
  **Test** Prec@1 49.520 Prec@5 89.050 Error@1 50.480
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 12:33:27] [Epoch=026/040] [Need: 00:15:27] [LR=0.0010] [Best : Accuracy=49.52, Error=50.48]
  Epoch: [026][000/500]   Time 17.960 (17.960)   Data 17.687 (17.687)   Loss 1.8959 (1.8959)   Prec@1 56.000 (56.000)   Prec@5 91.000 (91.000)   [2025-10-26 12:33:45]
  Epoch: [026][100/500]   Time 0.056 (0.231)   Data 0.000 (0.175)   Loss 1.8401 (1.9421)   Prec@1 61.000 (51.812)   Prec@5 91.000 (89.347)   [2025-10-26 12:33:50]
  Epoch: [026][200/500]   Time 0.056 (0.143)   Data 0.000 (0.088)   Loss 1.9403 (1.9437)   Prec@1 51.000 (51.562)   Prec@5 89.000 (89.493)   [2025-10-26 12:33:55]
  Epoch: [026][300/500]   Time 0.060 (0.114)   Data 0.000 (0.059)   Loss 1.8943 (1.9448)   Prec@1 57.000 (51.442)   Prec@5 90.000 (89.488)   [2025-10-26 12:34:01]
  Epoch: [026][400/500]   Time 0.055 (0.100)   Data 0.000 (0.044)   Loss 1.9043 (1.9447)   Prec@1 58.000 (51.479)   Prec@5 89.000 (89.439)   [2025-10-26 12:34:07]
  **Train** Prec@1 51.360 Prec@5 89.490 Error@1 48.640
  **Test** Prec@1 50.420 Prec@5 89.340 Error@1 49.580
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 12:34:32] [Epoch=027/040] [Need: 00:14:20] [LR=0.0010] [Best : Accuracy=50.42, Error=49.58]
  Epoch: [027][000/500]   Time 19.147 (19.147)   Data 18.876 (18.876)   Loss 1.9066 (1.9066)   Prec@1 55.000 (55.000)   Prec@5 93.000 (93.000)   [2025-10-26 12:34:52]
  Epoch: [027][100/500]   Time 0.052 (0.243)   Data 0.000 (0.187)   Loss 1.9428 (1.9431)   Prec@1 49.000 (51.634)   Prec@5 90.000 (89.713)   [2025-10-26 12:34:57]
  Epoch: [027][200/500]   Time 0.052 (0.149)   Data 0.000 (0.094)   Loss 1.8990 (1.9421)   Prec@1 56.000 (51.776)   Prec@5 86.000 (89.587)   [2025-10-26 12:35:02]
  Epoch: [027][300/500]   Time 0.053 (0.118)   Data 0.001 (0.063)   Loss 1.8899 (1.9416)   Prec@1 59.000 (51.870)   Prec@5 93.000 (89.615)   [2025-10-26 12:35:08]
  Epoch: [027][400/500]   Time 0.056 (0.103)   Data 0.000 (0.047)   Loss 1.9514 (1.9402)   Prec@1 52.000 (52.057)   Prec@5 85.000 (89.791)   [2025-10-26 12:35:13]
  **Train** Prec@1 51.842 Prec@5 89.544 Error@1 48.158
  **Test** Prec@1 50.170 Prec@5 88.810 Error@1 49.830

==>>[2025-10-26 12:35:40] [Epoch=028/040] [Need: 00:13:14] [LR=0.0010] [Best : Accuracy=50.42, Error=49.58]
  Epoch: [028][000/500]   Time 18.230 (18.230)   Data 18.025 (18.025)   Loss 1.9604 (1.9604)   Prec@1 50.000 (50.000)   Prec@5 92.000 (92.000)   [2025-10-26 12:35:58]
  Epoch: [028][100/500]   Time 0.052 (0.233)   Data 0.000 (0.179)   Loss 1.9385 (1.9415)   Prec@1 46.000 (51.861)   Prec@5 92.000 (89.713)   [2025-10-26 12:36:03]
  Epoch: [028][200/500]   Time 0.052 (0.144)   Data 0.001 (0.090)   Loss 2.0032 (1.9395)   Prec@1 48.000 (52.010)   Prec@5 88.000 (89.726)   [2025-10-26 12:36:08]
  Epoch: [028][300/500]   Time 0.055 (0.115)   Data 0.000 (0.060)   Loss 1.9280 (1.9410)   Prec@1 52.000 (51.934)   Prec@5 90.000 (89.585)   [2025-10-26 12:36:14]
  Epoch: [028][400/500]   Time 0.054 (0.100)   Data 0.000 (0.045)   Loss 1.8642 (1.9418)   Prec@1 59.000 (51.825)   Prec@5 92.000 (89.569)   [2025-10-26 12:36:20]
  **Train** Prec@1 51.880 Prec@5 89.528 Error@1 48.120
  **Test** Prec@1 49.840 Prec@5 89.870 Error@1 50.160

==>>[2025-10-26 12:36:45] [Epoch=029/040] [Need: 00:12:08] [LR=0.0010] [Best : Accuracy=50.42, Error=49.58]
  Epoch: [029][000/500]   Time 17.890 (17.890)   Data 17.612 (17.612)   Loss 1.9043 (1.9043)   Prec@1 58.000 (58.000)   Prec@5 88.000 (88.000)   [2025-10-26 12:37:03]
  Epoch: [029][100/500]   Time 0.056 (0.230)   Data 0.000 (0.174)   Loss 1.8978 (1.9413)   Prec@1 55.000 (51.723)   Prec@5 91.000 (89.307)   [2025-10-26 12:37:09]
  Epoch: [029][200/500]   Time 0.056 (0.143)   Data 0.000 (0.088)   Loss 1.9829 (1.9397)   Prec@1 49.000 (52.085)   Prec@5 89.000 (89.403)   [2025-10-26 12:37:14]
  Epoch: [029][300/500]   Time 0.058 (0.114)   Data 0.000 (0.059)   Loss 1.9873 (1.9383)   Prec@1 45.000 (52.252)   Prec@5 89.000 (89.542)   [2025-10-26 12:37:20]
  Epoch: [029][400/500]   Time 0.054 (0.099)   Data 0.000 (0.044)   Loss 1.9338 (1.9383)   Prec@1 52.000 (52.227)   Prec@5 91.000 (89.668)   [2025-10-26 12:37:25]
  **Train** Prec@1 52.212 Prec@5 89.694 Error@1 47.788
  **Test** Prec@1 49.530 Prec@5 88.690 Error@1 50.470

==>>[2025-10-26 12:37:51] [Epoch=030/040] [Need: 00:11:02] [LR=0.0010] [Best : Accuracy=50.42, Error=49.58]
  Epoch: [030][000/500]   Time 18.130 (18.130)   Data 17.891 (17.891)   Loss 1.8876 (1.8876)   Prec@1 59.000 (59.000)   Prec@5 92.000 (92.000)   [2025-10-26 12:38:09]
  Epoch: [030][100/500]   Time 0.054 (0.232)   Data 0.000 (0.177)   Loss 1.9208 (1.9385)   Prec@1 56.000 (52.257)   Prec@5 88.000 (89.327)   [2025-10-26 12:38:14]
  Epoch: [030][200/500]   Time 0.055 (0.144)   Data 0.000 (0.089)   Loss 1.8465 (1.9370)   Prec@1 65.000 (52.358)   Prec@5 97.000 (89.542)   [2025-10-26 12:38:20]
  Epoch: [030][300/500]   Time 0.053 (0.114)   Data 0.000 (0.060)   Loss 1.9873 (1.9387)   Prec@1 46.000 (52.146)   Prec@5 85.000 (89.402)   [2025-10-26 12:38:25]
  Epoch: [030][400/500]   Time 0.053 (0.100)   Data 0.000 (0.045)   Loss 1.9870 (1.9381)   Prec@1 45.000 (52.195)   Prec@5 91.000 (89.444)   [2025-10-26 12:38:31]
  **Train** Prec@1 52.206 Prec@5 89.498 Error@1 47.794
  **Test** Prec@1 48.230 Prec@5 88.230 Error@1 51.770

==>>[2025-10-26 12:38:57] [Epoch=031/040] [Need: 00:09:55] [LR=0.0010] [Best : Accuracy=50.42, Error=49.58]
  Epoch: [031][000/500]   Time 18.152 (18.152)   Data 17.929 (17.929)   Loss 1.9278 (1.9278)   Prec@1 52.000 (52.000)   Prec@5 88.000 (88.000)   [2025-10-26 12:39:15]
  Epoch: [031][100/500]   Time 0.055 (0.232)   Data 0.000 (0.178)   Loss 2.0433 (1.9344)   Prec@1 39.000 (52.604)   Prec@5 88.000 (89.644)   [2025-10-26 12:39:20]
  Epoch: [031][200/500]   Time 0.056 (0.144)   Data 0.000 (0.089)   Loss 1.9384 (1.9388)   Prec@1 52.000 (52.060)   Prec@5 86.000 (89.393)   [2025-10-26 12:39:26]
  Epoch: [031][300/500]   Time 0.054 (0.115)   Data 0.000 (0.060)   Loss 1.9211 (1.9361)   Prec@1 55.000 (52.449)   Prec@5 90.000 (89.545)   [2025-10-26 12:39:31]
  Epoch: [031][400/500]   Time 0.057 (0.100)   Data 0.000 (0.045)   Loss 1.8901 (1.9353)   Prec@1 57.000 (52.509)   Prec@5 88.000 (89.559)   [2025-10-26 12:39:37]
  **Train** Prec@1 52.418 Prec@5 89.554 Error@1 47.582
  **Test** Prec@1 49.060 Prec@5 87.980 Error@1 50.940

==>>[2025-10-26 12:40:03] [Epoch=032/040] [Need: 00:08:49] [LR=0.0010] [Best : Accuracy=50.42, Error=49.58]
  Epoch: [032][000/500]   Time 17.864 (17.864)   Data 17.658 (17.658)   Loss 1.9871 (1.9871)   Prec@1 50.000 (50.000)   Prec@5 88.000 (88.000)   [2025-10-26 12:40:21]
  Epoch: [032][100/500]   Time 0.057 (0.229)   Data 0.000 (0.175)   Loss 1.9023 (1.9334)   Prec@1 58.000 (52.822)   Prec@5 88.000 (89.644)   [2025-10-26 12:40:26]
  Epoch: [032][200/500]   Time 0.053 (0.142)   Data 0.000 (0.088)   Loss 1.8675 (1.9312)   Prec@1 61.000 (52.995)   Prec@5 90.000 (89.647)   [2025-10-26 12:40:31]
  Epoch: [032][300/500]   Time 0.053 (0.113)   Data 0.000 (0.059)   Loss 2.0147 (1.9326)   Prec@1 42.000 (52.894)   Prec@5 88.000 (89.581)   [2025-10-26 12:40:37]
  Epoch: [032][400/500]   Time 0.056 (0.099)   Data 0.000 (0.044)   Loss 1.8915 (1.9308)   Prec@1 59.000 (53.107)   Prec@5 91.000 (89.683)   [2025-10-26 12:40:43]
  **Train** Prec@1 52.960 Prec@5 89.620 Error@1 47.040
  **Test** Prec@1 49.030 Prec@5 87.930 Error@1 50.970

==>>[2025-10-26 12:41:09] [Epoch=033/040] [Need: 00:07:43] [LR=0.0010] [Best : Accuracy=50.42, Error=49.58]
  Epoch: [033][000/500]   Time 17.763 (17.763)   Data 17.486 (17.486)   Loss 1.9089 (1.9089)   Prec@1 56.000 (56.000)   Prec@5 89.000 (89.000)   [2025-10-26 12:41:27]
  Epoch: [033][100/500]   Time 0.055 (0.229)   Data 0.000 (0.173)   Loss 1.9434 (1.9314)   Prec@1 53.000 (52.941)   Prec@5 90.000 (89.644)   [2025-10-26 12:41:32]
  Epoch: [033][200/500]   Time 0.056 (0.142)   Data 0.000 (0.087)   Loss 1.9082 (1.9312)   Prec@1 53.000 (52.935)   Prec@5 88.000 (89.746)   [2025-10-26 12:41:38]
  Epoch: [033][300/500]   Time 0.056 (0.114)   Data 0.001 (0.058)   Loss 1.9851 (1.9336)   Prec@1 47.000 (52.698)   Prec@5 83.000 (89.545)   [2025-10-26 12:41:43]
  Epoch: [033][400/500]   Time 0.057 (0.099)   Data 0.000 (0.044)   Loss 1.9186 (1.9323)   Prec@1 54.000 (52.788)   Prec@5 87.000 (89.601)   [2025-10-26 12:41:49]
  **Train** Prec@1 52.798 Prec@5 89.570 Error@1 47.202
  **Test** Prec@1 49.570 Prec@5 88.530 Error@1 50.430

==>>[2025-10-26 12:42:15] [Epoch=034/040] [Need: 00:06:37] [LR=0.0010] [Best : Accuracy=50.42, Error=49.58]
  Epoch: [034][000/500]   Time 18.059 (18.059)   Data 17.772 (17.772)   Loss 1.9607 (1.9607)   Prec@1 52.000 (52.000)   Prec@5 90.000 (90.000)   [2025-10-26 12:42:33]
  Epoch: [034][100/500]   Time 0.053 (0.231)   Data 0.001 (0.176)   Loss 1.8449 (1.9243)   Prec@1 63.000 (53.970)   Prec@5 91.000 (89.604)   [2025-10-26 12:42:38]
  Epoch: [034][200/500]   Time 0.054 (0.144)   Data 0.000 (0.089)   Loss 1.9262 (1.9289)   Prec@1 55.000 (53.398)   Prec@5 91.000 (89.507)   [2025-10-26 12:42:44]
  Epoch: [034][300/500]   Time 0.057 (0.114)   Data 0.000 (0.059)   Loss 1.9560 (1.9329)   Prec@1 49.000 (52.910)   Prec@5 91.000 (89.545)   [2025-10-26 12:42:49]
  Epoch: [034][400/500]   Time 0.058 (0.100)   Data 0.000 (0.044)   Loss 1.9644 (1.9331)   Prec@1 52.000 (52.820)   Prec@5 87.000 (89.551)   [2025-10-26 12:42:55]
  **Train** Prec@1 52.890 Prec@5 89.672 Error@1 47.110
  **Test** Prec@1 51.140 Prec@5 89.640 Error@1 48.860
=> Obtain best accuracy, and update the best model

==>>[2025-10-26 12:43:21] [Epoch=035/040] [Need: 00:05:30] [LR=0.0010] [Best : Accuracy=51.14, Error=48.86]
  Epoch: [035][000/500]   Time 18.063 (18.063)   Data 17.777 (17.777)   Loss 1.9510 (1.9510)   Prec@1 52.000 (52.000)   Prec@5 87.000 (87.000)   [2025-10-26 12:43:39]
  Epoch: [035][100/500]   Time 0.054 (0.231)   Data 0.001 (0.176)   Loss 1.9783 (1.9360)   Prec@1 48.000 (52.238)   Prec@5 88.000 (89.495)   [2025-10-26 12:43:44]
  Epoch: [035][200/500]   Time 0.061 (0.143)   Data 0.001 (0.089)   Loss 1.9366 (1.9352)   Prec@1 53.000 (52.303)   Prec@5 88.000 (89.473)   [2025-10-26 12:43:50]
  Epoch: [035][300/500]   Time 0.055 (0.114)   Data 0.000 (0.059)   Loss 1.9059 (1.9345)   Prec@1 53.000 (52.425)   Prec@5 90.000 (89.296)   [2025-10-26 12:43:55]
  Epoch: [035][400/500]   Time 0.056 (0.100)   Data 0.000 (0.044)   Loss 1.9203 (1.9327)   Prec@1 54.000 (52.651)   Prec@5 89.000 (89.514)   [2025-10-26 12:44:01]
  **Train** Prec@1 52.660 Prec@5 89.566 Error@1 47.340
  **Test** Prec@1 50.120 Prec@5 88.500 Error@1 49.880

==>>[2025-10-26 12:44:26] [Epoch=036/040] [Need: 00:04:24] [LR=0.0010] [Best : Accuracy=51.14, Error=48.86]
  Epoch: [036][000/500]   Time 18.896 (18.896)   Data 18.636 (18.636)   Loss 1.9049 (1.9049)   Prec@1 53.000 (53.000)   Prec@5 89.000 (89.000)   [2025-10-26 12:44:45]
  Epoch: [036][100/500]   Time 0.056 (0.240)   Data 0.000 (0.185)   Loss 1.9236 (1.9205)   Prec@1 54.000 (54.287)   Prec@5 91.000 (90.208)   [2025-10-26 12:44:51]
  Epoch: [036][200/500]   Time 0.053 (0.148)   Data 0.000 (0.093)   Loss 1.9856 (1.9272)   Prec@1 44.000 (53.502)   Prec@5 96.000 (89.771)   [2025-10-26 12:44:56]
  Epoch: [036][300/500]   Time 0.054 (0.117)   Data 0.000 (0.062)   Loss 1.8475 (1.9299)   Prec@1 62.000 (53.146)   Prec@5 93.000 (89.837)   [2025-10-26 12:45:02]
  Epoch: [036][400/500]   Time 0.058 (0.102)   Data 0.000 (0.047)   Loss 1.9496 (1.9297)   Prec@1 50.000 (53.197)   Prec@5 88.000 (89.833)   [2025-10-26 12:45:07]
  **Train** Prec@1 53.194 Prec@5 89.794 Error@1 46.806
  **Test** Prec@1 48.410 Prec@5 87.450 Error@1 51.590

==>>[2025-10-26 12:45:33] [Epoch=037/040] [Need: 00:03:18] [LR=0.0010] [Best : Accuracy=51.14, Error=48.86]
  Epoch: [037][000/500]   Time 18.677 (18.677)   Data 18.392 (18.392)   Loss 1.9359 (1.9359)   Prec@1 51.000 (51.000)   Prec@5 94.000 (94.000)   [2025-10-26 12:45:52]
  Epoch: [037][100/500]   Time 0.055 (0.238)   Data 0.001 (0.182)   Loss 1.9273 (1.9269)   Prec@1 53.000 (53.188)   Prec@5 88.000 (89.545)   [2025-10-26 12:45:57]
  Epoch: [037][200/500]   Time 0.061 (0.147)   Data 0.000 (0.092)   Loss 1.9535 (1.9248)   Prec@1 51.000 (53.463)   Prec@5 87.000 (89.692)   [2025-10-26 12:46:02]
  Epoch: [037][300/500]   Time 0.060 (0.116)   Data 0.000 (0.061)   Loss 1.9447 (1.9264)   Prec@1 50.000 (53.339)   Prec@5 84.000 (89.767)   [2025-10-26 12:46:08]
  Epoch: [037][400/500]   Time 0.057 (0.101)   Data 0.000 (0.046)   Loss 1.8916 (1.9285)   Prec@1 59.000 (53.157)   Prec@5 85.000 (89.696)   [2025-10-26 12:46:14]
  **Train** Prec@1 53.128 Prec@5 89.740 Error@1 46.872
  **Test** Prec@1 49.880 Prec@5 88.580 Error@1 50.120

==>>[2025-10-26 12:46:39] [Epoch=038/040] [Need: 00:02:12] [LR=0.0010] [Best : Accuracy=51.14, Error=48.86]
  Epoch: [038][000/500]   Time 18.215 (18.215)   Data 17.942 (17.942)   Loss 1.8894 (1.8894)   Prec@1 57.000 (57.000)   Prec@5 87.000 (87.000)   [2025-10-26 12:46:57]
  Epoch: [038][100/500]   Time 0.053 (0.234)   Data 0.000 (0.178)   Loss 1.9057 (1.9330)   Prec@1 57.000 (52.772)   Prec@5 96.000 (89.693)   [2025-10-26 12:47:03]
  Epoch: [038][200/500]   Time 0.053 (0.145)   Data 0.000 (0.089)   Loss 1.8690 (1.9310)   Prec@1 59.000 (52.955)   Prec@5 91.000 (89.532)   [2025-10-26 12:47:08]
  Epoch: [038][300/500]   Time 0.058 (0.115)   Data 0.000 (0.060)   Loss 1.9300 (1.9294)   Prec@1 50.000 (53.169)   Prec@5 92.000 (89.681)   [2025-10-26 12:47:14]
  Epoch: [038][400/500]   Time 0.059 (0.100)   Data 0.000 (0.045)   Loss 1.8917 (1.9288)   Prec@1 54.000 (53.214)   Prec@5 90.000 (89.693)   [2025-10-26 12:47:19]
  **Train** Prec@1 53.122 Prec@5 89.726 Error@1 46.878
  **Test** Prec@1 49.800 Prec@5 88.650 Error@1 50.200

==>>[2025-10-26 12:47:45] [Epoch=039/040] [Need: 00:01:06] [LR=0.0010] [Best : Accuracy=51.14, Error=48.86]
  Epoch: [039][000/500]   Time 18.041 (18.041)   Data 17.849 (17.849)   Loss 1.9016 (1.9016)   Prec@1 57.000 (57.000)   Prec@5 91.000 (91.000)   [2025-10-26 12:48:03]
  Epoch: [039][100/500]   Time 0.053 (0.231)   Data 0.000 (0.177)   Loss 1.9597 (1.9187)   Prec@1 50.000 (54.267)   Prec@5 85.000 (90.059)   [2025-10-26 12:48:08]
  Epoch: [039][200/500]   Time 0.052 (0.143)   Data 0.000 (0.089)   Loss 1.9104 (1.9267)   Prec@1 54.000 (53.453)   Prec@5 90.000 (89.871)   [2025-10-26 12:48:14]
  Epoch: [039][300/500]   Time 0.056 (0.114)   Data 0.001 (0.059)   Loss 1.8967 (1.9294)   Prec@1 54.000 (53.037)   Prec@5 90.000 (89.860)   [2025-10-26 12:48:19]
  Epoch: [039][400/500]   Time 0.053 (0.100)   Data 0.000 (0.045)   Loss 2.0029 (1.9279)   Prec@1 45.000 (53.277)   Prec@5 88.000 (89.940)   [2025-10-26 12:48:25]
  **Train** Prec@1 53.374 Prec@5 89.976 Error@1 46.626
  **Test** Prec@1 48.570 Prec@5 88.370 Error@1 51.430
