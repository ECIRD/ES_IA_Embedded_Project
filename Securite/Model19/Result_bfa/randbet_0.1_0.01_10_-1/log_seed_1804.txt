save path : ./save/resnet9_quan/randbet_0.1_0.01_10_-1
{'data_path': './dataset', 'arch': 'resnet9_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': True, 'clipping_coeff': 0.1, 'learning_rate': 0.01, 'manualSeed': 1804, 'save_path': './save/resnet9_quan/randbet_0.1_0.01_10_-1', 'enable_bfa': False, 'resume': '', 'quan_bitwidth': None, 'reset_weight': False, 'evaluate': False, 'n_iter': 30, 'model_only': False, 'random_bfa': False, 'use_cuda': True}
Random Seed: 1804
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'resnet9_quan'
=> network :
 Sequential(
  (0): Sequential(
    (0): quan_Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (1): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (2): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (4): Dropout2d(p=0.2, inplace=False)
  (5): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (skip): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (6): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Dropout2d(p=0.3, inplace=False)
  (9): Sequential(
    (0): quan_Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (10): Sequential(
    (0): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (12): Dropout2d(p=0.3, inplace=False)
  (13): Sequential(
    (0): quan_Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (14): AdaptiveAvgPool2d(output_size=(1, 1))
  (15): Flatten()
  (16): quan_Linear(in_features=64, out_features=128, bias=False)
  (17): ReLU(inplace=True)
  (18): Dropout(p=0.35, inplace=False)
  (19): quan_Linear(in_features=128, out_features=10, bias=False)
  (20): Softmax(dim=1)
)
=> do not use any checkpoint for resnet9_quan model

==>>[2025-10-28 14:10:42] [Epoch=000/040] [Need: 00:00:00] [LR=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/500]   Time 21.806 (21.806)   Data 20.894 (20.894)   Loss 2.3024 (2.3024)   Prec@1 11.000 (11.000)   Prec@5 51.000 (51.000)   [2025-10-28 14:11:04]
  Epoch: [000][100/500]   Time 0.320 (0.560)   Data 0.001 (0.209)   Loss 2.3008 (2.3023)   Prec@1 13.000 (10.673)   Prec@5 57.000 (50.802)   [2025-10-28 14:11:39]
  Epoch: [000][200/500]   Time 0.309 (0.440)   Data 0.002 (0.106)   Loss 2.2979 (2.3010)   Prec@1 11.000 (11.950)   Prec@5 52.000 (53.721)   [2025-10-28 14:12:11]
  Epoch: [000][300/500]   Time 0.239 (0.401)   Data 0.002 (0.071)   Loss 2.2843 (2.2983)   Prec@1 17.000 (13.130)   Prec@5 66.000 (57.066)   [2025-10-28 14:12:43]
  Epoch: [000][400/500]   Time 0.337 (0.380)   Data 0.002 (0.054)   Loss 2.2788 (2.2939)   Prec@1 15.000 (14.182)   Prec@5 73.000 (60.272)   [2025-10-28 14:13:15]
  **Train** Prec@1 14.778 Prec@5 62.620 Error@1 85.222
  **Test** Prec@1 17.890 Prec@5 72.850 Error@1 82.110
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 14:14:12] [Epoch=001/040] [Need: 02:16:17] [LR=0.0100] [Best : Accuracy=17.89, Error=82.11]
  Epoch: [001][000/500]   Time 24.770 (24.770)   Data 24.238 (24.238)   Loss 2.2531 (2.2531)   Prec@1 21.000 (21.000)   Prec@5 76.000 (76.000)   [2025-10-28 14:14:37]
  Epoch: [001][100/500]   Time 0.342 (0.574)   Data 0.002 (0.242)   Loss 2.2466 (2.2576)   Prec@1 20.000 (17.842)   Prec@5 79.000 (74.257)   [2025-10-28 14:15:10]
  Epoch: [001][200/500]   Time 0.204 (0.454)   Data 0.001 (0.122)   Loss 2.2531 (2.2521)   Prec@1 19.000 (17.910)   Prec@5 76.000 (74.652)   [2025-10-28 14:15:44]
  Epoch: [001][300/500]   Time 0.400 (0.411)   Data 0.000 (0.082)   Loss 2.2569 (2.2485)   Prec@1 19.000 (17.960)   Prec@5 69.000 (74.967)   [2025-10-28 14:16:16]
  Epoch: [001][400/500]   Time 0.329 (0.389)   Data 0.001 (0.062)   Loss 2.2160 (2.2453)   Prec@1 18.000 (18.200)   Prec@5 76.000 (75.070)   [2025-10-28 14:16:49]
  **Train** Prec@1 18.682 Prec@5 75.156 Error@1 81.318
  **Test** Prec@1 21.890 Prec@5 73.340 Error@1 78.110
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 14:17:45] [Epoch=002/040] [Need: 02:13:47] [LR=0.0100] [Best : Accuracy=21.89, Error=78.11]
  Epoch: [002][000/500]   Time 23.429 (23.429)   Data 23.119 (23.119)   Loss 2.2041 (2.2041)   Prec@1 28.000 (28.000)   Prec@5 76.000 (76.000)   [2025-10-28 14:18:09]
  Epoch: [002][100/500]   Time 0.107 (0.330)   Data 0.001 (0.230)   Loss 2.2162 (2.2186)   Prec@1 22.000 (21.921)   Prec@5 77.000 (76.198)   [2025-10-28 14:18:19]
  Epoch: [002][200/500]   Time 0.099 (0.217)   Data 0.001 (0.116)   Loss 2.2011 (2.2157)   Prec@1 26.000 (21.985)   Prec@5 65.000 (76.134)   [2025-10-28 14:18:29]
  Epoch: [002][300/500]   Time 0.097 (0.179)   Data 0.000 (0.077)   Loss 2.1616 (2.2120)   Prec@1 28.000 (22.492)   Prec@5 79.000 (76.143)   [2025-10-28 14:18:39]
  Epoch: [002][400/500]   Time 0.098 (0.160)   Data 0.000 (0.058)   Loss 2.1801 (2.2102)   Prec@1 26.000 (22.661)   Prec@5 76.000 (76.257)   [2025-10-28 14:18:49]
  **Train** Prec@1 22.706 Prec@5 76.224 Error@1 77.294
  **Test** Prec@1 21.190 Prec@5 71.140 Error@1 78.810

==>>[2025-10-28 14:19:18] [Epoch=003/040] [Need: 01:46:01] [LR=0.0100] [Best : Accuracy=21.89, Error=78.11]
  Epoch: [003][000/500]   Time 17.699 (17.699)   Data 17.330 (17.330)   Loss 2.1968 (2.1968)   Prec@1 21.000 (21.000)   Prec@5 77.000 (77.000)   [2025-10-28 14:19:36]
  Epoch: [003][100/500]   Time 0.101 (0.273)   Data 0.001 (0.172)   Loss 2.2239 (2.1945)   Prec@1 17.000 (24.238)   Prec@5 79.000 (76.574)   [2025-10-28 14:19:46]
  Epoch: [003][200/500]   Time 0.100 (0.188)   Data 0.001 (0.087)   Loss 2.2205 (2.1928)   Prec@1 18.000 (24.139)   Prec@5 74.000 (76.881)   [2025-10-28 14:19:56]
  Epoch: [003][300/500]   Time 0.103 (0.160)   Data 0.000 (0.058)   Loss 2.1796 (2.1928)   Prec@1 26.000 (24.020)   Prec@5 82.000 (76.930)   [2025-10-28 14:20:06]
  Epoch: [003][400/500]   Time 0.099 (0.146)   Data 0.000 (0.044)   Loss 2.1586 (2.1921)   Prec@1 31.000 (24.062)   Prec@5 75.000 (76.913)   [2025-10-28 14:20:17]
  **Train** Prec@1 24.136 Prec@5 76.912 Error@1 75.864
  **Test** Prec@1 25.640 Prec@5 78.440 Error@1 74.360
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 14:20:47] [Epoch=004/040] [Need: 01:30:42] [LR=0.0100] [Best : Accuracy=25.64, Error=74.36]
  Epoch: [004][000/500]   Time 17.761 (17.761)   Data 17.389 (17.389)   Loss 2.2012 (2.2012)   Prec@1 18.000 (18.000)   Prec@5 74.000 (74.000)   [2025-10-28 14:21:05]
  Epoch: [004][100/500]   Time 0.104 (0.274)   Data 0.001 (0.173)   Loss 2.1530 (2.1828)   Prec@1 28.000 (24.614)   Prec@5 76.000 (78.089)   [2025-10-28 14:21:15]
  Epoch: [004][200/500]   Time 0.103 (0.189)   Data 0.000 (0.087)   Loss 2.1957 (2.1839)   Prec@1 24.000 (24.502)   Prec@5 77.000 (77.945)   [2025-10-28 14:21:25]
  Epoch: [004][300/500]   Time 0.101 (0.160)   Data 0.000 (0.058)   Loss 2.1901 (2.1829)   Prec@1 24.000 (24.565)   Prec@5 77.000 (77.910)   [2025-10-28 14:21:35]
  Epoch: [004][400/500]   Time 0.103 (0.146)   Data 0.001 (0.044)   Loss 2.1881 (2.1823)   Prec@1 30.000 (24.648)   Prec@5 75.000 (77.868)   [2025-10-28 14:21:46]
  **Train** Prec@1 24.672 Prec@5 77.758 Error@1 75.328
  **Test** Prec@1 25.980 Prec@5 79.950 Error@1 74.020
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 14:22:15] [Epoch=005/040] [Need: 01:20:47] [LR=0.0100] [Best : Accuracy=25.98, Error=74.02]
  Epoch: [005][000/500]   Time 17.627 (17.627)   Data 17.331 (17.331)   Loss 2.2044 (2.2044)   Prec@1 23.000 (23.000)   Prec@5 84.000 (84.000)   [2025-10-28 14:22:32]
  Epoch: [005][100/500]   Time 0.109 (0.273)   Data 0.001 (0.172)   Loss 2.1703 (2.1767)   Prec@1 26.000 (25.139)   Prec@5 76.000 (78.515)   [2025-10-28 14:22:42]
  Epoch: [005][200/500]   Time 0.107 (0.189)   Data 0.001 (0.087)   Loss 2.1910 (2.1752)   Prec@1 25.000 (25.259)   Prec@5 67.000 (78.507)   [2025-10-28 14:22:53]
  Epoch: [005][300/500]   Time 0.100 (0.161)   Data 0.000 (0.058)   Loss 2.2248 (2.1755)   Prec@1 21.000 (25.296)   Prec@5 76.000 (78.389)   [2025-10-28 14:23:03]
  Epoch: [005][400/500]   Time 0.099 (0.147)   Data 0.001 (0.044)   Loss 2.1827 (2.1750)   Prec@1 25.000 (25.389)   Prec@5 74.000 (78.656)   [2025-10-28 14:23:14]
  **Train** Prec@1 25.384 Prec@5 78.702 Error@1 74.616
  **Test** Prec@1 26.260 Prec@5 80.770 Error@1 73.740
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 14:23:43] [Epoch=006/040] [Need: 01:13:44] [LR=0.0100] [Best : Accuracy=26.26, Error=73.74]
  Epoch: [006][000/500]   Time 17.853 (17.853)   Data 17.555 (17.555)   Loss 2.1561 (2.1561)   Prec@1 25.000 (25.000)   Prec@5 76.000 (76.000)   [2025-10-28 14:24:01]
  Epoch: [006][100/500]   Time 0.103 (0.275)   Data 0.001 (0.174)   Loss 2.1905 (2.1752)   Prec@1 25.000 (25.297)   Prec@5 73.000 (78.921)   [2025-10-28 14:24:11]
  Epoch: [006][200/500]   Time 0.105 (0.189)   Data 0.000 (0.088)   Loss 2.1896 (2.1711)   Prec@1 22.000 (25.751)   Prec@5 77.000 (79.159)   [2025-10-28 14:24:21]
  Epoch: [006][300/500]   Time 0.100 (0.160)   Data 0.001 (0.059)   Loss 2.1432 (2.1698)   Prec@1 32.000 (25.927)   Prec@5 81.000 (79.236)   [2025-10-28 14:24:31]
  Epoch: [006][400/500]   Time 0.100 (0.145)   Data 0.001 (0.044)   Loss 2.1695 (2.1703)   Prec@1 24.000 (25.868)   Prec@5 80.000 (79.349)   [2025-10-28 14:24:41]
  **Train** Prec@1 26.042 Prec@5 79.436 Error@1 73.958
  **Test** Prec@1 26.090 Prec@5 78.960 Error@1 73.910

==>>[2025-10-28 14:25:10] [Epoch=007/040] [Need: 01:08:11] [LR=0.0100] [Best : Accuracy=26.26, Error=73.74]
  Epoch: [007][000/500]   Time 17.865 (17.865)   Data 17.480 (17.480)   Loss 2.1691 (2.1691)   Prec@1 24.000 (24.000)   Prec@5 81.000 (81.000)   [2025-10-28 14:25:28]
  Epoch: [007][100/500]   Time 0.097 (0.273)   Data 0.001 (0.174)   Loss 2.1660 (2.1604)   Prec@1 29.000 (27.356)   Prec@5 84.000 (80.465)   [2025-10-28 14:25:38]
  Epoch: [007][200/500]   Time 0.102 (0.188)   Data 0.000 (0.088)   Loss 2.1830 (2.1620)   Prec@1 25.000 (27.214)   Prec@5 75.000 (79.905)   [2025-10-28 14:25:48]
  Epoch: [007][300/500]   Time 0.104 (0.160)   Data 0.001 (0.059)   Loss 2.1564 (2.1622)   Prec@1 29.000 (27.282)   Prec@5 72.000 (79.977)   [2025-10-28 14:25:58]
  Epoch: [007][400/500]   Time 0.100 (0.146)   Data 0.001 (0.044)   Loss 2.1652 (2.1606)   Prec@1 27.000 (27.536)   Prec@5 81.000 (80.027)   [2025-10-28 14:26:09]
  **Train** Prec@1 27.638 Prec@5 80.154 Error@1 72.362
  **Test** Prec@1 29.400 Prec@5 80.850 Error@1 70.600
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 14:26:38] [Epoch=008/040] [Need: 01:03:43] [LR=0.0100] [Best : Accuracy=29.40, Error=70.60]
  Epoch: [008][000/500]   Time 17.649 (17.649)   Data 17.317 (17.317)   Loss 2.1012 (2.1012)   Prec@1 36.000 (36.000)   Prec@5 79.000 (79.000)   [2025-10-28 14:26:56]
  Epoch: [008][100/500]   Time 0.111 (0.276)   Data 0.001 (0.172)   Loss 2.1560 (2.1485)   Prec@1 27.000 (29.238)   Prec@5 80.000 (80.990)   [2025-10-28 14:27:06]
  Epoch: [008][200/500]   Time 0.103 (0.190)   Data 0.001 (0.087)   Loss 2.1387 (2.1488)   Prec@1 32.000 (29.239)   Prec@5 84.000 (80.776)   [2025-10-28 14:27:17]
  Epoch: [008][300/500]   Time 0.102 (0.162)   Data 0.001 (0.058)   Loss 2.2262 (2.1472)   Prec@1 18.000 (29.492)   Prec@5 79.000 (80.874)   [2025-10-28 14:27:27]
  Epoch: [008][400/500]   Time 0.104 (0.147)   Data 0.001 (0.044)   Loss 2.1314 (2.1471)   Prec@1 35.000 (29.546)   Prec@5 79.000 (80.706)   [2025-10-28 14:27:37]
  **Train** Prec@1 29.804 Prec@5 80.828 Error@1 70.196
  **Test** Prec@1 33.800 Prec@5 82.320 Error@1 66.200
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 14:28:06] [Epoch=009/040] [Need: 00:59:55] [LR=0.0100] [Best : Accuracy=33.80, Error=66.20]
  Epoch: [009][000/500]   Time 17.876 (17.876)   Data 17.622 (17.622)   Loss 2.1271 (2.1271)   Prec@1 37.000 (37.000)   Prec@5 77.000 (77.000)   [2025-10-28 14:28:24]
  Epoch: [009][100/500]   Time 0.101 (0.275)   Data 0.001 (0.175)   Loss 2.1347 (2.1403)   Prec@1 34.000 (30.693)   Prec@5 85.000 (80.634)   [2025-10-28 14:28:34]
  Epoch: [009][200/500]   Time 0.105 (0.189)   Data 0.001 (0.088)   Loss 2.0852 (2.1357)   Prec@1 41.000 (31.428)   Prec@5 86.000 (81.279)   [2025-10-28 14:28:44]
  Epoch: [009][300/500]   Time 0.105 (0.161)   Data 0.001 (0.059)   Loss 2.1035 (2.1309)   Prec@1 34.000 (32.053)   Prec@5 81.000 (81.512)   [2025-10-28 14:28:55]
  Epoch: [009][400/500]   Time 0.100 (0.147)   Data 0.000 (0.045)   Loss 2.0872 (2.1292)   Prec@1 37.000 (32.421)   Prec@5 85.000 (81.706)   [2025-10-28 14:29:05]
  **Train** Prec@1 32.770 Prec@5 81.908 Error@1 67.230
  **Test** Prec@1 34.330 Prec@5 82.310 Error@1 65.670
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 14:29:35] [Epoch=010/040] [Need: 00:56:36] [LR=0.0100] [Best : Accuracy=34.33, Error=65.67]
  Epoch: [010][000/500]   Time 17.707 (17.707)   Data 17.416 (17.416)   Loss 2.1514 (2.1514)   Prec@1 34.000 (34.000)   Prec@5 84.000 (84.000)   [2025-10-28 14:29:52]
  Epoch: [010][100/500]   Time 0.111 (0.273)   Data 0.000 (0.173)   Loss 2.1182 (2.1114)   Prec@1 33.000 (35.030)   Prec@5 82.000 (84.050)   [2025-10-28 14:30:02]
  Epoch: [010][200/500]   Time 0.105 (0.190)   Data 0.001 (0.087)   Loss 2.1343 (2.1109)   Prec@1 31.000 (34.766)   Prec@5 88.000 (83.637)   [2025-10-28 14:30:13]
  Epoch: [010][300/500]   Time 0.104 (0.161)   Data 0.000 (0.059)   Loss 2.1589 (2.1070)   Prec@1 27.000 (35.196)   Prec@5 78.000 (83.811)   [2025-10-28 14:30:23]
  Epoch: [010][400/500]   Time 0.103 (0.147)   Data 0.001 (0.044)   Loss 2.0900 (2.1065)   Prec@1 40.000 (35.426)   Prec@5 84.000 (83.945)   [2025-10-28 14:30:34]
  **Train** Prec@1 35.568 Prec@5 84.118 Error@1 64.432
  **Test** Prec@1 39.790 Prec@5 87.170 Error@1 60.210
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 14:31:03] [Epoch=011/040] [Need: 00:53:37] [LR=0.0100] [Best : Accuracy=39.79, Error=60.21]
  Epoch: [011][000/500]   Time 17.884 (17.884)   Data 17.525 (17.525)   Loss 2.0364 (2.0364)   Prec@1 45.000 (45.000)   Prec@5 88.000 (88.000)   [2025-10-28 14:31:21]
  Epoch: [011][100/500]   Time 0.108 (0.277)   Data 0.001 (0.174)   Loss 2.0847 (2.0922)   Prec@1 34.000 (36.703)   Prec@5 88.000 (84.624)   [2025-10-28 14:31:31]
  Epoch: [011][200/500]   Time 0.098 (0.193)   Data 0.001 (0.088)   Loss 2.0995 (2.0885)   Prec@1 37.000 (37.254)   Prec@5 84.000 (85.109)   [2025-10-28 14:31:42]
  Epoch: [011][300/500]   Time 0.101 (0.163)   Data 0.000 (0.059)   Loss 2.0401 (2.0874)   Prec@1 45.000 (37.269)   Prec@5 82.000 (85.272)   [2025-10-28 14:31:52]
  Epoch: [011][400/500]   Time 0.114 (0.148)   Data 0.001 (0.044)   Loss 2.0726 (2.0866)   Prec@1 36.000 (37.416)   Prec@5 92.000 (85.202)   [2025-10-28 14:32:02]
  **Train** Prec@1 37.496 Prec@5 85.268 Error@1 62.504
  **Test** Prec@1 40.610 Prec@5 87.630 Error@1 59.390
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 14:32:32] [Epoch=012/040] [Need: 00:50:54] [LR=0.0100] [Best : Accuracy=40.61, Error=59.39]
  Epoch: [012][000/500]   Time 17.857 (17.857)   Data 17.508 (17.508)   Loss 2.1023 (2.1023)   Prec@1 36.000 (36.000)   Prec@5 82.000 (82.000)   [2025-10-28 14:32:50]
  Epoch: [012][100/500]   Time 0.102 (0.276)   Data 0.001 (0.174)   Loss 2.0944 (2.0769)   Prec@1 36.000 (38.228)   Prec@5 81.000 (86.574)   [2025-10-28 14:32:59]
  Epoch: [012][200/500]   Time 0.103 (0.191)   Data 0.000 (0.088)   Loss 2.0609 (2.0767)   Prec@1 36.000 (38.363)   Prec@5 85.000 (86.413)   [2025-10-28 14:33:10]
  Epoch: [012][300/500]   Time 0.106 (0.165)   Data 0.000 (0.059)   Loss 2.0455 (2.0741)   Prec@1 41.000 (38.615)   Prec@5 88.000 (86.591)   [2025-10-28 14:33:21]
  Epoch: [012][400/500]   Time 0.120 (0.151)   Data 0.001 (0.044)   Loss 2.0508 (2.0713)   Prec@1 40.000 (39.007)   Prec@5 90.000 (86.678)   [2025-10-28 14:33:32]
  **Train** Prec@1 39.142 Prec@5 86.554 Error@1 60.858
  **Test** Prec@1 34.580 Prec@5 83.840 Error@1 65.420

==>>[2025-10-28 14:34:03] [Epoch=013/040] [Need: 00:48:28] [LR=0.0100] [Best : Accuracy=40.61, Error=59.39]
  Epoch: [013][000/500]   Time 18.847 (18.847)   Data 18.461 (18.461)   Loss 2.1332 (2.1332)   Prec@1 33.000 (33.000)   Prec@5 77.000 (77.000)   [2025-10-28 14:34:22]
  Epoch: [013][100/500]   Time 0.105 (0.290)   Data 0.001 (0.183)   Loss 2.0634 (2.0647)   Prec@1 39.000 (40.089)   Prec@5 84.000 (85.208)   [2025-10-28 14:34:32]
  Epoch: [013][200/500]   Time 0.120 (0.201)   Data 0.000 (0.093)   Loss 2.0643 (2.0620)   Prec@1 36.000 (40.269)   Prec@5 86.000 (85.507)   [2025-10-28 14:34:43]
  Epoch: [013][300/500]   Time 0.108 (0.171)   Data 0.000 (0.062)   Loss 2.0482 (2.0616)   Prec@1 42.000 (40.209)   Prec@5 89.000 (85.987)   [2025-10-28 14:34:54]
  Epoch: [013][400/500]   Time 0.102 (0.155)   Data 0.001 (0.047)   Loss 2.0141 (2.0621)   Prec@1 44.000 (40.115)   Prec@5 87.000 (85.985)   [2025-10-28 14:35:05]
  **Train** Prec@1 40.168 Prec@5 86.184 Error@1 59.832
  **Test** Prec@1 45.560 Prec@5 90.130 Error@1 54.440
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 14:35:36] [Epoch=014/040] [Need: 00:46:13] [LR=0.0100] [Best : Accuracy=45.56, Error=54.44]
  Epoch: [014][000/500]   Time 18.979 (18.979)   Data 18.641 (18.641)   Loss 2.0399 (2.0399)   Prec@1 43.000 (43.000)   Prec@5 87.000 (87.000)   [2025-10-28 14:35:55]
  Epoch: [014][100/500]   Time 0.105 (0.290)   Data 0.001 (0.185)   Loss 2.0314 (2.0513)   Prec@1 45.000 (41.198)   Prec@5 95.000 (87.208)   [2025-10-28 14:36:05]
  Epoch: [014][200/500]   Time 0.102 (0.199)   Data 0.000 (0.093)   Loss 2.0086 (2.0482)   Prec@1 43.000 (41.527)   Prec@5 84.000 (87.179)   [2025-10-28 14:36:16]
  Epoch: [014][300/500]   Time 0.107 (0.168)   Data 0.001 (0.062)   Loss 2.0584 (2.0498)   Prec@1 37.000 (41.229)   Prec@5 93.000 (86.997)   [2025-10-28 14:36:27]
  Epoch: [014][400/500]   Time 0.108 (0.153)   Data 0.001 (0.047)   Loss 2.0809 (2.0490)   Prec@1 37.000 (41.327)   Prec@5 85.000 (87.050)   [2025-10-28 14:36:38]
  **Train** Prec@1 41.332 Prec@5 87.036 Error@1 58.668
  **Test** Prec@1 44.100 Prec@5 90.010 Error@1 55.900

==>>[2025-10-28 14:37:08] [Epoch=015/040] [Need: 00:44:03] [LR=0.0100] [Best : Accuracy=45.56, Error=54.44]
  Epoch: [015][000/500]   Time 18.899 (18.899)   Data 18.623 (18.623)   Loss 2.0629 (2.0629)   Prec@1 39.000 (39.000)   Prec@5 83.000 (83.000)   [2025-10-28 14:37:27]
  Epoch: [015][100/500]   Time 0.106 (0.291)   Data 0.001 (0.185)   Loss 2.0399 (2.0432)   Prec@1 45.000 (41.950)   Prec@5 90.000 (87.337)   [2025-10-28 14:37:38]
  Epoch: [015][200/500]   Time 0.107 (0.199)   Data 0.000 (0.093)   Loss 2.0215 (2.0407)   Prec@1 45.000 (42.259)   Prec@5 92.000 (87.557)   [2025-10-28 14:37:48]
  Epoch: [015][300/500]   Time 0.104 (0.168)   Data 0.000 (0.063)   Loss 1.9514 (2.0379)   Prec@1 53.000 (42.565)   Prec@5 94.000 (87.635)   [2025-10-28 14:37:59]
  Epoch: [015][400/500]   Time 0.102 (0.152)   Data 0.000 (0.047)   Loss 2.0371 (2.0379)   Prec@1 44.000 (42.459)   Prec@5 82.000 (87.544)   [2025-10-28 14:38:09]
  **Train** Prec@1 42.374 Prec@5 87.508 Error@1 57.626
  **Test** Prec@1 41.830 Prec@5 87.680 Error@1 58.170

==>>[2025-10-28 14:38:38] [Epoch=016/040] [Need: 00:41:53] [LR=0.0100] [Best : Accuracy=45.56, Error=54.44]
  Epoch: [016][000/500]   Time 17.970 (17.970)   Data 17.695 (17.695)   Loss 2.0457 (2.0457)   Prec@1 46.000 (46.000)   Prec@5 83.000 (83.000)   [2025-10-28 14:38:56]
  Epoch: [016][100/500]   Time 0.101 (0.276)   Data 0.001 (0.176)   Loss 1.9958 (2.0288)   Prec@1 45.000 (43.545)   Prec@5 89.000 (87.614)   [2025-10-28 14:39:06]
  Epoch: [016][200/500]   Time 0.105 (0.190)   Data 0.001 (0.089)   Loss 2.0366 (2.0321)   Prec@1 47.000 (43.119)   Prec@5 89.000 (87.657)   [2025-10-28 14:39:17]
  Epoch: [016][300/500]   Time 0.107 (0.162)   Data 0.001 (0.059)   Loss 2.0285 (2.0309)   Prec@1 44.000 (43.276)   Prec@5 90.000 (87.731)   [2025-10-28 14:39:27]
  Epoch: [016][400/500]   Time 0.099 (0.147)   Data 0.000 (0.045)   Loss 1.9899 (2.0304)   Prec@1 47.000 (43.222)   Prec@5 90.000 (87.633)   [2025-10-28 14:39:38]
  **Train** Prec@1 43.218 Prec@5 87.616 Error@1 56.782
  **Test** Prec@1 43.800 Prec@5 87.380 Error@1 56.200

==>>[2025-10-28 14:40:07] [Epoch=017/040] [Need: 00:39:47] [LR=0.0100] [Best : Accuracy=45.56, Error=54.44]
  Epoch: [017][000/500]   Time 17.929 (17.929)   Data 17.659 (17.659)   Loss 2.0086 (2.0086)   Prec@1 46.000 (46.000)   Prec@5 91.000 (91.000)   [2025-10-28 14:40:25]
  Epoch: [017][100/500]   Time 0.102 (0.275)   Data 0.000 (0.175)   Loss 2.0176 (2.0202)   Prec@1 50.000 (44.347)   Prec@5 87.000 (87.861)   [2025-10-28 14:40:35]
  Epoch: [017][200/500]   Time 0.108 (0.190)   Data 0.000 (0.088)   Loss 1.9887 (2.0195)   Prec@1 52.000 (44.418)   Prec@5 88.000 (87.806)   [2025-10-28 14:40:45]
  Epoch: [017][300/500]   Time 0.101 (0.161)   Data 0.000 (0.059)   Loss 2.0986 (2.0197)   Prec@1 37.000 (44.369)   Prec@5 85.000 (88.010)   [2025-10-28 14:40:55]
  Epoch: [017][400/500]   Time 0.099 (0.147)   Data 0.000 (0.045)   Loss 2.0029 (2.0214)   Prec@1 46.000 (44.120)   Prec@5 84.000 (87.953)   [2025-10-28 14:41:06]
  **Train** Prec@1 44.206 Prec@5 88.072 Error@1 55.794
  **Test** Prec@1 45.030 Prec@5 90.420 Error@1 54.970

==>>[2025-10-28 14:41:36] [Epoch=018/040] [Need: 00:37:45] [LR=0.0100] [Best : Accuracy=45.56, Error=54.44]
  Epoch: [018][000/500]   Time 17.837 (17.837)   Data 17.477 (17.477)   Loss 1.9981 (1.9981)   Prec@1 49.000 (49.000)   Prec@5 87.000 (87.000)   [2025-10-28 14:41:54]
  Epoch: [018][100/500]   Time 0.103 (0.276)   Data 0.000 (0.174)   Loss 2.0354 (2.0142)   Prec@1 45.000 (45.050)   Prec@5 79.000 (88.564)   [2025-10-28 14:42:04]
  Epoch: [018][200/500]   Time 0.104 (0.191)   Data 0.001 (0.088)   Loss 2.0366 (2.0140)   Prec@1 41.000 (44.876)   Prec@5 90.000 (88.517)   [2025-10-28 14:42:14]
  Epoch: [018][300/500]   Time 0.102 (0.162)   Data 0.000 (0.059)   Loss 2.0489 (2.0163)   Prec@1 43.000 (44.581)   Prec@5 88.000 (88.532)   [2025-10-28 14:42:24]
  Epoch: [018][400/500]   Time 0.112 (0.148)   Data 0.000 (0.044)   Loss 2.0102 (2.0152)   Prec@1 46.000 (44.731)   Prec@5 84.000 (88.676)   [2025-10-28 14:42:35]
  **Train** Prec@1 44.768 Prec@5 88.670 Error@1 55.232
  **Test** Prec@1 45.620 Prec@5 85.980 Error@1 54.380
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 14:43:04] [Epoch=019/040] [Need: 00:35:46] [LR=0.0100] [Best : Accuracy=45.62, Error=54.38]
  Epoch: [019][000/500]   Time 17.815 (17.815)   Data 17.484 (17.484)   Loss 2.0347 (2.0347)   Prec@1 44.000 (44.000)   Prec@5 91.000 (91.000)   [2025-10-28 14:43:22]
  Epoch: [019][100/500]   Time 0.103 (0.274)   Data 0.000 (0.174)   Loss 1.9759 (2.0090)   Prec@1 48.000 (45.535)   Prec@5 92.000 (89.069)   [2025-10-28 14:43:32]
  Epoch: [019][200/500]   Time 0.102 (0.189)   Data 0.001 (0.088)   Loss 2.0196 (2.0076)   Prec@1 46.000 (45.677)   Prec@5 82.000 (88.831)   [2025-10-28 14:43:42]
  Epoch: [019][300/500]   Time 0.100 (0.160)   Data 0.000 (0.059)   Loss 1.9867 (2.0074)   Prec@1 50.000 (45.631)   Prec@5 92.000 (88.904)   [2025-10-28 14:43:53]
  Epoch: [019][400/500]   Time 0.098 (0.146)   Data 0.000 (0.044)   Loss 1.9811 (2.0086)   Prec@1 48.000 (45.476)   Prec@5 94.000 (88.726)   [2025-10-28 14:44:03]
  **Train** Prec@1 45.428 Prec@5 88.772 Error@1 54.572
  **Test** Prec@1 48.350 Prec@5 89.420 Error@1 51.650
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 14:44:32] [Epoch=020/040] [Need: 00:33:50] [LR=0.0100] [Best : Accuracy=48.35, Error=51.65]
  Epoch: [020][000/500]   Time 18.244 (18.244)   Data 17.894 (17.894)   Loss 1.9399 (1.9399)   Prec@1 51.000 (51.000)   Prec@5 92.000 (92.000)   [2025-10-28 14:44:51]
  Epoch: [020][100/500]   Time 0.101 (0.278)   Data 0.000 (0.178)   Loss 1.9870 (2.0112)   Prec@1 47.000 (44.663)   Prec@5 87.000 (89.099)   [2025-10-28 14:45:01]
  Epoch: [020][200/500]   Time 0.102 (0.191)   Data 0.000 (0.090)   Loss 2.0841 (2.0064)   Prec@1 36.000 (45.398)   Prec@5 91.000 (89.239)   [2025-10-28 14:45:11]
  Epoch: [020][300/500]   Time 0.102 (0.162)   Data 0.001 (0.060)   Loss 2.0016 (2.0050)   Prec@1 43.000 (45.691)   Prec@5 94.000 (89.140)   [2025-10-28 14:45:21]
  Epoch: [020][400/500]   Time 0.101 (0.147)   Data 0.001 (0.045)   Loss 2.0199 (2.0065)   Prec@1 44.000 (45.601)   Prec@5 92.000 (89.127)   [2025-10-28 14:45:31]
  **Train** Prec@1 45.754 Prec@5 89.068 Error@1 54.246
  **Test** Prec@1 48.430 Prec@5 88.970 Error@1 51.570
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 14:46:01] [Epoch=021/040] [Need: 00:31:56] [LR=0.0100] [Best : Accuracy=48.43, Error=51.57]
  Epoch: [021][000/500]   Time 17.624 (17.624)   Data 17.347 (17.347)   Loss 2.0656 (2.0656)   Prec@1 40.000 (40.000)   Prec@5 81.000 (81.000)   [2025-10-28 14:46:18]
  Epoch: [021][100/500]   Time 0.105 (0.273)   Data 0.001 (0.172)   Loss 2.0099 (1.9951)   Prec@1 45.000 (46.861)   Prec@5 89.000 (89.099)   [2025-10-28 14:46:28]
  Epoch: [021][200/500]   Time 0.106 (0.189)   Data 0.000 (0.087)   Loss 1.9846 (1.9956)   Prec@1 49.000 (46.612)   Prec@5 86.000 (89.169)   [2025-10-28 14:46:39]
  Epoch: [021][300/500]   Time 0.585 (0.174)   Data 0.002 (0.058)   Loss 2.0161 (1.9931)   Prec@1 42.000 (46.784)   Prec@5 89.000 (89.528)   [2025-10-28 14:46:53]
  Epoch: [021][400/500]   Time 0.235 (0.217)   Data 0.001 (0.044)   Loss 1.9790 (1.9955)   Prec@1 50.000 (46.546)   Prec@5 94.000 (89.392)   [2025-10-28 14:47:28]
  **Train** Prec@1 46.282 Prec@5 89.334 Error@1 53.718
  **Test** Prec@1 42.330 Prec@5 80.990 Error@1 57.670

==>>[2025-10-28 14:48:25] [Epoch=022/040] [Need: 00:30:51] [LR=0.0100] [Best : Accuracy=48.43, Error=51.57]
  Epoch: [022][000/500]   Time 26.547 (26.547)   Data 25.960 (25.960)   Loss 2.0130 (2.0130)   Prec@1 46.000 (46.000)   Prec@5 89.000 (89.000)   [2025-10-28 14:48:52]
  Epoch: [022][100/500]   Time 0.330 (0.585)   Data 0.002 (0.259)   Loss 2.0356 (1.9900)   Prec@1 43.000 (47.257)   Prec@5 87.000 (89.317)   [2025-10-28 14:49:24]
  Epoch: [022][200/500]   Time 0.333 (0.461)   Data 0.003 (0.131)   Loss 2.0055 (1.9902)   Prec@1 46.000 (47.124)   Prec@5 96.000 (89.473)   [2025-10-28 14:49:58]
  Epoch: [022][300/500]   Time 0.216 (0.414)   Data 0.003 (0.088)   Loss 1.9449 (1.9898)   Prec@1 54.000 (47.096)   Prec@5 93.000 (89.568)   [2025-10-28 14:50:30]
  Epoch: [022][400/500]   Time 0.362 (0.391)   Data 0.002 (0.066)   Loss 1.9196 (1.9922)   Prec@1 54.000 (46.878)   Prec@5 91.000 (89.486)   [2025-10-28 14:51:02]
  **Train** Prec@1 46.810 Prec@5 89.484 Error@1 53.190
  **Test** Prec@1 48.830 Prec@5 89.360 Error@1 51.170
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 14:51:58] [Epoch=023/040] [Need: 00:30:29] [LR=0.0100] [Best : Accuracy=48.83, Error=51.17]
  Epoch: [023][000/500]   Time 23.084 (23.084)   Data 22.489 (22.489)   Loss 2.0006 (2.0006)   Prec@1 48.000 (48.000)   Prec@5 90.000 (90.000)   [2025-10-28 14:52:21]
  Epoch: [023][100/500]   Time 0.342 (0.570)   Data 0.002 (0.224)   Loss 1.9791 (1.9892)   Prec@1 48.000 (47.168)   Prec@5 81.000 (88.446)   [2025-10-28 14:52:56]
  Epoch: [023][200/500]   Time 0.405 (0.462)   Data 0.001 (0.113)   Loss 2.0490 (1.9849)   Prec@1 40.000 (47.632)   Prec@5 89.000 (88.851)   [2025-10-28 14:53:31]
  Epoch: [023][300/500]   Time 0.310 (0.424)   Data 0.001 (0.076)   Loss 2.0028 (1.9852)   Prec@1 46.000 (47.658)   Prec@5 86.000 (89.083)   [2025-10-28 14:54:06]
  Epoch: [023][400/500]   Time 0.334 (0.405)   Data 0.001 (0.058)   Loss 2.0597 (1.9874)   Prec@1 40.000 (47.466)   Prec@5 85.000 (89.112)   [2025-10-28 14:54:41]
  **Train** Prec@1 47.542 Prec@5 89.206 Error@1 52.458
  **Test** Prec@1 44.980 Prec@5 82.330 Error@1 55.020

==>>[2025-10-28 14:55:37] [Epoch=024/040] [Need: 00:29:56] [LR=0.0100] [Best : Accuracy=48.83, Error=51.17]
  Epoch: [024][000/500]   Time 23.477 (23.477)   Data 22.891 (22.891)   Loss 2.0688 (2.0688)   Prec@1 37.000 (37.000)   Prec@5 88.000 (88.000)   [2025-10-28 14:56:00]
  Epoch: [024][100/500]   Time 0.313 (0.567)   Data 0.001 (0.228)   Loss 1.9592 (1.9874)   Prec@1 52.000 (47.267)   Prec@5 91.000 (89.079)   [2025-10-28 14:56:34]
  Epoch: [024][200/500]   Time 0.319 (0.459)   Data 0.002 (0.115)   Loss 2.0273 (1.9850)   Prec@1 42.000 (47.522)   Prec@5 94.000 (89.204)   [2025-10-28 14:57:09]
  Epoch: [024][300/500]   Time 0.310 (0.423)   Data 0.002 (0.078)   Loss 1.9706 (1.9879)   Prec@1 46.000 (47.266)   Prec@5 88.000 (89.076)   [2025-10-28 14:57:44]
  Epoch: [024][400/500]   Time 0.402 (0.403)   Data 0.001 (0.059)   Loss 1.9958 (1.9877)   Prec@1 49.000 (47.352)   Prec@5 88.000 (88.945)   [2025-10-28 14:58:19]
  **Train** Prec@1 47.552 Prec@5 89.048 Error@1 52.448
  **Test** Prec@1 43.710 Prec@5 82.660 Error@1 56.290

==>>[2025-10-28 14:59:15] [Epoch=025/040] [Need: 00:29:07] [LR=0.0010] [Best : Accuracy=48.83, Error=51.17]
  Epoch: [025][000/500]   Time 22.987 (22.987)   Data 22.401 (22.401)   Loss 1.9720 (1.9720)   Prec@1 49.000 (49.000)   Prec@5 86.000 (86.000)   [2025-10-28 14:59:38]
  Epoch: [025][100/500]   Time 0.347 (0.562)   Data 0.001 (0.223)   Loss 1.9423 (1.9640)   Prec@1 55.000 (49.683)   Prec@5 92.000 (89.871)   [2025-10-28 15:00:12]
  Epoch: [025][200/500]   Time 0.392 (0.455)   Data 0.001 (0.113)   Loss 1.8977 (1.9613)   Prec@1 59.000 (50.154)   Prec@5 94.000 (90.254)   [2025-10-28 15:00:46]
  Epoch: [025][300/500]   Time 0.349 (0.421)   Data 0.002 (0.076)   Loss 1.8959 (1.9586)   Prec@1 57.000 (50.402)   Prec@5 92.000 (90.575)   [2025-10-28 15:01:22]
  Epoch: [025][400/500]   Time 0.418 (0.404)   Data 0.002 (0.058)   Loss 1.9241 (1.9556)   Prec@1 54.000 (50.773)   Prec@5 91.000 (90.653)   [2025-10-28 15:01:57]
  **Train** Prec@1 50.918 Prec@5 90.872 Error@1 49.082
  **Test** Prec@1 49.590 Prec@5 86.620 Error@1 50.410
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 15:02:53] [Epoch=026/040] [Need: 00:28:05] [LR=0.0010] [Best : Accuracy=49.59, Error=50.41]
  Epoch: [026][000/500]   Time 24.319 (24.319)   Data 23.681 (23.681)   Loss 1.9384 (1.9384)   Prec@1 52.000 (52.000)   Prec@5 94.000 (94.000)   [2025-10-28 15:03:17]
  Epoch: [026][100/500]   Time 0.319 (0.603)   Data 0.001 (0.236)   Loss 1.9574 (1.9374)   Prec@1 50.000 (52.842)   Prec@5 91.000 (91.564)   [2025-10-28 15:03:54]
  Epoch: [026][200/500]   Time 0.112 (0.447)   Data 0.001 (0.119)   Loss 1.9307 (1.9391)   Prec@1 54.000 (52.537)   Prec@5 89.000 (91.652)   [2025-10-28 15:04:23]
  Epoch: [026][300/500]   Time 0.105 (0.334)   Data 0.000 (0.080)   Loss 1.8989 (1.9397)   Prec@1 59.000 (52.492)   Prec@5 93.000 (91.409)   [2025-10-28 15:04:34]
  Epoch: [026][400/500]   Time 0.105 (0.278)   Data 0.001 (0.060)   Loss 1.9540 (1.9383)   Prec@1 52.000 (52.666)   Prec@5 90.000 (91.449)   [2025-10-28 15:04:44]
  **Train** Prec@1 52.612 Prec@5 91.330 Error@1 47.388
  **Test** Prec@1 52.440 Prec@5 88.940 Error@1 47.560
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 15:05:15] [Epoch=027/040] [Need: 00:26:15] [LR=0.0010] [Best : Accuracy=52.44, Error=47.56]
  Epoch: [027][000/500]   Time 19.005 (19.005)   Data 18.664 (18.664)   Loss 1.9401 (1.9401)   Prec@1 53.000 (53.000)   Prec@5 96.000 (96.000)   [2025-10-28 15:05:34]
  Epoch: [027][100/500]   Time 0.110 (0.291)   Data 0.001 (0.185)   Loss 1.8088 (1.9392)   Prec@1 68.000 (52.376)   Prec@5 98.000 (91.257)   [2025-10-28 15:05:44]
  Epoch: [027][200/500]   Time 0.104 (0.199)   Data 0.001 (0.093)   Loss 1.9203 (1.9360)   Prec@1 55.000 (52.781)   Prec@5 86.000 (91.204)   [2025-10-28 15:05:55]
  Epoch: [027][300/500]   Time 0.108 (0.168)   Data 0.000 (0.063)   Loss 1.9250 (1.9353)   Prec@1 54.000 (52.904)   Prec@5 95.000 (91.332)   [2025-10-28 15:06:06]
  Epoch: [027][400/500]   Time 0.104 (0.153)   Data 0.001 (0.047)   Loss 1.9319 (1.9358)   Prec@1 53.000 (52.833)   Prec@5 97.000 (91.272)   [2025-10-28 15:06:16]
  **Train** Prec@1 52.918 Prec@5 91.348 Error@1 47.082
  **Test** Prec@1 50.020 Prec@5 87.750 Error@1 49.980

==>>[2025-10-28 15:06:47] [Epoch=028/040] [Need: 00:24:02] [LR=0.0010] [Best : Accuracy=52.44, Error=47.56]
  Epoch: [028][000/500]   Time 19.086 (19.086)   Data 18.706 (18.706)   Loss 1.9214 (1.9214)   Prec@1 54.000 (54.000)   Prec@5 88.000 (88.000)   [2025-10-28 15:07:06]
  Epoch: [028][100/500]   Time 0.110 (0.292)   Data 0.000 (0.186)   Loss 1.9338 (1.9271)   Prec@1 52.000 (53.881)   Prec@5 93.000 (91.604)   [2025-10-28 15:07:17]
  Epoch: [028][200/500]   Time 0.109 (0.201)   Data 0.000 (0.094)   Loss 1.9364 (1.9283)   Prec@1 51.000 (53.602)   Prec@5 92.000 (91.438)   [2025-10-28 15:07:27]
  Epoch: [028][300/500]   Time 0.101 (0.169)   Data 0.001 (0.063)   Loss 1.9738 (1.9327)   Prec@1 48.000 (53.163)   Prec@5 86.000 (91.332)   [2025-10-28 15:07:38]
  Epoch: [028][400/500]   Time 0.106 (0.153)   Data 0.001 (0.047)   Loss 1.9888 (1.9330)   Prec@1 48.000 (53.202)   Prec@5 92.000 (91.294)   [2025-10-28 15:07:49]
  **Train** Prec@1 53.124 Prec@5 91.266 Error@1 46.876
  **Test** Prec@1 49.560 Prec@5 87.910 Error@1 50.440

==>>[2025-10-28 15:08:17] [Epoch=029/040] [Need: 00:21:50] [LR=0.0010] [Best : Accuracy=52.44, Error=47.56]
  Epoch: [029][000/500]   Time 17.999 (17.999)   Data 17.636 (17.636)   Loss 1.9148 (1.9148)   Prec@1 55.000 (55.000)   Prec@5 87.000 (87.000)   [2025-10-28 15:08:35]
  Epoch: [029][100/500]   Time 0.100 (0.277)   Data 0.001 (0.175)   Loss 1.9205 (1.9289)   Prec@1 54.000 (53.614)   Prec@5 89.000 (91.634)   [2025-10-28 15:08:45]
  Epoch: [029][200/500]   Time 0.102 (0.190)   Data 0.001 (0.088)   Loss 1.8931 (1.9282)   Prec@1 58.000 (53.731)   Prec@5 94.000 (91.597)   [2025-10-28 15:08:56]
  Epoch: [029][300/500]   Time 0.098 (0.161)   Data 0.001 (0.059)   Loss 1.9030 (1.9306)   Prec@1 58.000 (53.422)   Prec@5 88.000 (91.449)   [2025-10-28 15:09:06]
  Epoch: [029][400/500]   Time 0.102 (0.146)   Data 0.001 (0.045)   Loss 1.9904 (1.9313)   Prec@1 48.000 (53.379)   Prec@5 91.000 (91.314)   [2025-10-28 15:09:16]
  **Train** Prec@1 53.494 Prec@5 91.398 Error@1 46.506
  **Test** Prec@1 55.490 Prec@5 91.570 Error@1 44.510
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 15:09:46] [Epoch=030/040] [Need: 00:19:41] [LR=0.0010] [Best : Accuracy=55.49, Error=44.51]
  Epoch: [030][000/500]   Time 17.758 (17.758)   Data 17.474 (17.474)   Loss 1.9578 (1.9578)   Prec@1 49.000 (49.000)   Prec@5 91.000 (91.000)   [2025-10-28 15:10:04]
  Epoch: [030][100/500]   Time 0.101 (0.275)   Data 0.001 (0.174)   Loss 1.9125 (1.9314)   Prec@1 55.000 (53.297)   Prec@5 93.000 (92.188)   [2025-10-28 15:10:14]
  Epoch: [030][200/500]   Time 0.107 (0.190)   Data 0.000 (0.088)   Loss 1.9257 (1.9283)   Prec@1 53.000 (53.532)   Prec@5 97.000 (92.035)   [2025-10-28 15:10:24]
  Epoch: [030][300/500]   Time 0.111 (0.162)   Data 0.001 (0.059)   Loss 1.9559 (1.9277)   Prec@1 50.000 (53.694)   Prec@5 94.000 (91.748)   [2025-10-28 15:10:34]
  Epoch: [030][400/500]   Time 0.101 (0.148)   Data 0.001 (0.044)   Loss 1.9257 (1.9288)   Prec@1 55.000 (53.596)   Prec@5 95.000 (91.601)   [2025-10-28 15:10:45]
  **Train** Prec@1 53.432 Prec@5 91.546 Error@1 46.568
  **Test** Prec@1 54.000 Prec@5 91.780 Error@1 46.000

==>>[2025-10-28 15:11:14] [Epoch=031/040] [Need: 00:17:34] [LR=0.0010] [Best : Accuracy=55.49, Error=44.51]
  Epoch: [031][000/500]   Time 18.113 (18.113)   Data 17.828 (17.828)   Loss 1.9329 (1.9329)   Prec@1 54.000 (54.000)   Prec@5 94.000 (94.000)   [2025-10-28 15:11:32]
  Epoch: [031][100/500]   Time 0.100 (0.280)   Data 0.001 (0.177)   Loss 1.8545 (1.9269)   Prec@1 61.000 (53.772)   Prec@5 90.000 (91.465)   [2025-10-28 15:11:43]
  Epoch: [031][200/500]   Time 0.105 (0.202)   Data 0.001 (0.089)   Loss 1.9102 (1.9292)   Prec@1 55.000 (53.567)   Prec@5 90.000 (91.308)   [2025-10-28 15:11:55]
  Epoch: [031][300/500]   Time 0.280 (0.179)   Data 0.002 (0.060)   Loss 1.9382 (1.9267)   Prec@1 50.000 (53.821)   Prec@5 88.000 (91.432)   [2025-10-28 15:12:08]
  Epoch: [031][400/500]   Time 0.276 (0.209)   Data 0.000 (0.045)   Loss 1.8684 (1.9277)   Prec@1 61.000 (53.718)   Prec@5 94.000 (91.464)   [2025-10-28 15:12:38]
  **Train** Prec@1 53.682 Prec@5 91.462 Error@1 46.318
  **Test** Prec@1 53.960 Prec@5 91.110 Error@1 46.040

==>>[2025-10-28 15:13:34] [Epoch=032/040] [Need: 00:15:42] [LR=0.0010] [Best : Accuracy=55.49, Error=44.51]
  Epoch: [032][000/500]   Time 27.452 (27.452)   Data 26.844 (26.844)   Loss 1.8967 (1.8967)   Prec@1 56.000 (56.000)   Prec@5 92.000 (92.000)   [2025-10-28 15:14:01]
  Epoch: [032][100/500]   Time 0.269 (0.577)   Data 0.002 (0.268)   Loss 1.9384 (1.9238)   Prec@1 51.000 (53.970)   Prec@5 93.000 (91.584)   [2025-10-28 15:14:32]
  Epoch: [032][200/500]   Time 0.318 (0.436)   Data 0.001 (0.135)   Loss 1.9780 (1.9283)   Prec@1 49.000 (53.438)   Prec@5 91.000 (91.438)   [2025-10-28 15:15:02]
  Epoch: [032][300/500]   Time 0.124 (0.373)   Data 0.001 (0.091)   Loss 1.8576 (1.9236)   Prec@1 64.000 (53.993)   Prec@5 92.000 (91.532)   [2025-10-28 15:15:26]
  Epoch: [032][400/500]   Time 0.123 (0.312)   Data 0.001 (0.068)   Loss 1.8349 (1.9235)   Prec@1 64.000 (53.995)   Prec@5 94.000 (91.571)   [2025-10-28 15:15:39]
  **Train** Prec@1 54.020 Prec@5 91.614 Error@1 45.980
  **Test** Prec@1 56.470 Prec@5 91.600 Error@1 43.530
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 15:16:17] [Epoch=033/040] [Need: 00:13:54] [LR=0.0010] [Best : Accuracy=56.47, Error=43.53]
  Epoch: [033][000/500]   Time 22.842 (22.842)   Data 22.454 (22.454)   Loss 1.8397 (1.8397)   Prec@1 63.000 (63.000)   Prec@5 88.000 (88.000)   [2025-10-28 15:16:40]
  Epoch: [033][100/500]   Time 0.124 (0.349)   Data 0.001 (0.223)   Loss 1.9108 (1.9233)   Prec@1 53.000 (54.208)   Prec@5 93.000 (91.713)   [2025-10-28 15:16:52]
  Epoch: [033][200/500]   Time 0.136 (0.238)   Data 0.002 (0.113)   Loss 1.9319 (1.9260)   Prec@1 52.000 (53.791)   Prec@5 92.000 (91.985)   [2025-10-28 15:17:05]
  Epoch: [033][300/500]   Time 0.123 (0.201)   Data 0.000 (0.075)   Loss 1.8869 (1.9227)   Prec@1 58.000 (54.183)   Prec@5 92.000 (91.950)   [2025-10-28 15:17:17]
  Epoch: [033][400/500]   Time 0.128 (0.184)   Data 0.000 (0.057)   Loss 1.9927 (1.9243)   Prec@1 48.000 (54.032)   Prec@5 88.000 (91.776)   [2025-10-28 15:17:31]
  **Train** Prec@1 54.164 Prec@5 91.812 Error@1 45.836
  **Test** Prec@1 54.950 Prec@5 92.080 Error@1 45.050

==>>[2025-10-28 15:18:42] [Epoch=034/040] [Need: 00:11:59] [LR=0.0010] [Best : Accuracy=56.47, Error=43.53]
  Epoch: [034][000/500]   Time 28.802 (28.802)   Data 28.193 (28.193)   Loss 1.9393 (1.9393)   Prec@1 54.000 (54.000)   Prec@5 95.000 (95.000)   [2025-10-28 15:19:10]
  Epoch: [034][100/500]   Time 0.425 (0.597)   Data 0.003 (0.281)   Loss 1.8543 (1.9218)   Prec@1 62.000 (54.129)   Prec@5 96.000 (91.851)   [2025-10-28 15:19:42]
  Epoch: [034][200/500]   Time 0.246 (0.467)   Data 0.002 (0.142)   Loss 1.9916 (1.9241)   Prec@1 45.000 (54.025)   Prec@5 90.000 (91.940)   [2025-10-28 15:20:15]
  Epoch: [034][300/500]   Time 0.256 (0.401)   Data 0.001 (0.096)   Loss 1.8992 (1.9224)   Prec@1 55.000 (54.216)   Prec@5 93.000 (92.003)   [2025-10-28 15:20:42]
  Epoch: [034][400/500]   Time 0.339 (0.374)   Data 0.002 (0.072)   Loss 1.9292 (1.9213)   Prec@1 55.000 (54.339)   Prec@5 91.000 (91.808)   [2025-10-28 15:21:11]
  **Train** Prec@1 54.386 Prec@5 91.708 Error@1 45.614
  **Test** Prec@1 55.510 Prec@5 92.220 Error@1 44.490

==>>[2025-10-28 15:22:12] [Epoch=035/040] [Need: 00:10:12] [LR=0.0010] [Best : Accuracy=56.47, Error=43.53]
  Epoch: [035][000/500]   Time 24.314 (24.314)   Data 23.715 (23.715)   Loss 1.9457 (1.9457)   Prec@1 52.000 (52.000)   Prec@5 94.000 (94.000)   [2025-10-28 15:22:36]
  Epoch: [035][100/500]   Time 0.257 (0.536)   Data 0.002 (0.237)   Loss 1.9442 (1.9266)   Prec@1 50.000 (53.802)   Prec@5 92.000 (91.574)   [2025-10-28 15:23:06]
  Epoch: [035][200/500]   Time 0.353 (0.461)   Data 0.001 (0.120)   Loss 1.9119 (1.9232)   Prec@1 54.000 (54.189)   Prec@5 92.000 (91.552)   [2025-10-28 15:23:44]
  Epoch: [035][300/500]   Time 0.241 (0.406)   Data 0.002 (0.081)   Loss 1.9141 (1.9238)   Prec@1 56.000 (54.163)   Prec@5 99.000 (91.608)   [2025-10-28 15:24:14]
  Epoch: [035][400/500]   Time 0.244 (0.383)   Data 0.002 (0.061)   Loss 1.8178 (1.9223)   Prec@1 65.000 (54.354)   Prec@5 95.000 (91.459)   [2025-10-28 15:24:46]
  **Train** Prec@1 54.338 Prec@5 91.522 Error@1 45.662
  **Test** Prec@1 54.380 Prec@5 92.070 Error@1 45.620

==>>[2025-10-28 15:25:50] [Epoch=036/040] [Need: 00:08:20] [LR=0.0010] [Best : Accuracy=56.47, Error=43.53]
  Epoch: [036][000/500]   Time 29.615 (29.615)   Data 28.652 (28.652)   Loss 1.8478 (1.8478)   Prec@1 63.000 (63.000)   Prec@5 96.000 (96.000)   [2025-10-28 15:26:20]
  Epoch: [036][100/500]   Time 0.265 (0.600)   Data 0.002 (0.285)   Loss 1.9801 (1.9178)   Prec@1 47.000 (54.584)   Prec@5 90.000 (91.970)   [2025-10-28 15:26:51]
  Epoch: [036][200/500]   Time 0.334 (0.466)   Data 0.000 (0.144)   Loss 1.9365 (1.9193)   Prec@1 51.000 (54.527)   Prec@5 96.000 (91.831)   [2025-10-28 15:27:24]
  Epoch: [036][300/500]   Time 0.369 (0.411)   Data 0.001 (0.097)   Loss 1.8500 (1.9158)   Prec@1 63.000 (54.907)   Prec@5 87.000 (91.890)   [2025-10-28 15:27:54]
  Epoch: [036][400/500]   Time 0.243 (0.378)   Data 0.002 (0.073)   Loss 1.9356 (1.9185)   Prec@1 51.000 (54.646)   Prec@5 90.000 (91.733)   [2025-10-28 15:28:22]
  **Train** Prec@1 54.644 Prec@5 91.774 Error@1 45.356
  **Test** Prec@1 55.850 Prec@5 92.030 Error@1 44.150

==>>[2025-10-28 15:29:24] [Epoch=037/040] [Need: 00:06:22] [LR=0.0010] [Best : Accuracy=56.47, Error=43.53]
  Epoch: [037][000/500]   Time 26.200 (26.200)   Data 25.506 (25.506)   Loss 1.8598 (1.8598)   Prec@1 60.000 (60.000)   Prec@5 94.000 (94.000)   [2025-10-28 15:29:50]
  Epoch: [037][100/500]   Time 0.325 (0.545)   Data 0.001 (0.255)   Loss 1.9470 (1.9260)   Prec@1 51.000 (53.812)   Prec@5 92.000 (91.327)   [2025-10-28 15:30:19]
  Epoch: [037][200/500]   Time 0.236 (0.407)   Data 0.001 (0.129)   Loss 1.8873 (1.9200)   Prec@1 60.000 (54.398)   Prec@5 87.000 (91.438)   [2025-10-28 15:30:45]
  Epoch: [037][300/500]   Time 0.251 (0.381)   Data 0.001 (0.087)   Loss 1.9607 (1.9187)   Prec@1 51.000 (54.475)   Prec@5 94.000 (91.621)   [2025-10-28 15:31:18]
  Epoch: [037][400/500]   Time 0.354 (0.359)   Data 0.002 (0.065)   Loss 1.8951 (1.9189)   Prec@1 60.000 (54.456)   Prec@5 93.000 (91.603)   [2025-10-28 15:31:48]
  **Train** Prec@1 54.616 Prec@5 91.592 Error@1 45.384
  **Test** Prec@1 55.770 Prec@5 92.380 Error@1 44.230

==>>[2025-10-28 15:32:44] [Epoch=038/040] [Need: 00:04:19] [LR=0.0010] [Best : Accuracy=56.47, Error=43.53]
  Epoch: [038][000/500]   Time 26.799 (26.799)   Data 26.207 (26.207)   Loss 1.9486 (1.9486)   Prec@1 51.000 (51.000)   Prec@5 88.000 (88.000)   [2025-10-28 15:33:11]
  Epoch: [038][100/500]   Time 0.109 (0.381)   Data 0.000 (0.260)   Loss 1.8769 (1.9223)   Prec@1 60.000 (54.356)   Prec@5 94.000 (91.495)   [2025-10-28 15:33:22]
  Epoch: [038][200/500]   Time 0.683 (0.293)   Data 0.001 (0.131)   Loss 1.9851 (1.9200)   Prec@1 47.000 (54.602)   Prec@5 89.000 (91.871)   [2025-10-28 15:33:43]
  Epoch: [038][300/500]   Time 0.258 (0.284)   Data 0.001 (0.088)   Loss 1.8872 (1.9191)   Prec@1 58.000 (54.591)   Prec@5 98.000 (91.751)   [2025-10-28 15:34:10]
  Epoch: [038][400/500]   Time 0.400 (0.282)   Data 0.001 (0.067)   Loss 1.8652 (1.9190)   Prec@1 61.000 (54.591)   Prec@5 91.000 (91.608)   [2025-10-28 15:34:37]
  **Train** Prec@1 54.694 Prec@5 91.708 Error@1 45.306
  **Test** Prec@1 54.470 Prec@5 91.010 Error@1 45.530

==>>[2025-10-28 15:35:33] [Epoch=039/040] [Need: 00:02:10] [LR=0.0010] [Best : Accuracy=56.47, Error=43.53]
  Epoch: [039][000/500]   Time 26.057 (26.057)   Data 25.574 (25.574)   Loss 1.9702 (1.9702)   Prec@1 49.000 (49.000)   Prec@5 89.000 (89.000)   [2025-10-28 15:35:59]
  Epoch: [039][100/500]   Time 0.258 (0.538)   Data 0.001 (0.255)   Loss 2.0005 (1.9156)   Prec@1 43.000 (54.970)   Prec@5 91.000 (91.762)   [2025-10-28 15:36:27]
  Epoch: [039][200/500]   Time 0.252 (0.410)   Data 0.001 (0.129)   Loss 1.9769 (1.9176)   Prec@1 47.000 (54.761)   Prec@5 93.000 (91.512)   [2025-10-28 15:36:56]
  Epoch: [039][300/500]   Time 0.290 (0.363)   Data 0.001 (0.087)   Loss 1.9229 (1.9165)   Prec@1 56.000 (54.870)   Prec@5 89.000 (91.598)   [2025-10-28 15:37:22]
  Epoch: [039][400/500]   Time 0.240 (0.344)   Data 0.001 (0.065)   Loss 1.8734 (1.9154)   Prec@1 61.000 (55.027)   Prec@5 91.000 (91.738)   [2025-10-28 15:37:51]
  **Train** Prec@1 55.084 Prec@5 91.752 Error@1 44.916
  **Test** Prec@1 55.320 Prec@5 91.830 Error@1 44.680
