save path : ./save/resnet9_quan/randbet_0.1_0.01_10_-1/results/4258
{'data_path': './dataset', 'arch': 'resnet9_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': True, 'clipping_coeff': 0.1, 'learning_rate': 0.01, 'manualSeed': 4258, 'save_path': './save/resnet9_quan/randbet_0.1_0.01_10_-1/results/4258', 'enable_bfa': True, 'resume': './save/resnet9_quan/randbet_0.1_0.01_10_-1/model_best.pth.tar', 'quan_bitwidth': None, 'reset_weight': True, 'evaluate': True, 'n_iter': 30, 'fine_tune': True, 'model_only': False, 'random_bfa': False, 'use_cuda': True}
Random Seed: 4258
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'resnet9_quan'
=> network :
 Sequential(
  (0): Sequential(
    (0): quan_Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (1): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (2): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (4): Dropout2d(p=0.2, inplace=False)
  (5): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (skip): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (6): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Dropout2d(p=0.3, inplace=False)
  (9): Sequential(
    (0): quan_Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (10): Sequential(
    (0): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (12): Dropout2d(p=0.3, inplace=False)
  (13): Sequential(
    (0): quan_Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (14): AdaptiveAvgPool2d(output_size=(1, 1))
  (15): Flatten()
  (16): quan_Linear(in_features=64, out_features=128, bias=False)
  (17): ReLU(inplace=True)
  (18): Dropout(p=0.35, inplace=False)
  (19): quan_Linear(in_features=128, out_features=10, bias=False)
  (20): Softmax(dim=1)
)
=> loading checkpoint './save/resnet9_quan/randbet_0.1_0.01_10_-1/model_best.pth.tar'
=> loaded checkpoint './save/resnet9_quan/randbet_0.1_0.01_10_-1/model_best.pth.tar' (epoch 0)
  **Test** Prec@1 54.450 Prec@5 91.450 Error@1 45.550
k_top=100
Attack_sample=100
************** ATTACK iteration *****************
Iteration: [001/030]   Attack Time 0.551 (0.551)  [2025-10-29 14:38:47]
loss before attack: 1.5592
loss after attack: 1.6946
bit flips: 1
hamming_dist: 1
  **Test** Prec@1 52.950 Prec@5 89.310 Error@1 47.050
iteration Time 19.425 (19.425)
************** ATTACK iteration *****************
Iteration: [002/030]   Attack Time 0.264 (0.407)  [2025-10-29 14:39:06]
loss before attack: 1.6946
loss after attack: 1.7853
bit flips: 2
hamming_dist: 2
  **Test** Prec@1 50.130 Prec@5 87.490 Error@1 49.870
iteration Time 19.530 (19.478)
************** ATTACK iteration *****************
Iteration: [003/030]   Attack Time 0.289 (0.368)  [2025-10-29 14:39:26]
loss before attack: 1.7853
loss after attack: 1.8574
bit flips: 3
hamming_dist: 3
  **Test** Prec@1 48.350 Prec@5 87.330 Error@1 51.650
iteration Time 20.535 (19.830)
************** ATTACK iteration *****************
Iteration: [004/030]   Attack Time 0.258 (0.340)  [2025-10-29 14:39:47]
loss before attack: 1.8574
loss after attack: 1.9291
bit flips: 4
hamming_dist: 4
  **Test** Prec@1 46.830 Prec@5 87.490 Error@1 53.170
iteration Time 19.876 (19.841)
************** ATTACK iteration *****************
Iteration: [005/030]   Attack Time 0.218 (0.316)  [2025-10-29 14:40:07]
loss before attack: 1.9291
loss after attack: 1.9954
bit flips: 5
hamming_dist: 5
  **Test** Prec@1 42.990 Prec@5 84.440 Error@1 57.010
iteration Time 19.585 (19.790)
************** ATTACK iteration *****************
Iteration: [006/030]   Attack Time 0.246 (0.304)  [2025-10-29 14:40:27]
loss before attack: 1.9954
loss after attack: 2.0472
bit flips: 6
hamming_dist: 6
  **Test** Prec@1 40.410 Prec@5 83.110 Error@1 59.590
iteration Time 20.123 (19.846)
************** ATTACK iteration *****************
Iteration: [007/030]   Attack Time 0.217 (0.292)  [2025-10-29 14:40:47]
loss before attack: 2.0472
loss after attack: 2.0817
bit flips: 7
hamming_dist: 7
  **Test** Prec@1 37.160 Prec@5 81.840 Error@1 62.840
iteration Time 19.039 (19.730)
************** ATTACK iteration *****************
Iteration: [008/030]   Attack Time 0.214 (0.282)  [2025-10-29 14:41:06]
loss before attack: 2.0817
loss after attack: 2.1262
bit flips: 8
hamming_dist: 8
  **Test** Prec@1 33.900 Prec@5 79.330 Error@1 66.100
iteration Time 18.417 (19.566)
************** ATTACK iteration *****************
Iteration: [009/030]   Attack Time 0.195 (0.272)  [2025-10-29 14:41:25]
loss before attack: 2.1262
loss after attack: 2.1718
bit flips: 9
hamming_dist: 9
  **Test** Prec@1 31.200 Prec@5 76.710 Error@1 68.800
iteration Time 18.356 (19.432)
************** ATTACK iteration *****************
Iteration: [010/030]   Attack Time 0.186 (0.264)  [2025-10-29 14:41:44]
loss before attack: 2.1718
loss after attack: 2.2104
bit flips: 10
hamming_dist: 10
  **Test** Prec@1 29.640 Prec@5 75.260 Error@1 70.360
iteration Time 18.380 (19.327)
************** ATTACK iteration *****************
Iteration: [011/030]   Attack Time 0.186 (0.257)  [2025-10-29 14:42:02]
loss before attack: 2.2104
loss after attack: 2.2632
bit flips: 11
hamming_dist: 11
  **Test** Prec@1 26.440 Prec@5 71.280 Error@1 73.560
iteration Time 18.213 (19.225)
************** ATTACK iteration *****************
Iteration: [012/030]   Attack Time 0.184 (0.251)  [2025-10-29 14:42:20]
loss before attack: 2.2632
loss after attack: 2.2977
bit flips: 12
hamming_dist: 12
  **Test** Prec@1 24.300 Prec@5 68.340 Error@1 75.700
iteration Time 18.428 (19.159)
************** ATTACK iteration *****************
Iteration: [013/030]   Attack Time 0.182 (0.245)  [2025-10-29 14:42:39]
loss before attack: 2.2977
loss after attack: 2.3178
bit flips: 13
hamming_dist: 13
  **Test** Prec@1 22.560 Prec@5 65.770 Error@1 77.440
iteration Time 18.264 (19.090)
************** ATTACK iteration *****************
Iteration: [014/030]   Attack Time 0.195 (0.242)  [2025-10-29 14:42:58]
loss before attack: 2.3178
loss after attack: 2.3334
bit flips: 14
hamming_dist: 14
  **Test** Prec@1 20.410 Prec@5 63.440 Error@1 79.590
iteration Time 18.537 (19.050)
************** ATTACK iteration *****************
Iteration: [015/030]   Attack Time 0.188 (0.238)  [2025-10-29 14:43:16]
loss before attack: 2.3334
loss after attack: 2.3480
bit flips: 15
hamming_dist: 15
  **Test** Prec@1 18.940 Prec@5 63.050 Error@1 81.060
iteration Time 18.181 (18.993)
************** ATTACK iteration *****************
Iteration: [016/030]   Attack Time 0.212 (0.237)  [2025-10-29 14:43:35]
loss before attack: 2.3480
loss after attack: 2.3530
bit flips: 16
hamming_dist: 16
  **Test** Prec@1 17.800 Prec@5 62.190 Error@1 82.200
iteration Time 18.186 (18.942)
************** ATTACK iteration *****************
Iteration: [017/030]   Attack Time 0.200 (0.234)  [2025-10-29 14:43:53]
loss before attack: 2.3530
loss after attack: 2.3606
bit flips: 17
hamming_dist: 17
  **Test** Prec@1 18.150 Prec@5 62.110 Error@1 81.850
iteration Time 18.215 (18.899)
************** ATTACK iteration *****************
Iteration: [018/030]   Attack Time 0.208 (0.233)  [2025-10-29 14:44:11]
loss before attack: 2.3606
loss after attack: 2.3670
bit flips: 18
hamming_dist: 18
  **Test** Prec@1 19.240 Prec@5 62.740 Error@1 80.760
iteration Time 18.385 (18.871)
************** ATTACK iteration *****************
Iteration: [019/030]   Attack Time 0.181 (0.230)  [2025-10-29 14:44:30]
loss before attack: 2.3670
loss after attack: 2.3913
bit flips: 19
hamming_dist: 19
  **Test** Prec@1 16.740 Prec@5 60.080 Error@1 83.260
iteration Time 19.226 (18.889)
************** ATTACK iteration *****************
Iteration: [020/030]   Attack Time 0.187 (0.228)  [2025-10-29 14:44:49]
loss before attack: 2.3913
loss after attack: 2.4012
bit flips: 20
hamming_dist: 20
  **Test** Prec@1 16.010 Prec@5 57.580 Error@1 83.990
iteration Time 18.439 (18.867)
************** ATTACK iteration *****************
Iteration: [021/030]   Attack Time 0.185 (0.226)  [2025-10-29 14:45:08]
loss before attack: 2.4012
loss after attack: 2.4090
bit flips: 21
hamming_dist: 21
  **Test** Prec@1 14.810 Prec@5 56.170 Error@1 85.190
iteration Time 18.343 (18.842)
************** ATTACK iteration *****************
Iteration: [022/030]   Attack Time 0.186 (0.224)  [2025-10-29 14:45:27]
loss before attack: 2.4090
loss after attack: 2.4136
bit flips: 22
hamming_dist: 22
  **Test** Prec@1 12.910 Prec@5 54.800 Error@1 87.090
iteration Time 19.144 (18.856)
************** ATTACK iteration *****************
Iteration: [023/030]   Attack Time 0.168 (0.222)  [2025-10-29 14:45:46]
loss before attack: 2.4136
loss after attack: 2.4164
bit flips: 23
hamming_dist: 23
  **Test** Prec@1 11.840 Prec@5 53.130 Error@1 88.160
iteration Time 18.428 (18.837)
************** ATTACK iteration *****************
Iteration: [024/030]   Attack Time 0.177 (0.220)  [2025-10-29 14:46:05]
loss before attack: 2.4164
loss after attack: 2.4182
bit flips: 24
hamming_dist: 24
  **Test** Prec@1 10.850 Prec@5 52.420 Error@1 89.150
iteration Time 18.210 (18.811)
