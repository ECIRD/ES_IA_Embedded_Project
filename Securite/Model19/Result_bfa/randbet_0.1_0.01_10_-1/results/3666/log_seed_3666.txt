save path : ./save/resnet9_quan/randbet_0.1_0.01_10_-1/results/3666
{'data_path': './dataset', 'arch': 'resnet9_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': True, 'clipping_coeff': 0.1, 'learning_rate': 0.01, 'manualSeed': 3666, 'save_path': './save/resnet9_quan/randbet_0.1_0.01_10_-1/results/3666', 'enable_bfa': True, 'resume': './save/resnet9_quan/randbet_0.1_0.01_10_-1/model_best.pth.tar', 'quan_bitwidth': None, 'reset_weight': True, 'evaluate': True, 'n_iter': 30, 'fine_tune': True, 'model_only': False, 'random_bfa': False, 'use_cuda': True}
Random Seed: 3666
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'resnet9_quan'
=> network :
 Sequential(
  (0): Sequential(
    (0): quan_Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (1): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (2): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (4): Dropout2d(p=0.2, inplace=False)
  (5): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (skip): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (6): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Dropout2d(p=0.3, inplace=False)
  (9): Sequential(
    (0): quan_Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (10): Sequential(
    (0): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (12): Dropout2d(p=0.3, inplace=False)
  (13): Sequential(
    (0): quan_Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (14): AdaptiveAvgPool2d(output_size=(1, 1))
  (15): Flatten()
  (16): quan_Linear(in_features=64, out_features=128, bias=False)
  (17): ReLU(inplace=True)
  (18): Dropout(p=0.35, inplace=False)
  (19): quan_Linear(in_features=128, out_features=10, bias=False)
  (20): Softmax(dim=1)
)
=> loading checkpoint './save/resnet9_quan/randbet_0.1_0.01_10_-1/model_best.pth.tar'
=> loaded checkpoint './save/resnet9_quan/randbet_0.1_0.01_10_-1/model_best.pth.tar' (epoch 0)
  **Test** Prec@1 54.450 Prec@5 91.450 Error@1 45.550
k_top=100
Attack_sample=100
************** ATTACK iteration *****************
Iteration: [001/030]   Attack Time 0.549 (0.549)  [2025-10-29 14:31:02]
loss before attack: 1.5583
loss after attack: 1.6723
bit flips: 1
hamming_dist: 1
  **Test** Prec@1 52.260 Prec@5 88.880 Error@1 47.740
iteration Time 19.582 (19.582)
************** ATTACK iteration *****************
Iteration: [002/030]   Attack Time 0.260 (0.405)  [2025-10-29 14:31:22]
loss before attack: 1.6723
loss after attack: 1.7309
bit flips: 2
hamming_dist: 2
  **Test** Prec@1 49.710 Prec@5 86.910 Error@1 50.290
iteration Time 19.683 (19.632)
************** ATTACK iteration *****************
Iteration: [003/030]   Attack Time 0.263 (0.357)  [2025-10-29 14:31:42]
loss before attack: 1.7309
loss after attack: 1.8016
bit flips: 3
hamming_dist: 3
  **Test** Prec@1 47.130 Prec@5 84.800 Error@1 52.870
iteration Time 19.783 (19.682)
************** ATTACK iteration *****************
Iteration: [004/030]   Attack Time 0.257 (0.332)  [2025-10-29 14:32:02]
loss before attack: 1.8016
loss after attack: 1.8977
bit flips: 4
hamming_dist: 4
  **Test** Prec@1 46.250 Prec@5 85.430 Error@1 53.750
iteration Time 19.670 (19.679)
************** ATTACK iteration *****************
Iteration: [005/030]   Attack Time 0.208 (0.307)  [2025-10-29 14:32:22]
loss before attack: 1.8977
loss after attack: 1.9590
bit flips: 5
hamming_dist: 5
  **Test** Prec@1 44.300 Prec@5 84.150 Error@1 55.700
iteration Time 19.672 (19.678)
************** ATTACK iteration *****************
Iteration: [006/030]   Attack Time 0.229 (0.294)  [2025-10-29 14:32:42]
loss before attack: 1.9590
loss after attack: 2.0245
bit flips: 6
hamming_dist: 6
  **Test** Prec@1 41.000 Prec@5 82.870 Error@1 59.000
iteration Time 19.920 (19.718)
************** ATTACK iteration *****************
Iteration: [007/030]   Attack Time 0.217 (0.283)  [2025-10-29 14:33:02]
loss before attack: 2.0245
loss after attack: 2.0831
bit flips: 7
hamming_dist: 7
  **Test** Prec@1 36.900 Prec@5 81.590 Error@1 63.100
iteration Time 19.947 (19.751)
************** ATTACK iteration *****************
Iteration: [008/030]   Attack Time 0.221 (0.276)  [2025-10-29 14:33:22]
loss before attack: 2.0831
loss after attack: 2.1364
bit flips: 8
hamming_dist: 8
  **Test** Prec@1 35.030 Prec@5 80.610 Error@1 64.970
iteration Time 19.867 (19.765)
************** ATTACK iteration *****************
Iteration: [009/030]   Attack Time 0.208 (0.268)  [2025-10-29 14:33:42]
loss before attack: 2.1364
loss after attack: 2.1640
bit flips: 9
hamming_dist: 9
  **Test** Prec@1 32.560 Prec@5 78.560 Error@1 67.440
iteration Time 19.653 (19.753)
************** ATTACK iteration *****************
Iteration: [010/030]   Attack Time 0.226 (0.264)  [2025-10-29 14:34:02]
loss before attack: 2.1640
loss after attack: 2.1916
bit flips: 10
hamming_dist: 10
  **Test** Prec@1 29.160 Prec@5 75.430 Error@1 70.840
iteration Time 19.718 (19.749)
************** ATTACK iteration *****************
Iteration: [011/030]   Attack Time 0.176 (0.256)  [2025-10-29 14:34:22]
loss before attack: 2.1916
loss after attack: 2.2263
bit flips: 11
hamming_dist: 11
  **Test** Prec@1 25.930 Prec@5 71.820 Error@1 74.070
iteration Time 19.825 (19.756)
************** ATTACK iteration *****************
Iteration: [012/030]   Attack Time 0.186 (0.250)  [2025-10-29 14:34:42]
loss before attack: 2.2263
loss after attack: 2.2554
bit flips: 12
hamming_dist: 12
  **Test** Prec@1 22.590 Prec@5 67.820 Error@1 77.410
iteration Time 20.675 (19.833)
************** ATTACK iteration *****************
Iteration: [013/030]   Attack Time 0.180 (0.245)  [2025-10-29 14:35:03]
loss before attack: 2.2554
loss after attack: 2.2953
bit flips: 13
hamming_dist: 13
  **Test** Prec@1 20.320 Prec@5 64.930 Error@1 79.680
iteration Time 19.189 (19.783)
************** ATTACK iteration *****************
Iteration: [014/030]   Attack Time 0.178 (0.240)  [2025-10-29 14:35:22]
loss before attack: 2.2953
loss after attack: 2.3183
bit flips: 14
hamming_dist: 14
  **Test** Prec@1 18.450 Prec@5 62.660 Error@1 81.550
iteration Time 19.703 (19.778)
************** ATTACK iteration *****************
Iteration: [015/030]   Attack Time 0.185 (0.236)  [2025-10-29 14:35:42]
loss before attack: 2.3183
loss after attack: 2.3390
bit flips: 15
hamming_dist: 15
  **Test** Prec@1 17.160 Prec@5 61.160 Error@1 82.840
iteration Time 19.947 (19.789)
************** ATTACK iteration *****************
Iteration: [016/030]   Attack Time 0.173 (0.232)  [2025-10-29 14:36:02]
loss before attack: 2.3390
loss after attack: 2.3572
bit flips: 16
hamming_dist: 16
  **Test** Prec@1 15.850 Prec@5 60.000 Error@1 84.150
iteration Time 19.631 (19.779)
************** ATTACK iteration *****************
Iteration: [017/030]   Attack Time 0.163 (0.228)  [2025-10-29 14:36:22]
loss before attack: 2.3572
loss after attack: 2.3694
bit flips: 17
hamming_dist: 17
  **Test** Prec@1 13.580 Prec@5 56.670 Error@1 86.420
iteration Time 19.367 (19.755)
************** ATTACK iteration *****************
Iteration: [018/030]   Attack Time 0.172 (0.225)  [2025-10-29 14:36:42]
loss before attack: 2.3694
loss after attack: 2.3897
bit flips: 18
hamming_dist: 18
  **Test** Prec@1 12.230 Prec@5 55.570 Error@1 87.770
iteration Time 19.237 (19.726)
************** ATTACK iteration *****************
Iteration: [019/030]   Attack Time 0.180 (0.223)  [2025-10-29 14:37:01]
loss before attack: 2.3897
loss after attack: 2.4052
bit flips: 19
hamming_dist: 19
  **Test** Prec@1 11.450 Prec@5 54.520 Error@1 88.550
iteration Time 19.044 (19.690)
************** ATTACK iteration *****************
Iteration: [020/030]   Attack Time 0.170 (0.220)  [2025-10-29 14:37:20]
loss before attack: 2.4052
loss after attack: 2.4198
bit flips: 20
hamming_dist: 20
  **Test** Prec@1 11.310 Prec@5 54.010 Error@1 88.690
iteration Time 19.589 (19.685)
************** ATTACK iteration *****************
Iteration: [021/030]   Attack Time 0.184 (0.218)  [2025-10-29 14:37:40]
loss before attack: 2.4198
loss after attack: 2.4268
bit flips: 21
hamming_dist: 21
  **Test** Prec@1 10.940 Prec@5 53.600 Error@1 89.060
iteration Time 19.629 (19.682)
