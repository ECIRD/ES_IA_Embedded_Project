save path : ./save/resnet9_quan/randbet_0.1_0.01_10_-1
{'data_path': './dataset', 'arch': 'resnet9_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': True, 'clipping_coeff': 0.1, 'learning_rate': 0.01, 'manualSeed': 6278, 'save_path': './save/resnet9_quan/randbet_0.1_0.01_10_-1', 'enable_bfa': False, 'resume': '', 'quan_bitwidth': None, 'reset_weight': False, 'evaluate': False, 'n_iter': 30, 'model_only': False, 'random_bfa': False, 'use_cuda': True}
Random Seed: 6278
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'resnet9_quan'
=> network :
 Sequential(
  (0): Sequential(
    (0): quan_Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (1): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (2): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (4): Dropout2d(p=0.2, inplace=False)
  (5): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (skip): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (6): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Dropout2d(p=0.3, inplace=False)
  (9): Sequential(
    (0): quan_Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (10): Sequential(
    (0): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (12): Dropout2d(p=0.3, inplace=False)
  (13): Sequential(
    (0): quan_Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (14): AdaptiveAvgPool2d(output_size=(1, 1))
  (15): Flatten()
  (16): quan_Linear(in_features=64, out_features=128, bias=False)
  (17): ReLU(inplace=True)
  (18): Dropout(p=0.35, inplace=False)
  (19): quan_Linear(in_features=128, out_features=10, bias=False)
  (20): Softmax(dim=1)
)
=> do not use any checkpoint for resnet9_quan model

==>>[2025-10-28 15:39:07] [Epoch=000/040] [Need: 00:00:00] [LR=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/500]   Time 29.316 (29.316)   Data 27.547 (27.547)   Loss 2.3042 (2.3042)   Prec@1 6.000 (6.000)   Prec@5 44.000 (44.000)   [2025-10-28 15:39:36]
  Epoch: [000][100/500]   Time 0.320 (0.586)   Data 0.002 (0.275)   Loss 2.3013 (2.3024)   Prec@1 11.000 (10.396)   Prec@5 58.000 (50.693)   [2025-10-28 15:40:06]
  Epoch: [000][200/500]   Time 0.267 (0.446)   Data 0.001 (0.139)   Loss 2.2954 (2.3014)   Prec@1 17.000 (11.030)   Prec@5 63.000 (52.821)   [2025-10-28 15:40:36]
  Epoch: [000][300/500]   Time 0.253 (0.388)   Data 0.002 (0.094)   Loss 2.2910 (2.2994)   Prec@1 15.000 (12.233)   Prec@5 66.000 (55.581)   [2025-10-28 15:41:03]
  Epoch: [000][400/500]   Time 0.372 (0.359)   Data 0.001 (0.071)   Loss 2.2781 (2.2953)   Prec@1 17.000 (13.219)   Prec@5 75.000 (58.387)   [2025-10-28 15:41:31]
  **Train** Prec@1 14.006 Prec@5 60.490 Error@1 85.994
  **Test** Prec@1 16.880 Prec@5 71.040 Error@1 83.120
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 15:42:26] [Epoch=001/040] [Need: 02:09:29] [LR=0.0100] [Best : Accuracy=16.88, Error=83.12]
  Epoch: [001][000/500]   Time 22.974 (22.974)   Data 22.374 (22.374)   Loss 2.2968 (2.2968)   Prec@1 16.000 (16.000)   Prec@5 60.000 (60.000)   [2025-10-28 15:42:49]
  Epoch: [001][100/500]   Time 0.403 (0.566)   Data 0.002 (0.223)   Loss 2.2607 (2.2675)   Prec@1 18.000 (16.941)   Prec@5 72.000 (69.317)   [2025-10-28 15:43:24]
  Epoch: [001][200/500]   Time 0.287 (0.460)   Data 0.001 (0.113)   Loss 2.2377 (2.2641)   Prec@1 24.000 (17.239)   Prec@5 74.000 (69.726)   [2025-10-28 15:43:59]
  Epoch: [001][300/500]   Time 0.334 (0.411)   Data 0.002 (0.076)   Loss 2.2721 (2.2608)   Prec@1 13.000 (17.412)   Prec@5 67.000 (70.259)   [2025-10-28 15:44:30]
  Epoch: [001][400/500]   Time 0.297 (0.386)   Data 0.002 (0.057)   Loss 2.2615 (2.2580)   Prec@1 16.000 (17.569)   Prec@5 72.000 (70.868)   [2025-10-28 15:45:01]
  **Train** Prec@1 17.576 Prec@5 71.120 Error@1 82.424
  **Test** Prec@1 18.430 Prec@5 72.870 Error@1 81.570
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 15:46:11] [Epoch=002/040] [Need: 02:14:10] [LR=0.0100] [Best : Accuracy=18.43, Error=81.57]
  Epoch: [002][000/500]   Time 28.519 (28.519)   Data 27.886 (27.886)   Loss 2.2485 (2.2485)   Prec@1 15.000 (15.000)   Prec@5 72.000 (72.000)   [2025-10-28 15:46:39]
  Epoch: [002][100/500]   Time 0.357 (0.661)   Data 0.001 (0.278)   Loss 2.2303 (2.2357)   Prec@1 20.000 (18.386)   Prec@5 67.000 (73.743)   [2025-10-28 15:47:18]
  Epoch: [002][200/500]   Time 0.118 (0.461)   Data 0.001 (0.140)   Loss 2.2563 (2.2361)   Prec@1 13.000 (18.423)   Prec@5 76.000 (73.776)   [2025-10-28 15:47:43]
  Epoch: [002][300/500]   Time 0.358 (0.409)   Data 0.002 (0.094)   Loss 2.2498 (2.2346)   Prec@1 13.000 (18.525)   Prec@5 80.000 (74.037)   [2025-10-28 15:48:14]
  Epoch: [002][400/500]   Time 0.557 (0.396)   Data 0.002 (0.071)   Loss 2.2491 (2.2346)   Prec@1 15.000 (18.364)   Prec@5 74.000 (74.000)   [2025-10-28 15:48:50]
  **Train** Prec@1 18.432 Prec@5 73.858 Error@1 81.568
  **Test** Prec@1 15.820 Prec@5 61.060 Error@1 84.180

==>>[2025-10-28 15:50:00] [Epoch=003/040] [Need: 02:14:15] [LR=0.0100] [Best : Accuracy=18.43, Error=81.57]
  Epoch: [003][000/500]   Time 34.025 (34.025)   Data 33.539 (33.539)   Loss 2.2107 (2.2107)   Prec@1 23.000 (23.000)   Prec@5 69.000 (69.000)   [2025-10-28 15:50:35]
  Epoch: [003][100/500]   Time 0.422 (0.710)   Data 0.002 (0.334)   Loss 2.1912 (2.2244)   Prec@1 21.000 (18.723)   Prec@5 78.000 (74.653)   [2025-10-28 15:51:12]
  Epoch: [003][200/500]   Time 0.327 (0.547)   Data 0.003 (0.169)   Loss 2.1952 (2.2251)   Prec@1 28.000 (18.607)   Prec@5 76.000 (74.701)   [2025-10-28 15:51:50]
  Epoch: [003][300/500]   Time 0.381 (0.485)   Data 0.002 (0.113)   Loss 2.2281 (2.2246)   Prec@1 14.000 (18.661)   Prec@5 78.000 (74.698)   [2025-10-28 15:52:27]
  Epoch: [003][400/500]   Time 0.372 (0.450)   Data 0.002 (0.086)   Loss 2.2045 (2.2225)   Prec@1 20.000 (18.993)   Prec@5 75.000 (74.738)   [2025-10-28 15:53:01]
  **Train** Prec@1 19.398 Prec@5 75.046 Error@1 80.602
  **Test** Prec@1 21.560 Prec@5 76.040 Error@1 78.440
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 15:54:18] [Epoch=004/040] [Need: 02:16:34] [LR=0.0100] [Best : Accuracy=21.56, Error=78.44]
  Epoch: [004][000/500]   Time 37.250 (37.250)   Data 36.842 (36.842)   Loss 2.2104 (2.2104)   Prec@1 20.000 (20.000)   Prec@5 75.000 (75.000)   [2025-10-28 15:54:55]
  Epoch: [004][100/500]   Time 0.360 (0.723)   Data 0.003 (0.367)   Loss 2.2058 (2.2121)   Prec@1 21.000 (20.782)   Prec@5 76.000 (75.525)   [2025-10-28 15:55:31]
  Epoch: [004][200/500]   Time 0.344 (0.542)   Data 0.004 (0.185)   Loss 2.2031 (2.2106)   Prec@1 20.000 (21.070)   Prec@5 77.000 (75.264)   [2025-10-28 15:56:06]
  Epoch: [004][300/500]   Time 0.339 (0.478)   Data 0.004 (0.124)   Loss 2.1906 (2.2103)   Prec@1 24.000 (21.076)   Prec@5 80.000 (75.229)   [2025-10-28 15:56:42]
  Epoch: [004][400/500]   Time 0.330 (0.449)   Data 0.002 (0.094)   Loss 2.2366 (2.2082)   Prec@1 18.000 (21.302)   Prec@5 79.000 (75.451)   [2025-10-28 15:57:17]
  **Train** Prec@1 21.476 Prec@5 75.576 Error@1 78.524
  **Test** Prec@1 22.600 Prec@5 76.670 Error@1 77.400
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 15:58:36] [Epoch=005/040] [Need: 02:16:21] [LR=0.0100] [Best : Accuracy=22.60, Error=77.40]
  Epoch: [005][000/500]   Time 37.151 (37.151)   Data 36.285 (36.285)   Loss 2.2132 (2.2132)   Prec@1 20.000 (20.000)   Prec@5 71.000 (71.000)   [2025-10-28 15:59:13]
  Epoch: [005][100/500]   Time 0.621 (1.291)   Data 0.001 (0.365)   Loss 2.1389 (2.2000)   Prec@1 31.000 (22.317)   Prec@5 86.000 (77.505)   [2025-10-28 16:00:46]
  Epoch: [005][200/500]   Time 0.931 (1.118)   Data 0.005 (0.186)   Loss 2.2279 (2.1979)   Prec@1 21.000 (22.597)   Prec@5 78.000 (76.841)   [2025-10-28 16:02:21]
  Epoch: [005][300/500]   Time 0.753 (1.037)   Data 0.004 (0.126)   Loss 2.1821 (2.1967)   Prec@1 24.000 (22.834)   Prec@5 78.000 (76.897)   [2025-10-28 16:03:48]
  Epoch: [005][400/500]   Time 0.812 (0.995)   Data 0.010 (0.096)   Loss 2.1920 (2.1956)   Prec@1 24.000 (23.195)   Prec@5 73.000 (76.803)   [2025-10-28 16:05:15]
  **Train** Prec@1 23.574 Prec@5 76.922 Error@1 76.426
  **Test** Prec@1 25.790 Prec@5 78.070 Error@1 74.210
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 16:07:22] [Epoch=006/040] [Need: 02:40:04] [LR=0.0100] [Best : Accuracy=25.79, Error=74.21]
  Epoch: [006][000/500]   Time 37.489 (37.489)   Data 36.745 (36.745)   Loss 2.1578 (2.1578)   Prec@1 30.000 (30.000)   Prec@5 78.000 (78.000)   [2025-10-28 16:08:00]
  Epoch: [006][100/500]   Time 0.804 (1.193)   Data 0.003 (0.368)   Loss 2.1638 (2.1820)   Prec@1 32.000 (26.505)   Prec@5 85.000 (78.059)   [2025-10-28 16:09:23]
  Epoch: [006][200/500]   Time 0.903 (1.020)   Data 0.002 (0.187)   Loss 2.1437 (2.1810)   Prec@1 28.000 (26.716)   Prec@5 77.000 (77.985)   [2025-10-28 16:10:47]
  Epoch: [006][300/500]   Time 0.959 (1.012)   Data 0.006 (0.127)   Loss 2.1688 (2.1779)   Prec@1 28.000 (27.306)   Prec@5 81.000 (78.056)   [2025-10-28 16:12:27]
  Epoch: [006][400/500]   Time 0.832 (0.987)   Data 0.001 (0.097)   Loss 2.1504 (2.1746)   Prec@1 27.000 (27.813)   Prec@5 83.000 (78.279)   [2025-10-28 16:13:58]
  **Train** Prec@1 28.066 Prec@5 78.296 Error@1 71.934
  **Test** Prec@1 31.860 Prec@5 80.520 Error@1 68.140
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 16:16:10] [Epoch=007/040] [Need: 02:54:39] [LR=0.0100] [Best : Accuracy=31.86, Error=68.14]
  Epoch: [007][000/500]   Time 37.374 (37.374)   Data 36.640 (36.640)   Loss 2.1498 (2.1498)   Prec@1 31.000 (31.000)   Prec@5 77.000 (77.000)   [2025-10-28 16:16:47]
  Epoch: [007][100/500]   Time 2.036 (1.253)   Data 0.023 (0.368)   Loss 2.1443 (2.1591)   Prec@1 30.000 (30.129)   Prec@5 83.000 (78.574)   [2025-10-28 16:18:17]
  Epoch: [007][200/500]   Time 0.922 (1.110)   Data 0.006 (0.188)   Loss 2.1605 (2.1542)   Prec@1 26.000 (30.577)   Prec@5 87.000 (78.816)   [2025-10-28 16:19:53]
  Epoch: [007][300/500]   Time 0.801 (1.085)   Data 0.014 (0.128)   Loss 2.1547 (2.1577)   Prec@1 31.000 (29.904)   Prec@5 77.000 (78.601)   [2025-10-28 16:21:37]
  Epoch: [007][400/500]   Time 0.793 (1.039)   Data 0.001 (0.098)   Loss 2.1720 (2.1574)   Prec@1 29.000 (29.833)   Prec@5 69.000 (78.551)   [2025-10-28 16:23:07]
  **Train** Prec@1 30.240 Prec@5 78.850 Error@1 69.760
  **Test** Prec@1 33.630 Prec@5 81.220 Error@1 66.370
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 16:25:25] [Epoch=008/040] [Need: 03:05:13] [LR=0.0100] [Best : Accuracy=33.63, Error=66.37]
  Epoch: [008][000/500]   Time 37.439 (37.439)   Data 35.547 (35.547)   Loss 2.0807 (2.0807)   Prec@1 39.000 (39.000)   Prec@5 86.000 (86.000)   [2025-10-28 16:26:03]
  Epoch: [008][100/500]   Time 1.282 (1.441)   Data 0.008 (0.360)   Loss 2.1628 (2.1369)   Prec@1 28.000 (31.109)   Prec@5 77.000 (79.465)   [2025-10-28 16:27:51]
  Epoch: [008][200/500]   Time 0.773 (1.188)   Data 0.004 (0.184)   Loss 2.1473 (2.1366)   Prec@1 29.000 (31.388)   Prec@5 79.000 (79.527)   [2025-10-28 16:29:24]
  Epoch: [008][300/500]   Time 0.855 (1.104)   Data 0.003 (0.125)   Loss 2.1958 (2.1372)   Prec@1 25.000 (31.405)   Prec@5 77.000 (79.206)   [2025-10-28 16:30:58]
  Epoch: [008][400/500]   Time 0.580 (1.045)   Data 0.003 (0.095)   Loss 2.1340 (2.1377)   Prec@1 31.000 (31.364)   Prec@5 80.000 (78.923)   [2025-10-28 16:32:25]
  **Train** Prec@1 31.592 Prec@5 79.082 Error@1 68.408
  **Test** Prec@1 33.180 Prec@5 79.400 Error@1 66.820

==>>[2025-10-28 16:33:54] [Epoch=009/040] [Need: 03:08:40] [LR=0.0100] [Best : Accuracy=33.63, Error=66.37]
  Epoch: [009][000/500]   Time 27.294 (27.294)   Data 26.777 (26.777)   Loss 2.1470 (2.1470)   Prec@1 27.000 (27.000)   Prec@5 75.000 (75.000)   [2025-10-28 16:34:21]
  Epoch: [009][100/500]   Time 0.393 (0.658)   Data 0.002 (0.267)   Loss 2.1480 (2.1303)   Prec@1 31.000 (31.990)   Prec@5 83.000 (79.307)   [2025-10-28 16:35:00]
  Epoch: [009][200/500]   Time 0.402 (0.519)   Data 0.001 (0.135)   Loss 2.1487 (2.1302)   Prec@1 31.000 (32.119)   Prec@5 74.000 (79.333)   [2025-10-28 16:35:38]
  Epoch: [009][300/500]   Time 0.332 (0.482)   Data 0.004 (0.091)   Loss 2.0941 (2.1301)   Prec@1 40.000 (32.219)   Prec@5 75.000 (79.458)   [2025-10-28 16:36:19]
  Epoch: [009][400/500]   Time 0.427 (0.454)   Data 0.002 (0.069)   Loss 2.1149 (2.1293)   Prec@1 35.000 (32.289)   Prec@5 86.000 (79.611)   [2025-10-28 16:36:56]
  **Train** Prec@1 32.444 Prec@5 79.790 Error@1 67.556
  **Test** Prec@1 33.080 Prec@5 81.790 Error@1 66.920

==>>[2025-10-28 16:40:45] [Epoch=010/040] [Need: 03:04:53] [LR=0.0100] [Best : Accuracy=33.63, Error=66.37]
  Epoch: [010][000/500]   Time 33.381 (33.381)   Data 31.395 (31.395)   Loss 2.1066 (2.1066)   Prec@1 34.000 (34.000)   Prec@5 82.000 (82.000)   [2025-10-28 16:41:18]
  Epoch: [010][100/500]   Time 2.179 (2.271)   Data 0.029 (0.336)   Loss 2.1129 (2.1239)   Prec@1 35.000 (32.851)   Prec@5 80.000 (80.228)   [2025-10-28 16:44:34]
  Epoch: [010][200/500]   Time 0.935 (1.869)   Data 0.007 (0.177)   Loss 2.1037 (2.1180)   Prec@1 37.000 (33.667)   Prec@5 85.000 (80.637)   [2025-10-28 16:47:01]
  Epoch: [010][300/500]   Time 1.833 (1.792)   Data 0.024 (0.124)   Loss 2.1466 (2.1173)   Prec@1 28.000 (33.658)   Prec@5 77.000 (80.704)   [2025-10-28 16:49:44]
  Epoch: [010][400/500]   Time 0.356 (1.544)   Data 0.003 (0.095)   Loss 2.1088 (2.1175)   Prec@1 33.000 (33.666)   Prec@5 75.000 (80.633)   [2025-10-28 16:51:04]
  **Train** Prec@1 33.822 Prec@5 80.868 Error@1 66.178
  **Test** Prec@1 35.430 Prec@5 81.800 Error@1 64.570
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 16:51:58] [Epoch=011/040] [Need: 03:12:02] [LR=0.0100] [Best : Accuracy=35.43, Error=64.57]
  Epoch: [011][000/500]   Time 28.128 (28.128)   Data 27.795 (27.795)   Loss 2.1431 (2.1431)   Prec@1 31.000 (31.000)   Prec@5 75.000 (75.000)   [2025-10-28 16:52:26]
  Epoch: [011][100/500]   Time 0.135 (0.411)   Data 0.002 (0.276)   Loss 2.0779 (2.1074)   Prec@1 37.000 (34.733)   Prec@5 85.000 (82.010)   [2025-10-28 16:52:39]
  Epoch: [011][200/500]   Time 0.120 (0.266)   Data 0.000 (0.139)   Loss 2.1608 (2.1079)   Prec@1 32.000 (34.582)   Prec@5 87.000 (81.990)   [2025-10-28 16:52:51]
  Epoch: [011][300/500]   Time 0.118 (0.217)   Data 0.001 (0.093)   Loss 2.1475 (2.1080)   Prec@1 31.000 (34.591)   Prec@5 83.000 (81.930)   [2025-10-28 16:53:03]
  Epoch: [011][400/500]   Time 0.118 (0.193)   Data 0.001 (0.070)   Loss 2.0964 (2.1075)   Prec@1 36.000 (34.566)   Prec@5 83.000 (82.060)   [2025-10-28 16:53:15]
  **Train** Prec@1 34.778 Prec@5 82.240 Error@1 65.222
  **Test** Prec@1 34.430 Prec@5 82.380 Error@1 65.570

==>>[2025-10-28 16:53:52] [Epoch=012/040] [Need: 02:54:25] [LR=0.0100] [Best : Accuracy=35.43, Error=64.57]
  Epoch: [012][000/500]   Time 24.561 (24.561)   Data 24.246 (24.246)   Loss 2.0404 (2.0404)   Prec@1 41.000 (41.000)   Prec@5 94.000 (94.000)   [2025-10-28 16:54:17]
  Epoch: [012][100/500]   Time 0.144 (0.363)   Data 0.001 (0.241)   Loss 2.1546 (2.1101)   Prec@1 28.000 (34.564)   Prec@5 86.000 (82.228)   [2025-10-28 16:54:29]
  Epoch: [012][200/500]   Time 0.151 (0.245)   Data 0.001 (0.121)   Loss 2.1658 (2.1085)   Prec@1 28.000 (34.741)   Prec@5 78.000 (82.318)   [2025-10-28 16:54:42]
  Epoch: [012][300/500]   Time 0.118 (0.205)   Data 0.001 (0.081)   Loss 2.0627 (2.1072)   Prec@1 38.000 (34.817)   Prec@5 87.000 (82.425)   [2025-10-28 16:54:54]
  Epoch: [012][400/500]   Time 0.114 (0.184)   Data 0.000 (0.061)   Loss 2.0842 (2.1031)   Prec@1 34.000 (35.239)   Prec@5 86.000 (82.559)   [2025-10-28 16:55:06]
  **Train** Prec@1 35.510 Prec@5 82.692 Error@1 64.490
  **Test** Prec@1 33.930 Prec@5 84.750 Error@1 66.070

==>>[2025-10-28 16:55:41] [Epoch=013/040] [Need: 02:39:01] [LR=0.0100] [Best : Accuracy=35.43, Error=64.57]
  Epoch: [013][000/500]   Time 21.401 (21.401)   Data 21.106 (21.106)   Loss 2.0470 (2.0470)   Prec@1 45.000 (45.000)   Prec@5 79.000 (79.000)   [2025-10-28 16:56:02]
  Epoch: [013][100/500]   Time 0.109 (0.322)   Data 0.001 (0.210)   Loss 2.0807 (2.0825)   Prec@1 39.000 (37.931)   Prec@5 83.000 (84.129)   [2025-10-28 16:56:14]
  Epoch: [013][200/500]   Time 0.138 (0.218)   Data 0.001 (0.106)   Loss 2.0835 (2.0893)   Prec@1 37.000 (37.075)   Prec@5 90.000 (83.672)   [2025-10-28 16:56:25]
  Epoch: [013][300/500]   Time 0.112 (0.184)   Data 0.001 (0.071)   Loss 2.0301 (2.0869)   Prec@1 43.000 (37.196)   Prec@5 90.000 (83.777)   [2025-10-28 16:56:36]
  Epoch: [013][400/500]   Time 0.124 (0.167)   Data 0.001 (0.053)   Loss 2.1008 (2.0880)   Prec@1 34.000 (37.107)   Prec@5 80.000 (83.678)   [2025-10-28 16:56:48]
  **Train** Prec@1 37.082 Prec@5 83.574 Error@1 62.918
  **Test** Prec@1 34.290 Prec@5 83.470 Error@1 65.710

==>>[2025-10-28 16:57:25] [Epoch=014/040] [Need: 02:25:24] [LR=0.0100] [Best : Accuracy=35.43, Error=64.57]
  Epoch: [014][000/500]   Time 23.170 (23.170)   Data 22.794 (22.794)   Loss 2.0789 (2.0789)   Prec@1 41.000 (41.000)   Prec@5 80.000 (80.000)   [2025-10-28 16:57:48]
  Epoch: [014][100/500]   Time 0.118 (0.344)   Data 0.001 (0.226)   Loss 2.0512 (2.0814)   Prec@1 40.000 (37.347)   Prec@5 84.000 (83.901)   [2025-10-28 16:58:00]
  Epoch: [014][200/500]   Time 0.132 (0.229)   Data 0.000 (0.114)   Loss 2.0535 (2.0799)   Prec@1 36.000 (37.736)   Prec@5 80.000 (83.846)   [2025-10-28 16:58:11]
  Epoch: [014][300/500]   Time 0.133 (0.193)   Data 0.001 (0.076)   Loss 2.0499 (2.0754)   Prec@1 40.000 (38.256)   Prec@5 89.000 (84.163)   [2025-10-28 16:58:23]
  Epoch: [014][400/500]   Time 0.109 (0.175)   Data 0.000 (0.058)   Loss 2.0767 (2.0760)   Prec@1 39.000 (38.202)   Prec@5 89.000 (84.197)   [2025-10-28 16:58:35]
  **Train** Prec@1 38.362 Prec@5 84.246 Error@1 61.638
  **Test** Prec@1 41.950 Prec@5 86.560 Error@1 58.050
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 16:59:12] [Epoch=015/040] [Need: 02:13:28] [LR=0.0100] [Best : Accuracy=41.95, Error=58.05]
  Epoch: [015][000/500]   Time 25.745 (25.745)   Data 25.556 (25.556)   Loss 2.0523 (2.0523)   Prec@1 43.000 (43.000)   Prec@5 84.000 (84.000)   [2025-10-28 16:59:38]
  Epoch: [015][100/500]   Time 0.328 (0.439)   Data 0.002 (0.254)   Loss 2.0103 (2.0722)   Prec@1 48.000 (38.921)   Prec@5 91.000 (83.851)   [2025-10-28 16:59:56]
  Epoch: [015][200/500]   Time 0.338 (0.392)   Data 0.004 (0.129)   Loss 2.1081 (2.0706)   Prec@1 33.000 (38.950)   Prec@5 86.000 (84.144)   [2025-10-28 17:00:31]
  Epoch: [015][300/500]   Time 0.338 (0.379)   Data 0.002 (0.087)   Loss 2.0368 (2.0691)   Prec@1 47.000 (39.116)   Prec@5 82.000 (84.332)   [2025-10-28 17:01:06]
  Epoch: [015][400/500]   Time 1.443 (0.488)   Data 0.020 (0.067)   Loss 2.0562 (2.0692)   Prec@1 39.000 (39.007)   Prec@5 86.000 (84.359)   [2025-10-28 17:02:28]
  **Train** Prec@1 39.008 Prec@5 84.376 Error@1 60.992
  **Test** Prec@1 42.910 Prec@5 85.620 Error@1 57.090
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 17:06:05] [Epoch=016/040] [Need: 02:10:27] [LR=0.0100] [Best : Accuracy=42.91, Error=57.09]
  Epoch: [016][000/500]   Time 36.138 (36.138)   Data 34.314 (34.314)   Loss 2.0470 (2.0470)   Prec@1 43.000 (43.000)   Prec@5 90.000 (90.000)   [2025-10-28 17:06:41]
  Epoch: [016][100/500]   Time 1.669 (2.060)   Data 0.004 (0.355)   Loss 2.0467 (2.0576)   Prec@1 38.000 (40.525)   Prec@5 88.000 (85.238)   [2025-10-28 17:09:33]
  Epoch: [016][200/500]   Time 1.547 (1.877)   Data 0.006 (0.186)   Loss 2.0474 (2.0566)   Prec@1 43.000 (40.363)   Prec@5 82.000 (85.303)   [2025-10-28 17:12:22]
  Epoch: [016][300/500]   Time 1.511 (1.786)   Data 0.022 (0.129)   Loss 2.0216 (2.0592)   Prec@1 42.000 (39.980)   Prec@5 85.000 (85.096)   [2025-10-28 17:15:03]
  Epoch: [016][400/500]   Time 0.652 (1.707)   Data 0.003 (0.100)   Loss 2.0103 (2.0591)   Prec@1 47.000 (39.928)   Prec@5 87.000 (85.406)   [2025-10-28 17:17:29]
  **Train** Prec@1 39.952 Prec@5 85.378 Error@1 60.048
  **Test** Prec@1 42.580 Prec@5 85.410 Error@1 57.420

==>>[2025-10-28 17:18:43] [Epoch=017/040] [Need: 02:14:45] [LR=0.0100] [Best : Accuracy=42.91, Error=57.09]
  Epoch: [017][000/500]   Time 29.075 (29.075)   Data 28.661 (28.661)   Loss 2.0180 (2.0180)   Prec@1 43.000 (43.000)   Prec@5 92.000 (92.000)   [2025-10-28 17:19:12]
  Epoch: [017][100/500]   Time 0.329 (0.633)   Data 0.002 (0.285)   Loss 2.0494 (2.0544)   Prec@1 40.000 (40.396)   Prec@5 88.000 (86.525)   [2025-10-28 17:19:47]
  Epoch: [017][200/500]   Time 0.329 (0.498)   Data 0.003 (0.144)   Loss 2.0427 (2.0535)   Prec@1 43.000 (40.522)   Prec@5 85.000 (86.532)   [2025-10-28 17:20:23]
  Epoch: [017][300/500]   Time 0.312 (0.447)   Data 0.001 (0.097)   Loss 1.9949 (2.0504)   Prec@1 49.000 (40.864)   Prec@5 88.000 (86.472)   [2025-10-28 17:20:58]
  Epoch: [017][400/500]   Time 0.377 (0.421)   Data 0.002 (0.073)   Loss 2.0699 (2.0505)   Prec@1 42.000 (40.825)   Prec@5 82.000 (86.501)   [2025-10-28 17:21:32]
  **Train** Prec@1 40.774 Prec@5 86.654 Error@1 59.226
  **Test** Prec@1 38.330 Prec@5 86.670 Error@1 61.670

==>>[2025-10-28 17:22:45] [Epoch=018/040] [Need: 02:06:39] [LR=0.0100] [Best : Accuracy=42.91, Error=57.09]
  Epoch: [018][000/500]   Time 35.272 (35.272)   Data 34.920 (34.920)   Loss 2.0912 (2.0912)   Prec@1 36.000 (36.000)   Prec@5 84.000 (84.000)   [2025-10-28 17:23:20]
  Epoch: [018][100/500]   Time 0.357 (0.691)   Data 0.003 (0.347)   Loss 1.9987 (2.0407)   Prec@1 46.000 (41.861)   Prec@5 91.000 (87.198)   [2025-10-28 17:23:55]
  Epoch: [018][200/500]   Time 0.745 (0.534)   Data 0.005 (0.175)   Loss 2.0768 (2.0420)   Prec@1 37.000 (41.721)   Prec@5 84.000 (87.249)   [2025-10-28 17:24:32]
  Epoch: [018][300/500]   Time 1.738 (0.859)   Data 0.020 (0.121)   Loss 2.0520 (2.0423)   Prec@1 42.000 (41.714)   Prec@5 91.000 (87.342)   [2025-10-28 17:27:04]
  Epoch: [018][400/500]   Time 1.749 (0.989)   Data 0.027 (0.094)   Loss 2.0341 (2.0438)   Prec@1 42.000 (41.576)   Prec@5 84.000 (87.307)   [2025-10-28 17:29:22]
  **Train** Prec@1 41.546 Prec@5 87.332 Error@1 58.454
  **Test** Prec@1 43.990 Prec@5 86.910 Error@1 56.010
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 17:32:32] [Epoch=019/040] [Need: 02:05:20] [LR=0.0100] [Best : Accuracy=43.99, Error=56.01]
  Epoch: [019][000/500]   Time 28.199 (28.199)   Data 27.571 (27.571)   Loss 2.0545 (2.0545)   Prec@1 38.000 (38.000)   Prec@5 89.000 (89.000)   [2025-10-28 17:33:00]
  Epoch: [019][100/500]   Time 0.322 (0.639)   Data 0.001 (0.275)   Loss 2.0756 (2.0369)   Prec@1 36.000 (42.287)   Prec@5 81.000 (87.851)   [2025-10-28 17:33:36]
  Epoch: [019][200/500]   Time 0.321 (0.495)   Data 0.001 (0.139)   Loss 2.0401 (2.0371)   Prec@1 41.000 (42.070)   Prec@5 87.000 (87.985)   [2025-10-28 17:34:11]
  Epoch: [019][300/500]   Time 0.293 (0.448)   Data 0.002 (0.093)   Loss 2.0885 (2.0373)   Prec@1 35.000 (42.143)   Prec@5 82.000 (87.834)   [2025-10-28 17:34:46]
  Epoch: [019][400/500]   Time 0.338 (0.422)   Data 0.001 (0.070)   Loss 2.0500 (2.0354)   Prec@1 39.000 (42.344)   Prec@5 92.000 (88.025)   [2025-10-28 17:35:21]
  **Train** Prec@1 42.342 Prec@5 88.034 Error@1 57.658
  **Test** Prec@1 44.540 Prec@5 89.030 Error@1 55.460
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 17:36:26] [Epoch=020/040] [Need: 01:57:18] [LR=0.0100] [Best : Accuracy=44.54, Error=55.46]
  Epoch: [020][000/500]   Time 26.889 (26.889)   Data 26.259 (26.259)   Loss 1.9025 (1.9025)   Prec@1 60.000 (60.000)   Prec@5 90.000 (90.000)   [2025-10-28 17:36:53]
  Epoch: [020][100/500]   Time 0.109 (0.392)   Data 0.000 (0.261)   Loss 2.0457 (2.0290)   Prec@1 44.000 (43.119)   Prec@5 87.000 (88.089)   [2025-10-28 17:37:05]
  Epoch: [020][200/500]   Time 0.136 (0.262)   Data 0.000 (0.131)   Loss 2.0124 (2.0322)   Prec@1 43.000 (42.831)   Prec@5 88.000 (87.896)   [2025-10-28 17:37:18]
  Epoch: [020][300/500]   Time 0.137 (0.219)   Data 0.001 (0.088)   Loss 2.0515 (2.0299)   Prec@1 41.000 (42.980)   Prec@5 86.000 (88.173)   [2025-10-28 17:37:32]
  Epoch: [020][400/500]   Time 0.122 (0.195)   Data 0.002 (0.066)   Loss 2.0494 (2.0268)   Prec@1 39.000 (43.349)   Prec@5 90.000 (88.157)   [2025-10-28 17:37:44]
  **Train** Prec@1 43.298 Prec@5 88.192 Error@1 56.702
  **Test** Prec@1 44.060 Prec@5 89.740 Error@1 55.940

==>>[2025-10-28 17:38:23] [Epoch=021/040] [Need: 01:47:54] [LR=0.0100] [Best : Accuracy=44.54, Error=55.46]
  Epoch: [021][000/500]   Time 23.361 (23.361)   Data 23.040 (23.040)   Loss 1.9784 (1.9784)   Prec@1 47.000 (47.000)   Prec@5 90.000 (90.000)   [2025-10-28 17:38:46]
  Epoch: [021][100/500]   Time 0.111 (0.346)   Data 0.001 (0.229)   Loss 2.0302 (2.0242)   Prec@1 42.000 (43.594)   Prec@5 85.000 (88.495)   [2025-10-28 17:38:58]
  Epoch: [021][200/500]   Time 0.138 (0.229)   Data 0.001 (0.115)   Loss 1.9518 (2.0216)   Prec@1 54.000 (43.960)   Prec@5 91.000 (88.468)   [2025-10-28 17:39:09]
  Epoch: [021][300/500]   Time 0.118 (0.193)   Data 0.000 (0.077)   Loss 1.9649 (2.0228)   Prec@1 52.000 (43.751)   Prec@5 95.000 (88.229)   [2025-10-28 17:39:21]
  Epoch: [021][400/500]   Time 0.119 (0.175)   Data 0.000 (0.058)   Loss 2.0940 (2.0221)   Prec@1 35.000 (43.781)   Prec@5 87.000 (88.449)   [2025-10-28 17:39:33]
  **Train** Prec@1 43.932 Prec@5 88.554 Error@1 56.068
  **Test** Prec@1 44.090 Prec@5 86.810 Error@1 55.910

==>>[2025-10-28 17:40:12] [Epoch=022/040] [Need: 01:39:04] [LR=0.0100] [Best : Accuracy=44.54, Error=55.46]
  Epoch: [022][000/500]   Time 25.839 (25.839)   Data 25.530 (25.530)   Loss 1.8943 (1.8943)   Prec@1 59.000 (59.000)   Prec@5 91.000 (91.000)   [2025-10-28 17:40:38]
  Epoch: [022][100/500]   Time 0.145 (0.399)   Data 0.001 (0.254)   Loss 2.0065 (2.0153)   Prec@1 42.000 (44.495)   Prec@5 86.000 (88.366)   [2025-10-28 17:40:53]
  Epoch: [022][200/500]   Time 0.122 (0.264)   Data 0.001 (0.128)   Loss 2.0423 (2.0173)   Prec@1 42.000 (44.229)   Prec@5 88.000 (88.249)   [2025-10-28 17:41:05]
  Epoch: [022][300/500]   Time 0.118 (0.217)   Data 0.001 (0.086)   Loss 2.0133 (2.0167)   Prec@1 47.000 (44.432)   Prec@5 89.000 (88.339)   [2025-10-28 17:41:18]
  Epoch: [022][400/500]   Time 0.144 (0.195)   Data 0.001 (0.065)   Loss 1.9513 (2.0177)   Prec@1 49.000 (44.294)   Prec@5 97.000 (88.322)   [2025-10-28 17:41:31]
  **Train** Prec@1 44.472 Prec@5 88.196 Error@1 55.528
  **Test** Prec@1 44.740 Prec@5 88.310 Error@1 55.260
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 17:42:24] [Epoch=023/040] [Need: 01:31:07] [LR=0.0100] [Best : Accuracy=44.74, Error=55.26]
  Epoch: [023][000/500]   Time 29.647 (29.647)   Data 29.462 (29.462)   Loss 2.0486 (2.0486)   Prec@1 41.000 (41.000)   Prec@5 86.000 (86.000)   [2025-10-28 17:42:54]
  Epoch: [023][100/500]   Time 0.169 (0.462)   Data 0.001 (0.293)   Loss 2.0064 (2.0059)   Prec@1 48.000 (45.267)   Prec@5 88.000 (88.208)   [2025-10-28 17:43:11]
  Epoch: [023][200/500]   Time 0.159 (0.312)   Data 0.001 (0.148)   Loss 2.0144 (2.0117)   Prec@1 45.000 (44.776)   Prec@5 89.000 (88.100)   [2025-10-28 17:43:27]
  Epoch: [023][300/500]   Time 0.172 (0.261)   Data 0.004 (0.099)   Loss 1.9821 (2.0099)   Prec@1 46.000 (44.950)   Prec@5 89.000 (88.375)   [2025-10-28 17:43:43]
  Epoch: [023][400/500]   Time 0.154 (0.237)   Data 0.001 (0.075)   Loss 1.9426 (2.0086)   Prec@1 53.000 (45.122)   Prec@5 91.000 (88.334)   [2025-10-28 17:43:59]
  **Train** Prec@1 44.872 Prec@5 88.290 Error@1 55.128
  **Test** Prec@1 41.330 Prec@5 82.980 Error@1 58.670

==>>[2025-10-28 17:44:47] [Epoch=024/040] [Need: 01:23:46] [LR=0.0100] [Best : Accuracy=44.74, Error=55.26]
  Epoch: [024][000/500]   Time 29.330 (29.330)   Data 29.153 (29.153)   Loss 2.0467 (2.0467)   Prec@1 43.000 (43.000)   Prec@5 91.000 (91.000)   [2025-10-28 17:45:17]
  Epoch: [024][100/500]   Time 0.148 (0.445)   Data 0.001 (0.290)   Loss 1.9616 (2.0120)   Prec@1 52.000 (44.832)   Prec@5 86.000 (88.465)   [2025-10-28 17:45:32]
  Epoch: [024][200/500]   Time 0.161 (0.305)   Data 0.001 (0.147)   Loss 2.0184 (2.0068)   Prec@1 43.000 (45.562)   Prec@5 91.000 (88.234)   [2025-10-28 17:45:48]
  Epoch: [024][300/500]   Time 0.169 (0.258)   Data 0.003 (0.098)   Loss 1.9837 (2.0068)   Prec@1 46.000 (45.462)   Prec@5 90.000 (88.395)   [2025-10-28 17:46:05]
  Epoch: [024][400/500]   Time 0.152 (0.233)   Data 0.002 (0.074)   Loss 2.0344 (2.0081)   Prec@1 41.000 (45.392)   Prec@5 83.000 (88.299)   [2025-10-28 17:46:21]
  **Train** Prec@1 45.444 Prec@5 88.392 Error@1 54.556
  **Test** Prec@1 46.180 Prec@5 89.410 Error@1 53.820
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 17:47:09] [Epoch=025/040] [Need: 01:16:49] [LR=0.0010] [Best : Accuracy=46.18, Error=53.82]
  Epoch: [025][000/500]   Time 30.699 (30.699)   Data 30.506 (30.506)   Loss 1.9840 (1.9840)   Prec@1 51.000 (51.000)   Prec@5 91.000 (91.000)   [2025-10-28 17:47:40]
  Epoch: [025][100/500]   Time 0.150 (0.460)   Data 0.001 (0.303)   Loss 1.9496 (1.9974)   Prec@1 53.000 (46.327)   Prec@5 90.000 (89.020)   [2025-10-28 17:47:55]
  Epoch: [025][200/500]   Time 0.183 (0.312)   Data 0.003 (0.153)   Loss 1.9951 (1.9884)   Prec@1 46.000 (47.214)   Prec@5 87.000 (89.139)   [2025-10-28 17:48:11]
  Epoch: [025][300/500]   Time 0.166 (0.261)   Data 0.001 (0.103)   Loss 1.9867 (1.9815)   Prec@1 47.000 (48.000)   Prec@5 94.000 (89.468)   [2025-10-28 17:48:27]
  Epoch: [025][400/500]   Time 0.156 (0.235)   Data 0.001 (0.077)   Loss 1.9628 (1.9781)   Prec@1 52.000 (48.359)   Prec@5 86.000 (89.703)   [2025-10-28 17:48:43]
  **Train** Prec@1 48.638 Prec@5 89.858 Error@1 51.362
  **Test** Prec@1 49.780 Prec@5 91.070 Error@1 50.220
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 17:49:30] [Epoch=026/040] [Need: 01:10:12] [LR=0.0010] [Best : Accuracy=49.78, Error=50.22]
  Epoch: [026][000/500]   Time 30.030 (30.030)   Data 29.845 (29.845)   Loss 1.9605 (1.9605)   Prec@1 50.000 (50.000)   Prec@5 90.000 (90.000)   [2025-10-28 17:50:00]
  Epoch: [026][100/500]   Time 0.159 (0.454)   Data 0.001 (0.297)   Loss 1.9933 (1.9614)   Prec@1 47.000 (50.139)   Prec@5 90.000 (90.267)   [2025-10-28 17:50:16]
  Epoch: [026][200/500]   Time 0.159 (0.305)   Data 0.000 (0.150)   Loss 2.0217 (1.9607)   Prec@1 42.000 (50.209)   Prec@5 88.000 (90.512)   [2025-10-28 17:50:31]
  Epoch: [026][300/500]   Time 0.164 (0.255)   Data 0.001 (0.100)   Loss 1.8943 (1.9606)   Prec@1 59.000 (50.256)   Prec@5 95.000 (90.555)   [2025-10-28 17:50:47]
  Epoch: [026][400/500]   Time 0.152 (0.230)   Data 0.001 (0.076)   Loss 2.0204 (1.9616)   Prec@1 43.000 (50.107)   Prec@5 90.000 (90.429)   [2025-10-28 17:51:03]
  **Train** Prec@1 50.208 Prec@5 90.520 Error@1 49.792
  **Test** Prec@1 48.580 Prec@5 88.020 Error@1 51.420

==>>[2025-10-28 17:51:50] [Epoch=027/040] [Need: 01:03:54] [LR=0.0010] [Best : Accuracy=49.78, Error=50.22]
  Epoch: [027][000/500]   Time 29.774 (29.774)   Data 29.587 (29.587)   Loss 1.9598 (1.9598)   Prec@1 52.000 (52.000)   Prec@5 90.000 (90.000)   [2025-10-28 17:52:20]
  Epoch: [027][100/500]   Time 0.156 (0.447)   Data 0.001 (0.294)   Loss 2.0118 (1.9583)   Prec@1 46.000 (50.525)   Prec@5 88.000 (90.564)   [2025-10-28 17:52:36]
  Epoch: [027][200/500]   Time 0.157 (0.301)   Data 0.001 (0.149)   Loss 2.0184 (1.9577)   Prec@1 44.000 (50.473)   Prec@5 91.000 (90.701)   [2025-10-28 17:52:51]
  Epoch: [027][300/500]   Time 0.147 (0.253)   Data 0.000 (0.100)   Loss 2.0016 (1.9573)   Prec@1 45.000 (50.558)   Prec@5 88.000 (90.708)   [2025-10-28 17:53:07]
  Epoch: [027][400/500]   Time 0.144 (0.228)   Data 0.001 (0.075)   Loss 1.9917 (1.9565)   Prec@1 48.000 (50.738)   Prec@5 87.000 (90.661)   [2025-10-28 17:53:22]
  **Train** Prec@1 50.834 Prec@5 90.714 Error@1 49.166
  **Test** Prec@1 49.570 Prec@5 87.960 Error@1 50.430

==>>[2025-10-28 17:54:10] [Epoch=028/040] [Need: 00:57:52] [LR=0.0010] [Best : Accuracy=49.78, Error=50.22]
  Epoch: [028][000/500]   Time 29.330 (29.330)   Data 29.121 (29.121)   Loss 1.9743 (1.9743)   Prec@1 52.000 (52.000)   Prec@5 88.000 (88.000)   [2025-10-28 17:54:39]
  Epoch: [028][100/500]   Time 0.147 (0.440)   Data 0.002 (0.290)   Loss 1.9729 (1.9578)   Prec@1 47.000 (50.396)   Prec@5 85.000 (90.743)   [2025-10-28 17:54:54]
  Epoch: [028][200/500]   Time 0.154 (0.300)   Data 0.001 (0.146)   Loss 1.9124 (1.9552)   Prec@1 56.000 (50.811)   Prec@5 94.000 (90.851)   [2025-10-28 17:55:10]
  Epoch: [028][300/500]   Time 0.158 (0.252)   Data 0.002 (0.098)   Loss 1.8370 (1.9550)   Prec@1 67.000 (50.937)   Prec@5 96.000 (90.678)   [2025-10-28 17:55:25]
  Epoch: [028][400/500]   Time 0.151 (0.228)   Data 0.001 (0.074)   Loss 1.9753 (1.9533)   Prec@1 48.000 (51.050)   Prec@5 93.000 (90.753)   [2025-10-28 17:55:41]
  **Train** Prec@1 50.928 Prec@5 90.724 Error@1 49.072
  **Test** Prec@1 51.400 Prec@5 89.710 Error@1 48.600
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 17:56:28] [Epoch=029/040] [Need: 00:52:06] [LR=0.0010] [Best : Accuracy=51.40, Error=48.60]
  Epoch: [029][000/500]   Time 29.970 (29.970)   Data 29.772 (29.772)   Loss 1.8985 (1.8985)   Prec@1 56.000 (56.000)   Prec@5 96.000 (96.000)   [2025-10-28 17:56:58]
  Epoch: [029][100/500]   Time 0.153 (0.448)   Data 0.001 (0.296)   Loss 1.9595 (1.9499)   Prec@1 50.000 (51.465)   Prec@5 89.000 (90.861)   [2025-10-28 17:57:14]
  Epoch: [029][200/500]   Time 0.168 (0.302)   Data 0.001 (0.150)   Loss 1.8738 (1.9471)   Prec@1 60.000 (51.632)   Prec@5 90.000 (90.910)   [2025-10-28 17:57:29]
  Epoch: [029][300/500]   Time 0.158 (0.254)   Data 0.003 (0.100)   Loss 1.9255 (1.9489)   Prec@1 52.000 (51.512)   Prec@5 91.000 (90.711)   [2025-10-28 17:57:45]
  Epoch: [029][400/500]   Time 0.164 (0.229)   Data 0.001 (0.076)   Loss 1.9615 (1.9497)   Prec@1 50.000 (51.429)   Prec@5 96.000 (90.748)   [2025-10-28 17:58:00]
  **Train** Prec@1 51.320 Prec@5 90.782 Error@1 48.680
  **Test** Prec@1 47.580 Prec@5 86.190 Error@1 52.420

==>>[2025-10-28 17:58:48] [Epoch=030/040] [Need: 00:46:33] [LR=0.0010] [Best : Accuracy=51.40, Error=48.60]
  Epoch: [030][000/500]   Time 30.220 (30.220)   Data 30.035 (30.035)   Loss 1.9825 (1.9825)   Prec@1 49.000 (49.000)   Prec@5 93.000 (93.000)   [2025-10-28 17:59:18]
  Epoch: [030][100/500]   Time 0.144 (0.451)   Data 0.002 (0.299)   Loss 1.9465 (1.9440)   Prec@1 54.000 (51.683)   Prec@5 94.000 (91.030)   [2025-10-28 17:59:33]
  Epoch: [030][200/500]   Time 0.158 (0.308)   Data 0.001 (0.151)   Loss 1.9320 (1.9477)   Prec@1 56.000 (51.532)   Prec@5 87.000 (90.915)   [2025-10-28 17:59:50]
  Epoch: [030][300/500]   Time 0.160 (0.259)   Data 0.000 (0.101)   Loss 1.8879 (1.9491)   Prec@1 60.000 (51.389)   Prec@5 90.000 (90.900)   [2025-10-28 18:00:06]
  Epoch: [030][400/500]   Time 0.151 (0.235)   Data 0.001 (0.076)   Loss 1.9420 (1.9498)   Prec@1 52.000 (51.307)   Prec@5 91.000 (90.868)   [2025-10-28 18:00:22]
  **Train** Prec@1 51.290 Prec@5 90.680 Error@1 48.710
  **Test** Prec@1 51.510 Prec@5 89.400 Error@1 48.490
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 18:01:10] [Epoch=031/040] [Need: 00:41:14] [LR=0.0010] [Best : Accuracy=51.51, Error=48.49]
  Epoch: [031][000/500]   Time 29.865 (29.865)   Data 29.679 (29.679)   Loss 1.9969 (1.9969)   Prec@1 44.000 (44.000)   Prec@5 91.000 (91.000)   [2025-10-28 18:01:40]
  Epoch: [031][100/500]   Time 0.172 (0.451)   Data 0.003 (0.295)   Loss 1.9761 (1.9482)   Prec@1 49.000 (51.832)   Prec@5 89.000 (91.238)   [2025-10-28 18:01:56]
  Epoch: [031][200/500]   Time 0.163 (0.307)   Data 0.002 (0.149)   Loss 1.9324 (1.9475)   Prec@1 52.000 (51.677)   Prec@5 89.000 (91.284)   [2025-10-28 18:02:12]
  Epoch: [031][300/500]   Time 0.149 (0.257)   Data 0.001 (0.100)   Loss 1.9235 (1.9492)   Prec@1 52.000 (51.462)   Prec@5 96.000 (91.213)   [2025-10-28 18:02:27]
  Epoch: [031][400/500]   Time 0.777 (0.281)   Data 0.005 (0.076)   Loss 1.9684 (1.9495)   Prec@1 49.000 (51.392)   Prec@5 90.000 (91.125)   [2025-10-28 18:03:03]
  **Train** Prec@1 51.448 Prec@5 91.102 Error@1 48.552
  **Test** Prec@1 49.950 Prec@5 88.040 Error@1 50.050

==>>[2025-10-28 18:05:26] [Epoch=032/040] [Need: 00:36:34] [LR=0.0010] [Best : Accuracy=51.51, Error=48.49]
  Epoch: [032][000/500]   Time 30.066 (30.066)   Data 29.566 (29.566)   Loss 1.9566 (1.9566)   Prec@1 49.000 (49.000)   Prec@5 94.000 (94.000)   [2025-10-28 18:05:56]
  Epoch: [032][100/500]   Time 0.887 (1.181)   Data 0.013 (0.299)   Loss 1.9569 (1.9491)   Prec@1 51.000 (51.238)   Prec@5 92.000 (90.644)   [2025-10-28 18:07:26]
  Epoch: [032][200/500]   Time 0.559 (0.971)   Data 0.005 (0.153)   Loss 1.9373 (1.9451)   Prec@1 52.000 (51.776)   Prec@5 92.000 (90.796)   [2025-10-28 18:08:41]
  Epoch: [032][300/500]   Time 1.302 (0.905)   Data 0.011 (0.103)   Loss 1.9602 (1.9452)   Prec@1 52.000 (51.854)   Prec@5 89.000 (90.831)   [2025-10-28 18:09:59]
  Epoch: [032][400/500]   Time 0.317 (0.880)   Data 0.001 (0.079)   Loss 2.0087 (1.9481)   Prec@1 43.000 (51.519)   Prec@5 91.000 (90.835)   [2025-10-28 18:11:19]
  **Train** Prec@1 51.622 Prec@5 90.840 Error@1 48.378
  **Test** Prec@1 49.580 Prec@5 87.530 Error@1 50.420

==>>[2025-10-28 18:12:05] [Epoch=033/040] [Need: 00:32:26] [LR=0.0010] [Best : Accuracy=51.51, Error=48.49]
  Epoch: [033][000/500]   Time 25.601 (25.601)   Data 25.208 (25.208)   Loss 1.8776 (1.8776)   Prec@1 59.000 (59.000)   Prec@5 87.000 (87.000)   [2025-10-28 18:12:31]
  Epoch: [033][100/500]   Time 0.138 (0.376)   Data 0.000 (0.250)   Loss 1.9555 (1.9481)   Prec@1 51.000 (51.663)   Prec@5 91.000 (90.604)   [2025-10-28 18:12:43]
  Epoch: [033][200/500]   Time 0.148 (0.254)   Data 0.001 (0.126)   Loss 1.9334 (1.9476)   Prec@1 52.000 (51.667)   Prec@5 93.000 (90.652)   [2025-10-28 18:12:56]
  Epoch: [033][300/500]   Time 0.114 (0.214)   Data 0.000 (0.085)   Loss 1.9231 (1.9471)   Prec@1 55.000 (51.688)   Prec@5 91.000 (90.641)   [2025-10-28 18:13:10]
  Epoch: [033][400/500]   Time 0.132 (0.192)   Data 0.002 (0.064)   Loss 1.9262 (1.9447)   Prec@1 54.000 (51.935)   Prec@5 88.000 (90.815)   [2025-10-28 18:13:23]
  **Train** Prec@1 51.852 Prec@5 90.906 Error@1 48.148
  **Test** Prec@1 52.270 Prec@5 90.040 Error@1 47.730
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 18:14:02] [Epoch=034/040] [Need: 00:27:20] [LR=0.0010] [Best : Accuracy=52.27, Error=47.73]
  Epoch: [034][000/500]   Time 23.139 (23.139)   Data 22.792 (22.792)   Loss 1.9741 (1.9741)   Prec@1 45.000 (45.000)   Prec@5 90.000 (90.000)   [2025-10-28 18:14:25]
  Epoch: [034][100/500]   Time 0.133 (0.352)   Data 0.001 (0.226)   Loss 1.8855 (1.9457)   Prec@1 59.000 (51.960)   Prec@5 96.000 (90.861)   [2025-10-28 18:14:37]
  Epoch: [034][200/500]   Time 0.128 (0.242)   Data 0.001 (0.114)   Loss 1.9495 (1.9451)   Prec@1 51.000 (51.930)   Prec@5 92.000 (91.184)   [2025-10-28 18:14:50]
  Epoch: [034][300/500]   Time 0.141 (0.205)   Data 0.001 (0.077)   Loss 1.9424 (1.9463)   Prec@1 56.000 (51.807)   Prec@5 93.000 (91.110)   [2025-10-28 18:15:03]
  Epoch: [034][400/500]   Time 0.148 (0.189)   Data 0.002 (0.058)   Loss 1.9524 (1.9450)   Prec@1 52.000 (51.955)   Prec@5 91.000 (91.052)   [2025-10-28 18:15:17]
  **Train** Prec@1 51.988 Prec@5 91.008 Error@1 48.012
  **Test** Prec@1 50.730 Prec@5 88.330 Error@1 49.270

==>>[2025-10-28 18:15:56] [Epoch=035/040] [Need: 00:22:24] [LR=0.0010] [Best : Accuracy=52.27, Error=47.73]
  Epoch: [035][000/500]   Time 24.228 (24.228)   Data 23.998 (23.998)   Loss 1.9396 (1.9396)   Prec@1 54.000 (54.000)   Prec@5 87.000 (87.000)   [2025-10-28 18:16:20]
  Epoch: [035][100/500]   Time 0.135 (0.365)   Data 0.000 (0.238)   Loss 1.9099 (1.9472)   Prec@1 56.000 (51.911)   Prec@5 93.000 (90.495)   [2025-10-28 18:16:32]
  Epoch: [035][200/500]   Time 0.127 (0.245)   Data 0.001 (0.120)   Loss 1.9585 (1.9452)   Prec@1 50.000 (51.896)   Prec@5 91.000 (90.856)   [2025-10-28 18:16:45]
  Epoch: [035][300/500]   Time 0.124 (0.204)   Data 0.000 (0.081)   Loss 1.9659 (1.9468)   Prec@1 49.000 (51.678)   Prec@5 91.000 (90.741)   [2025-10-28 18:16:57]
  Epoch: [035][400/500]   Time 0.128 (0.185)   Data 0.001 (0.061)   Loss 1.8940 (1.9452)   Prec@1 57.000 (51.873)   Prec@5 96.000 (90.903)   [2025-10-28 18:17:10]
  **Train** Prec@1 51.920 Prec@5 90.882 Error@1 48.080
  **Test** Prec@1 52.220 Prec@5 91.560 Error@1 47.780

==>>[2025-10-28 18:17:49] [Epoch=036/040] [Need: 00:17:37] [LR=0.0010] [Best : Accuracy=52.27, Error=47.73]
  Epoch: [036][000/500]   Time 24.702 (24.702)   Data 24.446 (24.446)   Loss 1.9131 (1.9131)   Prec@1 54.000 (54.000)   Prec@5 93.000 (93.000)   [2025-10-28 18:18:13]
  Epoch: [036][100/500]   Time 0.126 (0.384)   Data 0.000 (0.243)   Loss 1.9029 (1.9345)   Prec@1 58.000 (52.931)   Prec@5 91.000 (91.050)   [2025-10-28 18:18:28]
  Epoch: [036][200/500]   Time 0.171 (0.292)   Data 0.001 (0.123)   Loss 1.9756 (1.9388)   Prec@1 47.000 (52.358)   Prec@5 94.000 (91.015)   [2025-10-28 18:18:47]
  Epoch: [036][300/500]   Time 0.275 (0.251)   Data 0.000 (0.083)   Loss 1.9259 (1.9395)   Prec@1 56.000 (52.359)   Prec@5 91.000 (90.934)   [2025-10-28 18:19:04]
  Epoch: [036][400/500]   Time 0.178 (0.232)   Data 0.002 (0.062)   Loss 1.9634 (1.9395)   Prec@1 51.000 (52.446)   Prec@5 90.000 (91.025)   [2025-10-28 18:19:22]
  **Train** Prec@1 52.412 Prec@5 91.068 Error@1 47.588
  **Test** Prec@1 49.190 Prec@5 86.020 Error@1 50.810

==>>[2025-10-28 18:20:16] [Epoch=037/040] [Need: 00:13:04] [LR=0.0010] [Best : Accuracy=52.27, Error=47.73]
  Epoch: [037][000/500]   Time 29.234 (29.234)   Data 29.050 (29.050)   Loss 1.9878 (1.9878)   Prec@1 48.000 (48.000)   Prec@5 90.000 (90.000)   [2025-10-28 18:20:46]
  Epoch: [037][100/500]   Time 0.146 (0.438)   Data 0.001 (0.289)   Loss 1.9610 (1.9423)   Prec@1 50.000 (52.139)   Prec@5 90.000 (90.901)   [2025-10-28 18:21:01]
  Epoch: [037][200/500]   Time 0.146 (0.296)   Data 0.001 (0.146)   Loss 1.8908 (1.9447)   Prec@1 57.000 (51.796)   Prec@5 92.000 (90.577)   [2025-10-28 18:21:16]
  Epoch: [037][300/500]   Time 0.161 (0.249)   Data 0.002 (0.098)   Loss 2.0132 (1.9448)   Prec@1 46.000 (51.767)   Prec@5 88.000 (90.741)   [2025-10-28 18:21:31]
  Epoch: [037][400/500]   Time 0.154 (0.226)   Data 0.003 (0.074)   Loss 1.9088 (1.9427)   Prec@1 57.000 (51.998)   Prec@5 91.000 (90.898)   [2025-10-28 18:21:47]
  **Train** Prec@1 52.032 Prec@5 90.880 Error@1 47.968
  **Test** Prec@1 49.310 Prec@5 86.130 Error@1 50.690

==>>[2025-10-28 18:22:33] [Epoch=038/040] [Need: 00:08:36] [LR=0.0010] [Best : Accuracy=52.27, Error=47.73]
  Epoch: [038][000/500]   Time 28.329 (28.329)   Data 28.151 (28.151)   Loss 1.9027 (1.9027)   Prec@1 55.000 (55.000)   Prec@5 90.000 (90.000)   [2025-10-28 18:23:02]
  Epoch: [038][100/500]   Time 0.151 (0.426)   Data 0.003 (0.280)   Loss 1.8560 (1.9439)   Prec@1 61.000 (51.891)   Prec@5 94.000 (90.901)   [2025-10-28 18:23:16]
  Epoch: [038][200/500]   Time 0.148 (0.289)   Data 0.000 (0.141)   Loss 1.9621 (1.9427)   Prec@1 49.000 (52.025)   Prec@5 94.000 (90.856)   [2025-10-28 18:23:31]
  Epoch: [038][300/500]   Time 0.160 (0.244)   Data 0.003 (0.095)   Loss 1.8978 (1.9416)   Prec@1 58.000 (52.023)   Prec@5 97.000 (90.977)   [2025-10-28 18:23:47]
  Epoch: [038][400/500]   Time 0.157 (0.223)   Data 0.003 (0.072)   Loss 1.8691 (1.9400)   Prec@1 60.000 (52.214)   Prec@5 95.000 (90.985)   [2025-10-28 18:24:03]
  **Train** Prec@1 52.308 Prec@5 90.972 Error@1 47.692
  **Test** Prec@1 51.140 Prec@5 87.840 Error@1 48.860

==>>[2025-10-28 18:26:11] [Epoch=039/040] [Need: 00:04:17] [LR=0.0010] [Best : Accuracy=52.27, Error=47.73]
  Epoch: [039][000/500]   Time 27.517 (27.517)   Data 26.905 (26.905)   Loss 1.9409 (1.9409)   Prec@1 53.000 (53.000)   Prec@5 93.000 (93.000)   [2025-10-28 18:26:39]
  Epoch: [039][100/500]   Time 0.668 (0.911)   Data 0.004 (0.270)   Loss 1.9427 (1.9436)   Prec@1 50.000 (51.881)   Prec@5 92.000 (91.238)   [2025-10-28 18:27:43]
  Epoch: [039][200/500]   Time 0.549 (0.886)   Data 0.006 (0.138)   Loss 2.0394 (1.9413)   Prec@1 41.000 (52.060)   Prec@5 85.000 (91.055)   [2025-10-28 18:29:09]
  Epoch: [039][300/500]   Time 0.706 (0.892)   Data 0.001 (0.094)   Loss 1.9673 (1.9379)   Prec@1 50.000 (52.472)   Prec@5 90.000 (91.143)   [2025-10-28 18:30:40]
  Epoch: [039][400/500]   Time 1.019 (0.917)   Data 0.001 (0.072)   Loss 1.9389 (1.9386)   Prec@1 51.000 (52.419)   Prec@5 88.000 (91.219)   [2025-10-28 18:32:19]
  **Train** Prec@1 52.404 Prec@5 91.178 Error@1 47.596
  **Test** Prec@1 54.460 Prec@5 91.470 Error@1 45.540
=> Obtain best accuracy, and update the best model
