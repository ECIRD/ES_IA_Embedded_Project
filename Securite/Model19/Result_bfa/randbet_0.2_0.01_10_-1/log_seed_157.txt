save path : ./save/resnet9_quan/randbet_0.2_0.01_10_-1
{'data_path': './dataset', 'arch': 'resnet9_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': True, 'clipping_coeff': 0.2, 'learning_rate': 0.01, 'manualSeed': 157, 'save_path': './save/resnet9_quan/randbet_0.2_0.01_10_-1', 'enable_bfa': False, 'resume': '', 'quan_bitwidth': None, 'reset_weight': False, 'evaluate': False, 'n_iter': 30, 'model_only': False, 'random_bfa': False, 'use_cuda': True}
Random Seed: 157
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'resnet9_quan'
=> network :
 Sequential(
  (0): Sequential(
    (0): quan_Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (1): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (2): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (4): Dropout2d(p=0.2, inplace=False)
  (5): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (skip): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (6): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Dropout2d(p=0.3, inplace=False)
  (9): Sequential(
    (0): quan_Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (10): Sequential(
    (0): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (12): Dropout2d(p=0.3, inplace=False)
  (13): Sequential(
    (0): quan_Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (14): AdaptiveAvgPool2d(output_size=(1, 1))
  (15): Flatten()
  (16): quan_Linear(in_features=64, out_features=128, bias=False)
  (17): ReLU(inplace=True)
  (18): Dropout(p=0.35, inplace=False)
  (19): quan_Linear(in_features=128, out_features=10, bias=False)
  (20): Softmax(dim=1)
)
=> do not use any checkpoint for resnet9_quan model

==>>[2025-10-28 19:44:25] [Epoch=000/040] [Need: 00:00:00] [LR=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/500]   Time 18.530 (18.530)   Data 17.884 (17.884)   Loss 2.3042 (2.3042)   Prec@1 9.000 (9.000)   Prec@5 44.000 (44.000)   [2025-10-28 19:44:44]
  Epoch: [000][100/500]   Time 0.100 (0.284)   Data 0.001 (0.178)   Loss 2.3013 (2.3024)   Prec@1 8.000 (10.178)   Prec@5 52.000 (50.802)   [2025-10-28 19:44:54]
  Epoch: [000][200/500]   Time 0.102 (0.193)   Data 0.000 (0.090)   Loss 2.3002 (2.3017)   Prec@1 14.000 (10.622)   Prec@5 51.000 (51.960)   [2025-10-28 19:45:04]
  Epoch: [000][300/500]   Time 0.099 (0.163)   Data 0.001 (0.060)   Loss 2.3056 (2.2998)   Prec@1 7.000 (11.113)   Prec@5 56.000 (53.262)   [2025-10-28 19:45:14]
  Epoch: [000][400/500]   Time 0.099 (0.148)   Data 0.000 (0.045)   Loss 2.2863 (2.2961)   Prec@1 12.000 (11.546)   Prec@5 64.000 (54.676)   [2025-10-28 19:45:25]
  **Train** Prec@1 12.474 Prec@5 56.148 Error@1 87.526
  **Test** Prec@1 15.980 Prec@5 57.450 Error@1 84.020
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 19:45:54] [Epoch=001/040] [Need: 00:57:47] [LR=0.0100] [Best : Accuracy=15.98, Error=84.02]
  Epoch: [001][000/500]   Time 17.882 (17.882)   Data 17.521 (17.521)   Loss 2.2539 (2.2539)   Prec@1 17.000 (17.000)   Prec@5 63.000 (63.000)   [2025-10-28 19:46:12]
  Epoch: [001][100/500]   Time 0.102 (0.275)   Data 0.001 (0.174)   Loss 2.2341 (2.2612)   Prec@1 23.000 (17.871)   Prec@5 63.000 (64.594)   [2025-10-28 19:46:22]
  Epoch: [001][200/500]   Time 0.099 (0.190)   Data 0.001 (0.088)   Loss 2.2927 (2.2606)   Prec@1 15.000 (18.050)   Prec@5 72.000 (65.602)   [2025-10-28 19:46:32]
  Epoch: [001][300/500]   Time 0.100 (0.161)   Data 0.001 (0.059)   Loss 2.2799 (2.2592)   Prec@1 16.000 (18.206)   Prec@5 72.000 (66.615)   [2025-10-28 19:46:43]
  Epoch: [001][400/500]   Time 0.102 (0.147)   Data 0.001 (0.044)   Loss 2.2445 (2.2582)   Prec@1 16.000 (18.187)   Prec@5 70.000 (67.180)   [2025-10-28 19:46:53]
  **Train** Prec@1 18.266 Prec@5 67.870 Error@1 81.734
  **Test** Prec@1 16.410 Prec@5 60.700 Error@1 83.590
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 19:47:22] [Epoch=002/040] [Need: 00:56:05] [LR=0.0100] [Best : Accuracy=16.41, Error=83.59]
  Epoch: [002][000/500]   Time 18.062 (18.062)   Data 17.733 (17.733)   Loss 2.2267 (2.2267)   Prec@1 20.000 (20.000)   Prec@5 74.000 (74.000)   [2025-10-28 19:47:40]
  Epoch: [002][100/500]   Time 0.107 (0.277)   Data 0.000 (0.176)   Loss 2.2431 (2.2388)   Prec@1 15.000 (18.099)   Prec@5 68.000 (70.663)   [2025-10-28 19:47:50]
  Epoch: [002][200/500]   Time 0.102 (0.190)   Data 0.000 (0.089)   Loss 2.2407 (2.2312)   Prec@1 18.000 (18.930)   Prec@5 67.000 (71.423)   [2025-10-28 19:48:00]
  Epoch: [002][300/500]   Time 0.103 (0.161)   Data 0.001 (0.059)   Loss 2.2243 (2.2283)   Prec@1 21.000 (19.056)   Prec@5 69.000 (71.721)   [2025-10-28 19:48:11]
  Epoch: [002][400/500]   Time 0.102 (0.146)   Data 0.001 (0.045)   Loss 2.2047 (2.2265)   Prec@1 22.000 (19.309)   Prec@5 68.000 (71.833)   [2025-10-28 19:48:21]
  **Train** Prec@1 19.620 Prec@5 72.234 Error@1 80.380
  **Test** Prec@1 14.520 Prec@5 59.200 Error@1 85.480

==>>[2025-10-28 19:48:50] [Epoch=003/040] [Need: 00:54:28] [LR=0.0100] [Best : Accuracy=16.41, Error=83.59]
  Epoch: [003][000/500]   Time 18.479 (18.479)   Data 18.095 (18.095)   Loss 2.2418 (2.2418)   Prec@1 17.000 (17.000)   Prec@5 71.000 (71.000)   [2025-10-28 19:49:09]
  Epoch: [003][100/500]   Time 0.107 (0.283)   Data 0.000 (0.180)   Loss 2.2272 (2.2123)   Prec@1 20.000 (21.198)   Prec@5 69.000 (73.267)   [2025-10-28 19:49:19]
  Epoch: [003][200/500]   Time 0.102 (0.195)   Data 0.000 (0.091)   Loss 2.2013 (2.2102)   Prec@1 24.000 (21.592)   Prec@5 80.000 (73.801)   [2025-10-28 19:49:29]
  Epoch: [003][300/500]   Time 0.101 (0.164)   Data 0.000 (0.061)   Loss 2.2090 (2.2098)   Prec@1 20.000 (21.588)   Prec@5 70.000 (74.166)   [2025-10-28 19:49:40]
  Epoch: [003][400/500]   Time 0.102 (0.149)   Data 0.001 (0.046)   Loss 2.1560 (2.2078)   Prec@1 30.000 (21.833)   Prec@5 77.000 (74.628)   [2025-10-28 19:49:50]
  **Train** Prec@1 22.010 Prec@5 74.942 Error@1 77.990
  **Test** Prec@1 13.350 Prec@5 56.980 Error@1 86.650

==>>[2025-10-28 19:50:19] [Epoch=004/040] [Need: 00:53:02] [LR=0.0100] [Best : Accuracy=16.41, Error=83.59]
  Epoch: [004][000/500]   Time 17.957 (17.957)   Data 17.659 (17.659)   Loss 2.2192 (2.2192)   Prec@1 18.000 (18.000)   Prec@5 66.000 (66.000)   [2025-10-28 19:50:37]
  Epoch: [004][100/500]   Time 0.101 (0.276)   Data 0.001 (0.175)   Loss 2.1997 (2.1976)   Prec@1 23.000 (24.050)   Prec@5 73.000 (76.109)   [2025-10-28 19:50:47]
  Epoch: [004][200/500]   Time 0.104 (0.189)   Data 0.001 (0.088)   Loss 2.1856 (2.1976)   Prec@1 23.000 (23.831)   Prec@5 81.000 (76.602)   [2025-10-28 19:50:57]
  Epoch: [004][300/500]   Time 0.103 (0.160)   Data 0.001 (0.059)   Loss 2.2066 (2.1952)   Prec@1 19.000 (24.096)   Prec@5 76.000 (76.844)   [2025-10-28 19:51:07]
  Epoch: [004][400/500]   Time 0.104 (0.146)   Data 0.000 (0.045)   Loss 2.1809 (2.1933)   Prec@1 27.000 (24.456)   Prec@5 78.000 (77.185)   [2025-10-28 19:51:17]
  **Train** Prec@1 24.840 Prec@5 77.398 Error@1 75.160
  **Test** Prec@1 23.370 Prec@5 75.080 Error@1 76.630
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 19:51:47] [Epoch=005/040] [Need: 00:51:29] [LR=0.0100] [Best : Accuracy=23.37, Error=76.63]
  Epoch: [005][000/500]   Time 18.118 (18.118)   Data 17.769 (17.769)   Loss 2.1885 (2.1885)   Prec@1 23.000 (23.000)   Prec@5 86.000 (86.000)   [2025-10-28 19:52:05]
  Epoch: [005][100/500]   Time 0.109 (0.277)   Data 0.000 (0.176)   Loss 2.1997 (2.1802)   Prec@1 24.000 (26.426)   Prec@5 71.000 (78.495)   [2025-10-28 19:52:15]
  Epoch: [005][200/500]   Time 0.110 (0.190)   Data 0.001 (0.089)   Loss 2.2164 (2.1784)   Prec@1 22.000 (26.338)   Prec@5 72.000 (78.095)   [2025-10-28 19:52:25]
  Epoch: [005][300/500]   Time 0.100 (0.161)   Data 0.000 (0.060)   Loss 2.1856 (2.1778)   Prec@1 24.000 (26.445)   Prec@5 79.000 (78.352)   [2025-10-28 19:52:35]
  Epoch: [005][400/500]   Time 0.106 (0.146)   Data 0.001 (0.045)   Loss 2.1567 (2.1753)   Prec@1 32.000 (26.805)   Prec@5 81.000 (78.589)   [2025-10-28 19:52:45]
  **Train** Prec@1 27.080 Prec@5 78.710 Error@1 72.920
  **Test** Prec@1 17.800 Prec@5 66.840 Error@1 82.200

==>>[2025-10-28 19:53:14] [Epoch=006/040] [Need: 00:49:59] [LR=0.0100] [Best : Accuracy=23.37, Error=76.63]
  Epoch: [006][000/500]   Time 17.971 (17.971)   Data 17.595 (17.595)   Loss 2.2051 (2.2051)   Prec@1 23.000 (23.000)   Prec@5 74.000 (74.000)   [2025-10-28 19:53:32]
  Epoch: [006][100/500]   Time 0.103 (0.283)   Data 0.000 (0.175)   Loss 2.2316 (2.1589)   Prec@1 23.000 (29.129)   Prec@5 74.000 (79.158)   [2025-10-28 19:53:43]
  Epoch: [006][200/500]   Time 0.100 (0.193)   Data 0.001 (0.088)   Loss 2.1385 (2.1591)   Prec@1 28.000 (29.000)   Prec@5 81.000 (79.010)   [2025-10-28 19:53:53]
  Epoch: [006][300/500]   Time 0.108 (0.163)   Data 0.000 (0.059)   Loss 2.1522 (2.1566)   Prec@1 31.000 (29.495)   Prec@5 81.000 (78.977)   [2025-10-28 19:54:04]
  Epoch: [006][400/500]   Time 0.099 (0.149)   Data 0.000 (0.044)   Loss 2.1853 (2.1564)   Prec@1 28.000 (29.529)   Prec@5 79.000 (79.060)   [2025-10-28 19:54:14]
  **Train** Prec@1 29.624 Prec@5 79.074 Error@1 70.376
  **Test** Prec@1 23.310 Prec@5 74.380 Error@1 76.690

==>>[2025-10-28 19:54:44] [Epoch=007/040] [Need: 00:48:36] [LR=0.0100] [Best : Accuracy=23.37, Error=76.63]
  Epoch: [007][000/500]   Time 17.884 (17.884)   Data 17.539 (17.539)   Loss 2.1408 (2.1408)   Prec@1 30.000 (30.000)   Prec@5 76.000 (76.000)   [2025-10-28 19:55:02]
  Epoch: [007][100/500]   Time 0.100 (0.274)   Data 0.001 (0.174)   Loss 2.1248 (2.1472)   Prec@1 34.000 (30.307)   Prec@5 82.000 (79.743)   [2025-10-28 19:55:12]
  Epoch: [007][200/500]   Time 0.101 (0.189)   Data 0.001 (0.088)   Loss 2.0991 (2.1436)   Prec@1 33.000 (30.771)   Prec@5 83.000 (80.144)   [2025-10-28 19:55:22]
  Epoch: [007][300/500]   Time 0.101 (0.161)   Data 0.001 (0.059)   Loss 2.2073 (2.1423)   Prec@1 23.000 (30.827)   Prec@5 76.000 (80.462)   [2025-10-28 19:55:32]
  Epoch: [007][400/500]   Time 0.100 (0.147)   Data 0.000 (0.044)   Loss 2.1131 (2.1408)   Prec@1 38.000 (31.027)   Prec@5 82.000 (80.633)   [2025-10-28 19:55:43]
  **Train** Prec@1 31.010 Prec@5 80.854 Error@1 68.990
  **Test** Prec@1 21.940 Prec@5 75.160 Error@1 78.060

==>>[2025-10-28 19:56:12] [Epoch=008/040] [Need: 00:47:07] [LR=0.0100] [Best : Accuracy=23.37, Error=76.63]
  Epoch: [008][000/500]   Time 18.656 (18.656)   Data 18.273 (18.273)   Loss 2.1831 (2.1831)   Prec@1 26.000 (26.000)   Prec@5 83.000 (83.000)   [2025-10-28 19:56:31]
  Epoch: [008][100/500]   Time 0.105 (0.284)   Data 0.000 (0.181)   Loss 2.1443 (2.1263)   Prec@1 30.000 (32.376)   Prec@5 85.000 (82.347)   [2025-10-28 19:56:41]
  Epoch: [008][200/500]   Time 0.103 (0.194)   Data 0.002 (0.092)   Loss 2.1132 (2.1309)   Prec@1 30.000 (31.841)   Prec@5 81.000 (82.234)   [2025-10-28 19:56:51]
  Epoch: [008][300/500]   Time 0.107 (0.164)   Data 0.001 (0.061)   Loss 2.1021 (2.1293)   Prec@1 36.000 (32.066)   Prec@5 81.000 (82.243)   [2025-10-28 19:57:02]
  Epoch: [008][400/500]   Time 0.100 (0.149)   Data 0.001 (0.046)   Loss 2.1035 (2.1292)   Prec@1 37.000 (31.990)   Prec@5 89.000 (82.284)   [2025-10-28 19:57:12]
  **Train** Prec@1 32.214 Prec@5 82.316 Error@1 67.786
  **Test** Prec@1 26.410 Prec@5 83.700 Error@1 73.590
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 19:57:41] [Epoch=009/040] [Need: 00:45:42] [LR=0.0100] [Best : Accuracy=26.41, Error=73.59]
  Epoch: [009][000/500]   Time 18.657 (18.657)   Data 18.367 (18.367)   Loss 2.1306 (2.1306)   Prec@1 34.000 (34.000)   Prec@5 82.000 (82.000)   [2025-10-28 19:58:00]
  Epoch: [009][100/500]   Time 0.104 (0.283)   Data 0.000 (0.182)   Loss 2.1492 (2.1220)   Prec@1 34.000 (32.574)   Prec@5 75.000 (82.822)   [2025-10-28 19:58:10]
  Epoch: [009][200/500]   Time 0.104 (0.194)   Data 0.001 (0.092)   Loss 2.1445 (2.1235)   Prec@1 27.000 (32.532)   Prec@5 83.000 (82.970)   [2025-10-28 19:58:20]
  Epoch: [009][300/500]   Time 0.105 (0.164)   Data 0.000 (0.062)   Loss 2.0845 (2.1179)   Prec@1 38.000 (33.093)   Prec@5 84.000 (83.532)   [2025-10-28 19:58:31]
  Epoch: [009][400/500]   Time 0.101 (0.149)   Data 0.001 (0.046)   Loss 2.1259 (2.1178)   Prec@1 31.000 (33.142)   Prec@5 92.000 (83.683)   [2025-10-28 19:58:41]
  **Train** Prec@1 33.238 Prec@5 83.658 Error@1 66.762
  **Test** Prec@1 13.820 Prec@5 61.320 Error@1 86.180

==>>[2025-10-28 19:59:10] [Epoch=010/040] [Need: 00:44:15] [LR=0.0100] [Best : Accuracy=26.41, Error=73.59]
  Epoch: [010][000/500]   Time 17.905 (17.905)   Data 17.524 (17.524)   Loss 2.1671 (2.1671)   Prec@1 31.000 (31.000)   Prec@5 83.000 (83.000)   [2025-10-28 19:59:28]
  Epoch: [010][100/500]   Time 0.099 (0.274)   Data 0.001 (0.174)   Loss 2.0973 (2.1127)   Prec@1 36.000 (33.634)   Prec@5 92.000 (84.772)   [2025-10-28 19:59:38]
  Epoch: [010][200/500]   Time 0.106 (0.190)   Data 0.001 (0.088)   Loss 2.0772 (2.1134)   Prec@1 36.000 (33.542)   Prec@5 86.000 (84.537)   [2025-10-28 19:59:49]
  Epoch: [010][300/500]   Time 0.101 (0.161)   Data 0.001 (0.059)   Loss 2.0773 (2.1116)   Prec@1 37.000 (33.831)   Prec@5 86.000 (84.598)   [2025-10-28 19:59:59]
  Epoch: [010][400/500]   Time 0.110 (0.146)   Data 0.001 (0.044)   Loss 2.1167 (2.1107)   Prec@1 36.000 (34.002)   Prec@5 83.000 (84.703)   [2025-10-28 20:00:09]
  **Train** Prec@1 34.244 Prec@5 84.886 Error@1 65.756
  **Test** Prec@1 34.090 Prec@5 88.140 Error@1 65.910
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 20:00:39] [Epoch=011/040] [Need: 00:42:46] [LR=0.0100] [Best : Accuracy=34.09, Error=65.91]
  Epoch: [011][000/500]   Time 18.005 (18.005)   Data 17.645 (17.645)   Loss 2.0004 (2.0004)   Prec@1 49.000 (49.000)   Prec@5 86.000 (86.000)   [2025-10-28 20:00:57]
  Epoch: [011][100/500]   Time 0.097 (0.276)   Data 0.001 (0.175)   Loss 2.1134 (2.1008)   Prec@1 34.000 (35.257)   Prec@5 83.000 (85.139)   [2025-10-28 20:01:06]
  Epoch: [011][200/500]   Time 0.100 (0.189)   Data 0.001 (0.088)   Loss 2.1225 (2.1026)   Prec@1 34.000 (35.114)   Prec@5 83.000 (85.179)   [2025-10-28 20:01:17]
  Epoch: [011][300/500]   Time 0.102 (0.161)   Data 0.000 (0.059)   Loss 2.1269 (2.1004)   Prec@1 32.000 (35.216)   Prec@5 80.000 (85.316)   [2025-10-28 20:01:27]
  Epoch: [011][400/500]   Time 0.102 (0.146)   Data 0.000 (0.045)   Loss 2.0816 (2.0991)   Prec@1 38.000 (35.426)   Prec@5 82.000 (85.401)   [2025-10-28 20:01:37]
  **Train** Prec@1 35.446 Prec@5 85.310 Error@1 64.554
  **Test** Prec@1 33.260 Prec@5 85.670 Error@1 66.740

==>>[2025-10-28 20:02:06] [Epoch=012/040] [Need: 00:41:15] [LR=0.0100] [Best : Accuracy=34.09, Error=65.91]
  Epoch: [012][000/500]   Time 17.877 (17.877)   Data 17.568 (17.568)   Loss 2.0803 (2.0803)   Prec@1 37.000 (37.000)   Prec@5 90.000 (90.000)   [2025-10-28 20:02:24]
  Epoch: [012][100/500]   Time 0.099 (0.274)   Data 0.001 (0.175)   Loss 2.1193 (2.0943)   Prec@1 33.000 (35.832)   Prec@5 87.000 (85.931)   [2025-10-28 20:02:34]
  Epoch: [012][200/500]   Time 0.104 (0.189)   Data 0.001 (0.088)   Loss 2.0202 (2.0934)   Prec@1 45.000 (35.896)   Prec@5 85.000 (85.915)   [2025-10-28 20:02:44]
  Epoch: [012][300/500]   Time 0.112 (0.161)   Data 0.001 (0.059)   Loss 2.1527 (2.0928)   Prec@1 28.000 (35.957)   Prec@5 86.000 (85.887)   [2025-10-28 20:02:54]
  Epoch: [012][400/500]   Time 0.102 (0.146)   Data 0.001 (0.044)   Loss 2.0123 (2.0912)   Prec@1 46.000 (36.202)   Prec@5 87.000 (85.756)   [2025-10-28 20:03:05]
  **Train** Prec@1 36.232 Prec@5 85.906 Error@1 63.768
  **Test** Prec@1 29.080 Prec@5 80.250 Error@1 70.920

==>>[2025-10-28 20:03:34] [Epoch=013/040] [Need: 00:39:45] [LR=0.0100] [Best : Accuracy=34.09, Error=65.91]
  Epoch: [013][000/500]   Time 17.821 (17.821)   Data 17.441 (17.441)   Loss 2.0168 (2.0168)   Prec@1 47.000 (47.000)   Prec@5 92.000 (92.000)   [2025-10-28 20:03:51]
  Epoch: [013][100/500]   Time 0.098 (0.272)   Data 0.001 (0.173)   Loss 2.1651 (2.0836)   Prec@1 30.000 (36.970)   Prec@5 84.000 (86.604)   [2025-10-28 20:04:01]
  Epoch: [013][200/500]   Time 0.102 (0.187)   Data 0.000 (0.087)   Loss 2.0735 (2.0803)   Prec@1 35.000 (37.522)   Prec@5 88.000 (86.677)   [2025-10-28 20:04:11]
  Epoch: [013][300/500]   Time 0.108 (0.159)   Data 0.000 (0.059)   Loss 2.0839 (2.0786)   Prec@1 39.000 (37.671)   Prec@5 88.000 (86.884)   [2025-10-28 20:04:22]
  Epoch: [013][400/500]   Time 0.097 (0.145)   Data 0.000 (0.044)   Loss 2.0570 (2.0769)   Prec@1 36.000 (37.870)   Prec@5 90.000 (86.895)   [2025-10-28 20:04:32]
  **Train** Prec@1 38.064 Prec@5 86.918 Error@1 61.936
  **Test** Prec@1 32.550 Prec@5 84.940 Error@1 67.450

==>>[2025-10-28 20:05:01] [Epoch=014/040] [Need: 00:38:15] [LR=0.0100] [Best : Accuracy=34.09, Error=65.91]
  Epoch: [014][000/500]   Time 17.944 (17.944)   Data 17.666 (17.666)   Loss 2.0563 (2.0563)   Prec@1 39.000 (39.000)   Prec@5 86.000 (86.000)   [2025-10-28 20:05:19]
  Epoch: [014][100/500]   Time 0.096 (0.275)   Data 0.000 (0.176)   Loss 2.1183 (2.0716)   Prec@1 30.000 (38.386)   Prec@5 91.000 (86.891)   [2025-10-28 20:05:29]
  Epoch: [014][200/500]   Time 0.106 (0.190)   Data 0.001 (0.089)   Loss 2.0700 (2.0714)   Prec@1 41.000 (38.473)   Prec@5 86.000 (86.572)   [2025-10-28 20:05:39]
  Epoch: [014][300/500]   Time 0.101 (0.161)   Data 0.000 (0.059)   Loss 2.0542 (2.0673)   Prec@1 42.000 (38.841)   Prec@5 92.000 (86.977)   [2025-10-28 20:05:50]
  Epoch: [014][400/500]   Time 0.108 (0.147)   Data 0.001 (0.045)   Loss 2.0617 (2.0651)   Prec@1 39.000 (39.067)   Prec@5 89.000 (87.060)   [2025-10-28 20:06:00]
  **Train** Prec@1 39.284 Prec@5 87.042 Error@1 60.716
  **Test** Prec@1 40.770 Prec@5 88.610 Error@1 59.230
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 20:06:29] [Epoch=015/040] [Need: 00:36:46] [LR=0.0100] [Best : Accuracy=40.77, Error=59.23]
  Epoch: [015][000/500]   Time 18.004 (18.004)   Data 17.645 (17.645)   Loss 2.0941 (2.0941)   Prec@1 33.000 (33.000)   Prec@5 90.000 (90.000)   [2025-10-28 20:06:47]
  Epoch: [015][100/500]   Time 0.101 (0.278)   Data 0.001 (0.175)   Loss 1.9952 (2.0555)   Prec@1 44.000 (40.099)   Prec@5 88.000 (87.515)   [2025-10-28 20:06:57]
  Epoch: [015][200/500]   Time 0.104 (0.192)   Data 0.000 (0.088)   Loss 2.0495 (2.0555)   Prec@1 42.000 (40.348)   Prec@5 86.000 (87.677)   [2025-10-28 20:07:08]
  Epoch: [015][300/500]   Time 0.109 (0.163)   Data 0.001 (0.059)   Loss 2.0176 (2.0514)   Prec@1 47.000 (40.827)   Prec@5 88.000 (87.741)   [2025-10-28 20:07:18]
  Epoch: [015][400/500]   Time 0.099 (0.148)   Data 0.000 (0.045)   Loss 2.0378 (2.0489)   Prec@1 43.000 (41.092)   Prec@5 90.000 (87.865)   [2025-10-28 20:07:28]
  **Train** Prec@1 41.260 Prec@5 87.872 Error@1 58.740
  **Test** Prec@1 37.240 Prec@5 82.790 Error@1 62.760

==>>[2025-10-28 20:07:58] [Epoch=016/040] [Need: 00:35:19] [LR=0.0100] [Best : Accuracy=40.77, Error=59.23]
  Epoch: [016][000/500]   Time 18.003 (18.003)   Data 17.625 (17.625)   Loss 2.0931 (2.0931)   Prec@1 35.000 (35.000)   Prec@5 86.000 (86.000)   [2025-10-28 20:08:16]
  Epoch: [016][100/500]   Time 0.110 (0.274)   Data 0.001 (0.175)   Loss 2.0232 (2.0308)   Prec@1 44.000 (43.099)   Prec@5 88.000 (88.139)   [2025-10-28 20:08:26]
  Epoch: [016][200/500]   Time 0.101 (0.188)   Data 0.001 (0.088)   Loss 2.1186 (2.0339)   Prec@1 33.000 (42.721)   Prec@5 84.000 (88.070)   [2025-10-28 20:08:36]
  Epoch: [016][300/500]   Time 0.101 (0.159)   Data 0.000 (0.059)   Loss 2.0138 (2.0309)   Prec@1 46.000 (43.053)   Prec@5 82.000 (88.106)   [2025-10-28 20:08:46]
  Epoch: [016][400/500]   Time 0.103 (0.145)   Data 0.000 (0.045)   Loss 1.9206 (2.0277)   Prec@1 56.000 (43.374)   Prec@5 93.000 (88.257)   [2025-10-28 20:08:57]
  **Train** Prec@1 43.438 Prec@5 88.272 Error@1 56.562
  **Test** Prec@1 44.610 Prec@5 88.190 Error@1 55.390
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 20:09:26] [Epoch=017/040] [Need: 00:33:50] [LR=0.0100] [Best : Accuracy=44.61, Error=55.39]
  Epoch: [017][000/500]   Time 17.986 (17.986)   Data 17.614 (17.614)   Loss 2.0090 (2.0090)   Prec@1 42.000 (42.000)   Prec@5 93.000 (93.000)   [2025-10-28 20:09:44]
  Epoch: [017][100/500]   Time 0.104 (0.276)   Data 0.000 (0.175)   Loss 2.0367 (2.0155)   Prec@1 41.000 (44.337)   Prec@5 90.000 (89.208)   [2025-10-28 20:09:54]
  Epoch: [017][200/500]   Time 0.103 (0.190)   Data 0.001 (0.088)   Loss 2.0022 (2.0161)   Prec@1 47.000 (44.343)   Prec@5 90.000 (88.886)   [2025-10-28 20:10:04]
  Epoch: [017][300/500]   Time 0.111 (0.161)   Data 0.001 (0.059)   Loss 2.0361 (2.0164)   Prec@1 42.000 (44.445)   Prec@5 90.000 (88.681)   [2025-10-28 20:10:15]
  Epoch: [017][400/500]   Time 0.100 (0.147)   Data 0.001 (0.045)   Loss 2.0442 (2.0181)   Prec@1 43.000 (44.162)   Prec@5 86.000 (88.618)   [2025-10-28 20:10:25]
  **Train** Prec@1 44.262 Prec@5 88.656 Error@1 55.738
  **Test** Prec@1 38.290 Prec@5 82.520 Error@1 61.710

==>>[2025-10-28 20:10:54] [Epoch=018/040] [Need: 00:32:22] [LR=0.0100] [Best : Accuracy=44.61, Error=55.39]
  Epoch: [018][000/500]   Time 18.002 (18.002)   Data 17.609 (17.609)   Loss 2.0571 (2.0571)   Prec@1 36.000 (36.000)   Prec@5 91.000 (91.000)   [2025-10-28 20:11:12]
  Epoch: [018][100/500]   Time 0.103 (0.277)   Data 0.001 (0.175)   Loss 1.9260 (2.0045)   Prec@1 54.000 (45.317)   Prec@5 90.000 (88.554)   [2025-10-28 20:11:22]
  Epoch: [018][200/500]   Time 0.102 (0.190)   Data 0.000 (0.088)   Loss 1.9389 (2.0106)   Prec@1 55.000 (44.796)   Prec@5 88.000 (88.338)   [2025-10-28 20:11:33]
  Epoch: [018][300/500]   Time 0.103 (0.162)   Data 0.001 (0.059)   Loss 1.9691 (2.0086)   Prec@1 49.000 (44.900)   Prec@5 89.000 (88.528)   [2025-10-28 20:11:43]
  Epoch: [018][400/500]   Time 0.106 (0.148)   Data 0.000 (0.045)   Loss 2.0544 (2.0071)   Prec@1 41.000 (45.140)   Prec@5 83.000 (88.539)   [2025-10-28 20:11:54]
  **Train** Prec@1 45.020 Prec@5 88.430 Error@1 54.980
  **Test** Prec@1 48.980 Prec@5 89.910 Error@1 51.020
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 20:12:24] [Epoch=019/040] [Need: 00:30:55] [LR=0.0100] [Best : Accuracy=48.98, Error=51.02]
  Epoch: [019][000/500]   Time 18.030 (18.030)   Data 17.642 (17.642)   Loss 2.0556 (2.0556)   Prec@1 41.000 (41.000)   Prec@5 86.000 (86.000)   [2025-10-28 20:12:42]
  Epoch: [019][100/500]   Time 0.103 (0.276)   Data 0.000 (0.175)   Loss 2.0457 (2.0058)   Prec@1 40.000 (45.248)   Prec@5 89.000 (88.267)   [2025-10-28 20:12:51]
  Epoch: [019][200/500]   Time 0.103 (0.189)   Data 0.001 (0.088)   Loss 2.0357 (2.0043)   Prec@1 44.000 (45.622)   Prec@5 86.000 (88.448)   [2025-10-28 20:13:02]
  Epoch: [019][300/500]   Time 0.096 (0.160)   Data 0.001 (0.059)   Loss 1.9860 (2.0001)   Prec@1 47.000 (46.037)   Prec@5 88.000 (88.970)   [2025-10-28 20:13:12]
  Epoch: [019][400/500]   Time 0.107 (0.146)   Data 0.000 (0.045)   Loss 1.9269 (2.0001)   Prec@1 55.000 (45.953)   Prec@5 86.000 (88.893)   [2025-10-28 20:13:22]
  **Train** Prec@1 46.122 Prec@5 89.034 Error@1 53.878
  **Test** Prec@1 49.620 Prec@5 90.150 Error@1 50.380
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 20:13:51] [Epoch=020/040] [Need: 00:29:25] [LR=0.0100] [Best : Accuracy=49.62, Error=50.38]
  Epoch: [020][000/500]   Time 17.823 (17.823)   Data 17.533 (17.533)   Loss 1.9798 (1.9798)   Prec@1 48.000 (48.000)   Prec@5 91.000 (91.000)   [2025-10-28 20:14:09]
  Epoch: [020][100/500]   Time 0.105 (0.276)   Data 0.000 (0.174)   Loss 1.9741 (1.9990)   Prec@1 51.000 (46.198)   Prec@5 93.000 (88.683)   [2025-10-28 20:14:19]
  Epoch: [020][200/500]   Time 0.102 (0.192)   Data 0.000 (0.088)   Loss 1.9519 (1.9939)   Prec@1 51.000 (46.682)   Prec@5 89.000 (88.801)   [2025-10-28 20:14:30]
  Epoch: [020][300/500]   Time 0.104 (0.163)   Data 0.001 (0.059)   Loss 2.0778 (1.9953)   Prec@1 37.000 (46.538)   Prec@5 85.000 (88.910)   [2025-10-28 20:14:40]
  Epoch: [020][400/500]   Time 0.103 (0.149)   Data 0.000 (0.044)   Loss 2.0294 (1.9945)   Prec@1 43.000 (46.668)   Prec@5 86.000 (88.908)   [2025-10-28 20:14:51]
  **Train** Prec@1 46.818 Prec@5 89.094 Error@1 53.182
  **Test** Prec@1 48.370 Prec@5 88.970 Error@1 51.630

==>>[2025-10-28 20:15:21] [Epoch=021/040] [Need: 00:27:58] [LR=0.0100] [Best : Accuracy=49.62, Error=50.38]
  Epoch: [021][000/500]   Time 18.084 (18.084)   Data 17.774 (17.774)   Loss 2.0707 (2.0707)   Prec@1 40.000 (40.000)   Prec@5 90.000 (90.000)   [2025-10-28 20:15:39]
  Epoch: [021][100/500]   Time 0.106 (0.276)   Data 0.000 (0.177)   Loss 1.9358 (1.9952)   Prec@1 52.000 (46.198)   Prec@5 95.000 (89.416)   [2025-10-28 20:15:48]
  Epoch: [021][200/500]   Time 0.100 (0.189)   Data 0.000 (0.089)   Loss 2.0175 (1.9914)   Prec@1 45.000 (46.741)   Prec@5 85.000 (89.393)   [2025-10-28 20:15:59]
  Epoch: [021][300/500]   Time 0.102 (0.161)   Data 0.000 (0.060)   Loss 2.0276 (1.9912)   Prec@1 43.000 (46.787)   Prec@5 90.000 (89.329)   [2025-10-28 20:16:09]
  Epoch: [021][400/500]   Time 0.099 (0.146)   Data 0.000 (0.045)   Loss 1.9781 (1.9889)   Prec@1 45.000 (46.985)   Prec@5 93.000 (89.202)   [2025-10-28 20:16:19]
  **Train** Prec@1 47.218 Prec@5 89.174 Error@1 52.782
  **Test** Prec@1 50.450 Prec@5 90.080 Error@1 49.550
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 20:16:49] [Epoch=022/040] [Need: 00:26:30] [LR=0.0100] [Best : Accuracy=50.45, Error=49.55]
  Epoch: [022][000/500]   Time 17.983 (17.983)   Data 17.592 (17.592)   Loss 1.9710 (1.9710)   Prec@1 48.000 (48.000)   Prec@5 92.000 (92.000)   [2025-10-28 20:17:07]
  Epoch: [022][100/500]   Time 0.102 (0.273)   Data 0.001 (0.175)   Loss 1.9523 (1.9853)   Prec@1 54.000 (47.436)   Prec@5 87.000 (89.119)   [2025-10-28 20:17:16]
  Epoch: [022][200/500]   Time 0.107 (0.188)   Data 0.000 (0.088)   Loss 1.9567 (1.9842)   Prec@1 49.000 (47.537)   Prec@5 96.000 (89.124)   [2025-10-28 20:17:26]
  Epoch: [022][300/500]   Time 0.103 (0.159)   Data 0.001 (0.059)   Loss 1.9332 (1.9826)   Prec@1 53.000 (47.844)   Prec@5 90.000 (89.223)   [2025-10-28 20:17:37]
  Epoch: [022][400/500]   Time 0.101 (0.145)   Data 0.001 (0.044)   Loss 2.0229 (1.9824)   Prec@1 43.000 (47.823)   Prec@5 90.000 (89.352)   [2025-10-28 20:17:47]
  **Train** Prec@1 47.806 Prec@5 89.374 Error@1 52.194
  **Test** Prec@1 47.070 Prec@5 86.930 Error@1 52.930

==>>[2025-10-28 20:18:17] [Epoch=023/040] [Need: 00:25:01] [LR=0.0100] [Best : Accuracy=50.45, Error=49.55]
  Epoch: [023][000/500]   Time 18.151 (18.151)   Data 17.767 (17.767)   Loss 1.9665 (1.9665)   Prec@1 50.000 (50.000)   Prec@5 88.000 (88.000)   [2025-10-28 20:18:35]
  Epoch: [023][100/500]   Time 0.105 (0.279)   Data 0.001 (0.176)   Loss 2.0773 (1.9835)   Prec@1 35.000 (47.525)   Prec@5 86.000 (88.931)   [2025-10-28 20:18:45]
  Epoch: [023][200/500]   Time 0.106 (0.191)   Data 0.001 (0.089)   Loss 2.0381 (1.9832)   Prec@1 43.000 (47.721)   Prec@5 85.000 (89.055)   [2025-10-28 20:18:56]
  Epoch: [023][300/500]   Time 0.099 (0.162)   Data 0.001 (0.060)   Loss 1.9406 (1.9803)   Prec@1 55.000 (48.027)   Prec@5 88.000 (89.422)   [2025-10-28 20:19:06]
  Epoch: [023][400/500]   Time 0.104 (0.147)   Data 0.000 (0.045)   Loss 1.8855 (1.9812)   Prec@1 59.000 (47.868)   Prec@5 92.000 (89.419)   [2025-10-28 20:19:16]
  **Train** Prec@1 47.994 Prec@5 89.472 Error@1 52.006
  **Test** Prec@1 42.680 Prec@5 87.760 Error@1 57.320

==>>[2025-10-28 20:19:45] [Epoch=024/040] [Need: 00:23:33] [LR=0.0100] [Best : Accuracy=50.45, Error=49.55]
  Epoch: [024][000/500]   Time 18.077 (18.077)   Data 17.831 (17.831)   Loss 1.9759 (1.9759)   Prec@1 46.000 (46.000)   Prec@5 87.000 (87.000)   [2025-10-28 20:20:03]
  Epoch: [024][100/500]   Time 0.111 (0.278)   Data 0.000 (0.177)   Loss 1.9527 (1.9710)   Prec@1 49.000 (48.911)   Prec@5 93.000 (90.178)   [2025-10-28 20:20:13]
  Epoch: [024][200/500]   Time 0.099 (0.191)   Data 0.001 (0.089)   Loss 1.9872 (1.9695)   Prec@1 47.000 (49.169)   Prec@5 90.000 (90.199)   [2025-10-28 20:20:24]
  Epoch: [024][300/500]   Time 0.103 (0.162)   Data 0.001 (0.060)   Loss 1.9475 (1.9700)   Prec@1 51.000 (49.020)   Prec@5 88.000 (89.990)   [2025-10-28 20:20:34]
  Epoch: [024][400/500]   Time 0.099 (0.147)   Data 0.001 (0.045)   Loss 1.9524 (1.9709)   Prec@1 51.000 (48.960)   Prec@5 88.000 (89.880)   [2025-10-28 20:20:44]
  **Train** Prec@1 48.914 Prec@5 89.858 Error@1 51.086
  **Test** Prec@1 51.700 Prec@5 90.830 Error@1 48.300
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 20:21:14] [Epoch=025/040] [Need: 00:22:05] [LR=0.0010] [Best : Accuracy=51.70, Error=48.30]
  Epoch: [025][000/500]   Time 18.217 (18.217)   Data 17.952 (17.952)   Loss 1.9695 (1.9695)   Prec@1 51.000 (51.000)   Prec@5 88.000 (88.000)   [2025-10-28 20:21:32]
  Epoch: [025][100/500]   Time 0.099 (0.277)   Data 0.001 (0.178)   Loss 2.0356 (1.9578)   Prec@1 42.000 (50.129)   Prec@5 84.000 (90.119)   [2025-10-28 20:21:42]
  Epoch: [025][200/500]   Time 0.105 (0.191)   Data 0.001 (0.090)   Loss 1.9900 (1.9561)   Prec@1 46.000 (50.279)   Prec@5 92.000 (90.463)   [2025-10-28 20:21:52]
  Epoch: [025][300/500]   Time 0.100 (0.162)   Data 0.001 (0.060)   Loss 1.9229 (1.9499)   Prec@1 55.000 (50.997)   Prec@5 91.000 (90.711)   [2025-10-28 20:22:02]
  Epoch: [025][400/500]   Time 0.103 (0.148)   Data 0.001 (0.045)   Loss 1.9376 (1.9487)   Prec@1 53.000 (51.132)   Prec@5 86.000 (90.771)   [2025-10-28 20:22:13]
  **Train** Prec@1 51.362 Prec@5 90.906 Error@1 48.638
  **Test** Prec@1 51.340 Prec@5 92.130 Error@1 48.660

==>>[2025-10-28 20:22:42] [Epoch=026/040] [Need: 00:20:36] [LR=0.0010] [Best : Accuracy=51.70, Error=48.30]
  Epoch: [026][000/500]   Time 17.902 (17.902)   Data 17.583 (17.583)   Loss 1.9200 (1.9200)   Prec@1 56.000 (56.000)   Prec@5 92.000 (92.000)   [2025-10-28 20:23:00]
  Epoch: [026][100/500]   Time 0.096 (0.272)   Data 0.001 (0.175)   Loss 1.9087 (1.9382)   Prec@1 55.000 (52.554)   Prec@5 97.000 (91.178)   [2025-10-28 20:23:10]
  Epoch: [026][200/500]   Time 0.099 (0.188)   Data 0.000 (0.088)   Loss 1.9741 (1.9378)   Prec@1 47.000 (52.622)   Prec@5 89.000 (91.204)   [2025-10-28 20:23:20]
  Epoch: [026][300/500]   Time 0.103 (0.159)   Data 0.001 (0.059)   Loss 1.9022 (1.9351)   Prec@1 58.000 (52.880)   Prec@5 93.000 (91.229)   [2025-10-28 20:23:30]
  Epoch: [026][400/500]   Time 0.099 (0.145)   Data 0.000 (0.044)   Loss 1.8891 (1.9348)   Prec@1 57.000 (52.860)   Prec@5 91.000 (91.162)   [2025-10-28 20:23:40]
  **Train** Prec@1 52.800 Prec@5 91.200 Error@1 47.200
  **Test** Prec@1 53.110 Prec@5 91.770 Error@1 46.890
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 20:24:10] [Epoch=027/040] [Need: 00:19:07] [LR=0.0010] [Best : Accuracy=53.11, Error=46.89]
  Epoch: [027][000/500]   Time 18.043 (18.043)   Data 17.723 (17.723)   Loss 1.8414 (1.8414)   Prec@1 64.000 (64.000)   Prec@5 91.000 (91.000)   [2025-10-28 20:24:28]
  Epoch: [027][100/500]   Time 0.109 (0.275)   Data 0.001 (0.176)   Loss 1.9278 (1.9313)   Prec@1 53.000 (53.089)   Prec@5 96.000 (91.733)   [2025-10-28 20:24:37]
  Epoch: [027][200/500]   Time 0.121 (0.190)   Data 0.001 (0.089)   Loss 1.9494 (1.9321)   Prec@1 51.000 (53.040)   Prec@5 92.000 (91.736)   [2025-10-28 20:24:48]
  Epoch: [027][300/500]   Time 0.109 (0.161)   Data 0.001 (0.059)   Loss 1.8426 (1.9312)   Prec@1 61.000 (53.213)   Prec@5 94.000 (91.422)   [2025-10-28 20:24:58]
  Epoch: [027][400/500]   Time 0.100 (0.146)   Data 0.001 (0.045)   Loss 1.8813 (1.9302)   Prec@1 59.000 (53.344)   Prec@5 97.000 (91.374)   [2025-10-28 20:25:08]
  **Train** Prec@1 53.198 Prec@5 91.428 Error@1 46.802
  **Test** Prec@1 48.350 Prec@5 85.770 Error@1 51.650

==>>[2025-10-28 20:25:38] [Epoch=028/040] [Need: 00:17:39] [LR=0.0010] [Best : Accuracy=53.11, Error=46.89]
  Epoch: [028][000/500]   Time 18.072 (18.072)   Data 17.752 (17.752)   Loss 1.9390 (1.9390)   Prec@1 53.000 (53.000)   Prec@5 88.000 (88.000)   [2025-10-28 20:25:56]
  Epoch: [028][100/500]   Time 0.107 (0.279)   Data 0.001 (0.176)   Loss 2.0262 (1.9253)   Prec@1 44.000 (53.733)   Prec@5 97.000 (91.485)   [2025-10-28 20:26:06]
  Epoch: [028][200/500]   Time 0.110 (0.192)   Data 0.001 (0.089)   Loss 1.9077 (1.9252)   Prec@1 56.000 (53.687)   Prec@5 91.000 (91.577)   [2025-10-28 20:26:17]
  Epoch: [028][300/500]   Time 0.100 (0.163)   Data 0.001 (0.060)   Loss 1.8973 (1.9262)   Prec@1 57.000 (53.601)   Prec@5 90.000 (91.475)   [2025-10-28 20:26:27]
  Epoch: [028][400/500]   Time 0.104 (0.148)   Data 0.000 (0.045)   Loss 1.9062 (1.9269)   Prec@1 57.000 (53.549)   Prec@5 96.000 (91.476)   [2025-10-28 20:26:37]
  **Train** Prec@1 53.466 Prec@5 91.384 Error@1 46.534
  **Test** Prec@1 54.230 Prec@5 90.680 Error@1 45.770
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 20:27:07] [Epoch=029/040] [Need: 00:16:11] [LR=0.0010] [Best : Accuracy=54.23, Error=45.77]
  Epoch: [029][000/500]   Time 18.000 (18.000)   Data 17.630 (17.630)   Loss 1.9428 (1.9428)   Prec@1 54.000 (54.000)   Prec@5 91.000 (91.000)   [2025-10-28 20:27:25]
  Epoch: [029][100/500]   Time 0.107 (0.275)   Data 0.001 (0.175)   Loss 1.9695 (1.9262)   Prec@1 48.000 (53.406)   Prec@5 90.000 (91.188)   [2025-10-28 20:27:35]
  Epoch: [029][200/500]   Time 0.105 (0.190)   Data 0.001 (0.088)   Loss 1.8883 (1.9271)   Prec@1 58.000 (53.463)   Prec@5 95.000 (91.423)   [2025-10-28 20:27:45]
  Epoch: [029][300/500]   Time 0.102 (0.161)   Data 0.001 (0.059)   Loss 1.8639 (1.9263)   Prec@1 59.000 (53.591)   Prec@5 92.000 (91.482)   [2025-10-28 20:27:55]
  Epoch: [029][400/500]   Time 0.100 (0.146)   Data 0.000 (0.045)   Loss 1.9505 (1.9261)   Prec@1 51.000 (53.581)   Prec@5 93.000 (91.556)   [2025-10-28 20:28:05]
  **Train** Prec@1 53.678 Prec@5 91.594 Error@1 46.322
  **Test** Prec@1 53.720 Prec@5 92.190 Error@1 46.280

==>>[2025-10-28 20:28:36] [Epoch=030/040] [Need: 00:14:43] [LR=0.0010] [Best : Accuracy=54.23, Error=45.77]
  Epoch: [030][000/500]   Time 18.577 (18.577)   Data 18.284 (18.284)   Loss 1.9579 (1.9579)   Prec@1 49.000 (49.000)   Prec@5 93.000 (93.000)   [2025-10-28 20:28:54]
  Epoch: [030][100/500]   Time 0.098 (0.282)   Data 0.001 (0.182)   Loss 1.8620 (1.9263)   Prec@1 60.000 (53.624)   Prec@5 95.000 (91.248)   [2025-10-28 20:29:04]
  Epoch: [030][200/500]   Time 0.103 (0.192)   Data 0.001 (0.092)   Loss 1.9589 (1.9245)   Prec@1 49.000 (53.751)   Prec@5 89.000 (91.323)   [2025-10-28 20:29:14]
  Epoch: [030][300/500]   Time 0.099 (0.163)   Data 0.000 (0.061)   Loss 1.8935 (1.9242)   Prec@1 58.000 (53.817)   Prec@5 89.000 (91.422)   [2025-10-28 20:29:25]
  Epoch: [030][400/500]   Time 0.110 (0.149)   Data 0.000 (0.046)   Loss 1.8753 (1.9233)   Prec@1 60.000 (53.945)   Prec@5 91.000 (91.509)   [2025-10-28 20:29:35]
  **Train** Prec@1 54.028 Prec@5 91.602 Error@1 45.972
  **Test** Prec@1 55.420 Prec@5 93.010 Error@1 44.580
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 20:30:06] [Epoch=031/040] [Need: 00:13:15] [LR=0.0010] [Best : Accuracy=55.42, Error=44.58]
  Epoch: [031][000/500]   Time 19.544 (19.544)   Data 19.182 (19.182)   Loss 1.9335 (1.9335)   Prec@1 55.000 (55.000)   Prec@5 90.000 (90.000)   [2025-10-28 20:30:26]
  Epoch: [031][100/500]   Time 0.104 (0.298)   Data 0.000 (0.191)   Loss 1.9412 (1.9189)   Prec@1 52.000 (54.436)   Prec@5 90.000 (91.594)   [2025-10-28 20:30:36]
  Epoch: [031][200/500]   Time 0.103 (0.202)   Data 0.001 (0.096)   Loss 1.8682 (1.9196)   Prec@1 60.000 (54.398)   Prec@5 95.000 (91.791)   [2025-10-28 20:30:47]
  Epoch: [031][300/500]   Time 0.103 (0.169)   Data 0.000 (0.064)   Loss 1.8972 (1.9204)   Prec@1 61.000 (54.369)   Prec@5 91.000 (91.555)   [2025-10-28 20:30:57]
  Epoch: [031][400/500]   Time 0.102 (0.153)   Data 0.001 (0.048)   Loss 1.9637 (1.9221)   Prec@1 51.000 (54.157)   Prec@5 91.000 (91.544)   [2025-10-28 20:31:08]
  **Train** Prec@1 54.236 Prec@5 91.612 Error@1 45.764
  **Test** Prec@1 55.430 Prec@5 92.860 Error@1 44.570
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 20:31:37] [Epoch=032/040] [Need: 00:11:47] [LR=0.0010] [Best : Accuracy=55.43, Error=44.57]
  Epoch: [032][000/500]   Time 18.464 (18.464)   Data 18.122 (18.122)   Loss 1.8753 (1.8753)   Prec@1 59.000 (59.000)   Prec@5 95.000 (95.000)   [2025-10-28 20:31:56]
  Epoch: [032][100/500]   Time 0.115 (0.284)   Data 0.000 (0.180)   Loss 1.9452 (1.9168)   Prec@1 50.000 (54.396)   Prec@5 92.000 (91.733)   [2025-10-28 20:32:06]
  Epoch: [032][200/500]   Time 0.103 (0.196)   Data 0.001 (0.091)   Loss 1.9207 (1.9188)   Prec@1 55.000 (54.209)   Prec@5 88.000 (91.851)   [2025-10-28 20:32:17]
  Epoch: [032][300/500]   Time 0.111 (0.168)   Data 0.001 (0.061)   Loss 1.9615 (1.9192)   Prec@1 50.000 (54.322)   Prec@5 90.000 (91.767)   [2025-10-28 20:32:28]
  Epoch: [032][400/500]   Time 0.111 (0.153)   Data 0.000 (0.046)   Loss 1.9327 (1.9207)   Prec@1 54.000 (54.195)   Prec@5 89.000 (91.671)   [2025-10-28 20:32:38]
  **Train** Prec@1 54.368 Prec@5 91.654 Error@1 45.632
  **Test** Prec@1 54.300 Prec@5 91.980 Error@1 45.700

==>>[2025-10-28 20:33:09] [Epoch=033/040] [Need: 00:10:20] [LR=0.0010] [Best : Accuracy=55.43, Error=44.57]
  Epoch: [033][000/500]   Time 18.022 (18.022)   Data 17.735 (17.735)   Loss 1.9123 (1.9123)   Prec@1 56.000 (56.000)   Prec@5 89.000 (89.000)   [2025-10-28 20:33:27]
  Epoch: [033][100/500]   Time 0.099 (0.276)   Data 0.001 (0.176)   Loss 1.9628 (1.9187)   Prec@1 49.000 (54.436)   Prec@5 90.000 (91.604)   [2025-10-28 20:33:36]
  Epoch: [033][200/500]   Time 0.108 (0.189)   Data 0.000 (0.089)   Loss 1.9851 (1.9191)   Prec@1 50.000 (54.463)   Prec@5 90.000 (91.473)   [2025-10-28 20:33:47]
  Epoch: [033][300/500]   Time 0.099 (0.161)   Data 0.000 (0.060)   Loss 1.9765 (1.9163)   Prec@1 49.000 (54.764)   Prec@5 91.000 (91.598)   [2025-10-28 20:33:57]
  Epoch: [033][400/500]   Time 0.100 (0.147)   Data 0.001 (0.045)   Loss 1.8967 (1.9167)   Prec@1 57.000 (54.658)   Prec@5 90.000 (91.579)   [2025-10-28 20:34:08]
  **Train** Prec@1 54.728 Prec@5 91.688 Error@1 45.272
  **Test** Prec@1 55.570 Prec@5 93.150 Error@1 44.430
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 20:34:38] [Epoch=034/040] [Need: 00:08:51] [LR=0.0010] [Best : Accuracy=55.57, Error=44.43]
  Epoch: [034][000/500]   Time 19.528 (19.528)   Data 19.138 (19.138)   Loss 1.8465 (1.8465)   Prec@1 64.000 (64.000)   Prec@5 92.000 (92.000)   [2025-10-28 20:34:57]
  Epoch: [034][100/500]   Time 0.121 (0.294)   Data 0.001 (0.190)   Loss 1.9115 (1.9211)   Prec@1 54.000 (53.980)   Prec@5 92.000 (91.267)   [2025-10-28 20:35:08]
  Epoch: [034][200/500]   Time 0.132 (0.210)   Data 0.001 (0.096)   Loss 1.8934 (1.9172)   Prec@1 57.000 (54.473)   Prec@5 91.000 (91.562)   [2025-10-28 20:35:20]
  Epoch: [034][300/500]   Time 0.107 (0.178)   Data 0.000 (0.064)   Loss 1.9658 (1.9210)   Prec@1 50.000 (54.123)   Prec@5 94.000 (91.482)   [2025-10-28 20:35:31]
  Epoch: [034][400/500]   Time 0.108 (0.160)   Data 0.000 (0.048)   Loss 1.8533 (1.9190)   Prec@1 61.000 (54.357)   Prec@5 94.000 (91.596)   [2025-10-28 20:35:42]
  **Train** Prec@1 54.412 Prec@5 91.584 Error@1 45.588
  **Test** Prec@1 55.260 Prec@5 92.730 Error@1 44.740

==>>[2025-10-28 20:36:13] [Epoch=035/040] [Need: 00:07:23] [LR=0.0010] [Best : Accuracy=55.57, Error=44.43]
  Epoch: [035][000/500]   Time 18.233 (18.233)   Data 17.930 (17.930)   Loss 1.9219 (1.9219)   Prec@1 55.000 (55.000)   Prec@5 88.000 (88.000)   [2025-10-28 20:36:31]
  Epoch: [035][100/500]   Time 0.099 (0.283)   Data 0.000 (0.178)   Loss 1.8606 (1.9117)   Prec@1 59.000 (55.109)   Prec@5 93.000 (91.663)   [2025-10-28 20:36:41]
  Epoch: [035][200/500]   Time 0.104 (0.194)   Data 0.001 (0.090)   Loss 1.9460 (1.9156)   Prec@1 49.000 (54.716)   Prec@5 87.000 (91.577)   [2025-10-28 20:36:52]
  Epoch: [035][300/500]   Time 0.110 (0.164)   Data 0.000 (0.060)   Loss 1.9295 (1.9150)   Prec@1 54.000 (54.791)   Prec@5 91.000 (91.528)   [2025-10-28 20:37:02]
  Epoch: [035][400/500]   Time 0.100 (0.149)   Data 0.000 (0.045)   Loss 1.9069 (1.9165)   Prec@1 56.000 (54.621)   Prec@5 94.000 (91.569)   [2025-10-28 20:37:13]
  **Train** Prec@1 54.736 Prec@5 91.644 Error@1 45.264
  **Test** Prec@1 55.420 Prec@5 92.630 Error@1 44.580

==>>[2025-10-28 20:37:43] [Epoch=036/040] [Need: 00:05:55] [LR=0.0010] [Best : Accuracy=55.57, Error=44.43]
  Epoch: [036][000/500]   Time 18.257 (18.257)   Data 17.994 (17.994)   Loss 1.8694 (1.8694)   Prec@1 59.000 (59.000)   Prec@5 91.000 (91.000)   [2025-10-28 20:38:01]
  Epoch: [036][100/500]   Time 0.101 (0.279)   Data 0.000 (0.179)   Loss 1.8424 (1.9145)   Prec@1 63.000 (54.881)   Prec@5 96.000 (91.505)   [2025-10-28 20:38:11]
  Epoch: [036][200/500]   Time 0.101 (0.193)   Data 0.001 (0.090)   Loss 1.8867 (1.9180)   Prec@1 61.000 (54.612)   Prec@5 87.000 (91.502)   [2025-10-28 20:38:21]
  Epoch: [036][300/500]   Time 0.112 (0.164)   Data 0.000 (0.060)   Loss 1.8682 (1.9189)   Prec@1 59.000 (54.528)   Prec@5 92.000 (91.505)   [2025-10-28 20:38:32]
  Epoch: [036][400/500]   Time 0.111 (0.151)   Data 0.001 (0.046)   Loss 1.8932 (1.9172)   Prec@1 57.000 (54.633)   Prec@5 92.000 (91.633)   [2025-10-28 20:38:43]
  **Train** Prec@1 54.626 Prec@5 91.604 Error@1 45.374
  **Test** Prec@1 51.920 Prec@5 92.400 Error@1 48.080

==>>[2025-10-28 20:39:13] [Epoch=037/040] [Need: 00:04:26] [LR=0.0010] [Best : Accuracy=55.57, Error=44.43]
  Epoch: [037][000/500]   Time 18.292 (18.292)   Data 17.910 (17.910)   Loss 1.9509 (1.9509)   Prec@1 48.000 (48.000)   Prec@5 94.000 (94.000)   [2025-10-28 20:39:31]
  Epoch: [037][100/500]   Time 0.118 (0.282)   Data 0.001 (0.178)   Loss 1.8934 (1.9110)   Prec@1 58.000 (55.109)   Prec@5 95.000 (91.584)   [2025-10-28 20:39:42]
  Epoch: [037][200/500]   Time 0.101 (0.194)   Data 0.000 (0.090)   Loss 1.9300 (1.9097)   Prec@1 50.000 (55.343)   Prec@5 89.000 (91.920)   [2025-10-28 20:39:52]
  Epoch: [037][300/500]   Time 0.103 (0.164)   Data 0.000 (0.060)   Loss 1.9228 (1.9124)   Prec@1 57.000 (54.963)   Prec@5 87.000 (91.777)   [2025-10-28 20:40:02]
  Epoch: [037][400/500]   Time 0.101 (0.148)   Data 0.001 (0.045)   Loss 1.9267 (1.9136)   Prec@1 54.000 (54.813)   Prec@5 89.000 (91.711)   [2025-10-28 20:40:13]
  **Train** Prec@1 54.642 Prec@5 91.642 Error@1 45.358
  **Test** Prec@1 54.190 Prec@5 92.100 Error@1 45.810

==>>[2025-10-28 20:40:42] [Epoch=038/040] [Need: 00:02:57] [LR=0.0010] [Best : Accuracy=55.57, Error=44.43]
  Epoch: [038][000/500]   Time 18.066 (18.066)   Data 17.738 (17.738)   Loss 1.9688 (1.9688)   Prec@1 49.000 (49.000)   Prec@5 97.000 (97.000)   [2025-10-28 20:41:00]
  Epoch: [038][100/500]   Time 0.097 (0.276)   Data 0.000 (0.176)   Loss 2.0020 (1.9082)   Prec@1 45.000 (55.545)   Prec@5 88.000 (92.218)   [2025-10-28 20:41:10]
  Epoch: [038][200/500]   Time 0.103 (0.190)   Data 0.000 (0.089)   Loss 1.9177 (1.9116)   Prec@1 53.000 (55.010)   Prec@5 91.000 (92.065)   [2025-10-28 20:41:20]
  Epoch: [038][300/500]   Time 0.104 (0.161)   Data 0.000 (0.060)   Loss 1.8920 (1.9101)   Prec@1 57.000 (55.189)   Prec@5 97.000 (92.073)   [2025-10-28 20:41:31]
  Epoch: [038][400/500]   Time 0.107 (0.147)   Data 0.000 (0.045)   Loss 1.9229 (1.9095)   Prec@1 54.000 (55.297)   Prec@5 91.000 (91.963)   [2025-10-28 20:41:41]
  **Train** Prec@1 55.258 Prec@5 91.914 Error@1 44.742
  **Test** Prec@1 53.900 Prec@5 92.540 Error@1 46.100

==>>[2025-10-28 20:42:10] [Epoch=039/040] [Need: 00:01:28] [LR=0.0010] [Best : Accuracy=55.57, Error=44.43]
  Epoch: [039][000/500]   Time 17.961 (17.961)   Data 17.659 (17.659)   Loss 1.8562 (1.8562)   Prec@1 61.000 (61.000)   Prec@5 93.000 (93.000)   [2025-10-28 20:42:28]
  Epoch: [039][100/500]   Time 0.101 (0.275)   Data 0.001 (0.176)   Loss 1.9661 (1.9100)   Prec@1 50.000 (55.455)   Prec@5 89.000 (91.941)   [2025-10-28 20:42:38]
  Epoch: [039][200/500]   Time 0.103 (0.190)   Data 0.001 (0.089)   Loss 1.8589 (1.9064)   Prec@1 59.000 (55.846)   Prec@5 94.000 (92.060)   [2025-10-28 20:42:49]
  Epoch: [039][300/500]   Time 0.103 (0.161)   Data 0.001 (0.059)   Loss 1.8794 (1.9095)   Prec@1 57.000 (55.432)   Prec@5 93.000 (91.924)   [2025-10-28 20:42:59]
  Epoch: [039][400/500]   Time 0.109 (0.147)   Data 0.000 (0.045)   Loss 1.9257 (1.9100)   Prec@1 56.000 (55.342)   Prec@5 94.000 (91.993)   [2025-10-28 20:43:09]
  **Train** Prec@1 55.272 Prec@5 91.884 Error@1 44.728
  **Test** Prec@1 56.740 Prec@5 93.630 Error@1 43.260
=> Obtain best accuracy, and update the best model
