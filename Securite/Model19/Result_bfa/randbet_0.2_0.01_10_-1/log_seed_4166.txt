save path : ./save/resnet9_quan/randbet_0.2_0.01_10_-1
{'data_path': './dataset', 'arch': 'resnet9_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': True, 'clipping_coeff': 0.2, 'learning_rate': 0.01, 'manualSeed': 4166, 'save_path': './save/resnet9_quan/randbet_0.2_0.01_10_-1', 'enable_bfa': False, 'resume': '', 'quan_bitwidth': None, 'reset_weight': False, 'evaluate': False, 'n_iter': 30, 'model_only': False, 'random_bfa': False, 'use_cuda': True}
Random Seed: 4166
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'resnet9_quan'
=> network :
 Sequential(
  (0): Sequential(
    (0): quan_Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (1): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (2): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (4): Dropout2d(p=0.2, inplace=False)
  (5): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (skip): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (6): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Dropout2d(p=0.3, inplace=False)
  (9): Sequential(
    (0): quan_Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (10): Sequential(
    (0): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (12): Dropout2d(p=0.3, inplace=False)
  (13): Sequential(
    (0): quan_Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (14): AdaptiveAvgPool2d(output_size=(1, 1))
  (15): Flatten()
  (16): quan_Linear(in_features=64, out_features=128, bias=False)
  (17): ReLU(inplace=True)
  (18): Dropout(p=0.35, inplace=False)
  (19): quan_Linear(in_features=128, out_features=10, bias=False)
  (20): Softmax(dim=1)
)
=> do not use any checkpoint for resnet9_quan model

==>>[2025-10-28 18:34:37] [Epoch=000/040] [Need: 00:00:00] [LR=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/500]   Time 30.306 (30.306)   Data 28.087 (28.087)   Loss 2.3021 (2.3021)   Prec@1 5.000 (5.000)   Prec@5 50.000 (50.000)   [2025-10-28 18:35:08]
  Epoch: [000][100/500]   Time 0.561 (1.010)   Data 0.001 (0.284)   Loss 2.3023 (2.3022)   Prec@1 8.000 (10.089)   Prec@5 54.000 (49.931)   [2025-10-28 18:36:19]
  Epoch: [000][200/500]   Time 0.883 (1.050)   Data 0.006 (0.146)   Loss 2.3008 (2.3017)   Prec@1 10.000 (10.299)   Prec@5 46.000 (50.930)   [2025-10-28 18:38:08]
  Epoch: [000][300/500]   Time 0.494 (0.971)   Data 0.005 (0.099)   Loss 2.2840 (2.3008)   Prec@1 14.000 (10.432)   Prec@5 50.000 (51.591)   [2025-10-28 18:39:30]
  Epoch: [000][400/500]   Time 0.861 (0.982)   Data 0.004 (0.076)   Loss 2.2905 (2.2984)   Prec@1 7.000 (10.823)   Prec@5 53.000 (51.885)   [2025-10-28 18:41:11]
  **Train** Prec@1 11.068 Prec@5 52.432 Error@1 88.932
  **Test** Prec@1 11.700 Prec@5 52.290 Error@1 88.300
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 18:43:32] [Epoch=001/040] [Need: 05:47:26] [LR=0.0100] [Best : Accuracy=11.70, Error=88.30]
  Epoch: [001][000/500]   Time 30.005 (30.005)   Data 29.603 (29.603)   Loss 2.2924 (2.2924)   Prec@1 6.000 (6.000)   Prec@5 60.000 (60.000)   [2025-10-28 18:44:02]
  Epoch: [001][100/500]   Time 0.126 (0.436)   Data 0.001 (0.294)   Loss 2.2742 (2.2754)   Prec@1 14.000 (14.030)   Prec@5 65.000 (60.089)   [2025-10-28 18:44:16]
  Epoch: [001][200/500]   Time 0.126 (0.286)   Data 0.001 (0.148)   Loss 2.2885 (2.2737)   Prec@1 15.000 (14.736)   Prec@5 62.000 (62.184)   [2025-10-28 18:44:29]
  Epoch: [001][300/500]   Time 0.121 (0.238)   Data 0.000 (0.099)   Loss 2.2744 (2.2713)   Prec@1 15.000 (15.017)   Prec@5 64.000 (64.066)   [2025-10-28 18:44:44]
  Epoch: [001][400/500]   Time 0.171 (0.214)   Data 0.001 (0.075)   Loss 2.2545 (2.2680)   Prec@1 13.000 (15.559)   Prec@5 70.000 (65.823)   [2025-10-28 18:44:58]
  **Train** Prec@1 15.910 Prec@5 67.052 Error@1 84.090
  **Test** Prec@1 10.710 Prec@5 50.980 Error@1 89.290

==>>[2025-10-28 18:45:36] [Epoch=002/040] [Need: 03:28:28] [LR=0.0100] [Best : Accuracy=11.70, Error=88.30]
  Epoch: [002][000/500]   Time 23.782 (23.782)   Data 23.472 (23.472)   Loss 2.2393 (2.2393)   Prec@1 21.000 (21.000)   Prec@5 76.000 (76.000)   [2025-10-28 18:46:00]
  Epoch: [002][100/500]   Time 0.128 (0.357)   Data 0.001 (0.233)   Loss 2.2387 (2.2536)   Prec@1 20.000 (17.535)   Prec@5 77.000 (73.178)   [2025-10-28 18:46:12]
  Epoch: [002][200/500]   Time 0.120 (0.240)   Data 0.001 (0.118)   Loss 2.2269 (2.2469)   Prec@1 18.000 (18.234)   Prec@5 69.000 (73.716)   [2025-10-28 18:46:24]
  Epoch: [002][300/500]   Time 0.111 (0.200)   Data 0.001 (0.079)   Loss 2.2201 (2.2442)   Prec@1 19.000 (18.233)   Prec@5 77.000 (73.807)   [2025-10-28 18:46:36]
  Epoch: [002][400/500]   Time 0.133 (0.179)   Data 0.001 (0.059)   Loss 2.2635 (2.2426)   Prec@1 12.000 (18.110)   Prec@5 72.000 (73.950)   [2025-10-28 18:46:48]
  **Train** Prec@1 18.176 Prec@5 74.238 Error@1 81.824
  **Test** Prec@1 10.720 Prec@5 51.100 Error@1 89.280

==>>[2025-10-28 18:47:25] [Epoch=003/040] [Need: 02:37:50] [LR=0.0100] [Best : Accuracy=11.70, Error=88.30]
  Epoch: [003][000/500]   Time 21.947 (21.947)   Data 21.679 (21.679)   Loss 2.2140 (2.2140)   Prec@1 17.000 (17.000)   Prec@5 78.000 (78.000)   [2025-10-28 18:47:47]
  Epoch: [003][100/500]   Time 0.109 (0.332)   Data 0.001 (0.216)   Loss 2.2128 (2.2274)   Prec@1 18.000 (18.832)   Prec@5 79.000 (75.980)   [2025-10-28 18:47:59]
  Epoch: [003][200/500]   Time 0.114 (0.224)   Data 0.002 (0.109)   Loss 2.2205 (2.2260)   Prec@1 20.000 (18.915)   Prec@5 80.000 (76.363)   [2025-10-28 18:48:10]
  Epoch: [003][300/500]   Time 0.113 (0.187)   Data 0.002 (0.073)   Loss 2.2016 (2.2245)   Prec@1 17.000 (19.346)   Prec@5 89.000 (76.558)   [2025-10-28 18:48:22]
  Epoch: [003][400/500]   Time 0.112 (0.169)   Data 0.000 (0.055)   Loss 2.2278 (2.2218)   Prec@1 17.000 (20.007)   Prec@5 77.000 (76.818)   [2025-10-28 18:48:33]
  **Train** Prec@1 20.702 Prec@5 76.978 Error@1 79.298
  **Test** Prec@1 19.550 Prec@5 71.390 Error@1 80.450
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 18:49:08] [Epoch=004/040] [Need: 02:10:33] [LR=0.0100] [Best : Accuracy=19.55, Error=80.45]
  Epoch: [004][000/500]   Time 21.486 (21.486)   Data 21.303 (21.303)   Loss 2.2147 (2.2147)   Prec@1 24.000 (24.000)   Prec@5 74.000 (74.000)   [2025-10-28 18:49:29]
  Epoch: [004][100/500]   Time 0.122 (0.327)   Data 0.001 (0.212)   Loss 2.1915 (2.1947)   Prec@1 24.000 (23.901)   Prec@5 79.000 (77.426)   [2025-10-28 18:49:41]
  Epoch: [004][200/500]   Time 0.114 (0.220)   Data 0.001 (0.107)   Loss 2.2095 (2.1914)   Prec@1 19.000 (24.169)   Prec@5 81.000 (77.617)   [2025-10-28 18:49:52]
  Epoch: [004][300/500]   Time 0.109 (0.185)   Data 0.000 (0.072)   Loss 2.1674 (2.1893)   Prec@1 31.000 (24.312)   Prec@5 76.000 (78.030)   [2025-10-28 18:50:03]
  Epoch: [004][400/500]   Time 0.110 (0.167)   Data 0.003 (0.054)   Loss 2.2244 (2.1868)   Prec@1 19.000 (24.569)   Prec@5 74.000 (78.259)   [2025-10-28 18:50:15]
  **Train** Prec@1 24.606 Prec@5 78.350 Error@1 75.394
  **Test** Prec@1 23.490 Prec@5 75.930 Error@1 76.510
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 18:50:49] [Epoch=005/040] [Need: 01:53:22] [LR=0.0100] [Best : Accuracy=23.49, Error=76.51]
  Epoch: [005][000/500]   Time 21.559 (21.559)   Data 21.364 (21.364)   Loss 2.1980 (2.1980)   Prec@1 20.000 (20.000)   Prec@5 82.000 (82.000)   [2025-10-28 18:51:11]
  Epoch: [005][100/500]   Time 0.102 (0.324)   Data 0.001 (0.212)   Loss 2.1847 (2.1753)   Prec@1 25.000 (25.990)   Prec@5 83.000 (79.752)   [2025-10-28 18:51:22]
  Epoch: [005][200/500]   Time 0.106 (0.220)   Data 0.001 (0.107)   Loss 2.1759 (2.1759)   Prec@1 25.000 (25.761)   Prec@5 86.000 (79.557)   [2025-10-28 18:51:33]
  Epoch: [005][300/500]   Time 0.111 (0.185)   Data 0.000 (0.072)   Loss 2.1991 (2.1764)   Prec@1 19.000 (25.694)   Prec@5 80.000 (79.542)   [2025-10-28 18:51:45]
  Epoch: [005][400/500]   Time 0.117 (0.167)   Data 0.002 (0.054)   Loss 2.1899 (2.1743)   Prec@1 25.000 (25.988)   Prec@5 82.000 (79.733)   [2025-10-28 18:51:56]
  **Train** Prec@1 26.034 Prec@5 79.954 Error@1 73.966
  **Test** Prec@1 26.150 Prec@5 77.790 Error@1 73.850
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 18:52:31] [Epoch=006/040] [Need: 01:41:23] [LR=0.0100] [Best : Accuracy=26.15, Error=73.85]
  Epoch: [006][000/500]   Time 21.499 (21.499)   Data 21.317 (21.317)   Loss 2.1662 (2.1662)   Prec@1 25.000 (25.000)   Prec@5 78.000 (78.000)   [2025-10-28 18:52:53]
  Epoch: [006][100/500]   Time 0.119 (0.327)   Data 0.001 (0.212)   Loss 2.1543 (2.1634)   Prec@1 29.000 (27.416)   Prec@5 78.000 (80.525)   [2025-10-28 18:53:04]
  Epoch: [006][200/500]   Time 0.117 (0.221)   Data 0.001 (0.107)   Loss 2.1937 (2.1638)   Prec@1 21.000 (27.413)   Prec@5 80.000 (80.517)   [2025-10-28 18:53:15]
  Epoch: [006][300/500]   Time 0.120 (0.186)   Data 0.001 (0.072)   Loss 2.1445 (2.1628)   Prec@1 29.000 (27.615)   Prec@5 86.000 (80.794)   [2025-10-28 18:53:27]
  Epoch: [006][400/500]   Time 0.117 (0.169)   Data 0.000 (0.054)   Loss 2.1738 (2.1599)   Prec@1 29.000 (28.007)   Prec@5 82.000 (81.142)   [2025-10-28 18:53:39]
  **Train** Prec@1 28.270 Prec@5 81.342 Error@1 71.730
  **Test** Prec@1 29.360 Prec@5 79.140 Error@1 70.640
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 18:54:14] [Epoch=007/040] [Need: 01:32:26] [LR=0.0100] [Best : Accuracy=29.36, Error=70.64]
  Epoch: [007][000/500]   Time 21.588 (21.588)   Data 21.451 (21.451)   Loss 2.1655 (2.1655)   Prec@1 24.000 (24.000)   Prec@5 84.000 (84.000)   [2025-10-28 18:54:36]
  Epoch: [007][100/500]   Time 0.107 (0.328)   Data 0.001 (0.213)   Loss 2.1099 (2.1499)   Prec@1 28.000 (29.416)   Prec@5 86.000 (82.119)   [2025-10-28 18:54:47]
  Epoch: [007][200/500]   Time 0.113 (0.220)   Data 0.002 (0.108)   Loss 2.1973 (2.1492)   Prec@1 26.000 (29.413)   Prec@5 78.000 (82.224)   [2025-10-28 18:54:58]
  Epoch: [007][300/500]   Time 0.127 (0.187)   Data 0.001 (0.072)   Loss 2.1380 (2.1443)   Prec@1 29.000 (30.100)   Prec@5 79.000 (82.475)   [2025-10-28 18:55:10]
  Epoch: [007][400/500]   Time 0.115 (0.170)   Data 0.004 (0.054)   Loss 2.1204 (2.1412)   Prec@1 34.000 (30.556)   Prec@5 82.000 (82.589)   [2025-10-28 18:55:22]
  **Train** Prec@1 30.752 Prec@5 82.670 Error@1 69.248
  **Test** Prec@1 28.840 Prec@5 80.350 Error@1 71.160

==>>[2025-10-28 18:56:19] [Epoch=008/040] [Need: 01:26:44] [LR=0.0100] [Best : Accuracy=29.36, Error=70.64]
  Epoch: [008][000/500]   Time 21.200 (21.200)   Data 21.041 (21.041)   Loss 2.1135 (2.1135)   Prec@1 33.000 (33.000)   Prec@5 82.000 (82.000)   [2025-10-28 18:56:40]
  Epoch: [008][100/500]   Time 0.107 (0.320)   Data 0.002 (0.209)   Loss 2.1420 (2.1280)   Prec@1 31.000 (32.277)   Prec@5 85.000 (82.693)   [2025-10-28 18:56:51]
  Epoch: [008][200/500]   Time 0.107 (0.217)   Data 0.001 (0.106)   Loss 2.1034 (2.1254)   Prec@1 34.000 (32.856)   Prec@5 90.000 (83.254)   [2025-10-28 18:57:02]
  Epoch: [008][300/500]   Time 0.106 (0.182)   Data 0.000 (0.071)   Loss 2.1076 (2.1225)   Prec@1 33.000 (33.332)   Prec@5 85.000 (83.326)   [2025-10-28 18:57:13]
  Epoch: [008][400/500]   Time 0.115 (0.165)   Data 0.002 (0.053)   Loss 2.1311 (2.1204)   Prec@1 31.000 (33.608)   Prec@5 84.000 (83.307)   [2025-10-28 18:57:25]
  **Train** Prec@1 33.834 Prec@5 83.188 Error@1 66.166
  **Test** Prec@1 36.000 Prec@5 83.350 Error@1 64.000
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 18:57:59] [Epoch=009/040] [Need: 01:20:26] [LR=0.0100] [Best : Accuracy=36.00, Error=64.00]
  Epoch: [009][000/500]   Time 21.147 (21.147)   Data 20.848 (20.848)   Loss 2.0445 (2.0445)   Prec@1 42.000 (42.000)   Prec@5 82.000 (82.000)   [2025-10-28 18:58:20]
  Epoch: [009][100/500]   Time 0.117 (0.319)   Data 0.001 (0.207)   Loss 2.1056 (2.1037)   Prec@1 34.000 (35.634)   Prec@5 84.000 (82.921)   [2025-10-28 18:58:31]
  Epoch: [009][200/500]   Time 0.112 (0.215)   Data 0.001 (0.105)   Loss 2.1537 (2.1035)   Prec@1 30.000 (35.493)   Prec@5 80.000 (82.905)   [2025-10-28 18:58:42]
  Epoch: [009][300/500]   Time 0.109 (0.181)   Data 0.001 (0.070)   Loss 2.1356 (2.0996)   Prec@1 34.000 (36.096)   Prec@5 84.000 (83.063)   [2025-10-28 18:58:53]
  Epoch: [009][400/500]   Time 0.111 (0.163)   Data 0.000 (0.053)   Loss 2.0677 (2.0990)   Prec@1 41.000 (36.055)   Prec@5 84.000 (83.132)   [2025-10-28 18:59:04]
  **Train** Prec@1 36.304 Prec@5 83.242 Error@1 63.696
  **Test** Prec@1 33.610 Prec@5 80.800 Error@1 66.390

==>>[2025-10-28 18:59:38] [Epoch=010/040] [Need: 01:15:02] [LR=0.0100] [Best : Accuracy=36.00, Error=64.00]
  Epoch: [010][000/500]   Time 21.547 (21.547)   Data 21.396 (21.396)   Loss 2.0527 (2.0527)   Prec@1 40.000 (40.000)   Prec@5 88.000 (88.000)   [2025-10-28 19:00:00]
  Epoch: [010][100/500]   Time 0.117 (0.323)   Data 0.001 (0.213)   Loss 2.1347 (2.0858)   Prec@1 28.000 (37.099)   Prec@5 91.000 (84.248)   [2025-10-28 19:00:11]
  Epoch: [010][200/500]   Time 0.113 (0.218)   Data 0.000 (0.107)   Loss 2.1255 (2.0861)   Prec@1 31.000 (37.070)   Prec@5 82.000 (84.090)   [2025-10-28 19:00:22]
  Epoch: [010][300/500]   Time 0.112 (0.183)   Data 0.001 (0.072)   Loss 2.0548 (2.0844)   Prec@1 42.000 (37.219)   Prec@5 87.000 (84.126)   [2025-10-28 19:00:33]
  Epoch: [010][400/500]   Time 0.107 (0.165)   Data 0.000 (0.054)   Loss 2.0467 (2.0839)   Prec@1 42.000 (37.279)   Prec@5 84.000 (84.406)   [2025-10-28 19:00:45]
  **Train** Prec@1 37.536 Prec@5 84.518 Error@1 62.464
  **Test** Prec@1 40.280 Prec@5 86.440 Error@1 59.720
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 19:01:18] [Epoch=011/040] [Need: 01:10:20] [LR=0.0100] [Best : Accuracy=40.28, Error=59.72]
  Epoch: [011][000/500]   Time 21.153 (21.153)   Data 21.009 (21.009)   Loss 2.0585 (2.0585)   Prec@1 41.000 (41.000)   Prec@5 85.000 (85.000)   [2025-10-28 19:01:39]
  Epoch: [011][100/500]   Time 0.111 (0.321)   Data 0.000 (0.209)   Loss 2.0425 (2.0676)   Prec@1 42.000 (39.040)   Prec@5 89.000 (85.287)   [2025-10-28 19:01:51]
  Epoch: [011][200/500]   Time 0.108 (0.216)   Data 0.001 (0.105)   Loss 2.0699 (2.0653)   Prec@1 38.000 (39.378)   Prec@5 85.000 (85.408)   [2025-10-28 19:02:02]
  Epoch: [011][300/500]   Time 0.110 (0.183)   Data 0.001 (0.071)   Loss 2.1521 (2.0666)   Prec@1 29.000 (39.150)   Prec@5 76.000 (85.615)   [2025-10-28 19:02:13]
  Epoch: [011][400/500]   Time 0.110 (0.166)   Data 0.000 (0.053)   Loss 2.0050 (2.0649)   Prec@1 45.000 (39.374)   Prec@5 88.000 (85.756)   [2025-10-28 19:02:25]
  **Train** Prec@1 39.558 Prec@5 85.946 Error@1 60.442
  **Test** Prec@1 43.720 Prec@5 86.820 Error@1 56.280
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 19:02:55] [Epoch=012/040] [Need: 01:06:01] [LR=0.0100] [Best : Accuracy=43.72, Error=56.28]
  Epoch: [012][000/500]   Time 18.102 (18.102)   Data 17.719 (17.719)   Loss 2.0187 (2.0187)   Prec@1 46.000 (46.000)   Prec@5 88.000 (88.000)   [2025-10-28 19:03:13]
  Epoch: [012][100/500]   Time 0.105 (0.278)   Data 0.000 (0.176)   Loss 2.0438 (2.0515)   Prec@1 44.000 (40.208)   Prec@5 88.000 (86.723)   [2025-10-28 19:03:23]
  Epoch: [012][200/500]   Time 0.101 (0.191)   Data 0.001 (0.089)   Loss 2.0659 (2.0578)   Prec@1 41.000 (39.652)   Prec@5 92.000 (86.448)   [2025-10-28 19:03:34]
  Epoch: [012][300/500]   Time 0.105 (0.163)   Data 0.000 (0.059)   Loss 2.1334 (2.0539)   Prec@1 31.000 (40.156)   Prec@5 90.000 (86.661)   [2025-10-28 19:03:44]
  Epoch: [012][400/500]   Time 0.104 (0.150)   Data 0.001 (0.045)   Loss 2.0102 (2.0524)   Prec@1 46.000 (40.409)   Prec@5 84.000 (86.845)   [2025-10-28 19:03:55]
  **Train** Prec@1 40.746 Prec@5 86.950 Error@1 59.254
  **Test** Prec@1 40.240 Prec@5 86.580 Error@1 59.760

==>>[2025-10-28 19:04:25] [Epoch=013/040] [Need: 01:01:53] [LR=0.0100] [Best : Accuracy=43.72, Error=56.28]
  Epoch: [013][000/500]   Time 18.248 (18.248)   Data 17.864 (17.864)   Loss 2.0536 (2.0536)   Prec@1 37.000 (37.000)   Prec@5 90.000 (90.000)   [2025-10-28 19:04:43]
  Epoch: [013][100/500]   Time 0.099 (0.280)   Data 0.001 (0.178)   Loss 2.0243 (2.0428)   Prec@1 43.000 (41.267)   Prec@5 90.000 (87.535)   [2025-10-28 19:04:53]
  Epoch: [013][200/500]   Time 0.097 (0.191)   Data 0.001 (0.090)   Loss 2.0483 (2.0420)   Prec@1 42.000 (41.463)   Prec@5 89.000 (87.318)   [2025-10-28 19:05:04]
  Epoch: [013][300/500]   Time 0.100 (0.162)   Data 0.001 (0.060)   Loss 1.9498 (2.0385)   Prec@1 51.000 (41.834)   Prec@5 91.000 (87.239)   [2025-10-28 19:05:14]
  Epoch: [013][400/500]   Time 0.106 (0.148)   Data 0.001 (0.045)   Loss 2.0003 (2.0371)   Prec@1 45.000 (42.067)   Prec@5 81.000 (87.357)   [2025-10-28 19:05:24]
  **Train** Prec@1 42.098 Prec@5 87.400 Error@1 57.902
  **Test** Prec@1 35.480 Prec@5 84.240 Error@1 64.520

==>>[2025-10-28 19:05:54] [Epoch=014/040] [Need: 00:58:04] [LR=0.0100] [Best : Accuracy=43.72, Error=56.28]
  Epoch: [014][000/500]   Time 17.786 (17.786)   Data 17.427 (17.427)   Loss 2.0580 (2.0580)   Prec@1 39.000 (39.000)   Prec@5 85.000 (85.000)   [2025-10-28 19:06:11]
  Epoch: [014][100/500]   Time 0.103 (0.274)   Data 0.001 (0.173)   Loss 2.0816 (2.0386)   Prec@1 35.000 (41.683)   Prec@5 88.000 (87.772)   [2025-10-28 19:06:21]
  Epoch: [014][200/500]   Time 0.100 (0.189)   Data 0.000 (0.087)   Loss 2.0081 (2.0337)   Prec@1 45.000 (42.303)   Prec@5 88.000 (87.851)   [2025-10-28 19:06:32]
  Epoch: [014][300/500]   Time 0.106 (0.162)   Data 0.000 (0.059)   Loss 2.1091 (2.0318)   Prec@1 33.000 (42.558)   Prec@5 85.000 (88.073)   [2025-10-28 19:06:42]
  Epoch: [014][400/500]   Time 0.103 (0.148)   Data 0.001 (0.044)   Loss 2.0348 (2.0317)   Prec@1 40.000 (42.544)   Prec@5 86.000 (88.234)   [2025-10-28 19:06:53]
  **Train** Prec@1 42.930 Prec@5 88.250 Error@1 57.070
  **Test** Prec@1 42.270 Prec@5 88.730 Error@1 57.730

==>>[2025-10-28 19:07:22] [Epoch=015/040] [Need: 00:54:35] [LR=0.0100] [Best : Accuracy=43.72, Error=56.28]
  Epoch: [015][000/500]   Time 17.966 (17.966)   Data 17.586 (17.586)   Loss 2.0985 (2.0985)   Prec@1 36.000 (36.000)   Prec@5 81.000 (81.000)   [2025-10-28 19:07:40]
  Epoch: [015][100/500]   Time 0.104 (0.276)   Data 0.000 (0.175)   Loss 2.0493 (2.0211)   Prec@1 39.000 (43.386)   Prec@5 85.000 (88.752)   [2025-10-28 19:07:50]
  Epoch: [015][200/500]   Time 0.100 (0.191)   Data 0.001 (0.088)   Loss 2.0186 (2.0201)   Prec@1 46.000 (43.632)   Prec@5 88.000 (88.731)   [2025-10-28 19:08:01]
  Epoch: [015][300/500]   Time 0.099 (0.161)   Data 0.000 (0.059)   Loss 2.0605 (2.0221)   Prec@1 40.000 (43.455)   Prec@5 88.000 (88.910)   [2025-10-28 19:08:11]
  Epoch: [015][400/500]   Time 0.098 (0.147)   Data 0.001 (0.044)   Loss 1.9665 (2.0195)   Prec@1 50.000 (43.756)   Prec@5 94.000 (88.955)   [2025-10-28 19:08:21]
  **Train** Prec@1 44.034 Prec@5 88.984 Error@1 55.966
  **Test** Prec@1 46.560 Prec@5 90.970 Error@1 53.440
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 19:08:50] [Epoch=016/040] [Need: 00:51:19] [LR=0.0100] [Best : Accuracy=46.56, Error=53.44]
  Epoch: [016][000/500]   Time 17.871 (17.871)   Data 17.558 (17.558)   Loss 2.0002 (2.0002)   Prec@1 46.000 (46.000)   Prec@5 86.000 (86.000)   [2025-10-28 19:09:08]
  Epoch: [016][100/500]   Time 0.097 (0.276)   Data 0.000 (0.175)   Loss 2.0546 (2.0128)   Prec@1 40.000 (44.366)   Prec@5 90.000 (88.693)   [2025-10-28 19:09:18]
  Epoch: [016][200/500]   Time 0.105 (0.191)   Data 0.001 (0.088)   Loss 1.9618 (2.0135)   Prec@1 53.000 (44.294)   Prec@5 89.000 (88.891)   [2025-10-28 19:09:29]
  Epoch: [016][300/500]   Time 0.107 (0.162)   Data 0.000 (0.059)   Loss 2.0497 (2.0161)   Prec@1 39.000 (43.960)   Prec@5 89.000 (88.764)   [2025-10-28 19:09:39]
  Epoch: [016][400/500]   Time 0.123 (0.148)   Data 0.001 (0.044)   Loss 2.0653 (2.0161)   Prec@1 41.000 (43.970)   Prec@5 92.000 (88.828)   [2025-10-28 19:09:50]
  **Train** Prec@1 44.092 Prec@5 88.948 Error@1 55.908
  **Test** Prec@1 44.750 Prec@5 85.860 Error@1 55.250

==>>[2025-10-28 19:10:19] [Epoch=017/040] [Need: 00:48:17] [LR=0.0100] [Best : Accuracy=46.56, Error=53.44]
  Epoch: [017][000/500]   Time 17.830 (17.830)   Data 17.542 (17.542)   Loss 1.9604 (1.9604)   Prec@1 50.000 (50.000)   Prec@5 96.000 (96.000)   [2025-10-28 19:10:37]
  Epoch: [017][100/500]   Time 0.109 (0.277)   Data 0.001 (0.174)   Loss 2.0348 (2.0086)   Prec@1 42.000 (44.990)   Prec@5 90.000 (89.446)   [2025-10-28 19:10:47]
  Epoch: [017][200/500]   Time 0.102 (0.191)   Data 0.001 (0.088)   Loss 1.9796 (2.0065)   Prec@1 48.000 (45.279)   Prec@5 91.000 (89.189)   [2025-10-28 19:10:57]
  Epoch: [017][300/500]   Time 0.097 (0.162)   Data 0.000 (0.059)   Loss 1.9085 (2.0081)   Prec@1 58.000 (45.043)   Prec@5 91.000 (89.229)   [2025-10-28 19:11:08]
  Epoch: [017][400/500]   Time 0.106 (0.148)   Data 0.000 (0.044)   Loss 1.9711 (2.0093)   Prec@1 49.000 (44.843)   Prec@5 86.000 (89.112)   [2025-10-28 19:11:18]
  **Train** Prec@1 44.932 Prec@5 89.206 Error@1 55.068
  **Test** Prec@1 46.950 Prec@5 89.180 Error@1 53.050
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 19:11:49] [Epoch=018/040] [Need: 00:45:27] [LR=0.0100] [Best : Accuracy=46.95, Error=53.05]
  Epoch: [018][000/500]   Time 19.380 (19.380)   Data 18.992 (18.992)   Loss 1.9490 (1.9490)   Prec@1 49.000 (49.000)   Prec@5 90.000 (90.000)   [2025-10-28 19:12:09]
  Epoch: [018][100/500]   Time 0.103 (0.295)   Data 0.000 (0.189)   Loss 1.9916 (2.0007)   Prec@1 48.000 (45.960)   Prec@5 82.000 (89.634)   [2025-10-28 19:12:19]
  Epoch: [018][200/500]   Time 0.107 (0.199)   Data 0.001 (0.095)   Loss 2.0452 (2.0019)   Prec@1 39.000 (45.731)   Prec@5 88.000 (89.766)   [2025-10-28 19:12:29]
  Epoch: [018][300/500]   Time 0.097 (0.167)   Data 0.001 (0.064)   Loss 2.0367 (1.9986)   Prec@1 44.000 (46.066)   Prec@5 88.000 (89.970)   [2025-10-28 19:12:40]
  Epoch: [018][400/500]   Time 0.103 (0.151)   Data 0.001 (0.048)   Loss 1.9482 (1.9979)   Prec@1 51.000 (46.180)   Prec@5 91.000 (89.840)   [2025-10-28 19:12:50]
  **Train** Prec@1 45.868 Prec@5 89.624 Error@1 54.132
  **Test** Prec@1 44.230 Prec@5 85.460 Error@1 55.770

==>>[2025-10-28 19:13:20] [Epoch=019/040] [Need: 00:42:46] [LR=0.0100] [Best : Accuracy=46.95, Error=53.05]
  Epoch: [019][000/500]   Time 17.845 (17.845)   Data 17.494 (17.494)   Loss 1.9992 (1.9992)   Prec@1 44.000 (44.000)   Prec@5 92.000 (92.000)   [2025-10-28 19:13:38]
  Epoch: [019][100/500]   Time 0.097 (0.273)   Data 0.001 (0.174)   Loss 2.0575 (1.9955)   Prec@1 39.000 (46.327)   Prec@5 89.000 (90.050)   [2025-10-28 19:13:47]
  Epoch: [019][200/500]   Time 0.105 (0.189)   Data 0.000 (0.088)   Loss 2.0006 (1.9942)   Prec@1 43.000 (46.572)   Prec@5 85.000 (89.672)   [2025-10-28 19:13:58]
  Epoch: [019][300/500]   Time 0.108 (0.161)   Data 0.001 (0.059)   Loss 2.0176 (1.9960)   Prec@1 43.000 (46.365)   Prec@5 86.000 (89.678)   [2025-10-28 19:14:08]
  Epoch: [019][400/500]   Time 0.104 (0.147)   Data 0.000 (0.044)   Loss 2.0759 (1.9959)   Prec@1 37.000 (46.337)   Prec@5 82.000 (89.633)   [2025-10-28 19:14:19]
  **Train** Prec@1 46.586 Prec@5 89.794 Error@1 53.414
  **Test** Prec@1 46.910 Prec@5 90.610 Error@1 53.090

==>>[2025-10-28 19:14:48] [Epoch=020/040] [Need: 00:40:10] [LR=0.0100] [Best : Accuracy=46.95, Error=53.05]
  Epoch: [020][000/500]   Time 18.006 (18.006)   Data 17.707 (17.707)   Loss 2.0043 (2.0043)   Prec@1 45.000 (45.000)   Prec@5 90.000 (90.000)   [2025-10-28 19:15:06]
  Epoch: [020][100/500]   Time 0.101 (0.277)   Data 0.000 (0.176)   Loss 1.9830 (1.9974)   Prec@1 48.000 (46.099)   Prec@5 91.000 (89.317)   [2025-10-28 19:15:16]
  Epoch: [020][200/500]   Time 0.102 (0.191)   Data 0.000 (0.089)   Loss 1.9238 (1.9939)   Prec@1 55.000 (46.542)   Prec@5 88.000 (89.249)   [2025-10-28 19:15:27]
  Epoch: [020][300/500]   Time 0.101 (0.162)   Data 0.000 (0.059)   Loss 1.9629 (1.9928)   Prec@1 50.000 (46.701)   Prec@5 89.000 (89.555)   [2025-10-28 19:15:37]
  Epoch: [020][400/500]   Time 0.105 (0.147)   Data 0.000 (0.045)   Loss 1.9948 (1.9917)   Prec@1 46.000 (46.798)   Prec@5 92.000 (89.623)   [2025-10-28 19:15:47]
  **Train** Prec@1 46.894 Prec@5 89.624 Error@1 53.106
  **Test** Prec@1 44.150 Prec@5 85.970 Error@1 55.850

==>>[2025-10-28 19:16:17] [Epoch=021/040] [Need: 00:37:41] [LR=0.0100] [Best : Accuracy=46.95, Error=53.05]
  Epoch: [021][000/500]   Time 18.060 (18.060)   Data 17.770 (17.770)   Loss 1.9984 (1.9984)   Prec@1 47.000 (47.000)   Prec@5 87.000 (87.000)   [2025-10-28 19:16:35]
  Epoch: [021][100/500]   Time 0.099 (0.275)   Data 0.001 (0.177)   Loss 1.8960 (1.9832)   Prec@1 59.000 (47.842)   Prec@5 94.000 (90.594)   [2025-10-28 19:16:45]
  Epoch: [021][200/500]   Time 0.098 (0.190)   Data 0.000 (0.089)   Loss 2.0468 (1.9854)   Prec@1 41.000 (47.393)   Prec@5 84.000 (90.239)   [2025-10-28 19:16:55]
  Epoch: [021][300/500]   Time 0.106 (0.161)   Data 0.001 (0.060)   Loss 2.0141 (1.9845)   Prec@1 43.000 (47.498)   Prec@5 91.000 (90.216)   [2025-10-28 19:17:05]
  Epoch: [021][400/500]   Time 0.098 (0.147)   Data 0.001 (0.045)   Loss 2.0680 (1.9845)   Prec@1 38.000 (47.426)   Prec@5 88.000 (90.170)   [2025-10-28 19:17:16]
  **Train** Prec@1 47.336 Prec@5 90.176 Error@1 52.664
  **Test** Prec@1 38.320 Prec@5 82.790 Error@1 61.680

==>>[2025-10-28 19:17:45] [Epoch=022/040] [Need: 00:35:17] [LR=0.0100] [Best : Accuracy=46.95, Error=53.05]
  Epoch: [022][000/500]   Time 18.470 (18.470)   Data 18.196 (18.196)   Loss 1.9960 (1.9960)   Prec@1 47.000 (47.000)   Prec@5 88.000 (88.000)   [2025-10-28 19:18:03]
  Epoch: [022][100/500]   Time 0.102 (0.282)   Data 0.001 (0.181)   Loss 1.9532 (1.9882)   Prec@1 49.000 (46.772)   Prec@5 89.000 (89.436)   [2025-10-28 19:18:13]
  Epoch: [022][200/500]   Time 0.103 (0.193)   Data 0.001 (0.091)   Loss 1.9784 (1.9874)   Prec@1 50.000 (46.915)   Prec@5 89.000 (89.692)   [2025-10-28 19:18:24]
  Epoch: [022][300/500]   Time 0.104 (0.164)   Data 0.001 (0.061)   Loss 1.9568 (1.9856)   Prec@1 49.000 (47.206)   Prec@5 88.000 (89.691)   [2025-10-28 19:18:34]
  Epoch: [022][400/500]   Time 0.104 (0.149)   Data 0.001 (0.046)   Loss 1.9292 (1.9828)   Prec@1 56.000 (47.554)   Prec@5 86.000 (89.923)   [2025-10-28 19:18:45]
  **Train** Prec@1 47.638 Prec@5 90.018 Error@1 52.362
  **Test** Prec@1 43.890 Prec@5 86.580 Error@1 56.110

==>>[2025-10-28 19:19:14] [Epoch=023/040] [Need: 00:32:58] [LR=0.0100] [Best : Accuracy=46.95, Error=53.05]
  Epoch: [023][000/500]   Time 18.074 (18.074)   Data 17.687 (17.687)   Loss 1.9387 (1.9387)   Prec@1 55.000 (55.000)   Prec@5 92.000 (92.000)   [2025-10-28 19:19:32]
  Epoch: [023][100/500]   Time 0.099 (0.277)   Data 0.001 (0.176)   Loss 2.0564 (1.9732)   Prec@1 40.000 (48.832)   Prec@5 90.000 (90.069)   [2025-10-28 19:19:42]
  Epoch: [023][200/500]   Time 0.103 (0.191)   Data 0.000 (0.089)   Loss 1.9767 (1.9760)   Prec@1 46.000 (48.398)   Prec@5 89.000 (90.070)   [2025-10-28 19:19:52]
  Epoch: [023][300/500]   Time 0.106 (0.162)   Data 0.000 (0.059)   Loss 1.9142 (1.9753)   Prec@1 54.000 (48.452)   Prec@5 95.000 (90.159)   [2025-10-28 19:20:03]
  Epoch: [023][400/500]   Time 0.112 (0.148)   Data 0.000 (0.045)   Loss 1.9834 (1.9768)   Prec@1 48.000 (48.257)   Prec@5 88.000 (90.180)   [2025-10-28 19:20:13]
  **Train** Prec@1 48.206 Prec@5 90.126 Error@1 51.794
  **Test** Prec@1 46.390 Prec@5 86.780 Error@1 53.610

==>>[2025-10-28 19:20:42] [Epoch=024/040] [Need: 00:30:43] [LR=0.0100] [Best : Accuracy=46.95, Error=53.05]
  Epoch: [024][000/500]   Time 18.005 (18.005)   Data 17.626 (17.626)   Loss 2.0941 (2.0941)   Prec@1 35.000 (35.000)   Prec@5 86.000 (86.000)   [2025-10-28 19:21:00]
  Epoch: [024][100/500]   Time 0.100 (0.274)   Data 0.001 (0.175)   Loss 1.9724 (1.9735)   Prec@1 47.000 (48.396)   Prec@5 86.000 (89.891)   [2025-10-28 19:21:10]
  Epoch: [024][200/500]   Time 0.104 (0.189)   Data 0.000 (0.088)   Loss 1.9587 (1.9725)   Prec@1 52.000 (48.592)   Prec@5 87.000 (90.005)   [2025-10-28 19:21:20]
  Epoch: [024][300/500]   Time 0.110 (0.162)   Data 0.000 (0.059)   Loss 1.9517 (1.9730)   Prec@1 51.000 (48.631)   Prec@5 95.000 (90.140)   [2025-10-28 19:21:31]
  Epoch: [024][400/500]   Time 0.106 (0.148)   Data 0.001 (0.045)   Loss 1.9714 (1.9738)   Prec@1 51.000 (48.566)   Prec@5 92.000 (90.045)   [2025-10-28 19:21:42]
  **Train** Prec@1 48.780 Prec@5 90.146 Error@1 51.220
  **Test** Prec@1 42.300 Prec@5 82.630 Error@1 57.700

==>>[2025-10-28 19:22:11] [Epoch=025/040] [Need: 00:28:32] [LR=0.0010] [Best : Accuracy=46.95, Error=53.05]
  Epoch: [025][000/500]   Time 17.833 (17.833)   Data 17.453 (17.453)   Loss 2.0232 (2.0232)   Prec@1 43.000 (43.000)   Prec@5 82.000 (82.000)   [2025-10-28 19:22:29]
  Epoch: [025][100/500]   Time 0.107 (0.276)   Data 0.001 (0.173)   Loss 1.8872 (1.9649)   Prec@1 57.000 (49.347)   Prec@5 94.000 (90.050)   [2025-10-28 19:22:39]
  Epoch: [025][200/500]   Time 0.108 (0.190)   Data 0.001 (0.087)   Loss 1.9567 (1.9586)   Prec@1 52.000 (50.104)   Prec@5 86.000 (90.647)   [2025-10-28 19:22:49]
  Epoch: [025][300/500]   Time 0.102 (0.162)   Data 0.001 (0.059)   Loss 1.8883 (1.9551)   Prec@1 57.000 (50.561)   Prec@5 92.000 (90.930)   [2025-10-28 19:23:00]
  Epoch: [025][400/500]   Time 0.101 (0.147)   Data 0.001 (0.044)   Loss 1.9428 (1.9537)   Prec@1 52.000 (50.671)   Prec@5 88.000 (91.117)   [2025-10-28 19:23:10]
  **Train** Prec@1 50.820 Prec@5 91.232 Error@1 49.180
  **Test** Prec@1 51.510 Prec@5 89.930 Error@1 48.490
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 19:23:40] [Epoch=026/040] [Need: 00:26:24] [LR=0.0010] [Best : Accuracy=51.51, Error=48.49]
  Epoch: [026][000/500]   Time 18.037 (18.037)   Data 17.758 (17.758)   Loss 1.9490 (1.9490)   Prec@1 50.000 (50.000)   Prec@5 92.000 (92.000)   [2025-10-28 19:23:58]
  Epoch: [026][100/500]   Time 0.102 (0.275)   Data 0.001 (0.176)   Loss 1.9494 (1.9398)   Prec@1 50.000 (52.158)   Prec@5 92.000 (91.594)   [2025-10-28 19:24:07]
  Epoch: [026][200/500]   Time 0.102 (0.190)   Data 0.000 (0.089)   Loss 1.9717 (1.9410)   Prec@1 49.000 (52.085)   Prec@5 89.000 (91.547)   [2025-10-28 19:24:18]
  Epoch: [026][300/500]   Time 0.103 (0.161)   Data 0.000 (0.060)   Loss 1.8865 (1.9416)   Prec@1 60.000 (52.066)   Prec@5 94.000 (91.581)   [2025-10-28 19:24:28]
  Epoch: [026][400/500]   Time 0.116 (0.147)   Data 0.001 (0.045)   Loss 1.9040 (1.9419)   Prec@1 54.000 (52.027)   Prec@5 92.000 (91.534)   [2025-10-28 19:24:39]
  **Train** Prec@1 51.984 Prec@5 91.550 Error@1 48.016
  **Test** Prec@1 45.340 Prec@5 84.930 Error@1 54.660

==>>[2025-10-28 19:25:08] [Epoch=027/040] [Need: 00:24:19] [LR=0.0010] [Best : Accuracy=51.51, Error=48.49]
  Epoch: [027][000/500]   Time 18.012 (18.012)   Data 17.694 (17.694)   Loss 1.8846 (1.8846)   Prec@1 58.000 (58.000)   Prec@5 94.000 (94.000)   [2025-10-28 19:25:26]
  Epoch: [027][100/500]   Time 0.113 (0.277)   Data 0.001 (0.176)   Loss 2.0034 (1.9419)   Prec@1 45.000 (51.980)   Prec@5 84.000 (91.040)   [2025-10-28 19:25:36]
  Epoch: [027][200/500]   Time 0.099 (0.191)   Data 0.001 (0.089)   Loss 1.9862 (1.9417)   Prec@1 48.000 (52.025)   Prec@5 94.000 (91.234)   [2025-10-28 19:25:46]
  Epoch: [027][300/500]   Time 0.103 (0.163)   Data 0.001 (0.059)   Loss 1.9985 (1.9414)   Prec@1 48.000 (52.106)   Prec@5 89.000 (91.282)   [2025-10-28 19:25:57]
  Epoch: [027][400/500]   Time 0.099 (0.148)   Data 0.001 (0.045)   Loss 1.9302 (1.9414)   Prec@1 54.000 (52.082)   Prec@5 92.000 (91.511)   [2025-10-28 19:26:07]
  **Train** Prec@1 52.226 Prec@5 91.566 Error@1 47.774
  **Test** Prec@1 50.540 Prec@5 89.470 Error@1 49.460

==>>[2025-10-28 19:26:37] [Epoch=028/040] [Need: 00:22:16] [LR=0.0010] [Best : Accuracy=51.51, Error=48.49]
  Epoch: [028][000/500]   Time 17.936 (17.936)   Data 17.594 (17.594)   Loss 1.9774 (1.9774)   Prec@1 49.000 (49.000)   Prec@5 90.000 (90.000)   [2025-10-28 19:26:55]
  Epoch: [028][100/500]   Time 0.100 (0.276)   Data 0.001 (0.175)   Loss 2.0386 (1.9427)   Prec@1 40.000 (51.663)   Prec@5 87.000 (92.030)   [2025-10-28 19:27:05]
  Epoch: [028][200/500]   Time 0.104 (0.190)   Data 0.001 (0.088)   Loss 1.9301 (1.9391)   Prec@1 52.000 (52.169)   Prec@5 95.000 (91.741)   [2025-10-28 19:27:15]
  Epoch: [028][300/500]   Time 0.106 (0.161)   Data 0.001 (0.059)   Loss 1.9251 (1.9374)   Prec@1 54.000 (52.429)   Prec@5 90.000 (91.701)   [2025-10-28 19:27:25]
  Epoch: [028][400/500]   Time 0.109 (0.147)   Data 0.000 (0.044)   Loss 1.9186 (1.9375)   Prec@1 54.000 (52.434)   Prec@5 94.000 (91.581)   [2025-10-28 19:27:36]
  **Train** Prec@1 52.476 Prec@5 91.682 Error@1 47.524
  **Test** Prec@1 51.220 Prec@5 91.130 Error@1 48.780

==>>[2025-10-28 19:28:05] [Epoch=029/040] [Need: 00:20:16] [LR=0.0010] [Best : Accuracy=51.51, Error=48.49]
  Epoch: [029][000/500]   Time 18.067 (18.067)   Data 17.681 (17.681)   Loss 1.9464 (1.9464)   Prec@1 53.000 (53.000)   Prec@5 91.000 (91.000)   [2025-10-28 19:28:23]
  Epoch: [029][100/500]   Time 0.108 (0.278)   Data 0.001 (0.176)   Loss 1.9176 (1.9318)   Prec@1 55.000 (53.020)   Prec@5 95.000 (91.980)   [2025-10-28 19:28:33]
  Epoch: [029][200/500]   Time 0.098 (0.190)   Data 0.001 (0.089)   Loss 1.9426 (1.9335)   Prec@1 53.000 (52.771)   Prec@5 92.000 (91.985)   [2025-10-28 19:28:43]
  Epoch: [029][300/500]   Time 0.102 (0.162)   Data 0.001 (0.059)   Loss 1.9318 (1.9343)   Prec@1 52.000 (52.781)   Prec@5 91.000 (91.841)   [2025-10-28 19:28:54]
  Epoch: [029][400/500]   Time 0.103 (0.147)   Data 0.001 (0.045)   Loss 1.9427 (1.9367)   Prec@1 52.000 (52.534)   Prec@5 90.000 (91.796)   [2025-10-28 19:29:04]
  **Train** Prec@1 52.546 Prec@5 91.830 Error@1 47.454
  **Test** Prec@1 50.840 Prec@5 88.530 Error@1 49.160

==>>[2025-10-28 19:29:34] [Epoch=030/040] [Need: 00:18:18] [LR=0.0010] [Best : Accuracy=51.51, Error=48.49]
  Epoch: [030][000/500]   Time 18.095 (18.095)   Data 17.806 (17.806)   Loss 1.9257 (1.9257)   Prec@1 53.000 (53.000)   Prec@5 92.000 (92.000)   [2025-10-28 19:29:52]
  Epoch: [030][100/500]   Time 0.105 (0.279)   Data 0.001 (0.177)   Loss 1.9728 (1.9385)   Prec@1 47.000 (52.218)   Prec@5 98.000 (91.525)   [2025-10-28 19:30:02]
  Epoch: [030][200/500]   Time 0.118 (0.192)   Data 0.001 (0.089)   Loss 1.9213 (1.9371)   Prec@1 55.000 (52.448)   Prec@5 88.000 (91.692)   [2025-10-28 19:30:13]
  Epoch: [030][300/500]   Time 0.101 (0.163)   Data 0.001 (0.060)   Loss 1.9681 (1.9365)   Prec@1 47.000 (52.405)   Prec@5 88.000 (91.684)   [2025-10-28 19:30:23]
  Epoch: [030][400/500]   Time 0.110 (0.148)   Data 0.001 (0.045)   Loss 1.9722 (1.9355)   Prec@1 50.000 (52.531)   Prec@5 93.000 (91.686)   [2025-10-28 19:30:34]
  **Train** Prec@1 52.624 Prec@5 91.760 Error@1 47.376
  **Test** Prec@1 54.250 Prec@5 92.400 Error@1 45.750
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 19:31:03] [Epoch=031/040] [Need: 00:16:22] [LR=0.0010] [Best : Accuracy=54.25, Error=45.75]
  Epoch: [031][000/500]   Time 17.876 (17.876)   Data 17.524 (17.524)   Loss 1.9730 (1.9730)   Prec@1 47.000 (47.000)   Prec@5 88.000 (88.000)   [2025-10-28 19:31:21]
  Epoch: [031][100/500]   Time 0.107 (0.275)   Data 0.000 (0.174)   Loss 2.0222 (1.9361)   Prec@1 44.000 (52.455)   Prec@5 89.000 (91.663)   [2025-10-28 19:31:31]
  Epoch: [031][200/500]   Time 0.101 (0.190)   Data 0.000 (0.088)   Loss 1.9219 (1.9329)   Prec@1 55.000 (52.716)   Prec@5 90.000 (91.602)   [2025-10-28 19:31:41]
  Epoch: [031][300/500]   Time 0.100 (0.161)   Data 0.000 (0.059)   Loss 1.9082 (1.9330)   Prec@1 54.000 (52.817)   Prec@5 92.000 (91.638)   [2025-10-28 19:31:51]
  Epoch: [031][400/500]   Time 0.101 (0.146)   Data 0.000 (0.044)   Loss 1.9155 (1.9329)   Prec@1 53.000 (52.873)   Prec@5 96.000 (91.668)   [2025-10-28 19:32:01]
  **Train** Prec@1 53.026 Prec@5 91.742 Error@1 46.974
  **Test** Prec@1 51.950 Prec@5 89.770 Error@1 48.050

==>>[2025-10-28 19:32:31] [Epoch=032/040] [Need: 00:14:28] [LR=0.0010] [Best : Accuracy=54.25, Error=45.75]
  Epoch: [032][000/500]   Time 17.908 (17.908)   Data 17.555 (17.555)   Loss 1.9128 (1.9128)   Prec@1 53.000 (53.000)   Prec@5 93.000 (93.000)   [2025-10-28 19:32:49]
  Epoch: [032][100/500]   Time 0.107 (0.277)   Data 0.001 (0.174)   Loss 1.9033 (1.9300)   Prec@1 55.000 (53.257)   Prec@5 96.000 (91.723)   [2025-10-28 19:32:59]
  Epoch: [032][200/500]   Time 0.104 (0.190)   Data 0.000 (0.088)   Loss 1.9937 (1.9327)   Prec@1 49.000 (53.040)   Prec@5 92.000 (91.567)   [2025-10-28 19:33:09]
  Epoch: [032][300/500]   Time 0.105 (0.162)   Data 0.001 (0.059)   Loss 1.9420 (1.9331)   Prec@1 53.000 (52.944)   Prec@5 93.000 (91.731)   [2025-10-28 19:33:20]
  Epoch: [032][400/500]   Time 0.103 (0.147)   Data 0.000 (0.044)   Loss 1.8375 (1.9320)   Prec@1 61.000 (52.993)   Prec@5 91.000 (91.751)   [2025-10-28 19:33:30]
  **Train** Prec@1 53.152 Prec@5 91.878 Error@1 46.848
  **Test** Prec@1 51.330 Prec@5 89.700 Error@1 48.670

==>>[2025-10-28 19:33:59] [Epoch=033/040] [Need: 00:12:35] [LR=0.0010] [Best : Accuracy=54.25, Error=45.75]
  Epoch: [033][000/500]   Time 17.739 (17.739)   Data 17.422 (17.422)   Loss 1.9483 (1.9483)   Prec@1 50.000 (50.000)   Prec@5 93.000 (93.000)   [2025-10-28 19:34:17]
  Epoch: [033][100/500]   Time 0.104 (0.274)   Data 0.000 (0.173)   Loss 2.0040 (1.9261)   Prec@1 49.000 (53.495)   Prec@5 89.000 (91.980)   [2025-10-28 19:34:27]
  Epoch: [033][200/500]   Time 0.101 (0.190)   Data 0.001 (0.087)   Loss 1.8858 (1.9283)   Prec@1 59.000 (53.308)   Prec@5 97.000 (91.985)   [2025-10-28 19:34:37]
  Epoch: [033][300/500]   Time 0.103 (0.162)   Data 0.001 (0.059)   Loss 1.9131 (1.9289)   Prec@1 56.000 (53.209)   Prec@5 97.000 (92.103)   [2025-10-28 19:34:48]
  Epoch: [033][400/500]   Time 0.102 (0.147)   Data 0.001 (0.044)   Loss 2.0067 (1.9291)   Prec@1 45.000 (53.147)   Prec@5 85.000 (92.045)   [2025-10-28 19:34:58]
  **Train** Prec@1 53.098 Prec@5 91.924 Error@1 46.902
  **Test** Prec@1 49.350 Prec@5 87.830 Error@1 50.650

==>>[2025-10-28 19:35:28] [Epoch=034/040] [Need: 00:10:44] [LR=0.0010] [Best : Accuracy=54.25, Error=45.75]
  Epoch: [034][000/500]   Time 17.911 (17.911)   Data 17.568 (17.568)   Loss 1.8758 (1.8758)   Prec@1 61.000 (61.000)   Prec@5 94.000 (94.000)   [2025-10-28 19:35:46]
  Epoch: [034][100/500]   Time 0.103 (0.275)   Data 0.001 (0.174)   Loss 1.9355 (1.9258)   Prec@1 53.000 (53.594)   Prec@5 93.000 (91.891)   [2025-10-28 19:35:56]
  Epoch: [034][200/500]   Time 0.102 (0.190)   Data 0.001 (0.088)   Loss 1.8969 (1.9285)   Prec@1 59.000 (53.189)   Prec@5 91.000 (91.816)   [2025-10-28 19:36:06]
  Epoch: [034][300/500]   Time 0.105 (0.162)   Data 0.000 (0.059)   Loss 1.9546 (1.9263)   Prec@1 52.000 (53.462)   Prec@5 89.000 (91.867)   [2025-10-28 19:36:17]
  Epoch: [034][400/500]   Time 0.107 (0.147)   Data 0.000 (0.044)   Loss 1.9645 (1.9286)   Prec@1 49.000 (53.195)   Prec@5 88.000 (91.793)   [2025-10-28 19:36:27]
  **Train** Prec@1 53.276 Prec@5 91.796 Error@1 46.724
  **Test** Prec@1 52.950 Prec@5 91.090 Error@1 47.050

==>>[2025-10-28 19:36:56] [Epoch=035/040] [Need: 00:08:54] [LR=0.0010] [Best : Accuracy=54.25, Error=45.75]
  Epoch: [035][000/500]   Time 17.798 (17.798)   Data 17.412 (17.412)   Loss 1.9167 (1.9167)   Prec@1 53.000 (53.000)   Prec@5 93.000 (93.000)   [2025-10-28 19:37:14]
  Epoch: [035][100/500]   Time 0.107 (0.274)   Data 0.000 (0.173)   Loss 1.8785 (1.9259)   Prec@1 60.000 (53.703)   Prec@5 91.000 (92.287)   [2025-10-28 19:37:24]
  Epoch: [035][200/500]   Time 0.109 (0.189)   Data 0.000 (0.087)   Loss 1.9725 (1.9239)   Prec@1 47.000 (53.791)   Prec@5 91.000 (92.129)   [2025-10-28 19:37:34]
  Epoch: [035][300/500]   Time 0.099 (0.160)   Data 0.000 (0.058)   Loss 1.9024 (1.9243)   Prec@1 55.000 (53.814)   Prec@5 91.000 (92.050)   [2025-10-28 19:37:44]
  Epoch: [035][400/500]   Time 0.103 (0.146)   Data 0.000 (0.044)   Loss 1.9635 (1.9242)   Prec@1 49.000 (53.798)   Prec@5 94.000 (92.095)   [2025-10-28 19:37:55]
  **Train** Prec@1 53.776 Prec@5 92.120 Error@1 46.224
  **Test** Prec@1 53.050 Prec@5 90.930 Error@1 46.950

==>>[2025-10-28 19:38:24] [Epoch=036/040] [Need: 00:07:05] [LR=0.0010] [Best : Accuracy=54.25, Error=45.75]
  Epoch: [036][000/500]   Time 17.729 (17.729)   Data 17.355 (17.355)   Loss 1.9713 (1.9713)   Prec@1 49.000 (49.000)   Prec@5 87.000 (87.000)   [2025-10-28 19:38:42]
  Epoch: [036][100/500]   Time 0.105 (0.274)   Data 0.001 (0.172)   Loss 1.9456 (1.9237)   Prec@1 52.000 (53.812)   Prec@5 93.000 (91.881)   [2025-10-28 19:38:52]
  Epoch: [036][200/500]   Time 0.105 (0.189)   Data 0.001 (0.087)   Loss 1.9654 (1.9258)   Prec@1 49.000 (53.567)   Prec@5 89.000 (92.035)   [2025-10-28 19:39:02]
  Epoch: [036][300/500]   Time 0.099 (0.161)   Data 0.000 (0.058)   Loss 1.8996 (1.9259)   Prec@1 55.000 (53.595)   Prec@5 92.000 (92.073)   [2025-10-28 19:39:13]
  Epoch: [036][400/500]   Time 0.100 (0.147)   Data 0.000 (0.044)   Loss 1.9107 (1.9252)   Prec@1 57.000 (53.668)   Prec@5 96.000 (91.970)   [2025-10-28 19:39:23]
  **Train** Prec@1 53.740 Prec@5 91.992 Error@1 46.260
  **Test** Prec@1 46.420 Prec@5 84.920 Error@1 53.580

==>>[2025-10-28 19:39:53] [Epoch=037/040] [Need: 00:05:17] [LR=0.0010] [Best : Accuracy=54.25, Error=45.75]
  Epoch: [037][000/500]   Time 17.818 (17.818)   Data 17.536 (17.536)   Loss 1.8604 (1.8604)   Prec@1 61.000 (61.000)   Prec@5 95.000 (95.000)   [2025-10-28 19:40:11]
  Epoch: [037][100/500]   Time 0.102 (0.274)   Data 0.001 (0.174)   Loss 1.9618 (1.9240)   Prec@1 51.000 (53.762)   Prec@5 93.000 (92.327)   [2025-10-28 19:40:20]
  Epoch: [037][200/500]   Time 0.102 (0.188)   Data 0.000 (0.088)   Loss 1.9331 (1.9252)   Prec@1 54.000 (53.597)   Prec@5 90.000 (92.005)   [2025-10-28 19:40:31]
  Epoch: [037][300/500]   Time 0.098 (0.160)   Data 0.001 (0.059)   Loss 1.9297 (1.9260)   Prec@1 54.000 (53.578)   Prec@5 88.000 (91.900)   [2025-10-28 19:40:41]
  Epoch: [037][400/500]   Time 0.109 (0.145)   Data 0.000 (0.044)   Loss 1.9569 (1.9252)   Prec@1 51.000 (53.668)   Prec@5 93.000 (91.995)   [2025-10-28 19:40:51]
  **Train** Prec@1 53.704 Prec@5 92.100 Error@1 46.296
  **Test** Prec@1 48.360 Prec@5 88.220 Error@1 51.640

==>>[2025-10-28 19:41:21] [Epoch=038/040] [Need: 00:03:30] [LR=0.0010] [Best : Accuracy=54.25, Error=45.75]
  Epoch: [038][000/500]   Time 17.881 (17.881)   Data 17.628 (17.628)   Loss 1.8695 (1.8695)   Prec@1 58.000 (58.000)   Prec@5 93.000 (93.000)   [2025-10-28 19:41:39]
  Epoch: [038][100/500]   Time 0.108 (0.274)   Data 0.001 (0.175)   Loss 1.9358 (1.9261)   Prec@1 52.000 (53.614)   Prec@5 92.000 (92.139)   [2025-10-28 19:41:48]
  Epoch: [038][200/500]   Time 0.101 (0.190)   Data 0.000 (0.088)   Loss 1.9427 (1.9244)   Prec@1 51.000 (53.781)   Prec@5 91.000 (92.114)   [2025-10-28 19:41:59]
  Epoch: [038][300/500]   Time 0.105 (0.161)   Data 0.000 (0.059)   Loss 1.9151 (1.9241)   Prec@1 56.000 (53.890)   Prec@5 94.000 (92.133)   [2025-10-28 19:42:09]
  Epoch: [038][400/500]   Time 0.102 (0.147)   Data 0.001 (0.045)   Loss 1.9210 (1.9246)   Prec@1 55.000 (53.808)   Prec@5 95.000 (92.107)   [2025-10-28 19:42:20]
  **Train** Prec@1 53.850 Prec@5 92.080 Error@1 46.150
  **Test** Prec@1 54.560 Prec@5 92.930 Error@1 45.440
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 19:42:49] [Epoch=039/040] [Need: 00:01:44] [LR=0.0010] [Best : Accuracy=54.56, Error=45.44]
  Epoch: [039][000/500]   Time 17.836 (17.836)   Data 17.538 (17.538)   Loss 1.8922 (1.8922)   Prec@1 58.000 (58.000)   Prec@5 92.000 (92.000)   [2025-10-28 19:43:07]
  Epoch: [039][100/500]   Time 0.122 (0.277)   Data 0.001 (0.174)   Loss 1.9225 (1.9264)   Prec@1 52.000 (53.594)   Prec@5 95.000 (91.881)   [2025-10-28 19:43:17]
  Epoch: [039][200/500]   Time 0.098 (0.191)   Data 0.001 (0.088)   Loss 1.8321 (1.9223)   Prec@1 66.000 (53.990)   Prec@5 92.000 (91.995)   [2025-10-28 19:43:27]
  Epoch: [039][300/500]   Time 0.106 (0.162)   Data 0.000 (0.059)   Loss 1.9795 (1.9222)   Prec@1 47.000 (54.010)   Prec@5 90.000 (92.037)   [2025-10-28 19:43:38]
  Epoch: [039][400/500]   Time 0.102 (0.147)   Data 0.000 (0.044)   Loss 1.9389 (1.9222)   Prec@1 51.000 (53.973)   Prec@5 97.000 (92.025)   [2025-10-28 19:43:48]
  **Train** Prec@1 54.034 Prec@5 92.130 Error@1 45.966
  **Test** Prec@1 52.560 Prec@5 89.620 Error@1 47.440
