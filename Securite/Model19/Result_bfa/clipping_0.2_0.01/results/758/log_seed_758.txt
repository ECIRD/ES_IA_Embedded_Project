save path : ./save/resnet9_quan/clipping_0.2_0.01/results/758
{'data_path': './dataset', 'arch': 'resnet9_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': False, 'clipping_coeff': 0.2, 'learning_rate': 0.01, 'manualSeed': 758, 'save_path': './save/resnet9_quan/clipping_0.2_0.01/results/758', 'enable_bfa': True, 'resume': './save/resnet9_quan/clipping_0.2_0.01/model_best.pth.tar', 'quan_bitwidth': None, 'reset_weight': True, 'evaluate': True, 'n_iter': 30, 'fine_tune': True, 'model_only': False, 'random_bfa': False, 'use_cuda': True}
Random Seed: 758
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'resnet9_quan'
=> network :
 Sequential(
  (0): Sequential(
    (0): quan_Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (1): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (2): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (4): Dropout2d(p=0.2, inplace=False)
  (5): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (skip): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (6): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Dropout2d(p=0.3, inplace=False)
  (9): Sequential(
    (0): quan_Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (10): Sequential(
    (0): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (12): Dropout2d(p=0.3, inplace=False)
  (13): Sequential(
    (0): quan_Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (14): AdaptiveAvgPool2d(output_size=(1, 1))
  (15): Flatten()
  (16): quan_Linear(in_features=64, out_features=128, bias=False)
  (17): ReLU(inplace=True)
  (18): Dropout(p=0.35, inplace=False)
  (19): quan_Linear(in_features=128, out_features=10, bias=False)
  (20): Softmax(dim=1)
)
=> loading checkpoint './save/resnet9_quan/clipping_0.2_0.01/model_best.pth.tar'
=> loaded checkpoint './save/resnet9_quan/clipping_0.2_0.01/model_best.pth.tar' (epoch 0)
  **Test** Prec@1 55.120 Prec@5 89.860 Error@1 44.880
k_top=100
Attack_sample=100
************** ATTACK iteration *****************
Iteration: [001/030]   Attack Time 0.597 (0.597)  [2025-10-29 13:08:49]
loss before attack: 1.6209
loss after attack: 1.8019
bit flips: 1
hamming_dist: 1
  **Test** Prec@1 51.410 Prec@5 88.920 Error@1 48.590
iteration Time 21.374 (21.374)
************** ATTACK iteration *****************
Iteration: [002/030]   Attack Time 0.291 (0.444)  [2025-10-29 13:09:11]
loss before attack: 1.8019
loss after attack: 1.9636
bit flips: 2
hamming_dist: 2
  **Test** Prec@1 46.040 Prec@5 86.780 Error@1 53.960
iteration Time 21.246 (21.310)
************** ATTACK iteration *****************
Iteration: [003/030]   Attack Time 0.279 (0.389)  [2025-10-29 13:09:32]
loss before attack: 1.9636
loss after attack: 2.0942
bit flips: 3
hamming_dist: 3
  **Test** Prec@1 37.240 Prec@5 81.430 Error@1 62.760
iteration Time 21.884 (21.501)
************** ATTACK iteration *****************
Iteration: [004/030]   Attack Time 0.298 (0.366)  [2025-10-29 13:09:55]
loss before attack: 2.0942
loss after attack: 2.1755
bit flips: 4
hamming_dist: 4
  **Test** Prec@1 30.700 Prec@5 76.320 Error@1 69.300
iteration Time 21.711 (21.554)
************** ATTACK iteration *****************
Iteration: [005/030]   Attack Time 0.303 (0.354)  [2025-10-29 13:10:17]
loss before attack: 2.1755
loss after attack: 2.2452
bit flips: 5
hamming_dist: 5
  **Test** Prec@1 25.420 Prec@5 69.450 Error@1 74.580
iteration Time 21.525 (21.548)
************** ATTACK iteration *****************
Iteration: [006/030]   Attack Time 0.288 (0.343)  [2025-10-29 13:10:38]
loss before attack: 2.2452
loss after attack: 2.2946
bit flips: 6
hamming_dist: 6
  **Test** Prec@1 21.450 Prec@5 63.900 Error@1 78.550
iteration Time 21.565 (21.551)
************** ATTACK iteration *****************
Iteration: [007/030]   Attack Time 0.293 (0.336)  [2025-10-29 13:11:00]
loss before attack: 2.2946
loss after attack: 2.3146
bit flips: 7
hamming_dist: 7
  **Test** Prec@1 18.470 Prec@5 60.220 Error@1 81.530
iteration Time 21.528 (21.547)
************** ATTACK iteration *****************
Iteration: [008/030]   Attack Time 0.291 (0.330)  [2025-10-29 13:11:22]
loss before attack: 2.3146
loss after attack: 2.3445
bit flips: 8
hamming_dist: 8
  **Test** Prec@1 15.890 Prec@5 57.690 Error@1 84.110
iteration Time 21.545 (21.547)
************** ATTACK iteration *****************
Iteration: [009/030]   Attack Time 0.299 (0.326)  [2025-10-29 13:11:44]
loss before attack: 2.3445
loss after attack: 2.3572
bit flips: 9
hamming_dist: 9
  **Test** Prec@1 14.450 Prec@5 55.920 Error@1 85.550
iteration Time 21.378 (21.528)
************** ATTACK iteration *****************
Iteration: [010/030]   Attack Time 0.186 (0.312)  [2025-10-29 13:12:05]
loss before attack: 2.3572
loss after attack: 2.3685
bit flips: 10
hamming_dist: 10
  **Test** Prec@1 13.500 Prec@5 55.570 Error@1 86.500
iteration Time 20.791 (21.455)
************** ATTACK iteration *****************
Iteration: [011/030]   Attack Time 0.145 (0.297)  [2025-10-29 13:12:26]
loss before attack: 2.3685
loss after attack: 2.3700
bit flips: 11
hamming_dist: 11
  **Test** Prec@1 12.370 Prec@5 54.000 Error@1 87.630
iteration Time 20.824 (21.397)
************** ATTACK iteration *****************
Iteration: [012/030]   Attack Time 0.150 (0.285)  [2025-10-29 13:12:47]
loss before attack: 2.3700
loss after attack: 2.3707
bit flips: 12
hamming_dist: 12
  **Test** Prec@1 11.540 Prec@5 53.090 Error@1 88.460
iteration Time 20.881 (21.354)
************** ATTACK iteration *****************
Iteration: [013/030]   Attack Time 0.153 (0.275)  [2025-10-29 13:13:08]
loss before attack: 2.3707
loss after attack: 2.3709
bit flips: 13
hamming_dist: 13
  **Test** Prec@1 11.160 Prec@5 52.090 Error@1 88.840
iteration Time 21.159 (21.339)
************** ATTACK iteration *****************
Iteration: [014/030]   Attack Time 0.146 (0.266)  [2025-10-29 13:13:30]
loss before attack: 2.3709
loss after attack: 2.3710
bit flips: 14
hamming_dist: 14
  **Test** Prec@1 11.160 Prec@5 52.090 Error@1 88.840
iteration Time 20.858 (21.305)
************** ATTACK iteration *****************
Iteration: [015/030]   Attack Time 0.172 (0.259)  [2025-10-29 13:13:51]
loss before attack: 2.3710
loss after attack: 2.3710
bit flips: 15
hamming_dist: 15
  **Test** Prec@1 11.160 Prec@5 52.090 Error@1 88.840
iteration Time 20.660 (21.262)
************** ATTACK iteration *****************
Iteration: [016/030]   Attack Time 0.161 (0.253)  [2025-10-29 13:14:12]
loss before attack: 2.3710
loss after attack: 2.3711
bit flips: 16
hamming_dist: 16
  **Test** Prec@1 11.160 Prec@5 52.090 Error@1 88.840
iteration Time 20.739 (21.229)
************** ATTACK iteration *****************
Iteration: [017/030]   Attack Time 0.153 (0.247)  [2025-10-29 13:14:32]
loss before attack: 2.3711
loss after attack: 2.3711
bit flips: 17
hamming_dist: 17
  **Test** Prec@1 11.190 Prec@5 52.120 Error@1 88.810
iteration Time 21.174 (21.226)
************** ATTACK iteration *****************
Iteration: [018/030]   Attack Time 0.162 (0.242)  [2025-10-29 13:14:54]
loss before attack: 2.3711
loss after attack: 2.3711
bit flips: 18
hamming_dist: 18
  **Test** Prec@1 11.240 Prec@5 52.160 Error@1 88.760
iteration Time 20.818 (21.203)
************** ATTACK iteration *****************
Iteration: [019/030]   Attack Time 0.158 (0.238)  [2025-10-29 13:15:15]
loss before attack: 2.3711
loss after attack: 2.3711
bit flips: 19
hamming_dist: 19
  **Test** Prec@1 11.210 Prec@5 52.040 Error@1 88.790
iteration Time 20.828 (21.184)
************** ATTACK iteration *****************
Iteration: [020/030]   Attack Time 0.150 (0.234)  [2025-10-29 13:15:36]
loss before attack: 2.3711
loss after attack: 2.3711
bit flips: 20
hamming_dist: 20
  **Test** Prec@1 10.940 Prec@5 51.550 Error@1 89.060
iteration Time 21.222 (21.185)
