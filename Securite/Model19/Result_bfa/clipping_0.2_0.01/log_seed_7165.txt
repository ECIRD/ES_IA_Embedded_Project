save path : ./save/resnet9_quan/clipping_0.2_0.01
{'data_path': './dataset', 'arch': 'resnet9_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': False, 'clipping_coeff': 0.2, 'learning_rate': 0.01, 'manualSeed': 7165, 'save_path': './save/resnet9_quan/clipping_0.2_0.01', 'enable_bfa': False, 'resume': '', 'quan_bitwidth': None, 'reset_weight': False, 'evaluate': False, 'n_iter': 30, 'model_only': False, 'random_bfa': False, 'use_cuda': True}
Random Seed: 7165
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'resnet9_quan'
=> network :
 Sequential(
  (0): Sequential(
    (0): quan_Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (1): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (2): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (4): Dropout2d(p=0.2, inplace=False)
  (5): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (skip): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (6): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Dropout2d(p=0.3, inplace=False)
  (9): Sequential(
    (0): quan_Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (10): Sequential(
    (0): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (12): Dropout2d(p=0.3, inplace=False)
  (13): Sequential(
    (0): quan_Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (14): AdaptiveAvgPool2d(output_size=(1, 1))
  (15): Flatten()
  (16): quan_Linear(in_features=64, out_features=128, bias=False)
  (17): ReLU(inplace=True)
  (18): Dropout(p=0.35, inplace=False)
  (19): quan_Linear(in_features=128, out_features=10, bias=False)
  (20): Softmax(dim=1)
)
=> do not use any checkpoint for resnet9_quan model

==>>[2025-10-28 12:36:03] [Epoch=000/040] [Need: 00:00:00] [LR=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/500]   Time 19.060 (19.060)   Data 17.899 (17.899)   Loss 2.3054 (2.3054)   Prec@1 11.000 (11.000)   Prec@5 39.000 (39.000)   [2025-10-28 12:36:22]
  Epoch: [000][100/500]   Time 0.053 (0.241)   Data 0.000 (0.177)   Loss 2.3017 (2.3025)   Prec@1 14.000 (10.297)   Prec@5 54.000 (51.406)   [2025-10-28 12:36:28]
  Epoch: [000][200/500]   Time 0.052 (0.148)   Data 0.000 (0.089)   Loss 2.3035 (2.3019)   Prec@1 8.000 (10.846)   Prec@5 50.000 (52.184)   [2025-10-28 12:36:33]
  Epoch: [000][300/500]   Time 0.059 (0.117)   Data 0.001 (0.060)   Loss 2.3018 (2.3009)   Prec@1 6.000 (11.575)   Prec@5 54.000 (53.468)   [2025-10-28 12:36:39]
  Epoch: [000][400/500]   Time 0.056 (0.102)   Data 0.000 (0.045)   Loss 2.2815 (2.2983)   Prec@1 20.000 (12.414)   Prec@5 67.000 (55.496)   [2025-10-28 12:36:44]
  **Train** Prec@1 13.226 Prec@5 57.824 Error@1 86.774
  **Test** Prec@1 16.160 Prec@5 63.770 Error@1 83.840
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 12:37:10] [Epoch=001/040] [Need: 00:43:08] [LR=0.0100] [Best : Accuracy=16.16, Error=83.84]
  Epoch: [001][000/500]   Time 17.982 (17.982)   Data 17.704 (17.704)   Loss 2.2511 (2.2511)   Prec@1 17.000 (17.000)   Prec@5 69.000 (69.000)   [2025-10-28 12:37:28]
  Epoch: [001][100/500]   Time 0.055 (0.231)   Data 0.000 (0.175)   Loss 2.2546 (2.2632)   Prec@1 18.000 (16.911)   Prec@5 72.000 (70.218)   [2025-10-28 12:37:33]
  Epoch: [001][200/500]   Time 0.054 (0.144)   Data 0.000 (0.088)   Loss 2.2246 (2.2574)   Prec@1 23.000 (17.512)   Prec@5 74.000 (71.925)   [2025-10-28 12:37:39]
  Epoch: [001][300/500]   Time 0.057 (0.114)   Data 0.001 (0.059)   Loss 2.2649 (2.2540)   Prec@1 19.000 (17.525)   Prec@5 74.000 (72.718)   [2025-10-28 12:37:44]
  Epoch: [001][400/500]   Time 0.057 (0.100)   Data 0.000 (0.044)   Loss 2.2475 (2.2502)   Prec@1 15.000 (17.706)   Prec@5 73.000 (73.259)   [2025-10-28 12:37:50]
  **Train** Prec@1 17.830 Prec@5 73.616 Error@1 82.170
  **Test** Prec@1 12.650 Prec@5 54.080 Error@1 87.350

==>>[2025-10-28 12:38:18] [Epoch=002/040] [Need: 00:42:36] [LR=0.0100] [Best : Accuracy=16.16, Error=83.84]
  Epoch: [002][000/500]   Time 25.006 (25.006)   Data 24.747 (24.747)   Loss 2.2504 (2.2504)   Prec@1 15.000 (15.000)   Prec@5 81.000 (81.000)   [2025-10-28 12:38:43]
  Epoch: [002][100/500]   Time 0.076 (0.314)   Data 0.000 (0.245)   Loss 2.2652 (2.2291)   Prec@1 14.000 (18.347)   Prec@5 73.000 (75.218)   [2025-10-28 12:38:50]
  Epoch: [002][200/500]   Time 0.067 (0.191)   Data 0.002 (0.123)   Loss 2.2115 (2.2286)   Prec@1 26.000 (18.473)   Prec@5 80.000 (75.925)   [2025-10-28 12:38:57]
  Epoch: [002][300/500]   Time 0.066 (0.150)   Data 0.001 (0.083)   Loss 2.2668 (2.2269)   Prec@1 13.000 (18.678)   Prec@5 77.000 (75.870)   [2025-10-28 12:39:04]
  Epoch: [002][400/500]   Time 0.066 (0.130)   Data 0.000 (0.062)   Loss 2.2263 (2.2262)   Prec@1 21.000 (18.873)   Prec@5 75.000 (76.125)   [2025-10-28 12:39:11]
  **Train** Prec@1 18.940 Prec@5 76.500 Error@1 81.060
  **Test** Prec@1 14.320 Prec@5 56.730 Error@1 85.680

==>>[2025-10-28 12:39:44] [Epoch=003/040] [Need: 00:45:13] [LR=0.0100] [Best : Accuracy=16.16, Error=83.84]
  Epoch: [003][000/500]   Time 24.869 (24.869)   Data 24.570 (24.570)   Loss 2.2478 (2.2478)   Prec@1 17.000 (17.000)   Prec@5 70.000 (70.000)   [2025-10-28 12:40:09]
  Epoch: [003][100/500]   Time 0.069 (0.313)   Data 0.000 (0.244)   Loss 2.1939 (2.2213)   Prec@1 30.000 (19.861)   Prec@5 83.000 (77.139)   [2025-10-28 12:40:15]
  Epoch: [003][200/500]   Time 0.065 (0.191)   Data 0.000 (0.123)   Loss 2.2326 (2.2196)   Prec@1 15.000 (20.030)   Prec@5 75.000 (77.587)   [2025-10-28 12:40:22]
  Epoch: [003][300/500]   Time 0.071 (0.149)   Data 0.000 (0.082)   Loss 2.2248 (2.2176)   Prec@1 20.000 (20.153)   Prec@5 81.000 (77.688)   [2025-10-28 12:40:29]
  Epoch: [003][400/500]   Time 0.068 (0.129)   Data 0.000 (0.062)   Loss 2.2159 (2.2159)   Prec@1 21.000 (20.476)   Prec@5 79.000 (77.990)   [2025-10-28 12:40:35]
  **Train** Prec@1 20.784 Prec@5 78.148 Error@1 79.216
  **Test** Prec@1 18.830 Prec@5 72.940 Error@1 81.170
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 12:41:06] [Epoch=004/040] [Need: 00:45:24] [LR=0.0100] [Best : Accuracy=18.83, Error=81.17]
  Epoch: [004][000/500]   Time 24.333 (24.333)   Data 24.038 (24.038)   Loss 2.2134 (2.2134)   Prec@1 19.000 (19.000)   Prec@5 80.000 (80.000)   [2025-10-28 12:41:31]
  Epoch: [004][100/500]   Time 0.073 (0.307)   Data 0.001 (0.238)   Loss 2.2200 (2.2064)   Prec@1 22.000 (22.475)   Prec@5 79.000 (79.069)   [2025-10-28 12:41:37]
  Epoch: [004][200/500]   Time 0.064 (0.187)   Data 0.000 (0.120)   Loss 2.2224 (2.2068)   Prec@1 22.000 (22.662)   Prec@5 74.000 (78.577)   [2025-10-28 12:41:44]
  Epoch: [004][300/500]   Time 0.086 (0.148)   Data 0.000 (0.080)   Loss 2.1519 (2.2019)   Prec@1 34.000 (23.133)   Prec@5 89.000 (78.860)   [2025-10-28 12:41:51]
  Epoch: [004][400/500]   Time 0.078 (0.128)   Data 0.001 (0.060)   Loss 2.1897 (2.1999)   Prec@1 23.000 (23.374)   Prec@5 82.000 (78.938)   [2025-10-28 12:41:58]
  **Train** Prec@1 23.382 Prec@5 78.862 Error@1 76.618
  **Test** Prec@1 24.310 Prec@5 80.330 Error@1 75.690
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 12:42:28] [Epoch=005/040] [Need: 00:44:52] [LR=0.0100] [Best : Accuracy=24.31, Error=75.69]
  Epoch: [005][000/500]   Time 24.202 (24.202)   Data 23.937 (23.937)   Loss 2.1932 (2.1932)   Prec@1 20.000 (20.000)   Prec@5 84.000 (84.000)   [2025-10-28 12:42:53]
  Epoch: [005][100/500]   Time 0.066 (0.305)   Data 0.001 (0.237)   Loss 2.1810 (2.1866)   Prec@1 22.000 (24.604)   Prec@5 79.000 (79.871)   [2025-10-28 12:42:59]
  Epoch: [005][200/500]   Time 0.065 (0.186)   Data 0.000 (0.119)   Loss 2.1913 (2.1852)   Prec@1 20.000 (24.716)   Prec@5 77.000 (79.876)   [2025-10-28 12:43:06]
  Epoch: [005][300/500]   Time 0.074 (0.146)   Data 0.000 (0.080)   Loss 2.1794 (2.1827)   Prec@1 25.000 (25.166)   Prec@5 78.000 (79.990)   [2025-10-28 12:43:12]
  Epoch: [005][400/500]   Time 0.068 (0.126)   Data 0.000 (0.060)   Loss 2.1946 (2.1812)   Prec@1 27.000 (25.379)   Prec@5 80.000 (79.993)   [2025-10-28 12:43:19]
  **Train** Prec@1 25.690 Prec@5 80.058 Error@1 74.310
  **Test** Prec@1 24.910 Prec@5 81.170 Error@1 75.090
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 12:43:50] [Epoch=006/040] [Need: 00:44:02] [LR=0.0100] [Best : Accuracy=24.91, Error=75.09]
  Epoch: [006][000/500]   Time 22.889 (22.889)   Data 22.578 (22.578)   Loss 2.2003 (2.2003)   Prec@1 21.000 (21.000)   Prec@5 76.000 (76.000)   [2025-10-28 12:44:13]
  Epoch: [006][100/500]   Time 0.065 (0.293)   Data 0.000 (0.224)   Loss 2.1409 (2.1699)   Prec@1 27.000 (27.000)   Prec@5 88.000 (79.545)   [2025-10-28 12:44:20]
  Epoch: [006][200/500]   Time 0.071 (0.181)   Data 0.001 (0.113)   Loss 2.1407 (2.1690)   Prec@1 28.000 (27.005)   Prec@5 84.000 (79.970)   [2025-10-28 12:44:26]
  Epoch: [006][300/500]   Time 0.069 (0.143)   Data 0.000 (0.075)   Loss 2.1411 (2.1674)   Prec@1 31.000 (27.276)   Prec@5 83.000 (79.794)   [2025-10-28 12:44:33]
  Epoch: [006][400/500]   Time 0.065 (0.124)   Data 0.000 (0.057)   Loss 2.1598 (2.1649)   Prec@1 28.000 (27.603)   Prec@5 82.000 (79.833)   [2025-10-28 12:44:40]
  **Train** Prec@1 27.696 Prec@5 79.730 Error@1 72.304
  **Test** Prec@1 21.550 Prec@5 71.250 Error@1 78.450

==>>[2025-10-28 12:45:09] [Epoch=007/040] [Need: 00:42:52] [LR=0.0100] [Best : Accuracy=24.91, Error=75.09]
  Epoch: [007][000/500]   Time 22.541 (22.541)   Data 22.281 (22.281)   Loss 2.1334 (2.1334)   Prec@1 32.000 (32.000)   Prec@5 76.000 (76.000)   [2025-10-28 12:45:32]
  Epoch: [007][100/500]   Time 0.069 (0.288)   Data 0.001 (0.221)   Loss 2.1243 (2.1545)   Prec@1 31.000 (28.416)   Prec@5 79.000 (79.594)   [2025-10-28 12:45:38]
  Epoch: [007][200/500]   Time 0.068 (0.177)   Data 0.000 (0.111)   Loss 2.1158 (2.1543)   Prec@1 34.000 (28.866)   Prec@5 73.000 (79.353)   [2025-10-28 12:45:45]
  Epoch: [007][300/500]   Time 0.065 (0.141)   Data 0.001 (0.074)   Loss 2.1317 (2.1525)   Prec@1 35.000 (29.179)   Prec@5 78.000 (79.203)   [2025-10-28 12:45:52]
  Epoch: [007][400/500]   Time 0.065 (0.123)   Data 0.000 (0.056)   Loss 2.1507 (2.1510)   Prec@1 27.000 (29.337)   Prec@5 87.000 (79.262)   [2025-10-28 12:45:59]
  **Train** Prec@1 29.546 Prec@5 79.086 Error@1 70.454
  **Test** Prec@1 26.550 Prec@5 76.150 Error@1 73.450
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 12:46:30] [Epoch=008/040] [Need: 00:41:44] [LR=0.0100] [Best : Accuracy=26.55, Error=73.45]
  Epoch: [008][000/500]   Time 24.455 (24.455)   Data 24.244 (24.244)   Loss 2.1573 (2.1573)   Prec@1 26.000 (26.000)   Prec@5 82.000 (82.000)   [2025-10-28 12:46:54]
  Epoch: [008][100/500]   Time 0.053 (0.307)   Data 0.000 (0.240)   Loss 2.1312 (2.1347)   Prec@1 29.000 (31.238)   Prec@5 80.000 (79.861)   [2025-10-28 12:47:01]
  Epoch: [008][200/500]   Time 0.062 (0.188)   Data 0.001 (0.121)   Loss 2.1433 (2.1367)   Prec@1 33.000 (30.970)   Prec@5 78.000 (79.527)   [2025-10-28 12:47:08]
  Epoch: [008][300/500]   Time 0.077 (0.148)   Data 0.000 (0.081)   Loss 2.1357 (2.1363)   Prec@1 31.000 (31.226)   Prec@5 81.000 (79.462)   [2025-10-28 12:47:14]
  Epoch: [008][400/500]   Time 0.072 (0.128)   Data 0.000 (0.061)   Loss 2.1364 (2.1351)   Prec@1 34.000 (31.314)   Prec@5 75.000 (79.726)   [2025-10-28 12:47:21]
  **Train** Prec@1 31.572 Prec@5 79.778 Error@1 68.428
  **Test** Prec@1 29.770 Prec@5 79.390 Error@1 70.230
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 12:47:53] [Epoch=009/040] [Need: 00:40:42] [LR=0.0100] [Best : Accuracy=29.77, Error=70.23]
  Epoch: [009][000/500]   Time 22.779 (22.779)   Data 22.537 (22.537)   Loss 2.1119 (2.1119)   Prec@1 33.000 (33.000)   Prec@5 78.000 (78.000)   [2025-10-28 12:48:16]
  Epoch: [009][100/500]   Time 0.059 (0.293)   Data 0.000 (0.224)   Loss 2.0649 (2.1205)   Prec@1 43.000 (33.228)   Prec@5 83.000 (80.584)   [2025-10-28 12:48:22]
  Epoch: [009][200/500]   Time 0.075 (0.181)   Data 0.001 (0.113)   Loss 2.1566 (2.1172)   Prec@1 29.000 (33.751)   Prec@5 77.000 (80.542)   [2025-10-28 12:48:29]
  Epoch: [009][300/500]   Time 0.066 (0.143)   Data 0.000 (0.075)   Loss 2.0531 (2.1176)   Prec@1 44.000 (33.588)   Prec@5 82.000 (80.276)   [2025-10-28 12:48:36]
  Epoch: [009][400/500]   Time 0.064 (0.124)   Data 0.000 (0.057)   Loss 2.0888 (2.1162)   Prec@1 37.000 (33.576)   Prec@5 86.000 (80.439)   [2025-10-28 12:48:43]
  **Train** Prec@1 33.910 Prec@5 80.544 Error@1 66.090
  **Test** Prec@1 31.320 Prec@5 79.780 Error@1 68.680
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 12:49:15] [Epoch=010/040] [Need: 00:39:33] [LR=0.0100] [Best : Accuracy=31.32, Error=68.68]
  Epoch: [010][000/500]   Time 24.194 (24.194)   Data 23.941 (23.941)   Loss 2.0888 (2.0888)   Prec@1 35.000 (35.000)   Prec@5 83.000 (83.000)   [2025-10-28 12:49:39]
  Epoch: [010][100/500]   Time 0.073 (0.305)   Data 0.000 (0.237)   Loss 2.1430 (2.1012)   Prec@1 35.000 (35.663)   Prec@5 72.000 (80.495)   [2025-10-28 12:49:46]
  Epoch: [010][200/500]   Time 0.064 (0.186)   Data 0.000 (0.119)   Loss 2.0551 (2.1004)   Prec@1 41.000 (35.502)   Prec@5 79.000 (80.706)   [2025-10-28 12:49:53]
  Epoch: [010][300/500]   Time 0.069 (0.147)   Data 0.000 (0.080)   Loss 2.0802 (2.0963)   Prec@1 37.000 (35.924)   Prec@5 81.000 (80.807)   [2025-10-28 12:49:59]
  Epoch: [010][400/500]   Time 0.072 (0.127)   Data 0.000 (0.060)   Loss 2.0893 (2.0945)   Prec@1 36.000 (36.092)   Prec@5 80.000 (80.933)   [2025-10-28 12:50:06]
  **Train** Prec@1 36.284 Prec@5 81.122 Error@1 63.716
  **Test** Prec@1 35.390 Prec@5 80.910 Error@1 64.610
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 12:50:37] [Epoch=011/040] [Need: 00:38:22] [LR=0.0100] [Best : Accuracy=35.39, Error=64.61]
  Epoch: [011][000/500]   Time 24.097 (24.097)   Data 23.852 (23.852)   Loss 2.0292 (2.0292)   Prec@1 43.000 (43.000)   Prec@5 79.000 (79.000)   [2025-10-28 12:51:01]
  Epoch: [011][100/500]   Time 0.069 (0.303)   Data 0.000 (0.237)   Loss 2.1235 (2.0849)   Prec@1 31.000 (36.970)   Prec@5 79.000 (81.762)   [2025-10-28 12:51:08]
  Epoch: [011][200/500]   Time 0.063 (0.185)   Data 0.000 (0.119)   Loss 2.1345 (2.0842)   Prec@1 30.000 (36.995)   Prec@5 81.000 (81.632)   [2025-10-28 12:51:14]
  Epoch: [011][300/500]   Time 0.070 (0.146)   Data 0.000 (0.080)   Loss 2.0548 (2.0826)   Prec@1 41.000 (37.103)   Prec@5 86.000 (81.791)   [2025-10-28 12:51:21]
  Epoch: [011][400/500]   Time 0.065 (0.126)   Data 0.000 (0.060)   Loss 2.1274 (2.0803)   Prec@1 27.000 (37.394)   Prec@5 76.000 (81.965)   [2025-10-28 12:51:28]
  **Train** Prec@1 37.742 Prec@5 81.978 Error@1 62.258
  **Test** Prec@1 40.080 Prec@5 82.770 Error@1 59.920
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 12:51:59] [Epoch=012/040] [Need: 00:37:07] [LR=0.0100] [Best : Accuracy=40.08, Error=59.92]
  Epoch: [012][000/500]   Time 22.626 (22.626)   Data 22.355 (22.355)   Loss 2.0544 (2.0544)   Prec@1 38.000 (38.000)   Prec@5 83.000 (83.000)   [2025-10-28 12:52:21]
  Epoch: [012][100/500]   Time 0.065 (0.288)   Data 0.000 (0.222)   Loss 2.0590 (2.0686)   Prec@1 44.000 (38.713)   Prec@5 82.000 (81.881)   [2025-10-28 12:52:28]
  Epoch: [012][200/500]   Time 0.064 (0.178)   Data 0.000 (0.112)   Loss 2.1538 (2.0662)   Prec@1 30.000 (38.920)   Prec@5 81.000 (81.910)   [2025-10-28 12:52:34]
  Epoch: [012][300/500]   Time 0.068 (0.141)   Data 0.000 (0.075)   Loss 2.1297 (2.0636)   Prec@1 31.000 (39.203)   Prec@5 79.000 (82.189)   [2025-10-28 12:52:41]
  Epoch: [012][400/500]   Time 0.067 (0.123)   Data 0.000 (0.056)   Loss 2.0093 (2.0626)   Prec@1 44.000 (39.384)   Prec@5 87.000 (82.429)   [2025-10-28 12:52:48]
  **Train** Prec@1 39.660 Prec@5 82.476 Error@1 60.340
  **Test** Prec@1 41.080 Prec@5 83.010 Error@1 58.920
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 12:53:17] [Epoch=013/040] [Need: 00:35:47] [LR=0.0100] [Best : Accuracy=41.08, Error=58.92]
  Epoch: [013][000/500]   Time 22.431 (22.431)   Data 22.136 (22.136)   Loss 2.0668 (2.0668)   Prec@1 38.000 (38.000)   Prec@5 83.000 (83.000)   [2025-10-28 12:53:40]
  Epoch: [013][100/500]   Time 0.064 (0.288)   Data 0.000 (0.220)   Loss 1.9878 (2.0468)   Prec@1 45.000 (41.129)   Prec@5 84.000 (82.812)   [2025-10-28 12:53:47]
  Epoch: [013][200/500]   Time 0.067 (0.177)   Data 0.000 (0.110)   Loss 2.0817 (2.0488)   Prec@1 37.000 (40.846)   Prec@5 76.000 (82.751)   [2025-10-28 12:53:53]
  Epoch: [013][300/500]   Time 0.066 (0.141)   Data 0.001 (0.074)   Loss 2.0132 (2.0481)   Prec@1 46.000 (40.907)   Prec@5 87.000 (82.791)   [2025-10-28 12:54:00]
  Epoch: [013][400/500]   Time 0.062 (0.123)   Data 0.001 (0.056)   Loss 2.0527 (2.0471)   Prec@1 41.000 (40.993)   Prec@5 80.000 (82.828)   [2025-10-28 12:54:07]
  **Train** Prec@1 41.054 Prec@5 82.866 Error@1 58.946
  **Test** Prec@1 41.790 Prec@5 82.680 Error@1 58.210
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 12:54:36] [Epoch=014/040] [Need: 00:34:25] [LR=0.0100] [Best : Accuracy=41.79, Error=58.21]
  Epoch: [014][000/500]   Time 22.698 (22.698)   Data 22.415 (22.415)   Loss 2.0440 (2.0440)   Prec@1 37.000 (37.000)   Prec@5 79.000 (79.000)   [2025-10-28 12:54:59]
  Epoch: [014][100/500]   Time 0.070 (0.290)   Data 0.001 (0.222)   Loss 2.0052 (2.0384)   Prec@1 47.000 (41.713)   Prec@5 84.000 (83.386)   [2025-10-28 12:55:05]
  Epoch: [014][200/500]   Time 0.069 (0.178)   Data 0.000 (0.112)   Loss 2.0589 (2.0388)   Prec@1 38.000 (41.711)   Prec@5 83.000 (83.413)   [2025-10-28 12:55:12]
  Epoch: [014][300/500]   Time 0.069 (0.142)   Data 0.000 (0.075)   Loss 2.0472 (2.0367)   Prec@1 43.000 (41.950)   Prec@5 81.000 (83.289)   [2025-10-28 12:55:19]
  Epoch: [014][400/500]   Time 0.072 (0.123)   Data 0.000 (0.056)   Loss 2.0411 (2.0344)   Prec@1 41.000 (42.252)   Prec@5 78.000 (83.239)   [2025-10-28 12:55:26]
  **Train** Prec@1 42.546 Prec@5 83.328 Error@1 57.454
  **Test** Prec@1 41.250 Prec@5 80.320 Error@1 58.750

==>>[2025-10-28 12:55:55] [Epoch=015/040] [Need: 00:33:05] [LR=0.0100] [Best : Accuracy=41.79, Error=58.21]
  Epoch: [015][000/500]   Time 24.086 (24.086)   Data 23.793 (23.793)   Loss 1.9158 (1.9158)   Prec@1 55.000 (55.000)   Prec@5 86.000 (86.000)   [2025-10-28 12:56:19]
  Epoch: [015][100/500]   Time 0.072 (0.304)   Data 0.000 (0.236)   Loss 1.9731 (2.0250)   Prec@1 50.000 (43.594)   Prec@5 87.000 (84.109)   [2025-10-28 12:56:26]
  Epoch: [015][200/500]   Time 0.069 (0.186)   Data 0.002 (0.119)   Loss 2.0428 (2.0253)   Prec@1 41.000 (43.458)   Prec@5 80.000 (84.154)   [2025-10-28 12:56:33]
  Epoch: [015][300/500]   Time 0.070 (0.146)   Data 0.000 (0.079)   Loss 2.0469 (2.0270)   Prec@1 41.000 (43.279)   Prec@5 80.000 (84.133)   [2025-10-28 12:56:39]
  Epoch: [015][400/500]   Time 0.063 (0.126)   Data 0.000 (0.060)   Loss 1.9895 (2.0254)   Prec@1 46.000 (43.424)   Prec@5 90.000 (84.102)   [2025-10-28 12:56:46]
  **Train** Prec@1 43.644 Prec@5 84.142 Error@1 56.356
  **Test** Prec@1 40.890 Prec@5 81.480 Error@1 59.110

==>>[2025-10-28 12:57:15] [Epoch=016/040] [Need: 00:31:47] [LR=0.0100] [Best : Accuracy=41.79, Error=58.21]
  Epoch: [016][000/500]   Time 22.648 (22.648)   Data 22.362 (22.362)   Loss 1.9672 (1.9672)   Prec@1 50.000 (50.000)   Prec@5 87.000 (87.000)   [2025-10-28 12:57:38]
  Epoch: [016][100/500]   Time 0.067 (0.289)   Data 0.000 (0.222)   Loss 2.1109 (2.0180)   Prec@1 35.000 (44.050)   Prec@5 82.000 (84.337)   [2025-10-28 12:57:44]
  Epoch: [016][200/500]   Time 0.066 (0.179)   Data 0.000 (0.112)   Loss 1.9925 (2.0133)   Prec@1 46.000 (44.502)   Prec@5 85.000 (84.567)   [2025-10-28 12:57:51]
  Epoch: [016][300/500]   Time 0.068 (0.142)   Data 0.000 (0.075)   Loss 2.0225 (2.0128)   Prec@1 44.000 (44.555)   Prec@5 82.000 (84.794)   [2025-10-28 12:57:58]
  Epoch: [016][400/500]   Time 0.066 (0.123)   Data 0.000 (0.056)   Loss 2.0036 (2.0122)   Prec@1 44.000 (44.594)   Prec@5 90.000 (84.875)   [2025-10-28 12:58:05]
  **Train** Prec@1 44.772 Prec@5 84.966 Error@1 55.228
  **Test** Prec@1 42.650 Prec@5 82.780 Error@1 57.350
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 12:58:34] [Epoch=017/040] [Need: 00:30:26] [LR=0.0100] [Best : Accuracy=42.65, Error=57.35]
  Epoch: [017][000/500]   Time 22.990 (22.990)   Data 22.691 (22.691)   Loss 1.9838 (1.9838)   Prec@1 47.000 (47.000)   Prec@5 85.000 (85.000)   [2025-10-28 12:58:57]
  Epoch: [017][100/500]   Time 0.068 (0.292)   Data 0.001 (0.225)   Loss 2.0391 (2.0080)   Prec@1 40.000 (44.960)   Prec@5 88.000 (85.307)   [2025-10-28 12:59:04]
  Epoch: [017][200/500]   Time 0.067 (0.179)   Data 0.000 (0.113)   Loss 1.9687 (2.0078)   Prec@1 49.000 (45.129)   Prec@5 85.000 (85.383)   [2025-10-28 12:59:10]
  Epoch: [017][300/500]   Time 0.067 (0.142)   Data 0.000 (0.076)   Loss 2.0057 (2.0050)   Prec@1 46.000 (45.346)   Prec@5 81.000 (85.558)   [2025-10-28 12:59:17]
  Epoch: [017][400/500]   Time 0.061 (0.123)   Data 0.001 (0.057)   Loss 2.0101 (2.0053)   Prec@1 46.000 (45.434)   Prec@5 86.000 (85.259)   [2025-10-28 12:59:24]
  **Train** Prec@1 45.512 Prec@5 85.230 Error@1 54.488
  **Test** Prec@1 48.620 Prec@5 87.920 Error@1 51.380
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 12:59:53] [Epoch=018/040] [Need: 00:29:07] [LR=0.0100] [Best : Accuracy=48.62, Error=51.38]
  Epoch: [018][000/500]   Time 24.312 (24.312)   Data 24.009 (24.009)   Loss 2.0306 (2.0306)   Prec@1 42.000 (42.000)   Prec@5 81.000 (81.000)   [2025-10-28 13:00:18]
  Epoch: [018][100/500]   Time 0.063 (0.306)   Data 0.000 (0.238)   Loss 2.0034 (1.9914)   Prec@1 47.000 (47.139)   Prec@5 82.000 (85.545)   [2025-10-28 13:00:24]
  Epoch: [018][200/500]   Time 0.067 (0.187)   Data 0.000 (0.120)   Loss 1.9878 (1.9977)   Prec@1 48.000 (46.174)   Prec@5 89.000 (85.542)   [2025-10-28 13:00:31]
  Epoch: [018][300/500]   Time 0.064 (0.147)   Data 0.001 (0.080)   Loss 1.9787 (1.9987)   Prec@1 49.000 (45.987)   Prec@5 91.000 (85.346)   [2025-10-28 13:00:38]
  Epoch: [018][400/500]   Time 0.068 (0.127)   Data 0.001 (0.060)   Loss 2.0370 (1.9981)   Prec@1 45.000 (46.002)   Prec@5 80.000 (85.429)   [2025-10-28 13:00:44]
  **Train** Prec@1 46.074 Prec@5 85.586 Error@1 53.926
  **Test** Prec@1 43.280 Prec@5 82.200 Error@1 56.720

==>>[2025-10-28 13:01:11] [Epoch=019/040] [Need: 00:27:46] [LR=0.0100] [Best : Accuracy=48.62, Error=51.38]
  Epoch: [019][000/500]   Time 19.099 (19.099)   Data 18.854 (18.854)   Loss 2.0770 (2.0770)   Prec@1 37.000 (37.000)   Prec@5 80.000 (80.000)   [2025-10-28 13:01:30]
  Epoch: [019][100/500]   Time 0.053 (0.242)   Data 0.001 (0.187)   Loss 1.9974 (1.9925)   Prec@1 49.000 (46.574)   Prec@5 82.000 (85.980)   [2025-10-28 13:01:36]
  Epoch: [019][200/500]   Time 0.053 (0.149)   Data 0.000 (0.094)   Loss 1.9255 (1.9877)   Prec@1 55.000 (47.189)   Prec@5 94.000 (86.090)   [2025-10-28 13:01:41]
  Epoch: [019][300/500]   Time 0.057 (0.118)   Data 0.000 (0.063)   Loss 2.0148 (1.9894)   Prec@1 44.000 (47.033)   Prec@5 85.000 (86.113)   [2025-10-28 13:01:47]
  Epoch: [019][400/500]   Time 0.058 (0.102)   Data 0.000 (0.047)   Loss 2.0519 (1.9893)   Prec@1 39.000 (47.035)   Prec@5 85.000 (86.105)   [2025-10-28 13:01:52]
  **Train** Prec@1 47.262 Prec@5 86.258 Error@1 52.738
  **Test** Prec@1 46.400 Prec@5 86.900 Error@1 53.600

==>>[2025-10-28 13:02:19] [Epoch=020/040] [Need: 00:26:15] [LR=0.0100] [Best : Accuracy=48.62, Error=51.38]
  Epoch: [020][000/500]   Time 18.878 (18.878)   Data 18.598 (18.598)   Loss 1.9291 (1.9291)   Prec@1 53.000 (53.000)   Prec@5 86.000 (86.000)   [2025-10-28 13:02:38]
  Epoch: [020][100/500]   Time 0.056 (0.241)   Data 0.000 (0.184)   Loss 1.9932 (1.9800)   Prec@1 48.000 (48.040)   Prec@5 87.000 (87.069)   [2025-10-28 13:02:43]
  Epoch: [020][200/500]   Time 0.056 (0.148)   Data 0.000 (0.093)   Loss 2.0079 (1.9802)   Prec@1 48.000 (48.035)   Prec@5 86.000 (86.766)   [2025-10-28 13:02:48]
  Epoch: [020][300/500]   Time 0.054 (0.118)   Data 0.000 (0.062)   Loss 2.0517 (1.9823)   Prec@1 39.000 (47.751)   Prec@5 91.000 (86.711)   [2025-10-28 13:02:54]
  Epoch: [020][400/500]   Time 0.056 (0.102)   Data 0.000 (0.047)   Loss 1.9390 (1.9818)   Prec@1 54.000 (47.913)   Prec@5 87.000 (86.519)   [2025-10-28 13:03:00]
  **Train** Prec@1 47.814 Prec@5 86.512 Error@1 52.186
  **Test** Prec@1 44.380 Prec@5 86.840 Error@1 55.620

==>>[2025-10-28 13:03:26] [Epoch=021/040] [Need: 00:24:46] [LR=0.0100] [Best : Accuracy=48.62, Error=51.38]
  Epoch: [021][000/500]   Time 19.336 (19.336)   Data 19.059 (19.059)   Loss 1.9778 (1.9778)   Prec@1 49.000 (49.000)   Prec@5 88.000 (88.000)   [2025-10-28 13:03:46]
  Epoch: [021][100/500]   Time 0.051 (0.245)   Data 0.001 (0.189)   Loss 1.8963 (1.9794)   Prec@1 58.000 (47.891)   Prec@5 95.000 (87.386)   [2025-10-28 13:03:51]
  Epoch: [021][200/500]   Time 0.054 (0.151)   Data 0.000 (0.095)   Loss 1.9703 (1.9819)   Prec@1 50.000 (47.637)   Prec@5 90.000 (86.920)   [2025-10-28 13:03:57]
  Epoch: [021][300/500]   Time 0.053 (0.119)   Data 0.001 (0.063)   Loss 1.9542 (1.9831)   Prec@1 52.000 (47.538)   Prec@5 90.000 (86.708)   [2025-10-28 13:04:02]
  Epoch: [021][400/500]   Time 0.058 (0.103)   Data 0.000 (0.048)   Loss 1.9213 (1.9809)   Prec@1 55.000 (47.761)   Prec@5 88.000 (86.658)   [2025-10-28 13:04:08]
  **Train** Prec@1 47.860 Prec@5 86.776 Error@1 52.140
  **Test** Prec@1 46.960 Prec@5 85.710 Error@1 53.040

==>>[2025-10-28 13:04:34] [Epoch=022/040] [Need: 00:23:19] [LR=0.0100] [Best : Accuracy=48.62, Error=51.38]
  Epoch: [022][000/500]   Time 18.099 (18.099)   Data 17.902 (17.902)   Loss 2.0261 (2.0261)   Prec@1 45.000 (45.000)   Prec@5 90.000 (90.000)   [2025-10-28 13:04:52]
  Epoch: [022][100/500]   Time 0.052 (0.232)   Data 0.000 (0.177)   Loss 2.0849 (1.9693)   Prec@1 36.000 (49.178)   Prec@5 78.000 (87.604)   [2025-10-28 13:04:57]
  Epoch: [022][200/500]   Time 0.059 (0.144)   Data 0.001 (0.089)   Loss 2.0202 (1.9740)   Prec@1 44.000 (48.632)   Prec@5 79.000 (87.448)   [2025-10-28 13:05:03]
  Epoch: [022][300/500]   Time 0.055 (0.115)   Data 0.001 (0.060)   Loss 1.9581 (1.9765)   Prec@1 51.000 (48.346)   Prec@5 83.000 (87.186)   [2025-10-28 13:05:08]
  Epoch: [022][400/500]   Time 0.056 (0.100)   Data 0.000 (0.045)   Loss 1.9703 (1.9763)   Prec@1 49.000 (48.367)   Prec@5 80.000 (87.042)   [2025-10-28 13:05:14]
  **Train** Prec@1 48.488 Prec@5 87.088 Error@1 51.512
  **Test** Prec@1 45.570 Prec@5 85.090 Error@1 54.430

==>>[2025-10-28 13:05:39] [Epoch=023/040] [Need: 00:21:52] [LR=0.0100] [Best : Accuracy=48.62, Error=51.38]
  Epoch: [023][000/500]   Time 18.050 (18.050)   Data 17.770 (17.770)   Loss 1.9128 (1.9128)   Prec@1 58.000 (58.000)   Prec@5 89.000 (89.000)   [2025-10-28 13:05:58]
  Epoch: [023][100/500]   Time 0.057 (0.232)   Data 0.000 (0.176)   Loss 1.9194 (1.9756)   Prec@1 53.000 (48.376)   Prec@5 87.000 (87.089)   [2025-10-28 13:06:03]
  Epoch: [023][200/500]   Time 0.056 (0.144)   Data 0.001 (0.089)   Loss 1.9329 (1.9753)   Prec@1 51.000 (48.383)   Prec@5 87.000 (87.403)   [2025-10-28 13:06:08]
  Epoch: [023][300/500]   Time 0.055 (0.114)   Data 0.000 (0.059)   Loss 2.0058 (1.9758)   Prec@1 43.000 (48.322)   Prec@5 81.000 (87.422)   [2025-10-28 13:06:14]
  Epoch: [023][400/500]   Time 0.057 (0.100)   Data 0.000 (0.044)   Loss 1.9811 (1.9727)   Prec@1 49.000 (48.633)   Prec@5 83.000 (87.516)   [2025-10-28 13:06:20]
  **Train** Prec@1 48.578 Prec@5 87.530 Error@1 51.422
  **Test** Prec@1 44.500 Prec@5 85.070 Error@1 55.500

==>>[2025-10-28 13:06:45] [Epoch=024/040] [Need: 00:20:27] [LR=0.0100] [Best : Accuracy=48.62, Error=51.38]
  Epoch: [024][000/500]   Time 17.847 (17.847)   Data 17.565 (17.565)   Loss 1.9574 (1.9574)   Prec@1 52.000 (52.000)   Prec@5 94.000 (94.000)   [2025-10-28 13:07:03]
  Epoch: [024][100/500]   Time 0.051 (0.229)   Data 0.000 (0.174)   Loss 1.9630 (1.9704)   Prec@1 48.000 (48.752)   Prec@5 89.000 (87.564)   [2025-10-28 13:07:08]
  Epoch: [024][200/500]   Time 0.053 (0.142)   Data 0.000 (0.088)   Loss 2.0196 (1.9690)   Prec@1 44.000 (49.050)   Prec@5 89.000 (87.836)   [2025-10-28 13:07:14]
  Epoch: [024][300/500]   Time 0.055 (0.114)   Data 0.000 (0.059)   Loss 1.9225 (1.9694)   Prec@1 54.000 (49.066)   Prec@5 90.000 (87.894)   [2025-10-28 13:07:19]
  Epoch: [024][400/500]   Time 0.058 (0.099)   Data 0.000 (0.044)   Loss 2.0048 (1.9682)   Prec@1 46.000 (49.192)   Prec@5 87.000 (87.898)   [2025-10-28 13:07:25]
  **Train** Prec@1 49.168 Prec@5 87.928 Error@1 50.832
  **Test** Prec@1 41.940 Prec@5 81.600 Error@1 58.060

==>>[2025-10-28 13:07:50] [Epoch=025/040] [Need: 00:19:04] [LR=0.0010] [Best : Accuracy=48.62, Error=51.38]
  Epoch: [025][000/500]   Time 18.464 (18.464)   Data 18.254 (18.254)   Loss 1.9105 (1.9105)   Prec@1 58.000 (58.000)   Prec@5 88.000 (88.000)   [2025-10-28 13:08:09]
  Epoch: [025][100/500]   Time 0.054 (0.236)   Data 0.000 (0.181)   Loss 1.9047 (1.9430)   Prec@1 55.000 (52.059)   Prec@5 89.000 (88.970)   [2025-10-28 13:08:14]
  Epoch: [025][200/500]   Time 0.053 (0.146)   Data 0.000 (0.091)   Loss 2.0095 (1.9451)   Prec@1 45.000 (51.706)   Prec@5 83.000 (88.856)   [2025-10-28 13:08:20]
  Epoch: [025][300/500]   Time 0.054 (0.116)   Data 0.000 (0.061)   Loss 1.8889 (1.9421)   Prec@1 60.000 (52.043)   Prec@5 92.000 (89.086)   [2025-10-28 13:08:25]
  Epoch: [025][400/500]   Time 0.056 (0.101)   Data 0.000 (0.046)   Loss 1.9295 (1.9410)   Prec@1 55.000 (52.057)   Prec@5 88.000 (89.115)   [2025-10-28 13:08:31]
  **Train** Prec@1 52.126 Prec@5 89.254 Error@1 47.874
  **Test** Prec@1 51.490 Prec@5 89.190 Error@1 48.510
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 13:08:57] [Epoch=026/040] [Need: 00:17:42] [LR=0.0010] [Best : Accuracy=51.49, Error=48.51]
  Epoch: [026][000/500]   Time 17.961 (17.961)   Data 17.684 (17.684)   Loss 1.9986 (1.9986)   Prec@1 46.000 (46.000)   Prec@5 88.000 (88.000)   [2025-10-28 13:09:15]
  Epoch: [026][100/500]   Time 0.052 (0.230)   Data 0.000 (0.175)   Loss 2.0145 (1.9286)   Prec@1 44.000 (53.267)   Prec@5 85.000 (89.861)   [2025-10-28 13:09:20]
  Epoch: [026][200/500]   Time 0.053 (0.143)   Data 0.000 (0.088)   Loss 1.8616 (1.9335)   Prec@1 62.000 (52.622)   Prec@5 86.000 (89.677)   [2025-10-28 13:09:25]
  Epoch: [026][300/500]   Time 0.053 (0.114)   Data 0.001 (0.059)   Loss 1.9688 (1.9317)   Prec@1 48.000 (52.924)   Prec@5 92.000 (89.684)   [2025-10-28 13:09:31]
  Epoch: [026][400/500]   Time 0.058 (0.099)   Data 0.001 (0.044)   Loss 1.9172 (1.9315)   Prec@1 50.000 (52.880)   Prec@5 90.000 (89.596)   [2025-10-28 13:09:36]
  **Train** Prec@1 52.972 Prec@5 89.626 Error@1 47.028
  **Test** Prec@1 51.220 Prec@5 88.560 Error@1 48.780

==>>[2025-10-28 13:10:02] [Epoch=027/040] [Need: 00:16:21] [LR=0.0010] [Best : Accuracy=51.49, Error=48.51]
  Epoch: [027][000/500]   Time 17.894 (17.894)   Data 17.681 (17.681)   Loss 1.9573 (1.9573)   Prec@1 49.000 (49.000)   Prec@5 90.000 (90.000)   [2025-10-28 13:10:20]
  Epoch: [027][100/500]   Time 0.056 (0.230)   Data 0.000 (0.175)   Loss 1.8798 (1.9305)   Prec@1 58.000 (53.010)   Prec@5 89.000 (89.030)   [2025-10-28 13:10:25]
  Epoch: [027][200/500]   Time 0.056 (0.143)   Data 0.000 (0.088)   Loss 1.9496 (1.9306)   Prec@1 51.000 (53.030)   Prec@5 85.000 (89.388)   [2025-10-28 13:10:31]
  Epoch: [027][300/500]   Time 0.054 (0.114)   Data 0.000 (0.059)   Loss 1.8477 (1.9315)   Prec@1 62.000 (52.980)   Prec@5 91.000 (89.575)   [2025-10-28 13:10:37]
  Epoch: [027][400/500]   Time 0.060 (0.100)   Data 0.000 (0.044)   Loss 1.9164 (1.9293)   Prec@1 55.000 (53.182)   Prec@5 91.000 (89.818)   [2025-10-28 13:10:42]
  **Train** Prec@1 53.108 Prec@5 89.840 Error@1 46.892
  **Test** Prec@1 51.240 Prec@5 88.340 Error@1 48.760

==>>[2025-10-28 13:11:08] [Epoch=028/040] [Need: 00:15:01] [LR=0.0010] [Best : Accuracy=51.49, Error=48.51]
  Epoch: [028][000/500]   Time 17.923 (17.923)   Data 17.716 (17.716)   Loss 1.8842 (1.8842)   Prec@1 59.000 (59.000)   Prec@5 90.000 (90.000)   [2025-10-28 13:11:26]
  Epoch: [028][100/500]   Time 0.052 (0.230)   Data 0.000 (0.176)   Loss 1.8964 (1.9229)   Prec@1 57.000 (53.980)   Prec@5 94.000 (90.475)   [2025-10-28 13:11:31]
  Epoch: [028][200/500]   Time 0.052 (0.143)   Data 0.000 (0.088)   Loss 1.9895 (1.9264)   Prec@1 47.000 (53.557)   Prec@5 82.000 (90.025)   [2025-10-28 13:11:37]
  Epoch: [028][300/500]   Time 0.053 (0.114)   Data 0.000 (0.059)   Loss 1.9697 (1.9277)   Prec@1 47.000 (53.405)   Prec@5 88.000 (89.797)   [2025-10-28 13:11:42]
  Epoch: [028][400/500]   Time 0.054 (0.100)   Data 0.000 (0.044)   Loss 1.9165 (1.9290)   Prec@1 55.000 (53.224)   Prec@5 88.000 (89.796)   [2025-10-28 13:11:48]
  **Train** Prec@1 53.338 Prec@5 89.886 Error@1 46.662
  **Test** Prec@1 50.960 Prec@5 88.530 Error@1 49.040

==>>[2025-10-28 13:12:14] [Epoch=029/040] [Need: 00:13:43] [LR=0.0010] [Best : Accuracy=51.49, Error=48.51]
  Epoch: [029][000/500]   Time 18.034 (18.034)   Data 17.762 (17.762)   Loss 1.8860 (1.8860)   Prec@1 58.000 (58.000)   Prec@5 88.000 (88.000)   [2025-10-28 13:12:32]
  Epoch: [029][100/500]   Time 0.055 (0.232)   Data 0.001 (0.176)   Loss 1.8962 (1.9227)   Prec@1 57.000 (53.693)   Prec@5 89.000 (89.881)   [2025-10-28 13:12:37]
  Epoch: [029][200/500]   Time 0.056 (0.144)   Data 0.000 (0.089)   Loss 1.8753 (1.9234)   Prec@1 58.000 (53.562)   Prec@5 95.000 (90.119)   [2025-10-28 13:12:43]
  Epoch: [029][300/500]   Time 0.056 (0.115)   Data 0.000 (0.059)   Loss 1.9237 (1.9242)   Prec@1 56.000 (53.538)   Prec@5 85.000 (89.960)   [2025-10-28 13:12:48]
  Epoch: [029][400/500]   Time 0.054 (0.100)   Data 0.000 (0.044)   Loss 2.0090 (1.9241)   Prec@1 47.000 (53.616)   Prec@5 88.000 (90.000)   [2025-10-28 13:12:54]
  **Train** Prec@1 53.386 Prec@5 89.982 Error@1 46.614
  **Test** Prec@1 51.610 Prec@5 89.340 Error@1 48.390
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 13:13:19] [Epoch=030/040] [Need: 00:12:25] [LR=0.0010] [Best : Accuracy=51.61, Error=48.39]
  Epoch: [030][000/500]   Time 17.897 (17.897)   Data 17.629 (17.629)   Loss 1.9060 (1.9060)   Prec@1 59.000 (59.000)   Prec@5 91.000 (91.000)   [2025-10-28 13:13:37]
  Epoch: [030][100/500]   Time 0.055 (0.230)   Data 0.000 (0.175)   Loss 1.8397 (1.9169)   Prec@1 63.000 (54.752)   Prec@5 95.000 (89.891)   [2025-10-28 13:13:43]
  Epoch: [030][200/500]   Time 0.056 (0.143)   Data 0.000 (0.088)   Loss 1.9489 (1.9219)   Prec@1 51.000 (53.945)   Prec@5 94.000 (90.080)   [2025-10-28 13:13:48]
  Epoch: [030][300/500]   Time 0.058 (0.114)   Data 0.000 (0.059)   Loss 1.8363 (1.9211)   Prec@1 64.000 (53.967)   Prec@5 94.000 (90.163)   [2025-10-28 13:13:54]
  Epoch: [030][400/500]   Time 0.056 (0.100)   Data 0.000 (0.044)   Loss 1.9122 (1.9211)   Prec@1 53.000 (53.958)   Prec@5 88.000 (90.354)   [2025-10-28 13:13:59]
  **Train** Prec@1 53.864 Prec@5 90.346 Error@1 46.136
  **Test** Prec@1 52.200 Prec@5 89.390 Error@1 47.800
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 13:14:25] [Epoch=031/040] [Need: 00:11:08] [LR=0.0010] [Best : Accuracy=52.20, Error=47.80]
  Epoch: [031][000/500]   Time 17.804 (17.804)   Data 17.575 (17.575)   Loss 1.9425 (1.9425)   Prec@1 51.000 (51.000)   Prec@5 88.000 (88.000)   [2025-10-28 13:14:43]
  Epoch: [031][100/500]   Time 0.051 (0.229)   Data 0.000 (0.174)   Loss 1.8450 (1.9196)   Prec@1 64.000 (54.228)   Prec@5 89.000 (90.475)   [2025-10-28 13:14:48]
  Epoch: [031][200/500]   Time 0.054 (0.142)   Data 0.000 (0.088)   Loss 1.9542 (1.9221)   Prec@1 51.000 (53.985)   Prec@5 87.000 (90.264)   [2025-10-28 13:14:54]
  Epoch: [031][300/500]   Time 0.055 (0.114)   Data 0.000 (0.059)   Loss 1.8671 (1.9229)   Prec@1 60.000 (53.917)   Prec@5 93.000 (90.239)   [2025-10-28 13:14:59]
  Epoch: [031][400/500]   Time 0.058 (0.099)   Data 0.000 (0.044)   Loss 1.9356 (1.9226)   Prec@1 53.000 (53.935)   Prec@5 92.000 (90.242)   [2025-10-28 13:15:05]
  **Train** Prec@1 53.936 Prec@5 90.320 Error@1 46.064
  **Test** Prec@1 50.580 Prec@5 88.040 Error@1 49.420

==>>[2025-10-28 13:15:31] [Epoch=032/040] [Need: 00:09:51] [LR=0.0010] [Best : Accuracy=52.20, Error=47.80]
  Epoch: [032][000/500]   Time 17.891 (17.891)   Data 17.617 (17.617)   Loss 1.9176 (1.9176)   Prec@1 57.000 (57.000)   Prec@5 91.000 (91.000)   [2025-10-28 13:15:48]
  Epoch: [032][100/500]   Time 0.054 (0.231)   Data 0.000 (0.175)   Loss 1.8859 (1.9165)   Prec@1 58.000 (54.426)   Prec@5 89.000 (90.733)   [2025-10-28 13:15:54]
  Epoch: [032][200/500]   Time 0.056 (0.143)   Data 0.000 (0.088)   Loss 1.8622 (1.9193)   Prec@1 59.000 (54.104)   Prec@5 93.000 (90.612)   [2025-10-28 13:15:59]
  Epoch: [032][300/500]   Time 0.056 (0.114)   Data 0.001 (0.059)   Loss 1.9255 (1.9201)   Prec@1 52.000 (54.040)   Prec@5 93.000 (90.628)   [2025-10-28 13:16:05]
  Epoch: [032][400/500]   Time 0.058 (0.100)   Data 0.000 (0.044)   Loss 1.9515 (1.9217)   Prec@1 51.000 (53.833)   Prec@5 93.000 (90.671)   [2025-10-28 13:16:11]
  **Train** Prec@1 53.688 Prec@5 90.536 Error@1 46.312
  **Test** Prec@1 52.040 Prec@5 89.010 Error@1 47.960

==>>[2025-10-28 13:16:37] [Epoch=033/040] [Need: 00:08:36] [LR=0.0010] [Best : Accuracy=52.20, Error=47.80]
  Epoch: [033][000/500]   Time 18.037 (18.037)   Data 17.766 (17.766)   Loss 1.8959 (1.8959)   Prec@1 58.000 (58.000)   Prec@5 92.000 (92.000)   [2025-10-28 13:16:55]
  Epoch: [033][100/500]   Time 0.055 (0.233)   Data 0.000 (0.176)   Loss 1.8851 (1.9250)   Prec@1 60.000 (53.545)   Prec@5 90.000 (89.792)   [2025-10-28 13:17:00]
  Epoch: [033][200/500]   Time 0.055 (0.144)   Data 0.000 (0.089)   Loss 1.8765 (1.9204)   Prec@1 59.000 (54.134)   Prec@5 89.000 (90.000)   [2025-10-28 13:17:06]
  Epoch: [033][300/500]   Time 0.055 (0.115)   Data 0.000 (0.059)   Loss 1.9966 (1.9223)   Prec@1 45.000 (53.963)   Prec@5 86.000 (90.116)   [2025-10-28 13:17:11]
  Epoch: [033][400/500]   Time 0.057 (0.100)   Data 0.000 (0.044)   Loss 1.8961 (1.9218)   Prec@1 59.000 (53.998)   Prec@5 90.000 (90.177)   [2025-10-28 13:17:17]
  **Train** Prec@1 54.186 Prec@5 90.302 Error@1 45.814
  **Test** Prec@1 51.540 Prec@5 88.890 Error@1 48.460

==>>[2025-10-28 13:17:42] [Epoch=034/040] [Need: 00:07:20] [LR=0.0010] [Best : Accuracy=52.20, Error=47.80]
  Epoch: [034][000/500]   Time 17.891 (17.891)   Data 17.608 (17.608)   Loss 1.9075 (1.9075)   Prec@1 53.000 (53.000)   Prec@5 90.000 (90.000)   [2025-10-28 13:18:00]
  Epoch: [034][100/500]   Time 0.052 (0.230)   Data 0.001 (0.174)   Loss 1.9585 (1.9191)   Prec@1 49.000 (54.079)   Prec@5 89.000 (90.584)   [2025-10-28 13:18:05]
  Epoch: [034][200/500]   Time 0.057 (0.143)   Data 0.000 (0.088)   Loss 1.9029 (1.9157)   Prec@1 54.000 (54.512)   Prec@5 89.000 (90.657)   [2025-10-28 13:18:11]
  Epoch: [034][300/500]   Time 0.061 (0.114)   Data 0.000 (0.059)   Loss 1.9111 (1.9158)   Prec@1 57.000 (54.522)   Prec@5 91.000 (90.738)   [2025-10-28 13:18:17]
  Epoch: [034][400/500]   Time 0.059 (0.100)   Data 0.000 (0.044)   Loss 1.8371 (1.9168)   Prec@1 61.000 (54.339)   Prec@5 93.000 (90.711)   [2025-10-28 13:18:22]
  **Train** Prec@1 54.324 Prec@5 90.642 Error@1 45.676
  **Test** Prec@1 51.350 Prec@5 88.440 Error@1 48.650

==>>[2025-10-28 13:18:48] [Epoch=035/040] [Need: 00:06:06] [LR=0.0010] [Best : Accuracy=52.20, Error=47.80]
  Epoch: [035][000/500]   Time 18.022 (18.022)   Data 17.748 (17.748)   Loss 1.9716 (1.9716)   Prec@1 46.000 (46.000)   Prec@5 85.000 (85.000)   [2025-10-28 13:19:06]
  Epoch: [035][100/500]   Time 0.054 (0.232)   Data 0.000 (0.176)   Loss 1.9397 (1.9181)   Prec@1 50.000 (54.376)   Prec@5 86.000 (90.446)   [2025-10-28 13:19:11]
  Epoch: [035][200/500]   Time 0.057 (0.144)   Data 0.000 (0.088)   Loss 1.9525 (1.9168)   Prec@1 52.000 (54.388)   Prec@5 90.000 (90.871)   [2025-10-28 13:19:17]
  Epoch: [035][300/500]   Time 0.056 (0.115)   Data 0.000 (0.059)   Loss 1.9272 (1.9152)   Prec@1 54.000 (54.575)   Prec@5 90.000 (90.761)   [2025-10-28 13:19:22]
  Epoch: [035][400/500]   Time 0.056 (0.100)   Data 0.001 (0.044)   Loss 1.9251 (1.9147)   Prec@1 53.000 (54.638)   Prec@5 89.000 (90.763)   [2025-10-28 13:19:28]
  **Train** Prec@1 54.576 Prec@5 90.792 Error@1 45.424
  **Test** Prec@1 52.260 Prec@5 89.270 Error@1 47.740
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 13:19:54] [Epoch=036/040] [Need: 00:04:52] [LR=0.0010] [Best : Accuracy=52.26, Error=47.74]
  Epoch: [036][000/500]   Time 18.030 (18.030)   Data 17.812 (17.812)   Loss 1.8748 (1.8748)   Prec@1 56.000 (56.000)   Prec@5 94.000 (94.000)   [2025-10-28 13:20:12]
  Epoch: [036][100/500]   Time 0.055 (0.231)   Data 0.000 (0.176)   Loss 1.8887 (1.9065)   Prec@1 57.000 (55.238)   Prec@5 90.000 (91.030)   [2025-10-28 13:20:17]
  Epoch: [036][200/500]   Time 0.054 (0.144)   Data 0.000 (0.089)   Loss 1.9571 (1.9143)   Prec@1 49.000 (54.597)   Prec@5 90.000 (90.741)   [2025-10-28 13:20:23]
  Epoch: [036][300/500]   Time 0.057 (0.114)   Data 0.000 (0.059)   Loss 1.9149 (1.9176)   Prec@1 56.000 (54.216)   Prec@5 92.000 (90.691)   [2025-10-28 13:20:28]
  Epoch: [036][400/500]   Time 0.061 (0.100)   Data 0.000 (0.045)   Loss 2.0047 (1.9190)   Prec@1 45.000 (54.152)   Prec@5 86.000 (90.648)   [2025-10-28 13:20:34]
  **Train** Prec@1 54.218 Prec@5 90.606 Error@1 45.782
  **Test** Prec@1 52.790 Prec@5 89.480 Error@1 47.210
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 13:21:00] [Epoch=037/040] [Need: 00:03:38] [LR=0.0010] [Best : Accuracy=52.79, Error=47.21]
  Epoch: [037][000/500]   Time 18.010 (18.010)   Data 17.754 (17.754)   Loss 1.9578 (1.9578)   Prec@1 48.000 (48.000)   Prec@5 90.000 (90.000)   [2025-10-28 13:21:18]
  Epoch: [037][100/500]   Time 0.053 (0.231)   Data 0.000 (0.176)   Loss 1.9310 (1.9225)   Prec@1 52.000 (53.663)   Prec@5 92.000 (90.446)   [2025-10-28 13:21:23]
  Epoch: [037][200/500]   Time 0.056 (0.143)   Data 0.000 (0.089)   Loss 1.9332 (1.9214)   Prec@1 51.000 (53.841)   Prec@5 96.000 (90.502)   [2025-10-28 13:21:29]
  Epoch: [037][300/500]   Time 0.055 (0.114)   Data 0.000 (0.059)   Loss 1.8941 (1.9200)   Prec@1 58.000 (53.963)   Prec@5 92.000 (90.781)   [2025-10-28 13:21:34]
  Epoch: [037][400/500]   Time 0.056 (0.099)   Data 0.000 (0.044)   Loss 1.9303 (1.9207)   Prec@1 51.000 (53.870)   Prec@5 91.000 (90.681)   [2025-10-28 13:21:40]
  **Train** Prec@1 54.154 Prec@5 90.662 Error@1 45.846
  **Test** Prec@1 52.390 Prec@5 89.190 Error@1 47.610

==>>[2025-10-28 13:22:05] [Epoch=038/040] [Need: 00:02:25] [LR=0.0010] [Best : Accuracy=52.79, Error=47.21]
  Epoch: [038][000/500]   Time 17.780 (17.780)   Data 17.507 (17.507)   Loss 1.8870 (1.8870)   Prec@1 57.000 (57.000)   Prec@5 90.000 (90.000)   [2025-10-28 13:22:23]
  Epoch: [038][100/500]   Time 0.053 (0.230)   Data 0.000 (0.173)   Loss 1.9242 (1.9165)   Prec@1 53.000 (54.634)   Prec@5 94.000 (90.505)   [2025-10-28 13:22:28]
  Epoch: [038][200/500]   Time 0.057 (0.143)   Data 0.000 (0.087)   Loss 1.8843 (1.9139)   Prec@1 57.000 (54.861)   Prec@5 91.000 (90.652)   [2025-10-28 13:22:34]
  Epoch: [038][300/500]   Time 0.055 (0.114)   Data 0.000 (0.058)   Loss 1.8957 (1.9116)   Prec@1 57.000 (55.136)   Prec@5 89.000 (90.894)   [2025-10-28 13:22:39]
  Epoch: [038][400/500]   Time 0.057 (0.100)   Data 0.000 (0.044)   Loss 1.8824 (1.9122)   Prec@1 57.000 (54.975)   Prec@5 89.000 (90.988)   [2025-10-28 13:22:45]
  **Train** Prec@1 54.872 Prec@5 91.008 Error@1 45.128
  **Test** Prec@1 51.380 Prec@5 88.910 Error@1 48.620

==>>[2025-10-28 13:23:11] [Epoch=039/040] [Need: 00:01:12] [LR=0.0010] [Best : Accuracy=52.79, Error=47.21]
  Epoch: [039][000/500]   Time 18.140 (18.140)   Data 17.868 (17.868)   Loss 1.9307 (1.9307)   Prec@1 51.000 (51.000)   Prec@5 92.000 (92.000)   [2025-10-28 13:23:29]
  Epoch: [039][100/500]   Time 0.055 (0.233)   Data 0.000 (0.177)   Loss 1.9547 (1.9112)   Prec@1 52.000 (55.040)   Prec@5 90.000 (90.752)   [2025-10-28 13:23:34]
  Epoch: [039][200/500]   Time 0.050 (0.144)   Data 0.000 (0.089)   Loss 1.9818 (1.9143)   Prec@1 47.000 (54.721)   Prec@5 89.000 (90.945)   [2025-10-28 13:23:40]
  Epoch: [039][300/500]   Time 0.053 (0.115)   Data 0.000 (0.060)   Loss 1.8904 (1.9149)   Prec@1 56.000 (54.631)   Prec@5 95.000 (90.821)   [2025-10-28 13:23:45]
  Epoch: [039][400/500]   Time 0.056 (0.100)   Data 0.000 (0.045)   Loss 1.9049 (1.9145)   Prec@1 55.000 (54.691)   Prec@5 95.000 (90.830)   [2025-10-28 13:23:51]
  **Train** Prec@1 54.834 Prec@5 90.966 Error@1 45.166
  **Test** Prec@1 52.520 Prec@5 90.130 Error@1 47.480
