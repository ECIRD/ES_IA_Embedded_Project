save path : ./save/resnet9_quan/clipping_0.2_0.01
{'data_path': './dataset', 'arch': 'resnet9_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': False, 'clipping_coeff': 0.2, 'learning_rate': 0.01, 'manualSeed': 4211, 'save_path': './save/resnet9_quan/clipping_0.2_0.01', 'enable_bfa': False, 'resume': '', 'quan_bitwidth': None, 'reset_weight': False, 'evaluate': False, 'n_iter': 30, 'model_only': False, 'random_bfa': False, 'use_cuda': True}
Random Seed: 4211
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'resnet9_quan'
=> network :
 Sequential(
  (0): Sequential(
    (0): quan_Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (1): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (2): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (4): Dropout2d(p=0.2, inplace=False)
  (5): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (skip): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (6): Residual(
    (module): Sequential(
      (0): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Sequential(
        (0): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Dropout2d(p=0.3, inplace=False)
  (9): Sequential(
    (0): quan_Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (10): Sequential(
    (0): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (12): Dropout2d(p=0.3, inplace=False)
  (13): Sequential(
    (0): quan_Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (14): AdaptiveAvgPool2d(output_size=(1, 1))
  (15): Flatten()
  (16): quan_Linear(in_features=64, out_features=128, bias=False)
  (17): ReLU(inplace=True)
  (18): Dropout(p=0.35, inplace=False)
  (19): quan_Linear(in_features=128, out_features=10, bias=False)
  (20): Softmax(dim=1)
)
=> do not use any checkpoint for resnet9_quan model

==>>[2025-10-28 13:24:24] [Epoch=000/040] [Need: 00:00:00] [LR=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/500]   Time 18.408 (18.408)   Data 17.937 (17.937)   Loss 2.3063 (2.3063)   Prec@1 7.000 (7.000)   Prec@5 43.000 (43.000)   [2025-10-28 13:24:43]
  Epoch: [000][100/500]   Time 0.054 (0.235)   Data 0.000 (0.178)   Loss 2.3046 (2.3027)   Prec@1 7.000 (10.347)   Prec@5 46.000 (50.208)   [2025-10-28 13:24:48]
  Epoch: [000][200/500]   Time 0.055 (0.145)   Data 0.000 (0.089)   Loss 2.3006 (2.3022)   Prec@1 11.000 (10.781)   Prec@5 57.000 (50.831)   [2025-10-28 13:24:53]
  Epoch: [000][300/500]   Time 0.052 (0.115)   Data 0.000 (0.060)   Loss 2.2953 (2.3014)   Prec@1 14.000 (11.003)   Prec@5 60.000 (51.608)   [2025-10-28 13:24:59]
  Epoch: [000][400/500]   Time 0.060 (0.101)   Data 0.000 (0.045)   Loss 2.2879 (2.2994)   Prec@1 10.000 (11.294)   Prec@5 62.000 (52.466)   [2025-10-28 13:25:05]
  **Train** Prec@1 12.024 Prec@5 53.876 Error@1 87.976
  **Test** Prec@1 21.180 Prec@5 64.860 Error@1 78.820
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 13:25:30] [Epoch=001/040] [Need: 00:42:55] [LR=0.0100] [Best : Accuracy=21.18, Error=78.82]
  Epoch: [001][000/500]   Time 17.971 (17.971)   Data 17.699 (17.699)   Loss 2.2840 (2.2840)   Prec@1 17.000 (17.000)   Prec@5 67.000 (67.000)   [2025-10-28 13:25:48]
  Epoch: [001][100/500]   Time 0.054 (0.232)   Data 0.000 (0.175)   Loss 2.2556 (2.2693)   Prec@1 28.000 (17.752)   Prec@5 72.000 (66.347)   [2025-10-28 13:25:54]
  Epoch: [001][200/500]   Time 0.057 (0.144)   Data 0.000 (0.088)   Loss 2.2552 (2.2647)   Prec@1 20.000 (18.234)   Prec@5 68.000 (68.706)   [2025-10-28 13:25:59]
  Epoch: [001][300/500]   Time 0.056 (0.115)   Data 0.001 (0.059)   Loss 2.2639 (2.2568)   Prec@1 18.000 (19.279)   Prec@5 74.000 (70.635)   [2025-10-28 13:26:05]
  Epoch: [001][400/500]   Time 0.058 (0.100)   Data 0.000 (0.044)   Loss 2.2200 (2.2505)   Prec@1 20.000 (19.608)   Prec@5 84.000 (71.663)   [2025-10-28 13:26:10]
  **Train** Prec@1 19.944 Prec@5 72.484 Error@1 80.056
  **Test** Prec@1 21.620 Prec@5 73.860 Error@1 78.380
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 13:26:36] [Epoch=002/040] [Need: 00:41:44] [LR=0.0100] [Best : Accuracy=21.62, Error=78.38]
  Epoch: [002][000/500]   Time 17.775 (17.775)   Data 17.573 (17.573)   Loss 2.2061 (2.2061)   Prec@1 25.000 (25.000)   Prec@5 75.000 (75.000)   [2025-10-28 13:26:54]
  Epoch: [002][100/500]   Time 0.051 (0.229)   Data 0.000 (0.174)   Loss 2.2260 (2.2143)   Prec@1 18.000 (22.109)   Prec@5 81.000 (75.842)   [2025-10-28 13:26:59]
  Epoch: [002][200/500]   Time 0.054 (0.142)   Data 0.000 (0.088)   Loss 2.1870 (2.2120)   Prec@1 26.000 (22.134)   Prec@5 85.000 (76.104)   [2025-10-28 13:27:05]
  Epoch: [002][300/500]   Time 0.056 (0.114)   Data 0.001 (0.059)   Loss 2.1803 (2.2089)   Prec@1 26.000 (22.498)   Prec@5 77.000 (76.076)   [2025-10-28 13:27:10]
  Epoch: [002][400/500]   Time 0.055 (0.099)   Data 0.000 (0.044)   Loss 2.2219 (2.2065)   Prec@1 20.000 (22.733)   Prec@5 82.000 (76.155)   [2025-10-28 13:27:16]
  **Train** Prec@1 22.704 Prec@5 76.288 Error@1 77.296
  **Test** Prec@1 21.320 Prec@5 73.120 Error@1 78.680

==>>[2025-10-28 13:27:42] [Epoch=003/040] [Need: 00:40:34] [LR=0.0100] [Best : Accuracy=21.62, Error=78.38]
  Epoch: [003][000/500]   Time 18.015 (18.015)   Data 17.740 (17.740)   Loss 2.1750 (2.1750)   Prec@1 26.000 (26.000)   Prec@5 71.000 (71.000)   [2025-10-28 13:28:00]
  Epoch: [003][100/500]   Time 0.054 (0.232)   Data 0.000 (0.176)   Loss 2.1496 (2.1939)   Prec@1 34.000 (24.347)   Prec@5 89.000 (77.802)   [2025-10-28 13:28:05]
  Epoch: [003][200/500]   Time 0.056 (0.144)   Data 0.000 (0.088)   Loss 2.1951 (2.1936)   Prec@1 22.000 (24.090)   Prec@5 76.000 (77.801)   [2025-10-28 13:28:11]
  Epoch: [003][300/500]   Time 0.060 (0.114)   Data 0.000 (0.059)   Loss 2.2033 (2.1921)   Prec@1 24.000 (24.282)   Prec@5 73.000 (77.860)   [2025-10-28 13:28:16]
  Epoch: [003][400/500]   Time 0.059 (0.100)   Data 0.000 (0.044)   Loss 2.1856 (2.1911)   Prec@1 28.000 (24.401)   Prec@5 77.000 (77.716)   [2025-10-28 13:28:22]
  **Train** Prec@1 24.558 Prec@5 77.916 Error@1 75.442
  **Test** Prec@1 26.350 Prec@5 78.850 Error@1 73.650
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 13:28:47] [Epoch=004/040] [Need: 00:39:28] [LR=0.0100] [Best : Accuracy=26.35, Error=73.65]
  Epoch: [004][000/500]   Time 17.882 (17.882)   Data 17.652 (17.652)   Loss 2.2062 (2.2062)   Prec@1 23.000 (23.000)   Prec@5 80.000 (80.000)   [2025-10-28 13:29:05]
  Epoch: [004][100/500]   Time 0.053 (0.230)   Data 0.000 (0.175)   Loss 2.1615 (2.1831)   Prec@1 28.000 (25.495)   Prec@5 75.000 (78.515)   [2025-10-28 13:29:11]
  Epoch: [004][200/500]   Time 0.056 (0.143)   Data 0.000 (0.088)   Loss 2.1744 (2.1798)   Prec@1 25.000 (26.010)   Prec@5 80.000 (78.667)   [2025-10-28 13:29:16]
  Epoch: [004][300/500]   Time 0.054 (0.114)   Data 0.000 (0.059)   Loss 2.1792 (2.1774)   Prec@1 23.000 (26.229)   Prec@5 82.000 (78.983)   [2025-10-28 13:29:22]
  Epoch: [004][400/500]   Time 0.055 (0.099)   Data 0.000 (0.044)   Loss 2.1425 (2.1762)   Prec@1 30.000 (26.272)   Prec@5 79.000 (79.157)   [2025-10-28 13:29:27]
  **Train** Prec@1 26.212 Prec@5 79.200 Error@1 73.788
  **Test** Prec@1 28.610 Prec@5 80.920 Error@1 71.390
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 13:29:53] [Epoch=005/040] [Need: 00:38:22] [LR=0.0100] [Best : Accuracy=28.61, Error=71.39]
  Epoch: [005][000/500]   Time 17.827 (17.827)   Data 17.556 (17.556)   Loss 2.1938 (2.1938)   Prec@1 24.000 (24.000)   Prec@5 76.000 (76.000)   [2025-10-28 13:30:11]
  Epoch: [005][100/500]   Time 0.056 (0.230)   Data 0.000 (0.174)   Loss 2.1232 (2.1673)   Prec@1 33.000 (27.446)   Prec@5 76.000 (79.554)   [2025-10-28 13:30:16]
  Epoch: [005][200/500]   Time 0.051 (0.143)   Data 0.000 (0.088)   Loss 2.1284 (2.1657)   Prec@1 34.000 (27.537)   Prec@5 79.000 (79.582)   [2025-10-28 13:30:22]
  Epoch: [005][300/500]   Time 0.057 (0.114)   Data 0.000 (0.058)   Loss 2.1803 (2.1643)   Prec@1 22.000 (27.425)   Prec@5 77.000 (79.422)   [2025-10-28 13:30:28]
  Epoch: [005][400/500]   Time 0.058 (0.100)   Data 0.000 (0.044)   Loss 2.1641 (2.1623)   Prec@1 28.000 (27.701)   Prec@5 80.000 (79.387)   [2025-10-28 13:30:33]
  **Train** Prec@1 27.728 Prec@5 79.518 Error@1 72.272
  **Test** Prec@1 31.060 Prec@5 80.970 Error@1 68.940
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 13:30:59] [Epoch=006/040] [Need: 00:37:15] [LR=0.0100] [Best : Accuracy=31.06, Error=68.94]
  Epoch: [006][000/500]   Time 17.810 (17.810)   Data 17.561 (17.561)   Loss 2.1977 (2.1977)   Prec@1 23.000 (23.000)   Prec@5 69.000 (69.000)   [2025-10-28 13:31:17]
  Epoch: [006][100/500]   Time 0.051 (0.229)   Data 0.000 (0.174)   Loss 2.2102 (2.1542)   Prec@1 21.000 (28.762)   Prec@5 77.000 (79.861)   [2025-10-28 13:31:22]
  Epoch: [006][200/500]   Time 0.053 (0.143)   Data 0.000 (0.088)   Loss 2.1244 (2.1510)   Prec@1 32.000 (29.239)   Prec@5 78.000 (79.667)   [2025-10-28 13:31:27]
  Epoch: [006][300/500]   Time 0.059 (0.114)   Data 0.000 (0.058)   Loss 2.1793 (2.1513)   Prec@1 22.000 (29.156)   Prec@5 77.000 (79.598)   [2025-10-28 13:31:33]
  Epoch: [006][400/500]   Time 0.056 (0.099)   Data 0.000 (0.044)   Loss 2.1275 (2.1514)   Prec@1 33.000 (29.112)   Prec@5 80.000 (79.474)   [2025-10-28 13:31:39]
  **Train** Prec@1 29.404 Prec@5 79.276 Error@1 70.596
  **Test** Prec@1 27.020 Prec@5 76.230 Error@1 72.980

==>>[2025-10-28 13:32:04] [Epoch=007/040] [Need: 00:36:08] [LR=0.0100] [Best : Accuracy=31.06, Error=68.94]
  Epoch: [007][000/500]   Time 17.939 (17.939)   Data 17.713 (17.713)   Loss 2.1525 (2.1525)   Prec@1 28.000 (28.000)   Prec@5 79.000 (79.000)   [2025-10-28 13:32:22]
  Epoch: [007][100/500]   Time 0.056 (0.230)   Data 0.000 (0.176)   Loss 2.1334 (2.1395)   Prec@1 30.000 (30.554)   Prec@5 78.000 (78.891)   [2025-10-28 13:32:28]
  Epoch: [007][200/500]   Time 0.053 (0.143)   Data 0.000 (0.088)   Loss 2.1562 (2.1391)   Prec@1 31.000 (30.697)   Prec@5 76.000 (79.438)   [2025-10-28 13:32:33]
  Epoch: [007][300/500]   Time 0.053 (0.114)   Data 0.000 (0.059)   Loss 2.1434 (2.1401)   Prec@1 33.000 (30.757)   Prec@5 77.000 (79.435)   [2025-10-28 13:32:39]
  Epoch: [007][400/500]   Time 0.056 (0.100)   Data 0.000 (0.044)   Loss 2.1090 (2.1381)   Prec@1 30.000 (30.993)   Prec@5 83.000 (79.594)   [2025-10-28 13:32:44]
  **Train** Prec@1 31.192 Prec@5 79.628 Error@1 68.808
  **Test** Prec@1 33.440 Prec@5 80.670 Error@1 66.560
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 13:33:10] [Epoch=008/040] [Need: 00:35:01] [LR=0.0100] [Best : Accuracy=33.44, Error=66.56]
  Epoch: [008][000/500]   Time 17.984 (17.984)   Data 17.711 (17.711)   Loss 2.1643 (2.1643)   Prec@1 26.000 (26.000)   Prec@5 76.000 (76.000)   [2025-10-28 13:33:28]
  Epoch: [008][100/500]   Time 0.053 (0.231)   Data 0.001 (0.175)   Loss 2.1697 (2.1308)   Prec@1 27.000 (32.248)   Prec@5 80.000 (80.208)   [2025-10-28 13:33:33]
  Epoch: [008][200/500]   Time 0.052 (0.143)   Data 0.000 (0.088)   Loss 2.0848 (2.1305)   Prec@1 39.000 (32.323)   Prec@5 79.000 (79.920)   [2025-10-28 13:33:39]
  Epoch: [008][300/500]   Time 0.056 (0.114)   Data 0.000 (0.059)   Loss 2.1540 (2.1294)   Prec@1 28.000 (32.329)   Prec@5 73.000 (79.877)   [2025-10-28 13:33:44]
  Epoch: [008][400/500]   Time 0.057 (0.100)   Data 0.000 (0.044)   Loss 2.1498 (2.1289)   Prec@1 29.000 (32.324)   Prec@5 80.000 (79.883)   [2025-10-28 13:33:50]
  **Train** Prec@1 32.626 Prec@5 79.928 Error@1 67.374
  **Test** Prec@1 29.250 Prec@5 78.190 Error@1 70.750

==>>[2025-10-28 13:34:15] [Epoch=009/040] [Need: 00:33:55] [LR=0.0100] [Best : Accuracy=33.44, Error=66.56]
  Epoch: [009][000/500]   Time 17.928 (17.928)   Data 17.650 (17.650)   Loss 2.1162 (2.1162)   Prec@1 31.000 (31.000)   Prec@5 82.000 (82.000)   [2025-10-28 13:34:33]
  Epoch: [009][100/500]   Time 0.058 (0.231)   Data 0.001 (0.175)   Loss 2.1433 (2.1157)   Prec@1 29.000 (33.356)   Prec@5 75.000 (79.653)   [2025-10-28 13:34:39]
  Epoch: [009][200/500]   Time 0.055 (0.144)   Data 0.001 (0.088)   Loss 2.0696 (2.1118)   Prec@1 38.000 (34.025)   Prec@5 79.000 (80.358)   [2025-10-28 13:34:44]
  Epoch: [009][300/500]   Time 0.060 (0.115)   Data 0.001 (0.059)   Loss 2.1472 (2.1098)   Prec@1 30.000 (34.299)   Prec@5 81.000 (80.585)   [2025-10-28 13:34:50]
  Epoch: [009][400/500]   Time 0.056 (0.100)   Data 0.000 (0.044)   Loss 2.1766 (2.1116)   Prec@1 23.000 (34.087)   Prec@5 79.000 (80.364)   [2025-10-28 13:34:55]
  **Train** Prec@1 34.372 Prec@5 80.492 Error@1 65.628
  **Test** Prec@1 37.840 Prec@5 83.240 Error@1 62.160
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 13:35:21] [Epoch=010/040] [Need: 00:32:50] [LR=0.0100] [Best : Accuracy=37.84, Error=62.16]
  Epoch: [010][000/500]   Time 17.847 (17.847)   Data 17.573 (17.573)   Loss 2.1471 (2.1471)   Prec@1 27.000 (27.000)   Prec@5 74.000 (74.000)   [2025-10-28 13:35:39]
  Epoch: [010][100/500]   Time 0.054 (0.230)   Data 0.000 (0.174)   Loss 2.1435 (2.1038)   Prec@1 30.000 (35.287)   Prec@5 79.000 (80.277)   [2025-10-28 13:35:44]
  Epoch: [010][200/500]   Time 0.055 (0.143)   Data 0.000 (0.088)   Loss 2.1120 (2.0984)   Prec@1 34.000 (35.801)   Prec@5 84.000 (80.751)   [2025-10-28 13:35:50]
  Epoch: [010][300/500]   Time 0.055 (0.114)   Data 0.000 (0.059)   Loss 2.0923 (2.0997)   Prec@1 36.000 (35.495)   Prec@5 79.000 (80.618)   [2025-10-28 13:35:55]
  Epoch: [010][400/500]   Time 0.055 (0.099)   Data 0.000 (0.044)   Loss 2.1094 (2.0989)   Prec@1 33.000 (35.529)   Prec@5 79.000 (80.601)   [2025-10-28 13:36:01]
  **Train** Prec@1 35.542 Prec@5 80.636 Error@1 64.458
  **Test** Prec@1 36.880 Prec@5 82.930 Error@1 63.120

==>>[2025-10-28 13:36:27] [Epoch=011/040] [Need: 00:31:45] [LR=0.0100] [Best : Accuracy=37.84, Error=62.16]
  Epoch: [011][000/500]   Time 17.906 (17.906)   Data 17.692 (17.692)   Loss 2.1124 (2.1124)   Prec@1 37.000 (37.000)   Prec@5 78.000 (78.000)   [2025-10-28 13:36:45]
  Epoch: [011][100/500]   Time 0.054 (0.230)   Data 0.000 (0.175)   Loss 2.0626 (2.0961)   Prec@1 40.000 (35.366)   Prec@5 88.000 (80.535)   [2025-10-28 13:36:50]
  Epoch: [011][200/500]   Time 0.057 (0.143)   Data 0.000 (0.088)   Loss 2.0777 (2.0899)   Prec@1 38.000 (36.055)   Prec@5 82.000 (80.891)   [2025-10-28 13:36:56]
  Epoch: [011][300/500]   Time 0.057 (0.114)   Data 0.000 (0.059)   Loss 2.0931 (2.0888)   Prec@1 35.000 (36.123)   Prec@5 84.000 (80.664)   [2025-10-28 13:37:01]
  Epoch: [011][400/500]   Time 0.058 (0.099)   Data 0.000 (0.044)   Loss 2.0525 (2.0870)   Prec@1 40.000 (36.479)   Prec@5 86.000 (80.858)   [2025-10-28 13:37:07]
  **Train** Prec@1 36.642 Prec@5 80.960 Error@1 63.358
  **Test** Prec@1 40.150 Prec@5 84.380 Error@1 59.850
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 13:37:32] [Epoch=012/040] [Need: 00:30:39] [LR=0.0100] [Best : Accuracy=40.15, Error=59.85]
  Epoch: [012][000/500]   Time 17.964 (17.964)   Data 17.689 (17.689)   Loss 2.0779 (2.0779)   Prec@1 38.000 (38.000)   Prec@5 79.000 (79.000)   [2025-10-28 13:37:50]
  Epoch: [012][100/500]   Time 0.052 (0.231)   Data 0.000 (0.175)   Loss 2.0663 (2.0787)   Prec@1 41.000 (37.347)   Prec@5 81.000 (80.723)   [2025-10-28 13:37:56]
  Epoch: [012][200/500]   Time 0.053 (0.144)   Data 0.000 (0.088)   Loss 2.1122 (2.0757)   Prec@1 32.000 (37.716)   Prec@5 76.000 (81.030)   [2025-10-28 13:38:01]
  Epoch: [012][300/500]   Time 0.057 (0.114)   Data 0.000 (0.059)   Loss 2.1103 (2.0761)   Prec@1 34.000 (37.621)   Prec@5 79.000 (81.216)   [2025-10-28 13:38:07]
  Epoch: [012][400/500]   Time 0.055 (0.100)   Data 0.000 (0.044)   Loss 2.0043 (2.0773)   Prec@1 47.000 (37.658)   Prec@5 85.000 (81.190)   [2025-10-28 13:38:13]
  **Train** Prec@1 37.834 Prec@5 81.388 Error@1 62.166
  **Test** Prec@1 40.130 Prec@5 85.150 Error@1 59.870

==>>[2025-10-28 13:38:38] [Epoch=013/040] [Need: 00:29:33] [LR=0.0100] [Best : Accuracy=40.15, Error=59.85]
  Epoch: [013][000/500]   Time 17.881 (17.881)   Data 17.600 (17.600)   Loss 2.0527 (2.0527)   Prec@1 43.000 (43.000)   Prec@5 79.000 (79.000)   [2025-10-28 13:38:56]
  Epoch: [013][100/500]   Time 0.057 (0.230)   Data 0.000 (0.174)   Loss 2.0552 (2.0746)   Prec@1 46.000 (38.188)   Prec@5 86.000 (81.594)   [2025-10-28 13:39:01]
  Epoch: [013][200/500]   Time 0.054 (0.143)   Data 0.000 (0.088)   Loss 2.0731 (2.0706)   Prec@1 39.000 (38.498)   Prec@5 85.000 (81.771)   [2025-10-28 13:39:07]
  Epoch: [013][300/500]   Time 0.058 (0.114)   Data 0.000 (0.059)   Loss 2.0277 (2.0655)   Prec@1 45.000 (39.090)   Prec@5 89.000 (81.937)   [2025-10-28 13:39:12]
  Epoch: [013][400/500]   Time 0.055 (0.100)   Data 0.000 (0.044)   Loss 2.0219 (2.0654)   Prec@1 43.000 (39.055)   Prec@5 86.000 (82.082)   [2025-10-28 13:39:18]
  **Train** Prec@1 39.236 Prec@5 82.150 Error@1 60.764
  **Test** Prec@1 40.780 Prec@5 84.680 Error@1 59.220
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 13:39:44] [Epoch=014/040] [Need: 00:28:27] [LR=0.0100] [Best : Accuracy=40.78, Error=59.22]
  Epoch: [014][000/500]   Time 17.916 (17.916)   Data 17.628 (17.628)   Loss 2.0416 (2.0416)   Prec@1 43.000 (43.000)   Prec@5 78.000 (78.000)   [2025-10-28 13:40:02]
  Epoch: [014][100/500]   Time 0.057 (0.230)   Data 0.000 (0.175)   Loss 2.0552 (2.0647)   Prec@1 40.000 (39.000)   Prec@5 76.000 (82.178)   [2025-10-28 13:40:07]
  Epoch: [014][200/500]   Time 0.051 (0.143)   Data 0.000 (0.088)   Loss 2.0248 (2.0616)   Prec@1 41.000 (39.095)   Prec@5 88.000 (82.393)   [2025-10-28 13:40:12]
  Epoch: [014][300/500]   Time 0.055 (0.114)   Data 0.000 (0.059)   Loss 2.0255 (2.0618)   Prec@1 45.000 (39.179)   Prec@5 78.000 (82.455)   [2025-10-28 13:40:18]
  Epoch: [014][400/500]   Time 0.058 (0.099)   Data 0.000 (0.044)   Loss 2.0922 (2.0591)   Prec@1 33.000 (39.397)   Prec@5 80.000 (82.446)   [2025-10-28 13:40:24]
  **Train** Prec@1 39.454 Prec@5 82.468 Error@1 60.546
  **Test** Prec@1 44.050 Prec@5 85.040 Error@1 55.950
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 13:40:49] [Epoch=015/040] [Need: 00:27:21] [LR=0.0100] [Best : Accuracy=44.05, Error=55.95]
  Epoch: [015][000/500]   Time 17.798 (17.798)   Data 17.588 (17.588)   Loss 2.0444 (2.0444)   Prec@1 40.000 (40.000)   Prec@5 82.000 (82.000)   [2025-10-28 13:41:07]
  Epoch: [015][100/500]   Time 0.055 (0.229)   Data 0.000 (0.174)   Loss 2.1211 (2.0533)   Prec@1 35.000 (40.158)   Prec@5 76.000 (82.485)   [2025-10-28 13:41:12]
  Epoch: [015][200/500]   Time 0.052 (0.142)   Data 0.000 (0.088)   Loss 2.0604 (2.0559)   Prec@1 40.000 (39.751)   Prec@5 80.000 (82.662)   [2025-10-28 13:41:18]
  Epoch: [015][300/500]   Time 0.054 (0.113)   Data 0.000 (0.059)   Loss 2.0351 (2.0541)   Prec@1 43.000 (40.010)   Prec@5 86.000 (82.724)   [2025-10-28 13:41:23]
  Epoch: [015][400/500]   Time 0.055 (0.099)   Data 0.000 (0.044)   Loss 1.9702 (2.0516)   Prec@1 48.000 (40.309)   Prec@5 83.000 (82.830)   [2025-10-28 13:41:29]
  **Train** Prec@1 40.348 Prec@5 82.806 Error@1 59.652
  **Test** Prec@1 35.200 Prec@5 79.800 Error@1 64.800

==>>[2025-10-28 13:41:55] [Epoch=016/040] [Need: 00:26:15] [LR=0.0100] [Best : Accuracy=44.05, Error=55.95]
  Epoch: [016][000/500]   Time 17.694 (17.694)   Data 17.492 (17.492)   Loss 2.0643 (2.0643)   Prec@1 39.000 (39.000)   Prec@5 78.000 (78.000)   [2025-10-28 13:42:12]
  Epoch: [016][100/500]   Time 0.054 (0.228)   Data 0.001 (0.173)   Loss 2.0516 (2.0372)   Prec@1 43.000 (41.822)   Prec@5 79.000 (83.485)   [2025-10-28 13:42:18]
  Epoch: [016][200/500]   Time 0.056 (0.142)   Data 0.000 (0.087)   Loss 2.0742 (2.0417)   Prec@1 37.000 (41.493)   Prec@5 78.000 (83.473)   [2025-10-28 13:42:23]
  Epoch: [016][300/500]   Time 0.057 (0.113)   Data 0.001 (0.058)   Loss 2.0233 (2.0421)   Prec@1 42.000 (41.349)   Prec@5 86.000 (83.472)   [2025-10-28 13:42:29]
  Epoch: [016][400/500]   Time 0.057 (0.099)   Data 0.000 (0.044)   Loss 2.0005 (2.0427)   Prec@1 43.000 (41.224)   Prec@5 87.000 (83.377)   [2025-10-28 13:42:34]
  **Train** Prec@1 41.316 Prec@5 83.444 Error@1 58.684
  **Test** Prec@1 39.640 Prec@5 83.690 Error@1 60.360

==>>[2025-10-28 13:43:00] [Epoch=017/040] [Need: 00:25:09] [LR=0.0100] [Best : Accuracy=44.05, Error=55.95]
  Epoch: [017][000/500]   Time 17.869 (17.869)   Data 17.666 (17.666)   Loss 2.0528 (2.0528)   Prec@1 43.000 (43.000)   Prec@5 81.000 (81.000)   [2025-10-28 13:43:18]
  Epoch: [017][100/500]   Time 0.054 (0.230)   Data 0.000 (0.175)   Loss 1.9856 (2.0337)   Prec@1 50.000 (42.257)   Prec@5 89.000 (83.564)   [2025-10-28 13:43:23]
  Epoch: [017][200/500]   Time 0.055 (0.143)   Data 0.000 (0.088)   Loss 2.0630 (2.0376)   Prec@1 37.000 (41.881)   Prec@5 83.000 (83.318)   [2025-10-28 13:43:29]
  Epoch: [017][300/500]   Time 0.058 (0.114)   Data 0.001 (0.059)   Loss 2.0267 (2.0408)   Prec@1 42.000 (41.585)   Prec@5 89.000 (83.252)   [2025-10-28 13:43:34]
  Epoch: [017][400/500]   Time 0.057 (0.100)   Data 0.000 (0.044)   Loss 2.0304 (2.0373)   Prec@1 40.000 (41.893)   Prec@5 78.000 (83.332)   [2025-10-28 13:43:40]
  **Train** Prec@1 42.062 Prec@5 83.506 Error@1 57.938
  **Test** Prec@1 45.300 Prec@5 86.040 Error@1 54.700
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 13:44:06] [Epoch=018/040] [Need: 00:24:04] [LR=0.0100] [Best : Accuracy=45.30, Error=54.70]
  Epoch: [018][000/500]   Time 18.248 (18.248)   Data 17.984 (17.984)   Loss 2.0344 (2.0344)   Prec@1 42.000 (42.000)   Prec@5 87.000 (87.000)   [2025-10-28 13:44:24]
  Epoch: [018][100/500]   Time 0.056 (0.234)   Data 0.000 (0.178)   Loss 2.0778 (2.0302)   Prec@1 38.000 (42.505)   Prec@5 82.000 (83.743)   [2025-10-28 13:44:30]
  Epoch: [018][200/500]   Time 0.057 (0.145)   Data 0.000 (0.090)   Loss 2.0029 (2.0329)   Prec@1 46.000 (42.343)   Prec@5 77.000 (83.721)   [2025-10-28 13:44:35]
  Epoch: [018][300/500]   Time 0.054 (0.115)   Data 0.000 (0.060)   Loss 2.0743 (2.0290)   Prec@1 40.000 (42.884)   Prec@5 86.000 (83.711)   [2025-10-28 13:44:41]
  Epoch: [018][400/500]   Time 0.056 (0.100)   Data 0.000 (0.045)   Loss 1.9954 (2.0252)   Prec@1 48.000 (43.219)   Prec@5 82.000 (83.771)   [2025-10-28 13:44:46]
  **Train** Prec@1 43.396 Prec@5 83.756 Error@1 56.604
  **Test** Prec@1 48.110 Prec@5 85.470 Error@1 51.890
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 13:45:12] [Epoch=019/040] [Need: 00:22:59] [LR=0.0100] [Best : Accuracy=48.11, Error=51.89]
  Epoch: [019][000/500]   Time 17.985 (17.985)   Data 17.704 (17.704)   Loss 2.0038 (2.0038)   Prec@1 39.000 (39.000)   Prec@5 87.000 (87.000)   [2025-10-28 13:45:30]
  Epoch: [019][100/500]   Time 0.061 (0.231)   Data 0.001 (0.175)   Loss 1.9761 (2.0154)   Prec@1 49.000 (44.406)   Prec@5 87.000 (84.455)   [2025-10-28 13:45:35]
  Epoch: [019][200/500]   Time 0.056 (0.143)   Data 0.001 (0.088)   Loss 1.9637 (2.0150)   Prec@1 51.000 (44.289)   Prec@5 84.000 (84.383)   [2025-10-28 13:45:41]
  Epoch: [019][300/500]   Time 0.056 (0.114)   Data 0.000 (0.059)   Loss 2.0508 (2.0132)   Prec@1 40.000 (44.485)   Prec@5 79.000 (84.342)   [2025-10-28 13:45:46]
  Epoch: [019][400/500]   Time 0.059 (0.100)   Data 0.000 (0.044)   Loss 2.0195 (2.0125)   Prec@1 41.000 (44.576)   Prec@5 84.000 (84.416)   [2025-10-28 13:45:52]
  **Train** Prec@1 44.498 Prec@5 84.350 Error@1 55.502
  **Test** Prec@1 48.100 Prec@5 87.330 Error@1 51.900

==>>[2025-10-28 13:46:18] [Epoch=020/040] [Need: 00:21:53] [LR=0.0100] [Best : Accuracy=48.11, Error=51.89]
  Epoch: [020][000/500]   Time 18.000 (18.000)   Data 17.715 (17.715)   Loss 2.0628 (2.0628)   Prec@1 38.000 (38.000)   Prec@5 84.000 (84.000)   [2025-10-28 13:46:36]
  Epoch: [020][100/500]   Time 0.056 (0.232)   Data 0.000 (0.176)   Loss 2.0494 (2.0088)   Prec@1 42.000 (44.950)   Prec@5 90.000 (84.535)   [2025-10-28 13:46:41]
  Epoch: [020][200/500]   Time 0.053 (0.144)   Data 0.000 (0.088)   Loss 2.0105 (2.0057)   Prec@1 44.000 (45.318)   Prec@5 83.000 (84.731)   [2025-10-28 13:46:46]
  Epoch: [020][300/500]   Time 0.055 (0.115)   Data 0.000 (0.059)   Loss 1.9839 (2.0054)   Prec@1 49.000 (45.309)   Prec@5 86.000 (84.834)   [2025-10-28 13:46:52]
  Epoch: [020][400/500]   Time 0.055 (0.100)   Data 0.001 (0.044)   Loss 2.0504 (2.0044)   Prec@1 40.000 (45.389)   Prec@5 86.000 (84.753)   [2025-10-28 13:46:58]
  **Train** Prec@1 45.534 Prec@5 84.854 Error@1 54.466
  **Test** Prec@1 36.160 Prec@5 81.730 Error@1 63.840

==>>[2025-10-28 13:47:23] [Epoch=021/040] [Need: 00:20:47] [LR=0.0100] [Best : Accuracy=48.11, Error=51.89]
  Epoch: [021][000/500]   Time 18.276 (18.276)   Data 18.073 (18.073)   Loss 2.0521 (2.0521)   Prec@1 39.000 (39.000)   Prec@5 89.000 (89.000)   [2025-10-28 13:47:42]
  Epoch: [021][100/500]   Time 0.055 (0.234)   Data 0.001 (0.179)   Loss 2.0210 (1.9988)   Prec@1 43.000 (45.812)   Prec@5 84.000 (85.000)   [2025-10-28 13:47:47]
  Epoch: [021][200/500]   Time 0.056 (0.145)   Data 0.000 (0.090)   Loss 2.0404 (1.9956)   Prec@1 41.000 (46.219)   Prec@5 84.000 (84.836)   [2025-10-28 13:47:52]
  Epoch: [021][300/500]   Time 0.056 (0.115)   Data 0.000 (0.060)   Loss 2.0403 (1.9960)   Prec@1 39.000 (46.246)   Prec@5 85.000 (84.741)   [2025-10-28 13:47:58]
  Epoch: [021][400/500]   Time 0.058 (0.101)   Data 0.000 (0.045)   Loss 2.0520 (1.9975)   Prec@1 38.000 (46.142)   Prec@5 79.000 (84.746)   [2025-10-28 13:48:04]
  **Train** Prec@1 46.088 Prec@5 84.770 Error@1 53.912
  **Test** Prec@1 46.020 Prec@5 85.680 Error@1 53.980

==>>[2025-10-28 13:48:29] [Epoch=022/040] [Need: 00:19:42] [LR=0.0100] [Best : Accuracy=48.11, Error=51.89]
  Epoch: [022][000/500]   Time 18.069 (18.069)   Data 17.789 (17.789)   Loss 2.0168 (2.0168)   Prec@1 39.000 (39.000)   Prec@5 89.000 (89.000)   [2025-10-28 13:48:48]
  Epoch: [022][100/500]   Time 0.056 (0.232)   Data 0.001 (0.176)   Loss 2.0410 (1.9911)   Prec@1 43.000 (46.455)   Prec@5 86.000 (86.119)   [2025-10-28 13:48:53]
  Epoch: [022][200/500]   Time 0.057 (0.143)   Data 0.000 (0.089)   Loss 2.0551 (1.9909)   Prec@1 40.000 (46.781)   Prec@5 82.000 (85.771)   [2025-10-28 13:48:58]
  Epoch: [022][300/500]   Time 0.056 (0.114)   Data 0.000 (0.059)   Loss 1.9790 (1.9913)   Prec@1 47.000 (46.734)   Prec@5 89.000 (85.588)   [2025-10-28 13:49:04]
  Epoch: [022][400/500]   Time 0.057 (0.100)   Data 0.000 (0.045)   Loss 2.0091 (1.9931)   Prec@1 44.000 (46.579)   Prec@5 90.000 (85.426)   [2025-10-28 13:49:10]
  **Train** Prec@1 46.764 Prec@5 85.410 Error@1 53.236
  **Test** Prec@1 50.870 Prec@5 87.000 Error@1 49.130
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 13:49:35] [Epoch=023/040] [Need: 00:18:36] [LR=0.0100] [Best : Accuracy=50.87, Error=49.13]
  Epoch: [023][000/500]   Time 18.000 (18.000)   Data 17.713 (17.713)   Loss 1.9921 (1.9921)   Prec@1 46.000 (46.000)   Prec@5 92.000 (92.000)   [2025-10-28 13:49:53]
  Epoch: [023][100/500]   Time 0.056 (0.231)   Data 0.000 (0.176)   Loss 2.0021 (1.9852)   Prec@1 47.000 (47.505)   Prec@5 83.000 (85.634)   [2025-10-28 13:49:59]
  Epoch: [023][200/500]   Time 0.059 (0.144)   Data 0.000 (0.088)   Loss 1.9431 (1.9855)   Prec@1 49.000 (47.328)   Prec@5 89.000 (85.667)   [2025-10-28 13:50:04]
  Epoch: [023][300/500]   Time 0.053 (0.114)   Data 0.000 (0.059)   Loss 1.9729 (1.9866)   Prec@1 51.000 (47.312)   Prec@5 93.000 (85.565)   [2025-10-28 13:50:10]
  Epoch: [023][400/500]   Time 0.060 (0.100)   Data 0.000 (0.044)   Loss 1.9823 (1.9859)   Prec@1 48.000 (47.397)   Prec@5 83.000 (85.441)   [2025-10-28 13:50:15]
  **Train** Prec@1 47.344 Prec@5 85.598 Error@1 52.656
  **Test** Prec@1 40.920 Prec@5 85.170 Error@1 59.080

==>>[2025-10-28 13:50:41] [Epoch=024/040] [Need: 00:17:31] [LR=0.0100] [Best : Accuracy=50.87, Error=49.13]
  Epoch: [024][000/500]   Time 17.878 (17.878)   Data 17.635 (17.635)   Loss 1.9545 (1.9545)   Prec@1 51.000 (51.000)   Prec@5 86.000 (86.000)   [2025-10-28 13:50:59]
  Epoch: [024][100/500]   Time 0.052 (0.230)   Data 0.000 (0.175)   Loss 2.0280 (1.9842)   Prec@1 43.000 (47.386)   Prec@5 83.000 (85.713)   [2025-10-28 13:51:04]
  Epoch: [024][200/500]   Time 0.057 (0.143)   Data 0.000 (0.088)   Loss 1.9957 (1.9790)   Prec@1 47.000 (48.035)   Prec@5 88.000 (86.010)   [2025-10-28 13:51:10]
  Epoch: [024][300/500]   Time 0.054 (0.114)   Data 0.000 (0.059)   Loss 1.9683 (1.9777)   Prec@1 51.000 (48.252)   Prec@5 82.000 (85.774)   [2025-10-28 13:51:15]
  Epoch: [024][400/500]   Time 0.058 (0.100)   Data 0.000 (0.044)   Loss 2.0328 (1.9791)   Prec@1 41.000 (48.045)   Prec@5 88.000 (85.766)   [2025-10-28 13:51:21]
  **Train** Prec@1 48.190 Prec@5 85.856 Error@1 51.810
  **Test** Prec@1 53.380 Prec@5 88.790 Error@1 46.620
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 13:51:47] [Epoch=025/040] [Need: 00:16:25] [LR=0.0010] [Best : Accuracy=53.38, Error=46.62]
  Epoch: [025][000/500]   Time 17.913 (17.913)   Data 17.686 (17.686)   Loss 1.9579 (1.9579)   Prec@1 52.000 (52.000)   Prec@5 89.000 (89.000)   [2025-10-28 13:52:05]
  Epoch: [025][100/500]   Time 0.054 (0.230)   Data 0.000 (0.175)   Loss 1.9395 (1.9596)   Prec@1 52.000 (50.257)   Prec@5 88.000 (86.307)   [2025-10-28 13:52:10]
  Epoch: [025][200/500]   Time 0.057 (0.143)   Data 0.000 (0.088)   Loss 1.9170 (1.9560)   Prec@1 53.000 (50.547)   Prec@5 93.000 (86.632)   [2025-10-28 13:52:15]
  Epoch: [025][300/500]   Time 0.059 (0.114)   Data 0.000 (0.059)   Loss 2.0163 (1.9529)   Prec@1 43.000 (50.844)   Prec@5 84.000 (86.897)   [2025-10-28 13:52:21]
  Epoch: [025][400/500]   Time 0.057 (0.099)   Data 0.000 (0.044)   Loss 1.9930 (1.9537)   Prec@1 49.000 (50.845)   Prec@5 86.000 (86.863)   [2025-10-28 13:52:27]
  **Train** Prec@1 51.036 Prec@5 86.828 Error@1 48.964
  **Test** Prec@1 53.420 Prec@5 88.930 Error@1 46.580
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 13:52:53] [Epoch=026/040] [Need: 00:15:19] [LR=0.0010] [Best : Accuracy=53.42, Error=46.58]
  Epoch: [026][000/500]   Time 17.870 (17.870)   Data 17.684 (17.684)   Loss 1.8847 (1.8847)   Prec@1 59.000 (59.000)   Prec@5 92.000 (92.000)   [2025-10-28 13:53:10]
  Epoch: [026][100/500]   Time 0.056 (0.230)   Data 0.000 (0.175)   Loss 1.9665 (1.9385)   Prec@1 49.000 (52.317)   Prec@5 92.000 (87.366)   [2025-10-28 13:53:16]
  Epoch: [026][200/500]   Time 0.056 (0.143)   Data 0.000 (0.088)   Loss 1.9595 (1.9410)   Prec@1 49.000 (52.050)   Prec@5 89.000 (87.368)   [2025-10-28 13:53:21]
  Epoch: [026][300/500]   Time 0.057 (0.114)   Data 0.000 (0.059)   Loss 1.8720 (1.9424)   Prec@1 59.000 (51.924)   Prec@5 89.000 (87.326)   [2025-10-28 13:53:27]
  Epoch: [026][400/500]   Time 0.056 (0.100)   Data 0.000 (0.044)   Loss 1.9594 (1.9422)   Prec@1 49.000 (51.958)   Prec@5 85.000 (87.349)   [2025-10-28 13:53:33]
  **Train** Prec@1 52.126 Prec@5 87.358 Error@1 47.874
  **Test** Prec@1 53.100 Prec@5 89.290 Error@1 46.900

==>>[2025-10-28 13:53:58] [Epoch=027/040] [Need: 00:14:14] [LR=0.0010] [Best : Accuracy=53.42, Error=46.58]
  Epoch: [027][000/500]   Time 17.845 (17.845)   Data 17.560 (17.560)   Loss 1.9872 (1.9872)   Prec@1 45.000 (45.000)   Prec@5 85.000 (85.000)   [2025-10-28 13:54:16]
  Epoch: [027][100/500]   Time 0.052 (0.230)   Data 0.000 (0.174)   Loss 1.9624 (1.9447)   Prec@1 49.000 (51.653)   Prec@5 87.000 (87.248)   [2025-10-28 13:54:22]
  Epoch: [027][200/500]   Time 0.057 (0.143)   Data 0.000 (0.088)   Loss 1.9413 (1.9399)   Prec@1 54.000 (52.259)   Prec@5 90.000 (87.264)   [2025-10-28 13:54:27]
  Epoch: [027][300/500]   Time 0.056 (0.114)   Data 0.001 (0.058)   Loss 1.8593 (1.9395)   Prec@1 62.000 (52.246)   Prec@5 90.000 (87.213)   [2025-10-28 13:54:33]
  Epoch: [027][400/500]   Time 0.053 (0.100)   Data 0.000 (0.044)   Loss 1.9885 (1.9384)   Prec@1 47.000 (52.404)   Prec@5 86.000 (87.244)   [2025-10-28 13:54:38]
  **Train** Prec@1 52.312 Prec@5 87.266 Error@1 47.688
  **Test** Prec@1 53.140 Prec@5 89.350 Error@1 46.860

==>>[2025-10-28 13:55:04] [Epoch=028/040] [Need: 00:13:08] [LR=0.0010] [Best : Accuracy=53.42, Error=46.58]
  Epoch: [028][000/500]   Time 17.935 (17.935)   Data 17.691 (17.691)   Loss 2.0078 (2.0078)   Prec@1 44.000 (44.000)   Prec@5 90.000 (90.000)   [2025-10-28 13:55:22]
  Epoch: [028][100/500]   Time 0.051 (0.230)   Data 0.000 (0.175)   Loss 1.9683 (1.9368)   Prec@1 50.000 (52.455)   Prec@5 90.000 (87.644)   [2025-10-28 13:55:27]
  Epoch: [028][200/500]   Time 0.055 (0.143)   Data 0.000 (0.088)   Loss 1.8775 (1.9337)   Prec@1 60.000 (52.801)   Prec@5 89.000 (87.826)   [2025-10-28 13:55:33]
  Epoch: [028][300/500]   Time 0.056 (0.114)   Data 0.000 (0.059)   Loss 1.9241 (1.9374)   Prec@1 56.000 (52.455)   Prec@5 84.000 (87.458)   [2025-10-28 13:55:38]
  Epoch: [028][400/500]   Time 0.055 (0.100)   Data 0.000 (0.044)   Loss 1.9145 (1.9360)   Prec@1 59.000 (52.656)   Prec@5 90.000 (87.536)   [2025-10-28 13:55:44]
  **Train** Prec@1 52.666 Prec@5 87.580 Error@1 47.334
  **Test** Prec@1 52.630 Prec@5 88.950 Error@1 47.370

==>>[2025-10-28 13:56:10] [Epoch=029/040] [Need: 00:12:02] [LR=0.0010] [Best : Accuracy=53.42, Error=46.58]
  Epoch: [029][000/500]   Time 18.055 (18.055)   Data 17.771 (17.771)   Loss 1.9928 (1.9928)   Prec@1 46.000 (46.000)   Prec@5 83.000 (83.000)   [2025-10-28 13:56:28]
  Epoch: [029][100/500]   Time 0.055 (0.232)   Data 0.000 (0.176)   Loss 1.9406 (1.9344)   Prec@1 52.000 (52.604)   Prec@5 88.000 (87.851)   [2025-10-28 13:56:33]
  Epoch: [029][200/500]   Time 0.058 (0.144)   Data 0.000 (0.089)   Loss 1.9297 (1.9336)   Prec@1 53.000 (52.721)   Prec@5 89.000 (87.607)   [2025-10-28 13:56:39]
  Epoch: [029][300/500]   Time 0.054 (0.114)   Data 0.000 (0.059)   Loss 1.9101 (1.9324)   Prec@1 57.000 (52.917)   Prec@5 88.000 (87.761)   [2025-10-28 13:56:44]
  Epoch: [029][400/500]   Time 0.060 (0.100)   Data 0.001 (0.044)   Loss 1.9144 (1.9337)   Prec@1 57.000 (52.768)   Prec@5 87.000 (87.743)   [2025-10-28 13:56:50]
  **Train** Prec@1 52.838 Prec@5 87.630 Error@1 47.162
  **Test** Prec@1 52.760 Prec@5 89.630 Error@1 47.240

==>>[2025-10-28 13:57:21] [Epoch=030/040] [Need: 00:10:58] [LR=0.0010] [Best : Accuracy=53.42, Error=46.58]
  Epoch: [030][000/500]   Time 25.647 (25.647)   Data 25.334 (25.334)   Loss 2.0256 (2.0256)   Prec@1 42.000 (42.000)   Prec@5 84.000 (84.000)   [2025-10-28 13:57:46]
  Epoch: [030][100/500]   Time 0.063 (0.321)   Data 0.001 (0.251)   Loss 1.9503 (1.9363)   Prec@1 50.000 (52.584)   Prec@5 89.000 (87.139)   [2025-10-28 13:57:53]
  Epoch: [030][200/500]   Time 0.066 (0.195)   Data 0.000 (0.127)   Loss 1.9841 (1.9350)   Prec@1 46.000 (52.597)   Prec@5 86.000 (87.318)   [2025-10-28 13:58:00]
  Epoch: [030][300/500]   Time 0.067 (0.153)   Data 0.000 (0.085)   Loss 1.9051 (1.9323)   Prec@1 54.000 (52.963)   Prec@5 83.000 (87.342)   [2025-10-28 13:58:07]
  Epoch: [030][400/500]   Time 0.067 (0.132)   Data 0.002 (0.064)   Loss 1.9036 (1.9341)   Prec@1 56.000 (52.781)   Prec@5 89.000 (87.274)   [2025-10-28 13:58:14]
  **Train** Prec@1 52.760 Prec@5 87.420 Error@1 47.240
  **Test** Prec@1 54.350 Prec@5 89.110 Error@1 45.650
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 13:58:43] [Epoch=031/040] [Need: 00:09:57] [LR=0.0010] [Best : Accuracy=54.35, Error=45.65]
  Epoch: [031][000/500]   Time 22.649 (22.649)   Data 22.361 (22.361)   Loss 1.8838 (1.8838)   Prec@1 61.000 (61.000)   Prec@5 88.000 (88.000)   [2025-10-28 13:59:06]
  Epoch: [031][100/500]   Time 0.066 (0.289)   Data 0.000 (0.222)   Loss 1.9128 (1.9221)   Prec@1 52.000 (54.307)   Prec@5 86.000 (87.624)   [2025-10-28 13:59:12]
  Epoch: [031][200/500]   Time 0.067 (0.178)   Data 0.000 (0.112)   Loss 1.8921 (1.9283)   Prec@1 55.000 (53.463)   Prec@5 85.000 (87.507)   [2025-10-28 13:59:19]
  Epoch: [031][300/500]   Time 0.068 (0.141)   Data 0.000 (0.075)   Loss 1.8839 (1.9299)   Prec@1 59.000 (53.259)   Prec@5 89.000 (87.757)   [2025-10-28 13:59:26]
  Epoch: [031][400/500]   Time 0.064 (0.123)   Data 0.000 (0.056)   Loss 1.9536 (1.9293)   Prec@1 52.000 (53.334)   Prec@5 83.000 (87.778)   [2025-10-28 13:59:32]
  **Train** Prec@1 53.204 Prec@5 87.702 Error@1 46.796
  **Test** Prec@1 53.630 Prec@5 89.090 Error@1 46.370

==>>[2025-10-28 14:00:02] [Epoch=032/040] [Need: 00:08:54] [LR=0.0010] [Best : Accuracy=54.35, Error=45.65]
  Epoch: [032][000/500]   Time 23.215 (23.215)   Data 22.919 (22.919)   Loss 1.9833 (1.9833)   Prec@1 49.000 (49.000)   Prec@5 82.000 (82.000)   [2025-10-28 14:00:26]
  Epoch: [032][100/500]   Time 0.064 (0.295)   Data 0.000 (0.227)   Loss 1.9656 (1.9306)   Prec@1 48.000 (52.772)   Prec@5 87.000 (87.257)   [2025-10-28 14:00:32]
  Epoch: [032][200/500]   Time 0.063 (0.180)   Data 0.000 (0.114)   Loss 1.9356 (1.9333)   Prec@1 52.000 (52.453)   Prec@5 89.000 (87.259)   [2025-10-28 14:00:39]
  Epoch: [032][300/500]   Time 0.062 (0.142)   Data 0.000 (0.076)   Loss 2.0023 (1.9318)   Prec@1 45.000 (52.761)   Prec@5 85.000 (87.585)   [2025-10-28 14:00:45]
  Epoch: [032][400/500]   Time 0.066 (0.124)   Data 0.000 (0.057)   Loss 1.9517 (1.9332)   Prec@1 51.000 (52.623)   Prec@5 86.000 (87.608)   [2025-10-28 14:00:52]
  **Train** Prec@1 52.730 Prec@5 87.658 Error@1 47.270
  **Test** Prec@1 52.590 Prec@5 89.110 Error@1 47.410

==>>[2025-10-28 14:01:22] [Epoch=033/040] [Need: 00:07:50] [LR=0.0010] [Best : Accuracy=54.35, Error=45.65]
  Epoch: [033][000/500]   Time 22.313 (22.313)   Data 22.092 (22.092)   Loss 1.8815 (1.8815)   Prec@1 57.000 (57.000)   Prec@5 90.000 (90.000)   [2025-10-28 14:01:44]
  Epoch: [033][100/500]   Time 0.063 (0.286)   Data 0.001 (0.219)   Loss 1.8746 (1.9255)   Prec@1 59.000 (53.881)   Prec@5 90.000 (87.604)   [2025-10-28 14:01:51]
  Epoch: [033][200/500]   Time 0.066 (0.178)   Data 0.000 (0.110)   Loss 1.9321 (1.9276)   Prec@1 51.000 (53.552)   Prec@5 89.000 (87.667)   [2025-10-28 14:01:58]
  Epoch: [033][300/500]   Time 0.072 (0.141)   Data 0.001 (0.074)   Loss 1.9226 (1.9272)   Prec@1 57.000 (53.492)   Prec@5 87.000 (87.681)   [2025-10-28 14:02:04]
  Epoch: [033][400/500]   Time 0.062 (0.123)   Data 0.001 (0.056)   Loss 1.8861 (1.9285)   Prec@1 58.000 (53.369)   Prec@5 87.000 (87.608)   [2025-10-28 14:02:11]
  **Train** Prec@1 53.224 Prec@5 87.600 Error@1 46.776
  **Test** Prec@1 54.240 Prec@5 89.380 Error@1 45.760

==>>[2025-10-28 14:02:41] [Epoch=034/040] [Need: 00:06:45] [LR=0.0010] [Best : Accuracy=54.35, Error=45.65]
  Epoch: [034][000/500]   Time 22.505 (22.505)   Data 22.194 (22.194)   Loss 1.9153 (1.9153)   Prec@1 54.000 (54.000)   Prec@5 91.000 (91.000)   [2025-10-28 14:03:03]
  Epoch: [034][100/500]   Time 0.065 (0.287)   Data 0.000 (0.220)   Loss 1.9422 (1.9282)   Prec@1 50.000 (53.475)   Prec@5 86.000 (88.267)   [2025-10-28 14:03:10]
  Epoch: [034][200/500]   Time 0.060 (0.177)   Data 0.001 (0.111)   Loss 1.9456 (1.9273)   Prec@1 49.000 (53.517)   Prec@5 89.000 (88.169)   [2025-10-28 14:03:16]
  Epoch: [034][300/500]   Time 0.071 (0.140)   Data 0.001 (0.074)   Loss 1.9310 (1.9276)   Prec@1 53.000 (53.518)   Prec@5 90.000 (88.090)   [2025-10-28 14:03:23]
  Epoch: [034][400/500]   Time 0.067 (0.122)   Data 0.000 (0.056)   Loss 1.9403 (1.9263)   Prec@1 52.000 (53.611)   Prec@5 87.000 (88.085)   [2025-10-28 14:03:29]
  **Train** Prec@1 53.590 Prec@5 88.120 Error@1 46.410
  **Test** Prec@1 54.620 Prec@5 89.560 Error@1 45.380
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 14:03:59] [Epoch=035/040] [Need: 00:05:39] [LR=0.0010] [Best : Accuracy=54.62, Error=45.38]
  Epoch: [035][000/500]   Time 23.063 (23.063)   Data 22.768 (22.768)   Loss 1.9195 (1.9195)   Prec@1 54.000 (54.000)   Prec@5 90.000 (90.000)   [2025-10-28 14:04:22]
  Epoch: [035][100/500]   Time 0.069 (0.294)   Data 0.000 (0.226)   Loss 1.9988 (1.9263)   Prec@1 44.000 (53.782)   Prec@5 87.000 (87.832)   [2025-10-28 14:04:29]
  Epoch: [035][200/500]   Time 0.066 (0.180)   Data 0.000 (0.114)   Loss 1.9540 (1.9261)   Prec@1 49.000 (53.716)   Prec@5 87.000 (87.896)   [2025-10-28 14:04:35]
  Epoch: [035][300/500]   Time 0.065 (0.142)   Data 0.000 (0.076)   Loss 1.9282 (1.9258)   Prec@1 55.000 (53.684)   Prec@5 91.000 (87.791)   [2025-10-28 14:04:42]
  Epoch: [035][400/500]   Time 0.064 (0.124)   Data 0.000 (0.057)   Loss 1.9409 (1.9257)   Prec@1 53.000 (53.676)   Prec@5 90.000 (87.810)   [2025-10-28 14:04:49]
  **Train** Prec@1 53.614 Prec@5 87.848 Error@1 46.386
  **Test** Prec@1 54.580 Prec@5 89.180 Error@1 45.420

==>>[2025-10-28 14:05:18] [Epoch=036/040] [Need: 00:04:32] [LR=0.0010] [Best : Accuracy=54.62, Error=45.38]
  Epoch: [036][000/500]   Time 23.174 (23.174)   Data 22.867 (22.867)   Loss 1.9074 (1.9074)   Prec@1 56.000 (56.000)   Prec@5 89.000 (89.000)   [2025-10-28 14:05:41]
  Epoch: [036][100/500]   Time 0.066 (0.294)   Data 0.001 (0.227)   Loss 1.9483 (1.9277)   Prec@1 50.000 (53.535)   Prec@5 83.000 (87.584)   [2025-10-28 14:05:48]
  Epoch: [036][200/500]   Time 0.077 (0.181)   Data 0.000 (0.114)   Loss 1.9178 (1.9279)   Prec@1 56.000 (53.388)   Prec@5 89.000 (87.607)   [2025-10-28 14:05:54]
  Epoch: [036][300/500]   Time 0.069 (0.143)   Data 0.000 (0.076)   Loss 1.8703 (1.9261)   Prec@1 63.000 (53.585)   Prec@5 90.000 (87.880)   [2025-10-28 14:06:01]
  Epoch: [036][400/500]   Time 0.064 (0.124)   Data 0.000 (0.057)   Loss 2.0050 (1.9259)   Prec@1 42.000 (53.616)   Prec@5 87.000 (87.955)   [2025-10-28 14:06:08]
  **Train** Prec@1 53.528 Prec@5 87.938 Error@1 46.472
  **Test** Prec@1 54.290 Prec@5 90.130 Error@1 45.710

==>>[2025-10-28 14:06:38] [Epoch=037/040] [Need: 00:03:25] [LR=0.0010] [Best : Accuracy=54.62, Error=45.38]
  Epoch: [037][000/500]   Time 22.889 (22.889)   Data 22.583 (22.583)   Loss 1.9191 (1.9191)   Prec@1 52.000 (52.000)   Prec@5 87.000 (87.000)   [2025-10-28 14:07:01]
  Epoch: [037][100/500]   Time 0.061 (0.291)   Data 0.000 (0.224)   Loss 1.9968 (1.9230)   Prec@1 45.000 (53.871)   Prec@5 84.000 (88.059)   [2025-10-28 14:07:07]
  Epoch: [037][200/500]   Time 0.064 (0.178)   Data 0.001 (0.113)   Loss 1.9214 (1.9243)   Prec@1 56.000 (53.716)   Prec@5 92.000 (88.139)   [2025-10-28 14:07:14]
  Epoch: [037][300/500]   Time 0.067 (0.141)   Data 0.000 (0.075)   Loss 1.9414 (1.9244)   Prec@1 51.000 (53.631)   Prec@5 85.000 (87.950)   [2025-10-28 14:07:20]
  Epoch: [037][400/500]   Time 0.060 (0.123)   Data 0.000 (0.057)   Loss 1.8515 (1.9251)   Prec@1 61.000 (53.534)   Prec@5 89.000 (87.950)   [2025-10-28 14:07:27]
  **Train** Prec@1 53.576 Prec@5 87.916 Error@1 46.424
  **Test** Prec@1 55.120 Prec@5 89.860 Error@1 44.880
=> Obtain best accuracy, and update the best model

==>>[2025-10-28 14:07:57] [Epoch=038/040] [Need: 00:02:17] [LR=0.0010] [Best : Accuracy=55.12, Error=44.88]
  Epoch: [038][000/500]   Time 23.111 (23.111)   Data 22.845 (22.845)   Loss 1.9398 (1.9398)   Prec@1 54.000 (54.000)   Prec@5 88.000 (88.000)   [2025-10-28 14:08:20]
  Epoch: [038][100/500]   Time 0.069 (0.293)   Data 0.000 (0.227)   Loss 1.9403 (1.9182)   Prec@1 51.000 (54.535)   Prec@5 89.000 (88.812)   [2025-10-28 14:08:26]
  Epoch: [038][200/500]   Time 0.053 (0.180)   Data 0.000 (0.114)   Loss 1.8628 (1.9206)   Prec@1 60.000 (54.274)   Prec@5 88.000 (88.174)   [2025-10-28 14:08:33]
  Epoch: [038][300/500]   Time 0.070 (0.143)   Data 0.001 (0.076)   Loss 1.8937 (1.9231)   Prec@1 59.000 (53.927)   Prec@5 83.000 (88.083)   [2025-10-28 14:08:40]
  Epoch: [038][400/500]   Time 0.068 (0.124)   Data 0.003 (0.057)   Loss 1.9316 (1.9238)   Prec@1 50.000 (53.858)   Prec@5 88.000 (88.142)   [2025-10-28 14:08:46]
  **Train** Prec@1 53.854 Prec@5 88.072 Error@1 46.146
  **Test** Prec@1 54.180 Prec@5 89.520 Error@1 45.820

==>>[2025-10-28 14:09:16] [Epoch=039/040] [Need: 00:01:09] [LR=0.0010] [Best : Accuracy=55.12, Error=44.88]
  Epoch: [039][000/500]   Time 22.734 (22.734)   Data 22.422 (22.422)   Loss 1.9662 (1.9662)   Prec@1 50.000 (50.000)   Prec@5 87.000 (87.000)   [2025-10-28 14:09:38]
  Epoch: [039][100/500]   Time 0.067 (0.290)   Data 0.000 (0.222)   Loss 1.8392 (1.9187)   Prec@1 63.000 (54.188)   Prec@5 94.000 (87.881)   [2025-10-28 14:09:45]
  Epoch: [039][200/500]   Time 0.066 (0.178)   Data 0.000 (0.112)   Loss 1.8664 (1.9232)   Prec@1 58.000 (53.721)   Prec@5 96.000 (87.736)   [2025-10-28 14:09:51]
  Epoch: [039][300/500]   Time 0.063 (0.142)   Data 0.000 (0.075)   Loss 1.9164 (1.9253)   Prec@1 54.000 (53.488)   Prec@5 88.000 (87.635)   [2025-10-28 14:09:58]
  Epoch: [039][400/500]   Time 0.075 (0.123)   Data 0.000 (0.056)   Loss 1.9149 (1.9246)   Prec@1 53.000 (53.653)   Prec@5 86.000 (87.845)   [2025-10-28 14:10:05]
  **Train** Prec@1 53.758 Prec@5 88.006 Error@1 46.242
  **Test** Prec@1 53.510 Prec@5 89.970 Error@1 46.490
