save path : ./save/tinyvgg_quan/clipping_0.1_0.01
{'data_path': './dataset', 'arch': 'tinyvgg_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': False, 'clipping_coeff': 0.1, 'learning_rate': 0.01, 'manualSeed': 6542, 'save_path': './save/tinyvgg_quan/clipping_0.1_0.01', 'enable_bfa': False, 'resume': '', 'quan_bitwidth': None, 'reset_weight': False, 'evaluate': False, 'n_iter': 30, 'model_only': False, 'random_bfa': False, 'use_cuda': True}
Random Seed: 6542
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'tinyvgg_quan'
=> network :
 TinyVGG(
  (features): Sequential(
    (0): quan_Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Dropout2d(p=0.25, inplace=False)
    (6): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace=True)
    (8): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace=True)
    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (11): Dropout2d(p=0.25, inplace=False)
  )
  (classifier): Sequential(
    (0): quan_Linear(in_features=2048, out_features=10, bias=True)
  )
)
=> do not use any checkpoint for tinyvgg_quan model

==>>[2025-10-23 00:00:25] [Epoch=000/040] [Need: 00:00:00] [LR=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/500]   Time 17.998 (17.998)   Data 17.651 (17.651)   Loss 2.3038 (2.3038)   Prec@1 9.000 (9.000)   Prec@5 52.000 (52.000)   [2025-10-23 00:00:43]
  Epoch: [000][100/500]   Time 0.008 (0.190)   Data 0.000 (0.175)   Loss 2.0989 (2.2117)   Prec@1 20.000 (16.911)   Prec@5 78.000 (63.574)   [2025-10-23 00:00:44]
  Epoch: [000][200/500]   Time 0.010 (0.100)   Data 0.000 (0.088)   Loss 2.0455 (2.1112)   Prec@1 24.000 (21.741)   Prec@5 77.000 (71.104)   [2025-10-23 00:00:45]
  Epoch: [000][300/500]   Time 0.010 (0.070)   Data 0.000 (0.059)   Loss 1.8205 (2.0408)   Prec@1 35.000 (24.910)   Prec@5 85.000 (74.847)   [2025-10-23 00:00:46]
  Epoch: [000][400/500]   Time 0.010 (0.055)   Data 0.000 (0.044)   Loss 1.7472 (1.9730)   Prec@1 40.000 (27.723)   Prec@5 86.000 (77.658)   [2025-10-23 00:00:47]
  **Train** Prec@1 29.638 Prec@5 79.606 Error@1 70.362
  **Test** Prec@1 45.750 Prec@5 91.670 Error@1 54.250
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:01:06] [Epoch=001/040] [Need: 00:26:58] [LR=0.0100] [Best : Accuracy=45.75, Error=54.25]
  Epoch: [001][000/500]   Time 17.615 (17.615)   Data 17.571 (17.571)   Loss 1.7717 (1.7717)   Prec@1 40.000 (40.000)   Prec@5 89.000 (89.000)   [2025-10-23 00:01:24]
  Epoch: [001][100/500]   Time 0.009 (0.187)   Data 0.000 (0.174)   Loss 1.6747 (1.6594)   Prec@1 38.000 (39.792)   Prec@5 89.000 (88.683)   [2025-10-23 00:01:25]
  Epoch: [001][200/500]   Time 0.009 (0.099)   Data 0.000 (0.088)   Loss 1.5660 (1.6464)   Prec@1 44.000 (40.149)   Prec@5 92.000 (88.896)   [2025-10-23 00:01:26]
  Epoch: [001][300/500]   Time 0.010 (0.069)   Data 0.000 (0.059)   Loss 1.6343 (1.6297)   Prec@1 39.000 (40.807)   Prec@5 88.000 (89.206)   [2025-10-23 00:01:27]
  Epoch: [001][400/500]   Time 0.009 (0.054)   Data 0.000 (0.044)   Loss 1.5790 (1.6076)   Prec@1 40.000 (41.561)   Prec@5 91.000 (89.648)   [2025-10-23 00:01:28]
  **Train** Prec@1 42.104 Prec@5 89.988 Error@1 57.896
  **Test** Prec@1 52.260 Prec@5 93.570 Error@1 47.740
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:01:48] [Epoch=002/040] [Need: 00:26:14] [LR=0.0100] [Best : Accuracy=52.26, Error=47.74]
  Epoch: [002][000/500]   Time 17.442 (17.442)   Data 17.398 (17.398)   Loss 1.7940 (1.7940)   Prec@1 39.000 (39.000)   Prec@5 86.000 (86.000)   [2025-10-23 00:02:05]
  Epoch: [002][100/500]   Time 0.009 (0.185)   Data 0.000 (0.172)   Loss 1.4870 (1.5070)   Prec@1 37.000 (45.366)   Prec@5 90.000 (91.693)   [2025-10-23 00:02:06]
  Epoch: [002][200/500]   Time 0.009 (0.098)   Data 0.000 (0.087)   Loss 1.5488 (1.5006)   Prec@1 47.000 (45.801)   Prec@5 90.000 (91.642)   [2025-10-23 00:02:07]
  Epoch: [002][300/500]   Time 0.012 (0.069)   Data 0.000 (0.058)   Loss 1.3829 (1.4881)   Prec@1 45.000 (46.213)   Prec@5 92.000 (91.794)   [2025-10-23 00:02:08]
  Epoch: [002][400/500]   Time 0.009 (0.054)   Data 0.000 (0.044)   Loss 1.3672 (1.4813)   Prec@1 50.000 (46.606)   Prec@5 94.000 (91.830)   [2025-10-23 00:02:09]
  **Train** Prec@1 46.946 Prec@5 91.884 Error@1 53.054
  **Test** Prec@1 57.160 Prec@5 95.270 Error@1 42.840
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:02:29] [Epoch=003/040] [Need: 00:25:30] [LR=0.0100] [Best : Accuracy=57.16, Error=42.84]
  Epoch: [003][000/500]   Time 17.441 (17.441)   Data 17.397 (17.397)   Loss 1.4908 (1.4908)   Prec@1 48.000 (48.000)   Prec@5 93.000 (93.000)   [2025-10-23 00:02:46]
  Epoch: [003][100/500]   Time 0.010 (0.185)   Data 0.001 (0.172)   Loss 1.2314 (1.4157)   Prec@1 60.000 (49.020)   Prec@5 97.000 (92.703)   [2025-10-23 00:02:47]
  Epoch: [003][200/500]   Time 0.011 (0.098)   Data 0.000 (0.087)   Loss 1.2653 (1.4010)   Prec@1 54.000 (49.692)   Prec@5 97.000 (92.886)   [2025-10-23 00:02:48]
  Epoch: [003][300/500]   Time 0.011 (0.069)   Data 0.000 (0.058)   Loss 1.3718 (1.4046)   Prec@1 55.000 (49.611)   Prec@5 91.000 (92.801)   [2025-10-23 00:02:49]
  Epoch: [003][400/500]   Time 0.010 (0.054)   Data 0.000 (0.044)   Loss 1.2898 (1.3991)   Prec@1 55.000 (49.840)   Prec@5 94.000 (92.803)   [2025-10-23 00:02:50]
  **Train** Prec@1 50.244 Prec@5 92.934 Error@1 49.756
  **Test** Prec@1 58.680 Prec@5 95.410 Error@1 41.320
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:03:10] [Epoch=004/040] [Need: 00:24:45] [LR=0.0100] [Best : Accuracy=58.68, Error=41.32]
  Epoch: [004][000/500]   Time 17.471 (17.471)   Data 17.429 (17.429)   Loss 1.3029 (1.3029)   Prec@1 55.000 (55.000)   Prec@5 97.000 (97.000)   [2025-10-23 00:03:27]
  Epoch: [004][100/500]   Time 0.012 (0.185)   Data 0.000 (0.173)   Loss 1.1284 (1.3432)   Prec@1 63.000 (52.178)   Prec@5 98.000 (93.257)   [2025-10-23 00:03:28]
  Epoch: [004][200/500]   Time 0.012 (0.098)   Data 0.000 (0.087)   Loss 1.4305 (1.3405)   Prec@1 49.000 (52.090)   Prec@5 90.000 (93.249)   [2025-10-23 00:03:29]
  Epoch: [004][300/500]   Time 0.009 (0.069)   Data 0.000 (0.058)   Loss 1.1911 (1.3385)   Prec@1 55.000 (52.349)   Prec@5 93.000 (93.405)   [2025-10-23 00:03:30]
  Epoch: [004][400/500]   Time 0.012 (0.054)   Data 0.000 (0.044)   Loss 1.6057 (1.3324)   Prec@1 46.000 (52.491)   Prec@5 91.000 (93.549)   [2025-10-23 00:03:31]
  **Train** Prec@1 52.674 Prec@5 93.664 Error@1 47.326
  **Test** Prec@1 62.030 Prec@5 96.170 Error@1 37.970
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:03:51] [Epoch=005/040] [Need: 00:24:05] [LR=0.0100] [Best : Accuracy=62.03, Error=37.97]
  Epoch: [005][000/500]   Time 17.761 (17.761)   Data 17.716 (17.716)   Loss 1.4224 (1.4224)   Prec@1 47.000 (47.000)   Prec@5 92.000 (92.000)   [2025-10-23 00:04:09]
  Epoch: [005][100/500]   Time 0.010 (0.188)   Data 0.001 (0.176)   Loss 1.2134 (1.2932)   Prec@1 66.000 (53.901)   Prec@5 93.000 (93.832)   [2025-10-23 00:04:10]
  Epoch: [005][200/500]   Time 0.009 (0.099)   Data 0.000 (0.088)   Loss 1.2479 (1.2818)   Prec@1 55.000 (54.229)   Prec@5 96.000 (93.905)   [2025-10-23 00:04:11]
  Epoch: [005][300/500]   Time 0.010 (0.070)   Data 0.000 (0.059)   Loss 1.1805 (1.2793)   Prec@1 55.000 (54.179)   Prec@5 95.000 (93.973)   [2025-10-23 00:04:12]
  Epoch: [005][400/500]   Time 0.009 (0.055)   Data 0.000 (0.044)   Loss 1.3581 (1.2758)   Prec@1 57.000 (54.464)   Prec@5 92.000 (94.022)   [2025-10-23 00:04:13]
  **Train** Prec@1 54.658 Prec@5 94.102 Error@1 45.342
  **Test** Prec@1 64.470 Prec@5 96.790 Error@1 35.530
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:04:33] [Epoch=006/040] [Need: 00:23:27] [LR=0.0100] [Best : Accuracy=64.47, Error=35.53]
  Epoch: [006][000/500]   Time 19.148 (19.148)   Data 19.103 (19.103)   Loss 1.3775 (1.3775)   Prec@1 46.000 (46.000)   Prec@5 93.000 (93.000)   [2025-10-23 00:04:52]
  Epoch: [006][100/500]   Time 0.010 (0.202)   Data 0.000 (0.189)   Loss 1.2840 (1.2407)   Prec@1 54.000 (55.713)   Prec@5 93.000 (94.515)   [2025-10-23 00:04:53]
  Epoch: [006][200/500]   Time 0.010 (0.107)   Data 0.000 (0.095)   Loss 1.3126 (1.2316)   Prec@1 53.000 (56.403)   Prec@5 92.000 (94.642)   [2025-10-23 00:04:54]
  Epoch: [006][300/500]   Time 0.009 (0.075)   Data 0.000 (0.064)   Loss 1.1386 (1.2339)   Prec@1 62.000 (56.223)   Prec@5 95.000 (94.605)   [2025-10-23 00:04:55]
  Epoch: [006][400/500]   Time 0.013 (0.059)   Data 0.000 (0.048)   Loss 1.1640 (1.2345)   Prec@1 54.000 (56.239)   Prec@5 96.000 (94.589)   [2025-10-23 00:04:57]
  **Train** Prec@1 56.478 Prec@5 94.672 Error@1 43.522
  **Test** Prec@1 66.420 Prec@5 96.960 Error@1 33.580
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:05:16] [Epoch=007/040] [Need: 00:22:55] [LR=0.0100] [Best : Accuracy=66.42, Error=33.58]
  Epoch: [007][000/500]   Time 17.626 (17.626)   Data 17.582 (17.582)   Loss 1.1241 (1.1241)   Prec@1 60.000 (60.000)   Prec@5 95.000 (95.000)   [2025-10-23 00:05:34]
  Epoch: [007][100/500]   Time 0.009 (0.186)   Data 0.000 (0.174)   Loss 1.2792 (1.1987)   Prec@1 56.000 (57.455)   Prec@5 98.000 (94.752)   [2025-10-23 00:05:35]
  Epoch: [007][200/500]   Time 0.011 (0.099)   Data 0.000 (0.088)   Loss 1.2767 (1.1874)   Prec@1 51.000 (57.721)   Prec@5 98.000 (94.905)   [2025-10-23 00:05:36]
  Epoch: [007][300/500]   Time 0.017 (0.070)   Data 0.001 (0.059)   Loss 1.4366 (1.1868)   Prec@1 55.000 (57.635)   Prec@5 95.000 (95.040)   [2025-10-23 00:05:38]
  Epoch: [007][400/500]   Time 0.012 (0.056)   Data 0.000 (0.044)   Loss 1.3394 (1.1867)   Prec@1 54.000 (57.666)   Prec@5 94.000 (95.015)   [2025-10-23 00:05:39]
  **Train** Prec@1 57.652 Prec@5 94.998 Error@1 42.348
  **Test** Prec@1 67.430 Prec@5 97.360 Error@1 32.570
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:05:59] [Epoch=008/040] [Need: 00:22:15] [LR=0.0100] [Best : Accuracy=67.43, Error=32.57]
  Epoch: [008][000/500]   Time 18.676 (18.676)   Data 18.629 (18.629)   Loss 1.1907 (1.1907)   Prec@1 55.000 (55.000)   Prec@5 95.000 (95.000)   [2025-10-23 00:06:17]
  Epoch: [008][100/500]   Time 0.011 (0.198)   Data 0.001 (0.185)   Loss 1.0900 (1.1780)   Prec@1 58.000 (57.941)   Prec@5 97.000 (95.287)   [2025-10-23 00:06:19]
  Epoch: [008][200/500]   Time 0.013 (0.105)   Data 0.000 (0.093)   Loss 1.1938 (1.1767)   Prec@1 57.000 (58.368)   Prec@5 94.000 (95.075)   [2025-10-23 00:06:20]
  Epoch: [008][300/500]   Time 0.009 (0.073)   Data 0.000 (0.062)   Loss 1.1907 (1.1737)   Prec@1 65.000 (58.452)   Prec@5 94.000 (95.060)   [2025-10-23 00:06:21]
  Epoch: [008][400/500]   Time 0.010 (0.058)   Data 0.000 (0.047)   Loss 1.2397 (1.1655)   Prec@1 57.000 (58.776)   Prec@5 92.000 (95.102)   [2025-10-23 00:06:22]
  **Train** Prec@1 59.082 Prec@5 95.020 Error@1 40.918
  **Test** Prec@1 67.730 Prec@5 97.420 Error@1 32.270
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:06:43] [Epoch=009/040] [Need: 00:21:44] [LR=0.0100] [Best : Accuracy=67.73, Error=32.27]
  Epoch: [009][000/500]   Time 18.235 (18.235)   Data 18.192 (18.192)   Loss 1.0625 (1.0625)   Prec@1 64.000 (64.000)   Prec@5 99.000 (99.000)   [2025-10-23 00:07:02]
  Epoch: [009][100/500]   Time 0.010 (0.194)   Data 0.000 (0.180)   Loss 1.1695 (1.1293)   Prec@1 58.000 (60.505)   Prec@5 95.000 (95.614)   [2025-10-23 00:07:03]
  Epoch: [009][200/500]   Time 0.009 (0.102)   Data 0.000 (0.091)   Loss 1.0369 (1.1348)   Prec@1 68.000 (60.269)   Prec@5 96.000 (95.448)   [2025-10-23 00:07:04]
  Epoch: [009][300/500]   Time 0.010 (0.072)   Data 0.000 (0.061)   Loss 1.1322 (1.1384)   Prec@1 65.000 (59.947)   Prec@5 95.000 (95.445)   [2025-10-23 00:07:05]
  Epoch: [009][400/500]   Time 0.009 (0.056)   Data 0.000 (0.046)   Loss 1.1196 (1.1378)   Prec@1 57.000 (60.047)   Prec@5 96.000 (95.359)   [2025-10-23 00:07:06]
  **Train** Prec@1 59.932 Prec@5 95.392 Error@1 40.068
  **Test** Prec@1 68.680 Prec@5 97.500 Error@1 31.320
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:07:26] [Epoch=010/040] [Need: 00:21:03] [LR=0.0100] [Best : Accuracy=68.68, Error=31.32]
  Epoch: [010][000/500]   Time 17.620 (17.620)   Data 17.574 (17.574)   Loss 1.2723 (1.2723)   Prec@1 58.000 (58.000)   Prec@5 96.000 (96.000)   [2025-10-23 00:07:44]
  Epoch: [010][100/500]   Time 0.012 (0.187)   Data 0.001 (0.174)   Loss 1.1959 (1.1409)   Prec@1 63.000 (59.970)   Prec@5 97.000 (95.960)   [2025-10-23 00:07:45]
  Epoch: [010][200/500]   Time 0.014 (0.099)   Data 0.000 (0.088)   Loss 1.0646 (1.1215)   Prec@1 65.000 (60.463)   Prec@5 97.000 (95.761)   [2025-10-23 00:07:46]
  Epoch: [010][300/500]   Time 0.009 (0.070)   Data 0.000 (0.059)   Loss 1.0287 (1.1202)   Prec@1 63.000 (60.532)   Prec@5 96.000 (95.641)   [2025-10-23 00:07:47]
  Epoch: [010][400/500]   Time 0.014 (0.055)   Data 0.000 (0.044)   Loss 1.0430 (1.1199)   Prec@1 55.000 (60.581)   Prec@5 98.000 (95.559)   [2025-10-23 00:07:48]
  **Train** Prec@1 60.608 Prec@5 95.560 Error@1 39.392
  **Test** Prec@1 68.860 Prec@5 97.450 Error@1 31.140
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:08:08] [Epoch=011/040] [Need: 00:20:21] [LR=0.0100] [Best : Accuracy=68.86, Error=31.14]
  Epoch: [011][000/500]   Time 17.689 (17.689)   Data 17.645 (17.645)   Loss 1.1253 (1.1253)   Prec@1 58.000 (58.000)   Prec@5 97.000 (97.000)   [2025-10-23 00:08:26]
  Epoch: [011][100/500]   Time 0.013 (0.187)   Data 0.000 (0.175)   Loss 0.9679 (1.1025)   Prec@1 65.000 (61.149)   Prec@5 98.000 (95.792)   [2025-10-23 00:08:27]
  Epoch: [011][200/500]   Time 0.011 (0.099)   Data 0.000 (0.088)   Loss 0.9773 (1.0961)   Prec@1 66.000 (61.458)   Prec@5 99.000 (95.831)   [2025-10-23 00:08:28]
  Epoch: [011][300/500]   Time 0.009 (0.070)   Data 0.000 (0.059)   Loss 1.0757 (1.1024)   Prec@1 58.000 (61.150)   Prec@5 95.000 (95.904)   [2025-10-23 00:08:29]
  Epoch: [011][400/500]   Time 0.010 (0.055)   Data 0.000 (0.044)   Loss 1.2471 (1.1012)   Prec@1 58.000 (61.027)   Prec@5 91.000 (95.873)   [2025-10-23 00:08:30]
  **Train** Prec@1 61.092 Prec@5 95.838 Error@1 38.908
  **Test** Prec@1 70.910 Prec@5 97.640 Error@1 29.090
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:08:50] [Epoch=012/040] [Need: 00:19:39] [LR=0.0100] [Best : Accuracy=70.91, Error=29.09]
  Epoch: [012][000/500]   Time 17.720 (17.720)   Data 17.674 (17.674)   Loss 1.3789 (1.3789)   Prec@1 52.000 (52.000)   Prec@5 94.000 (94.000)   [2025-10-23 00:09:08]
  Epoch: [012][100/500]   Time 0.010 (0.188)   Data 0.000 (0.175)   Loss 1.0310 (1.0858)   Prec@1 61.000 (62.030)   Prec@5 96.000 (95.802)   [2025-10-23 00:09:09]
  Epoch: [012][200/500]   Time 0.010 (0.100)   Data 0.000 (0.088)   Loss 1.0223 (1.0870)   Prec@1 68.000 (62.030)   Prec@5 96.000 (95.970)   [2025-10-23 00:09:10]
  Epoch: [012][300/500]   Time 0.010 (0.070)   Data 0.000 (0.059)   Loss 1.0243 (1.0838)   Prec@1 65.000 (61.950)   Prec@5 97.000 (95.957)   [2025-10-23 00:09:11]
  Epoch: [012][400/500]   Time 0.010 (0.055)   Data 0.000 (0.044)   Loss 1.0369 (1.0866)   Prec@1 62.000 (61.731)   Prec@5 98.000 (96.005)   [2025-10-23 00:09:12]
  **Train** Prec@1 61.746 Prec@5 95.980 Error@1 38.254
  **Test** Prec@1 71.160 Prec@5 97.810 Error@1 28.840
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:09:32] [Epoch=013/040] [Need: 00:18:57] [LR=0.0100] [Best : Accuracy=71.16, Error=28.84]
  Epoch: [013][000/500]   Time 18.619 (18.619)   Data 18.576 (18.576)   Loss 1.0347 (1.0347)   Prec@1 67.000 (67.000)   Prec@5 95.000 (95.000)   [2025-10-23 00:09:51]
  Epoch: [013][100/500]   Time 0.008 (0.197)   Data 0.000 (0.184)   Loss 1.0577 (1.0855)   Prec@1 61.000 (61.980)   Prec@5 98.000 (95.921)   [2025-10-23 00:09:52]
  Epoch: [013][200/500]   Time 0.009 (0.104)   Data 0.000 (0.093)   Loss 1.0360 (1.0801)   Prec@1 68.000 (62.313)   Prec@5 94.000 (95.935)   [2025-10-23 00:09:53]
  Epoch: [013][300/500]   Time 0.013 (0.073)   Data 0.001 (0.062)   Loss 1.0770 (1.0749)   Prec@1 60.000 (62.199)   Prec@5 98.000 (95.997)   [2025-10-23 00:09:54]
  Epoch: [013][400/500]   Time 0.014 (0.057)   Data 0.000 (0.046)   Loss 1.1641 (1.0711)   Prec@1 56.000 (62.354)   Prec@5 96.000 (96.022)   [2025-10-23 00:09:55]
  **Train** Prec@1 62.244 Prec@5 96.084 Error@1 37.756
  **Test** Prec@1 70.580 Prec@5 97.880 Error@1 29.420

==>>[2025-10-23 00:10:15] [Epoch=014/040] [Need: 00:18:16] [LR=0.0100] [Best : Accuracy=71.16, Error=28.84]
  Epoch: [014][000/500]   Time 17.779 (17.779)   Data 17.736 (17.736)   Loss 1.0405 (1.0405)   Prec@1 64.000 (64.000)   Prec@5 98.000 (98.000)   [2025-10-23 00:10:33]
  Epoch: [014][100/500]   Time 0.010 (0.189)   Data 0.000 (0.176)   Loss 0.9751 (1.0574)   Prec@1 67.000 (62.881)   Prec@5 96.000 (95.752)   [2025-10-23 00:10:34]
  Epoch: [014][200/500]   Time 0.010 (0.100)   Data 0.000 (0.088)   Loss 0.9863 (1.0594)   Prec@1 64.000 (62.587)   Prec@5 96.000 (95.806)   [2025-10-23 00:10:35]
  Epoch: [014][300/500]   Time 0.012 (0.070)   Data 0.000 (0.059)   Loss 0.9280 (1.0595)   Prec@1 69.000 (62.668)   Prec@5 96.000 (95.854)   [2025-10-23 00:10:36]
  Epoch: [014][400/500]   Time 0.011 (0.055)   Data 0.001 (0.044)   Loss 1.0521 (1.0590)   Prec@1 68.000 (62.661)   Prec@5 92.000 (95.883)   [2025-10-23 00:10:37]
  **Train** Prec@1 62.610 Prec@5 95.962 Error@1 37.390
  **Test** Prec@1 71.560 Prec@5 97.790 Error@1 28.440
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:10:57] [Epoch=015/040] [Need: 00:17:33] [LR=0.0100] [Best : Accuracy=71.56, Error=28.44]
  Epoch: [015][000/500]   Time 18.202 (18.202)   Data 18.157 (18.157)   Loss 0.8691 (0.8691)   Prec@1 69.000 (69.000)   Prec@5 96.000 (96.000)   [2025-10-23 00:11:15]
  Epoch: [015][100/500]   Time 0.010 (0.193)   Data 0.000 (0.180)   Loss 1.2344 (1.0522)   Prec@1 61.000 (62.851)   Prec@5 94.000 (96.218)   [2025-10-23 00:11:16]
  Epoch: [015][200/500]   Time 0.011 (0.103)   Data 0.000 (0.091)   Loss 1.0879 (1.0459)   Prec@1 64.000 (63.020)   Prec@5 99.000 (96.249)   [2025-10-23 00:11:18]
  Epoch: [015][300/500]   Time 0.009 (0.072)   Data 0.000 (0.061)   Loss 0.9925 (1.0491)   Prec@1 70.000 (62.920)   Prec@5 94.000 (96.299)   [2025-10-23 00:11:19]
  Epoch: [015][400/500]   Time 0.010 (0.057)   Data 0.000 (0.045)   Loss 1.1262 (1.0532)   Prec@1 54.000 (62.830)   Prec@5 99.000 (96.212)   [2025-10-23 00:11:20]
  **Train** Prec@1 62.930 Prec@5 96.244 Error@1 37.070
  **Test** Prec@1 72.160 Prec@5 97.770 Error@1 27.840
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:11:39] [Epoch=016/040] [Need: 00:16:52] [LR=0.0100] [Best : Accuracy=72.16, Error=27.84]
  Epoch: [016][000/500]   Time 18.205 (18.205)   Data 18.160 (18.160)   Loss 1.0846 (1.0846)   Prec@1 60.000 (60.000)   Prec@5 95.000 (95.000)   [2025-10-23 00:11:58]
  Epoch: [016][100/500]   Time 0.012 (0.194)   Data 0.000 (0.180)   Loss 0.9255 (1.0527)   Prec@1 70.000 (63.772)   Prec@5 97.000 (96.158)   [2025-10-23 00:11:59]
  Epoch: [016][200/500]   Time 0.010 (0.103)   Data 0.000 (0.091)   Loss 0.9618 (1.0531)   Prec@1 66.000 (63.532)   Prec@5 99.000 (96.164)   [2025-10-23 00:12:00]
  Epoch: [016][300/500]   Time 0.016 (0.072)   Data 0.001 (0.061)   Loss 1.1458 (1.0503)   Prec@1 58.000 (63.495)   Prec@5 97.000 (96.163)   [2025-10-23 00:12:01]
  Epoch: [016][400/500]   Time 0.009 (0.057)   Data 0.000 (0.046)   Loss 1.0577 (1.0427)   Prec@1 67.000 (63.576)   Prec@5 97.000 (96.187)   [2025-10-23 00:12:02]
  **Train** Prec@1 63.366 Prec@5 96.148 Error@1 36.634
  **Test** Prec@1 71.550 Prec@5 97.900 Error@1 28.450

==>>[2025-10-23 00:12:22] [Epoch=017/040] [Need: 00:16:10] [LR=0.0100] [Best : Accuracy=72.16, Error=27.84]
  Epoch: [017][000/500]   Time 17.604 (17.604)   Data 17.560 (17.560)   Loss 1.1894 (1.1894)   Prec@1 57.000 (57.000)   Prec@5 91.000 (91.000)   [2025-10-23 00:12:40]
  Epoch: [017][100/500]   Time 0.010 (0.187)   Data 0.000 (0.174)   Loss 1.1084 (1.0356)   Prec@1 61.000 (63.812)   Prec@5 96.000 (96.277)   [2025-10-23 00:12:41]
  Epoch: [017][200/500]   Time 0.009 (0.099)   Data 0.000 (0.088)   Loss 0.9622 (1.0383)   Prec@1 65.000 (63.557)   Prec@5 98.000 (96.104)   [2025-10-23 00:12:42]
  Epoch: [017][300/500]   Time 0.009 (0.070)   Data 0.000 (0.059)   Loss 1.0065 (1.0387)   Prec@1 56.000 (63.748)   Prec@5 99.000 (96.080)   [2025-10-23 00:12:43]
  Epoch: [017][400/500]   Time 0.010 (0.055)   Data 0.001 (0.044)   Loss 0.8847 (1.0337)   Prec@1 75.000 (63.808)   Prec@5 100.000 (96.204)   [2025-10-23 00:12:44]
  **Train** Prec@1 63.810 Prec@5 96.200 Error@1 36.190
  **Test** Prec@1 72.590 Prec@5 98.020 Error@1 27.410
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:13:05] [Epoch=018/040] [Need: 00:15:28] [LR=0.0100] [Best : Accuracy=72.59, Error=27.41]
  Epoch: [018][000/500]   Time 17.647 (17.647)   Data 17.602 (17.602)   Loss 0.8986 (0.8986)   Prec@1 68.000 (68.000)   Prec@5 100.000 (100.000)   [2025-10-23 00:13:22]
  Epoch: [018][100/500]   Time 0.008 (0.187)   Data 0.000 (0.174)   Loss 1.0773 (1.0417)   Prec@1 64.000 (63.248)   Prec@5 97.000 (96.307)   [2025-10-23 00:13:24]
  Epoch: [018][200/500]   Time 0.010 (0.099)   Data 0.000 (0.088)   Loss 0.9922 (1.0369)   Prec@1 61.000 (63.338)   Prec@5 98.000 (96.348)   [2025-10-23 00:13:25]
  Epoch: [018][300/500]   Time 0.011 (0.070)   Data 0.000 (0.059)   Loss 0.9740 (1.0302)   Prec@1 72.000 (63.651)   Prec@5 92.000 (96.336)   [2025-10-23 00:13:26]
  Epoch: [018][400/500]   Time 0.013 (0.055)   Data 0.001 (0.044)   Loss 1.0146 (1.0303)   Prec@1 61.000 (63.773)   Prec@5 97.000 (96.344)   [2025-10-23 00:13:27]
  **Train** Prec@1 63.854 Prec@5 96.340 Error@1 36.146
  **Test** Prec@1 72.880 Prec@5 98.120 Error@1 27.120
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:13:47] [Epoch=019/040] [Need: 00:14:46] [LR=0.0100] [Best : Accuracy=72.88, Error=27.12]
  Epoch: [019][000/500]   Time 17.608 (17.608)   Data 17.565 (17.565)   Loss 1.0413 (1.0413)   Prec@1 62.000 (62.000)   Prec@5 97.000 (97.000)   [2025-10-23 00:14:04]
  Epoch: [019][100/500]   Time 0.012 (0.188)   Data 0.000 (0.174)   Loss 1.0873 (1.0433)   Prec@1 67.000 (63.723)   Prec@5 94.000 (96.545)   [2025-10-23 00:14:06]
  Epoch: [019][200/500]   Time 0.008 (0.099)   Data 0.000 (0.088)   Loss 1.1798 (1.0383)   Prec@1 65.000 (63.771)   Prec@5 93.000 (96.393)   [2025-10-23 00:14:07]
  Epoch: [019][300/500]   Time 0.010 (0.070)   Data 0.001 (0.059)   Loss 1.1055 (1.0327)   Prec@1 64.000 (64.113)   Prec@5 97.000 (96.392)   [2025-10-23 00:14:08]
  Epoch: [019][400/500]   Time 0.012 (0.055)   Data 0.000 (0.044)   Loss 1.1630 (1.0261)   Prec@1 56.000 (64.177)   Prec@5 95.000 (96.499)   [2025-10-23 00:14:09]
  **Train** Prec@1 64.344 Prec@5 96.484 Error@1 35.656
  **Test** Prec@1 71.300 Prec@5 97.800 Error@1 28.700

==>>[2025-10-23 00:14:28] [Epoch=020/040] [Need: 00:14:03] [LR=0.0100] [Best : Accuracy=72.88, Error=27.12]
  Epoch: [020][000/500]   Time 18.301 (18.301)   Data 18.255 (18.255)   Loss 0.9611 (0.9611)   Prec@1 65.000 (65.000)   Prec@5 98.000 (98.000)   [2025-10-23 00:14:47]
  Epoch: [020][100/500]   Time 0.011 (0.194)   Data 0.000 (0.181)   Loss 0.9633 (1.0233)   Prec@1 68.000 (64.248)   Prec@5 96.000 (96.416)   [2025-10-23 00:14:48]
  Epoch: [020][200/500]   Time 0.009 (0.103)   Data 0.000 (0.091)   Loss 1.0658 (1.0209)   Prec@1 65.000 (64.254)   Prec@5 97.000 (96.473)   [2025-10-23 00:14:49]
  Epoch: [020][300/500]   Time 0.010 (0.072)   Data 0.000 (0.061)   Loss 1.0449 (1.0190)   Prec@1 59.000 (64.385)   Prec@5 98.000 (96.445)   [2025-10-23 00:14:50]
  Epoch: [020][400/500]   Time 0.011 (0.057)   Data 0.000 (0.046)   Loss 0.9501 (1.0202)   Prec@1 70.000 (64.416)   Prec@5 98.000 (96.424)   [2025-10-23 00:14:51]
  **Train** Prec@1 64.448 Prec@5 96.398 Error@1 35.552
  **Test** Prec@1 72.240 Prec@5 98.060 Error@1 27.760

==>>[2025-10-23 00:15:11] [Epoch=021/040] [Need: 00:13:21] [LR=0.0100] [Best : Accuracy=72.88, Error=27.12]
  Epoch: [021][000/500]   Time 17.603 (17.603)   Data 17.558 (17.558)   Loss 1.0032 (1.0032)   Prec@1 65.000 (65.000)   Prec@5 100.000 (100.000)   [2025-10-23 00:15:28]
  Epoch: [021][100/500]   Time 0.010 (0.188)   Data 0.000 (0.174)   Loss 0.8936 (1.0296)   Prec@1 66.000 (64.010)   Prec@5 97.000 (96.535)   [2025-10-23 00:15:29]
  Epoch: [021][200/500]   Time 0.011 (0.099)   Data 0.000 (0.088)   Loss 1.0432 (1.0242)   Prec@1 68.000 (64.025)   Prec@5 99.000 (96.522)   [2025-10-23 00:15:30]
  Epoch: [021][300/500]   Time 0.011 (0.070)   Data 0.001 (0.058)   Loss 1.0064 (1.0238)   Prec@1 65.000 (64.153)   Prec@5 92.000 (96.472)   [2025-10-23 00:15:31]
  Epoch: [021][400/500]   Time 0.011 (0.055)   Data 0.000 (0.044)   Loss 0.9452 (1.0225)   Prec@1 68.000 (64.222)   Prec@5 97.000 (96.464)   [2025-10-23 00:15:33]
  **Train** Prec@1 64.286 Prec@5 96.452 Error@1 35.714
  **Test** Prec@1 72.120 Prec@5 97.870 Error@1 27.880

==>>[2025-10-23 00:15:53] [Epoch=022/040] [Need: 00:12:39] [LR=0.0100] [Best : Accuracy=72.88, Error=27.12]
  Epoch: [022][000/500]   Time 18.309 (18.309)   Data 18.262 (18.262)   Loss 1.0690 (1.0690)   Prec@1 62.000 (62.000)   Prec@5 98.000 (98.000)   [2025-10-23 00:16:12]
  Epoch: [022][100/500]   Time 0.012 (0.194)   Data 0.000 (0.181)   Loss 0.8341 (1.0014)   Prec@1 71.000 (65.158)   Prec@5 98.000 (96.584)   [2025-10-23 00:16:13]
  Epoch: [022][200/500]   Time 0.012 (0.103)   Data 0.001 (0.091)   Loss 0.8885 (0.9979)   Prec@1 70.000 (65.134)   Prec@5 100.000 (96.527)   [2025-10-23 00:16:14]
  Epoch: [022][300/500]   Time 0.011 (0.072)   Data 0.000 (0.061)   Loss 0.9592 (1.0035)   Prec@1 67.000 (64.877)   Prec@5 97.000 (96.429)   [2025-10-23 00:16:15]
  Epoch: [022][400/500]   Time 0.009 (0.057)   Data 0.000 (0.046)   Loss 0.9447 (1.0087)   Prec@1 67.000 (64.673)   Prec@5 98.000 (96.446)   [2025-10-23 00:16:16]
  **Train** Prec@1 64.646 Prec@5 96.416 Error@1 35.354
  **Test** Prec@1 71.860 Prec@5 97.940 Error@1 28.140

==>>[2025-10-23 00:16:36] [Epoch=023/040] [Need: 00:11:58] [LR=0.0100] [Best : Accuracy=72.88, Error=27.12]
  Epoch: [023][000/500]   Time 18.521 (18.521)   Data 18.475 (18.475)   Loss 0.8665 (0.8665)   Prec@1 69.000 (69.000)   Prec@5 100.000 (100.000)   [2025-10-23 00:16:55]
  Epoch: [023][100/500]   Time 0.009 (0.196)   Data 0.000 (0.183)   Loss 1.1634 (1.0145)   Prec@1 64.000 (64.634)   Prec@5 94.000 (96.406)   [2025-10-23 00:16:56]
  Epoch: [023][200/500]   Time 0.008 (0.104)   Data 0.000 (0.092)   Loss 0.9683 (1.0106)   Prec@1 66.000 (64.816)   Prec@5 99.000 (96.562)   [2025-10-23 00:16:57]
  Epoch: [023][300/500]   Time 0.010 (0.073)   Data 0.000 (0.062)   Loss 0.8107 (1.0051)   Prec@1 74.000 (64.844)   Prec@5 96.000 (96.645)   [2025-10-23 00:16:58]
  Epoch: [023][400/500]   Time 0.010 (0.057)   Data 0.000 (0.046)   Loss 0.8319 (1.0062)   Prec@1 71.000 (64.978)   Prec@5 99.000 (96.621)   [2025-10-23 00:16:59]
  **Train** Prec@1 64.896 Prec@5 96.660 Error@1 35.104
  **Test** Prec@1 72.840 Prec@5 98.050 Error@1 27.160

==>>[2025-10-23 00:17:19] [Epoch=024/040] [Need: 00:11:16] [LR=0.0100] [Best : Accuracy=72.88, Error=27.12]
  Epoch: [024][000/500]   Time 17.588 (17.588)   Data 17.543 (17.543)   Loss 0.9683 (0.9683)   Prec@1 67.000 (67.000)   Prec@5 98.000 (98.000)   [2025-10-23 00:17:37]
  Epoch: [024][100/500]   Time 0.009 (0.187)   Data 0.000 (0.174)   Loss 1.0344 (0.9872)   Prec@1 60.000 (65.733)   Prec@5 96.000 (96.604)   [2025-10-23 00:17:38]
  Epoch: [024][200/500]   Time 0.011 (0.099)   Data 0.000 (0.087)   Loss 0.9797 (0.9927)   Prec@1 64.000 (65.438)   Prec@5 97.000 (96.547)   [2025-10-23 00:17:39]
  Epoch: [024][300/500]   Time 0.010 (0.070)   Data 0.000 (0.058)   Loss 1.1505 (0.9994)   Prec@1 60.000 (65.166)   Prec@5 94.000 (96.548)   [2025-10-23 00:17:40]
  Epoch: [024][400/500]   Time 0.011 (0.055)   Data 0.000 (0.044)   Loss 0.9468 (0.9968)   Prec@1 68.000 (65.217)   Prec@5 98.000 (96.589)   [2025-10-23 00:17:41]
  **Train** Prec@1 65.228 Prec@5 96.646 Error@1 34.772
  **Test** Prec@1 73.000 Prec@5 97.900 Error@1 27.000
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:18:01] [Epoch=025/040] [Need: 00:10:33] [LR=0.0010] [Best : Accuracy=73.00, Error=27.00]
  Epoch: [025][000/500]   Time 17.542 (17.542)   Data 17.498 (17.498)   Loss 1.0492 (1.0492)   Prec@1 61.000 (61.000)   Prec@5 98.000 (98.000)   [2025-10-23 00:18:18]
  Epoch: [025][100/500]   Time 0.012 (0.187)   Data 0.000 (0.173)   Loss 0.8272 (0.9792)   Prec@1 71.000 (65.228)   Prec@5 98.000 (96.505)   [2025-10-23 00:18:20]
  Epoch: [025][200/500]   Time 0.011 (0.099)   Data 0.000 (0.087)   Loss 1.0658 (0.9634)   Prec@1 61.000 (66.015)   Prec@5 96.000 (96.801)   [2025-10-23 00:18:21]
  Epoch: [025][300/500]   Time 0.012 (0.070)   Data 0.000 (0.058)   Loss 0.9164 (0.9481)   Prec@1 72.000 (66.588)   Prec@5 95.000 (96.937)   [2025-10-23 00:18:22]
  Epoch: [025][400/500]   Time 0.009 (0.055)   Data 0.001 (0.044)   Loss 0.8812 (0.9392)   Prec@1 73.000 (66.980)   Prec@5 96.000 (96.993)   [2025-10-23 00:18:23]
  **Train** Prec@1 67.146 Prec@5 97.042 Error@1 32.854
  **Test** Prec@1 74.090 Prec@5 98.170 Error@1 25.910
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:18:43] [Epoch=026/040] [Need: 00:09:51] [LR=0.0010] [Best : Accuracy=74.09, Error=25.91]
  Epoch: [026][000/500]   Time 17.678 (17.678)   Data 17.634 (17.634)   Loss 0.8177 (0.8177)   Prec@1 72.000 (72.000)   Prec@5 98.000 (98.000)   [2025-10-23 00:19:00]
  Epoch: [026][100/500]   Time 0.011 (0.188)   Data 0.000 (0.175)   Loss 0.9348 (0.8971)   Prec@1 66.000 (68.208)   Prec@5 97.000 (97.297)   [2025-10-23 00:19:02]
  Epoch: [026][200/500]   Time 0.009 (0.100)   Data 0.000 (0.088)   Loss 1.1713 (0.9090)   Prec@1 58.000 (67.930)   Prec@5 96.000 (97.229)   [2025-10-23 00:19:03]
  Epoch: [026][300/500]   Time 0.012 (0.070)   Data 0.000 (0.059)   Loss 0.8764 (0.9192)   Prec@1 68.000 (67.718)   Prec@5 97.000 (97.186)   [2025-10-23 00:19:04]
  Epoch: [026][400/500]   Time 0.013 (0.055)   Data 0.000 (0.044)   Loss 0.8732 (0.9208)   Prec@1 70.000 (67.741)   Prec@5 96.000 (97.175)   [2025-10-23 00:19:05]
  **Train** Prec@1 67.658 Prec@5 97.140 Error@1 32.342
  **Test** Prec@1 74.020 Prec@5 98.220 Error@1 25.980

==>>[2025-10-23 00:19:25] [Epoch=027/040] [Need: 00:09:08] [LR=0.0010] [Best : Accuracy=74.09, Error=25.91]
  Epoch: [027][000/500]   Time 17.895 (17.895)   Data 17.850 (17.850)   Loss 0.8987 (0.8987)   Prec@1 67.000 (67.000)   Prec@5 98.000 (98.000)   [2025-10-23 00:19:43]
  Epoch: [027][100/500]   Time 0.013 (0.191)   Data 0.000 (0.177)   Loss 0.8500 (0.9171)   Prec@1 67.000 (68.000)   Prec@5 97.000 (97.119)   [2025-10-23 00:19:44]
  Epoch: [027][200/500]   Time 0.010 (0.101)   Data 0.000 (0.089)   Loss 1.0175 (0.9167)   Prec@1 64.000 (68.030)   Prec@5 96.000 (97.204)   [2025-10-23 00:19:45]
  Epoch: [027][300/500]   Time 0.012 (0.072)   Data 0.000 (0.059)   Loss 0.9962 (0.9174)   Prec@1 64.000 (67.993)   Prec@5 96.000 (97.262)   [2025-10-23 00:19:46]
  Epoch: [027][400/500]   Time 0.012 (0.057)   Data 0.000 (0.045)   Loss 1.0117 (0.9154)   Prec@1 65.000 (68.072)   Prec@5 97.000 (97.244)   [2025-10-23 00:19:47]
  **Train** Prec@1 68.178 Prec@5 97.242 Error@1 31.822
  **Test** Prec@1 74.680 Prec@5 98.160 Error@1 25.320
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:20:09] [Epoch=028/040] [Need: 00:08:27] [LR=0.0010] [Best : Accuracy=74.68, Error=25.32]
  Epoch: [028][000/500]   Time 18.371 (18.371)   Data 18.327 (18.327)   Loss 1.0191 (1.0191)   Prec@1 63.000 (63.000)   Prec@5 96.000 (96.000)   [2025-10-23 00:20:28]
  Epoch: [028][100/500]   Time 0.009 (0.194)   Data 0.000 (0.182)   Loss 0.9770 (0.8978)   Prec@1 69.000 (68.842)   Prec@5 98.000 (97.455)   [2025-10-23 00:20:29]
  Epoch: [028][200/500]   Time 0.011 (0.103)   Data 0.001 (0.091)   Loss 0.9524 (0.8979)   Prec@1 65.000 (68.657)   Prec@5 98.000 (97.403)   [2025-10-23 00:20:30]
  Epoch: [028][300/500]   Time 0.016 (0.072)   Data 0.001 (0.061)   Loss 0.9044 (0.9011)   Prec@1 69.000 (68.701)   Prec@5 97.000 (97.379)   [2025-10-23 00:20:31]
  Epoch: [028][400/500]   Time 0.010 (0.057)   Data 0.000 (0.046)   Loss 0.9642 (0.9041)   Prec@1 68.000 (68.589)   Prec@5 96.000 (97.312)   [2025-10-23 00:20:32]
  **Train** Prec@1 68.366 Prec@5 97.356 Error@1 31.634
  **Test** Prec@1 74.740 Prec@5 98.200 Error@1 25.260
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:20:53] [Epoch=029/040] [Need: 00:07:45] [LR=0.0010] [Best : Accuracy=74.74, Error=25.26]
  Epoch: [029][000/500]   Time 18.123 (18.123)   Data 18.077 (18.077)   Loss 0.9396 (0.9396)   Prec@1 63.000 (63.000)   Prec@5 97.000 (97.000)   [2025-10-23 00:21:11]
  Epoch: [029][100/500]   Time 0.009 (0.192)   Data 0.000 (0.179)   Loss 0.8949 (0.9248)   Prec@1 66.000 (67.248)   Prec@5 99.000 (97.238)   [2025-10-23 00:21:12]
  Epoch: [029][200/500]   Time 0.008 (0.102)   Data 0.000 (0.090)   Loss 1.0492 (0.9166)   Prec@1 65.000 (67.647)   Prec@5 95.000 (97.338)   [2025-10-23 00:21:13]
  Epoch: [029][300/500]   Time 0.014 (0.072)   Data 0.000 (0.060)   Loss 0.8801 (0.9083)   Prec@1 66.000 (68.066)   Prec@5 99.000 (97.332)   [2025-10-23 00:21:14]
  Epoch: [029][400/500]   Time 0.010 (0.057)   Data 0.001 (0.045)   Loss 0.8604 (0.9058)   Prec@1 71.000 (68.125)   Prec@5 97.000 (97.297)   [2025-10-23 00:21:16]
  **Train** Prec@1 68.272 Prec@5 97.236 Error@1 31.728
  **Test** Prec@1 74.970 Prec@5 98.240 Error@1 25.030
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:21:35] [Epoch=030/040] [Need: 00:07:03] [LR=0.0010] [Best : Accuracy=74.97, Error=25.03]
  Epoch: [030][000/500]   Time 18.075 (18.075)   Data 18.030 (18.030)   Loss 0.9833 (0.9833)   Prec@1 69.000 (69.000)   Prec@5 97.000 (97.000)   [2025-10-23 00:21:54]
  Epoch: [030][100/500]   Time 0.009 (0.191)   Data 0.000 (0.179)   Loss 0.6975 (0.9003)   Prec@1 76.000 (68.743)   Prec@5 99.000 (97.307)   [2025-10-23 00:21:55]
  Epoch: [030][200/500]   Time 0.011 (0.101)   Data 0.000 (0.090)   Loss 0.9230 (0.9110)   Prec@1 71.000 (68.179)   Prec@5 99.000 (97.174)   [2025-10-23 00:21:56]
  Epoch: [030][300/500]   Time 0.010 (0.071)   Data 0.000 (0.060)   Loss 1.0576 (0.9114)   Prec@1 64.000 (68.176)   Prec@5 96.000 (97.229)   [2025-10-23 00:21:57]
  Epoch: [030][400/500]   Time 0.010 (0.056)   Data 0.000 (0.045)   Loss 0.8703 (0.9047)   Prec@1 69.000 (68.476)   Prec@5 96.000 (97.227)   [2025-10-23 00:21:58]
  **Train** Prec@1 68.476 Prec@5 97.234 Error@1 31.524
  **Test** Prec@1 74.930 Prec@5 98.220 Error@1 25.070

==>>[2025-10-23 00:22:18] [Epoch=031/040] [Need: 00:06:21] [LR=0.0010] [Best : Accuracy=74.97, Error=25.03]
  Epoch: [031][000/500]   Time 17.577 (17.577)   Data 17.533 (17.533)   Loss 0.9097 (0.9097)   Prec@1 73.000 (73.000)   Prec@5 96.000 (96.000)   [2025-10-23 00:22:35]
  Epoch: [031][100/500]   Time 0.011 (0.187)   Data 0.000 (0.174)   Loss 0.8236 (0.8837)   Prec@1 70.000 (69.515)   Prec@5 99.000 (97.356)   [2025-10-23 00:22:37]
  Epoch: [031][200/500]   Time 0.010 (0.099)   Data 0.000 (0.087)   Loss 0.8093 (0.9003)   Prec@1 76.000 (68.502)   Prec@5 99.000 (97.114)   [2025-10-23 00:22:38]
  Epoch: [031][300/500]   Time 0.010 (0.070)   Data 0.001 (0.058)   Loss 0.8636 (0.9023)   Prec@1 69.000 (68.342)   Prec@5 97.000 (97.143)   [2025-10-23 00:22:39]
  Epoch: [031][400/500]   Time 0.010 (0.055)   Data 0.001 (0.044)   Loss 0.9706 (0.8995)   Prec@1 66.000 (68.506)   Prec@5 98.000 (97.229)   [2025-10-23 00:22:40]
  **Train** Prec@1 68.606 Prec@5 97.270 Error@1 31.394
  **Test** Prec@1 75.080 Prec@5 98.210 Error@1 24.920
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:23:00] [Epoch=032/040] [Need: 00:05:38] [LR=0.0010] [Best : Accuracy=75.08, Error=24.92]
  Epoch: [032][000/500]   Time 17.756 (17.756)   Data 17.711 (17.711)   Loss 1.0153 (1.0153)   Prec@1 67.000 (67.000)   Prec@5 97.000 (97.000)   [2025-10-23 00:23:18]
  Epoch: [032][100/500]   Time 0.012 (0.188)   Data 0.000 (0.176)   Loss 0.8522 (0.8959)   Prec@1 67.000 (68.950)   Prec@5 99.000 (97.584)   [2025-10-23 00:23:19]
  Epoch: [032][200/500]   Time 0.012 (0.100)   Data 0.000 (0.088)   Loss 0.9880 (0.8947)   Prec@1 68.000 (69.214)   Prec@5 96.000 (97.378)   [2025-10-23 00:23:20]
  Epoch: [032][300/500]   Time 0.010 (0.070)   Data 0.000 (0.059)   Loss 1.0586 (0.8994)   Prec@1 60.000 (68.880)   Prec@5 98.000 (97.339)   [2025-10-23 00:23:21]
  Epoch: [032][400/500]   Time 0.011 (0.055)   Data 0.000 (0.044)   Loss 0.7386 (0.8994)   Prec@1 75.000 (68.805)   Prec@5 98.000 (97.269)   [2025-10-23 00:23:22]
  **Train** Prec@1 68.838 Prec@5 97.266 Error@1 31.162
  **Test** Prec@1 75.090 Prec@5 98.240 Error@1 24.910
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:23:42] [Epoch=033/040] [Need: 00:04:56] [LR=0.0010] [Best : Accuracy=75.09, Error=24.91]
  Epoch: [033][000/500]   Time 17.840 (17.840)   Data 17.797 (17.797)   Loss 0.9126 (0.9126)   Prec@1 68.000 (68.000)   Prec@5 99.000 (99.000)   [2025-10-23 00:24:00]
  Epoch: [033][100/500]   Time 0.009 (0.189)   Data 0.000 (0.176)   Loss 0.7708 (0.9043)   Prec@1 69.000 (68.604)   Prec@5 99.000 (97.515)   [2025-10-23 00:24:01]
  Epoch: [033][200/500]   Time 0.012 (0.100)   Data 0.001 (0.089)   Loss 0.9050 (0.8958)   Prec@1 64.000 (68.806)   Prec@5 96.000 (97.498)   [2025-10-23 00:24:02]
  Epoch: [033][300/500]   Time 0.013 (0.070)   Data 0.001 (0.059)   Loss 0.9905 (0.8977)   Prec@1 68.000 (68.608)   Prec@5 93.000 (97.392)   [2025-10-23 00:24:03]
  Epoch: [033][400/500]   Time 0.010 (0.055)   Data 0.001 (0.045)   Loss 0.7494 (0.8955)   Prec@1 72.000 (68.738)   Prec@5 100.000 (97.392)   [2025-10-23 00:24:04]
  **Train** Prec@1 68.480 Prec@5 97.374 Error@1 31.520
  **Test** Prec@1 75.700 Prec@5 98.240 Error@1 24.300
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:24:24] [Epoch=034/040] [Need: 00:04:14] [LR=0.0010] [Best : Accuracy=75.70, Error=24.30]
  Epoch: [034][000/500]   Time 17.853 (17.853)   Data 17.807 (17.807)   Loss 1.1552 (1.1552)   Prec@1 58.000 (58.000)   Prec@5 96.000 (96.000)   [2025-10-23 00:24:42]
  Epoch: [034][100/500]   Time 0.010 (0.190)   Data 0.001 (0.177)   Loss 0.7572 (0.8882)   Prec@1 74.000 (68.802)   Prec@5 97.000 (97.574)   [2025-10-23 00:24:43]
  Epoch: [034][200/500]   Time 0.009 (0.101)   Data 0.000 (0.089)   Loss 0.8470 (0.8957)   Prec@1 71.000 (68.672)   Prec@5 97.000 (97.522)   [2025-10-23 00:24:45]
  Epoch: [034][300/500]   Time 0.013 (0.071)   Data 0.000 (0.059)   Loss 0.9603 (0.8903)   Prec@1 67.000 (69.013)   Prec@5 97.000 (97.488)   [2025-10-23 00:24:46]
  Epoch: [034][400/500]   Time 0.009 (0.056)   Data 0.000 (0.045)   Loss 1.1594 (0.8882)   Prec@1 63.000 (69.072)   Prec@5 94.000 (97.486)   [2025-10-23 00:24:47]
  **Train** Prec@1 68.944 Prec@5 97.386 Error@1 31.056
  **Test** Prec@1 74.930 Prec@5 98.280 Error@1 25.070

==>>[2025-10-23 00:25:07] [Epoch=035/040] [Need: 00:03:31] [LR=0.0010] [Best : Accuracy=75.70, Error=24.30]
  Epoch: [035][000/500]   Time 17.868 (17.868)   Data 17.824 (17.824)   Loss 1.1299 (1.1299)   Prec@1 66.000 (66.000)   Prec@5 95.000 (95.000)   [2025-10-23 00:25:25]
  Epoch: [035][100/500]   Time 0.009 (0.190)   Data 0.000 (0.177)   Loss 0.8727 (0.8721)   Prec@1 66.000 (69.297)   Prec@5 98.000 (97.525)   [2025-10-23 00:25:26]
  Epoch: [035][200/500]   Time 0.010 (0.100)   Data 0.000 (0.089)   Loss 0.8442 (0.8738)   Prec@1 75.000 (69.438)   Prec@5 98.000 (97.542)   [2025-10-23 00:25:27]
  Epoch: [035][300/500]   Time 0.010 (0.071)   Data 0.000 (0.059)   Loss 0.7276 (0.8802)   Prec@1 73.000 (69.306)   Prec@5 99.000 (97.508)   [2025-10-23 00:25:28]
  Epoch: [035][400/500]   Time 0.009 (0.056)   Data 0.000 (0.045)   Loss 0.8811 (0.8822)   Prec@1 71.000 (69.185)   Prec@5 97.000 (97.531)   [2025-10-23 00:25:29]
  **Train** Prec@1 69.134 Prec@5 97.418 Error@1 30.866
  **Test** Prec@1 75.500 Prec@5 98.300 Error@1 24.500

==>>[2025-10-23 00:25:49] [Epoch=036/040] [Need: 00:02:49] [LR=0.0010] [Best : Accuracy=75.70, Error=24.30]
  Epoch: [036][000/500]   Time 17.578 (17.578)   Data 17.533 (17.533)   Loss 0.9069 (0.9069)   Prec@1 67.000 (67.000)   Prec@5 99.000 (99.000)   [2025-10-23 00:26:06]
  Epoch: [036][100/500]   Time 0.010 (0.187)   Data 0.000 (0.174)   Loss 0.8273 (0.8758)   Prec@1 69.000 (69.644)   Prec@5 96.000 (97.475)   [2025-10-23 00:26:07]
  Epoch: [036][200/500]   Time 0.013 (0.099)   Data 0.000 (0.087)   Loss 0.6657 (0.8824)   Prec@1 75.000 (69.493)   Prec@5 99.000 (97.453)   [2025-10-23 00:26:09]
  Epoch: [036][300/500]   Time 0.013 (0.070)   Data 0.000 (0.058)   Loss 1.0106 (0.8869)   Prec@1 64.000 (69.156)   Prec@5 96.000 (97.442)   [2025-10-23 00:26:10]
  Epoch: [036][400/500]   Time 0.012 (0.055)   Data 0.001 (0.044)   Loss 0.8593 (0.8850)   Prec@1 68.000 (69.309)   Prec@5 97.000 (97.469)   [2025-10-23 00:26:11]
  **Train** Prec@1 69.202 Prec@5 97.420 Error@1 30.798
  **Test** Prec@1 75.300 Prec@5 98.380 Error@1 24.700

==>>[2025-10-23 00:26:31] [Epoch=037/040] [Need: 00:02:06] [LR=0.0010] [Best : Accuracy=75.70, Error=24.30]
  Epoch: [037][000/500]   Time 17.821 (17.821)   Data 17.773 (17.773)   Loss 0.9566 (0.9566)   Prec@1 69.000 (69.000)   Prec@5 96.000 (96.000)   [2025-10-23 00:26:49]
  Epoch: [037][100/500]   Time 0.011 (0.191)   Data 0.000 (0.176)   Loss 0.8602 (0.8617)   Prec@1 73.000 (70.119)   Prec@5 98.000 (97.495)   [2025-10-23 00:26:50]
  Epoch: [037][200/500]   Time 0.009 (0.102)   Data 0.000 (0.089)   Loss 0.8337 (0.8703)   Prec@1 71.000 (69.766)   Prec@5 99.000 (97.547)   [2025-10-23 00:26:51]
  Epoch: [037][300/500]   Time 0.010 (0.072)   Data 0.000 (0.059)   Loss 0.9362 (0.8796)   Prec@1 64.000 (69.326)   Prec@5 97.000 (97.458)   [2025-10-23 00:26:52]
  Epoch: [037][400/500]   Time 0.008 (0.057)   Data 0.000 (0.045)   Loss 0.8110 (0.8798)   Prec@1 70.000 (69.262)   Prec@5 100.000 (97.419)   [2025-10-23 00:26:53]
  **Train** Prec@1 69.134 Prec@5 97.354 Error@1 30.866
  **Test** Prec@1 75.620 Prec@5 98.310 Error@1 24.380

==>>[2025-10-23 00:27:13] [Epoch=038/040] [Need: 00:01:24] [LR=0.0010] [Best : Accuracy=75.70, Error=24.30]
  Epoch: [038][000/500]   Time 17.675 (17.675)   Data 17.628 (17.628)   Loss 0.8288 (0.8288)   Prec@1 67.000 (67.000)   Prec@5 99.000 (99.000)   [2025-10-23 00:27:31]
  Epoch: [038][100/500]   Time 0.011 (0.188)   Data 0.001 (0.175)   Loss 0.7854 (0.8921)   Prec@1 70.000 (68.703)   Prec@5 99.000 (97.287)   [2025-10-23 00:27:32]
  Epoch: [038][200/500]   Time 0.009 (0.099)   Data 0.000 (0.088)   Loss 0.9278 (0.8902)   Prec@1 69.000 (68.990)   Prec@5 96.000 (97.323)   [2025-10-23 00:27:33]
  Epoch: [038][300/500]   Time 0.010 (0.070)   Data 0.000 (0.059)   Loss 0.9290 (0.8853)   Prec@1 69.000 (69.073)   Prec@5 98.000 (97.458)   [2025-10-23 00:27:34]
  Epoch: [038][400/500]   Time 0.010 (0.055)   Data 0.000 (0.044)   Loss 0.9136 (0.8857)   Prec@1 67.000 (69.137)   Prec@5 99.000 (97.469)   [2025-10-23 00:27:35]
  **Train** Prec@1 69.220 Prec@5 97.456 Error@1 30.780
  **Test** Prec@1 75.760 Prec@5 98.360 Error@1 24.240
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:27:55] [Epoch=039/040] [Need: 00:00:42] [LR=0.0010] [Best : Accuracy=75.76, Error=24.24]
  Epoch: [039][000/500]   Time 17.621 (17.621)   Data 17.577 (17.577)   Loss 0.8074 (0.8074)   Prec@1 72.000 (72.000)   Prec@5 98.000 (98.000)   [2025-10-23 00:28:13]
  Epoch: [039][100/500]   Time 0.010 (0.187)   Data 0.000 (0.174)   Loss 0.8713 (0.8897)   Prec@1 74.000 (68.584)   Prec@5 95.000 (97.366)   [2025-10-23 00:28:14]
  Epoch: [039][200/500]   Time 0.012 (0.099)   Data 0.000 (0.088)   Loss 0.8180 (0.8918)   Prec@1 71.000 (68.517)   Prec@5 97.000 (97.358)   [2025-10-23 00:28:15]
  Epoch: [039][300/500]   Time 0.014 (0.070)   Data 0.000 (0.059)   Loss 0.8455 (0.8834)   Prec@1 68.000 (68.860)   Prec@5 99.000 (97.488)   [2025-10-23 00:28:16]
  Epoch: [039][400/500]   Time 0.011 (0.055)   Data 0.001 (0.044)   Loss 0.8579 (0.8830)   Prec@1 76.000 (68.998)   Prec@5 96.000 (97.471)   [2025-10-23 00:28:17]
  **Train** Prec@1 68.986 Prec@5 97.472 Error@1 31.014
  **Test** Prec@1 75.630 Prec@5 98.380 Error@1 24.370
