save path : ./save/tinyvgg_quan/randbet_0.1_0.01_10_-1
{'data_path': './dataset', 'arch': 'tinyvgg_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 1, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': True, 'clipping_coeff': 0.1, 'learning_rate': 0.01, 'manualSeed': 344, 'save_path': './save/tinyvgg_quan/randbet_0.1_0.01_10_-1', 'enable_bfa': False, 'resume': '', 'quan_bitwidth': None, 'reset_weight': False, 'evaluate': False, 'n_iter': 30, 'model_only': False, 'random_bfa': False, 'use_cuda': False}
Random Seed: 344
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'tinyvgg_quan'
=> network :
 TinyVGG(
  (features): Sequential(
    (0): quan_Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Dropout2d(p=0.25, inplace=False)
    (6): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace=True)
    (8): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace=True)
    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (11): Dropout2d(p=0.25, inplace=False)
  )
  (classifier): Sequential(
    (0): quan_Linear(in_features=2048, out_features=10, bias=True)
  )
)
=> do not use any checkpoint for tinyvgg_quan model

==>>[2025-10-22 21:55:16] [Epoch=000/040] [Need: 00:00:00] [LR=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/500]   Time 17.741 (17.741)   Data 17.616 (17.616)   Loss 2.3011 (2.3011)   Prec@1 8.000 (8.000)   Prec@5 51.000 (51.000)   [2025-10-22 21:55:34]
  Epoch: [000][100/500]   Time 0.101 (0.278)   Data 0.001 (0.175)   Loss 2.1198 (2.2528)   Prec@1 20.000 (14.624)   Prec@5 76.000 (60.366)   [2025-10-22 21:55:44]
  Epoch: [000][200/500]   Time 0.104 (0.244)   Data 0.000 (0.088)   Loss 1.8606 (2.1429)   Prec@1 33.000 (20.299)   Prec@5 79.000 (68.433)   [2025-10-22 21:56:05]
  Epoch: [000][300/500]   Time 0.104 (0.198)   Data 0.001 (0.059)   Loss 1.8787 (2.0635)   Prec@1 35.000 (23.801)   Prec@5 85.000 (73.186)   [2025-10-22 21:56:15]
  Epoch: [000][400/500]   Time 0.102 (0.174)   Data 0.000 (0.045)   Loss 1.8214 (1.9962)   Prec@1 30.000 (26.676)   Prec@5 88.000 (76.324)   [2025-10-22 21:56:26]
  **Train** Prec@1 28.594 Prec@5 78.300 Error@1 71.406
  **Test** Prec@1 44.420 Prec@5 90.370 Error@1 55.580
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 21:56:56] [Epoch=001/040] [Need: 01:04:47] [LR=0.0100] [Best : Accuracy=44.42, Error=55.58]
  Epoch: [001][000/500]   Time 17.759 (17.759)   Data 17.638 (17.638)   Loss 1.5294 (1.5294)   Prec@1 43.000 (43.000)   Prec@5 93.000 (93.000)   [2025-10-22 21:57:13]
  Epoch: [001][100/500]   Time 0.101 (0.279)   Data 0.001 (0.175)   Loss 1.7665 (1.6877)   Prec@1 37.000 (38.109)   Prec@5 83.000 (88.525)   [2025-10-22 21:57:24]
  Epoch: [001][200/500]   Time 0.104 (0.192)   Data 0.000 (0.088)   Loss 1.5890 (1.6646)   Prec@1 39.000 (39.254)   Prec@5 93.000 (88.746)   [2025-10-22 21:57:34]
  Epoch: [001][300/500]   Time 0.115 (0.163)   Data 0.001 (0.059)   Loss 1.4847 (1.6482)   Prec@1 50.000 (39.774)   Prec@5 92.000 (88.887)   [2025-10-22 21:57:45]
  Epoch: [001][400/500]   Time 0.106 (0.149)   Data 0.001 (0.045)   Loss 1.6578 (1.6348)   Prec@1 33.000 (40.389)   Prec@5 87.000 (89.160)   [2025-10-22 21:57:55]
  **Train** Prec@1 41.152 Prec@5 89.452 Error@1 58.848
  **Test** Prec@1 53.470 Prec@5 94.070 Error@1 46.530
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 21:58:31] [Epoch=002/040] [Need: 01:01:51] [LR=0.0100] [Best : Accuracy=53.47, Error=46.53]
  Epoch: [002][000/500]   Time 19.149 (19.149)   Data 19.031 (19.031)   Loss 1.3668 (1.3668)   Prec@1 50.000 (50.000)   Prec@5 96.000 (96.000)   [2025-10-22 21:58:50]
  Epoch: [002][100/500]   Time 0.104 (0.297)   Data 0.000 (0.189)   Loss 1.5050 (1.5264)   Prec@1 47.000 (44.218)   Prec@5 93.000 (91.010)   [2025-10-22 21:59:01]
  Epoch: [002][200/500]   Time 0.115 (0.206)   Data 0.001 (0.095)   Loss 1.5334 (1.5133)   Prec@1 46.000 (44.970)   Prec@5 93.000 (91.139)   [2025-10-22 21:59:13]
  Epoch: [002][300/500]   Time 0.108 (0.175)   Data 0.000 (0.064)   Loss 1.5702 (1.5089)   Prec@1 46.000 (45.113)   Prec@5 89.000 (91.193)   [2025-10-22 21:59:24]
  Epoch: [002][400/500]   Time 0.113 (0.158)   Data 0.001 (0.048)   Loss 1.7007 (1.5006)   Prec@1 40.000 (45.648)   Prec@5 90.000 (91.317)   [2025-10-22 21:59:35]
  **Train** Prec@1 46.140 Prec@5 91.522 Error@1 53.860
  **Test** Prec@1 55.640 Prec@5 95.350 Error@1 44.360
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 22:00:07] [Epoch=003/040] [Need: 00:59:48] [LR=0.0100] [Best : Accuracy=55.64, Error=44.36]
  Epoch: [003][000/500]   Time 19.425 (19.425)   Data 19.309 (19.309)   Loss 1.5845 (1.5845)   Prec@1 44.000 (44.000)   Prec@5 95.000 (95.000)   [2025-10-22 22:00:26]
  Epoch: [003][100/500]   Time 0.112 (0.300)   Data 0.001 (0.192)   Loss 1.5225 (1.4202)   Prec@1 44.000 (48.792)   Prec@5 94.000 (92.485)   [2025-10-22 22:00:37]
  Epoch: [003][200/500]   Time 0.114 (0.205)   Data 0.000 (0.097)   Loss 1.4233 (1.4175)   Prec@1 50.000 (49.154)   Prec@5 94.000 (92.592)   [2025-10-22 22:00:48]
  Epoch: [003][300/500]   Time 0.116 (0.174)   Data 0.000 (0.065)   Loss 1.5658 (1.4115)   Prec@1 35.000 (49.256)   Prec@5 94.000 (92.535)   [2025-10-22 22:00:59]
  Epoch: [003][400/500]   Time 0.109 (0.159)   Data 0.001 (0.049)   Loss 1.3971 (1.4069)   Prec@1 53.000 (49.564)   Prec@5 93.000 (92.621)   [2025-10-22 22:01:10]
  **Train** Prec@1 49.998 Prec@5 92.832 Error@1 50.002
  **Test** Prec@1 59.830 Prec@5 95.670 Error@1 40.170
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 22:01:43] [Epoch=004/040] [Need: 00:58:05] [LR=0.0100] [Best : Accuracy=59.83, Error=40.17]
  Epoch: [004][000/500]   Time 21.791 (21.791)   Data 21.664 (21.664)   Loss 1.1755 (1.1755)   Prec@1 55.000 (55.000)   Prec@5 98.000 (98.000)   [2025-10-22 22:02:05]
  Epoch: [004][100/500]   Time 0.113 (0.342)   Data 0.001 (0.215)   Loss 1.5437 (1.3292)   Prec@1 49.000 (53.050)   Prec@5 93.000 (93.356)   [2025-10-22 22:02:18]
  Epoch: [004][200/500]   Time 0.117 (0.230)   Data 0.001 (0.108)   Loss 1.2336 (1.3291)   Prec@1 51.000 (52.866)   Prec@5 98.000 (93.468)   [2025-10-22 22:02:29]
  Epoch: [004][300/500]   Time 0.113 (0.191)   Data 0.001 (0.073)   Loss 1.3256 (1.3219)   Prec@1 55.000 (53.116)   Prec@5 92.000 (93.542)   [2025-10-22 22:02:41]
  Epoch: [004][400/500]   Time 0.111 (0.171)   Data 0.001 (0.055)   Loss 1.3649 (1.3151)   Prec@1 51.000 (53.406)   Prec@5 92.000 (93.621)   [2025-10-22 22:02:52]
  **Train** Prec@1 53.456 Prec@5 93.668 Error@1 46.544
  **Test** Prec@1 63.210 Prec@5 96.470 Error@1 36.790
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 22:03:24] [Epoch=005/040] [Need: 00:56:58] [LR=0.0100] [Best : Accuracy=63.21, Error=36.79]
  Epoch: [005][000/500]   Time 19.559 (19.559)   Data 19.438 (19.438)   Loss 1.4225 (1.4225)   Prec@1 48.000 (48.000)   Prec@5 93.000 (93.000)   [2025-10-22 22:03:44]
  Epoch: [005][100/500]   Time 0.107 (0.299)   Data 0.001 (0.193)   Loss 1.3585 (1.3021)   Prec@1 52.000 (54.208)   Prec@5 97.000 (93.792)   [2025-10-22 22:03:55]
  Epoch: [005][200/500]   Time 0.117 (0.204)   Data 0.001 (0.097)   Loss 1.1287 (1.2830)   Prec@1 58.000 (54.393)   Prec@5 97.000 (94.020)   [2025-10-22 22:04:05]
  Epoch: [005][300/500]   Time 0.106 (0.172)   Data 0.000 (0.065)   Loss 1.2190 (1.2686)   Prec@1 58.000 (54.960)   Prec@5 96.000 (94.279)   [2025-10-22 22:04:16]
  Epoch: [005][400/500]   Time 0.108 (0.156)   Data 0.000 (0.049)   Loss 1.2681 (1.2596)   Prec@1 52.000 (55.252)   Prec@5 93.000 (94.362)   [2025-10-22 22:04:27]
  **Train** Prec@1 55.560 Prec@5 94.390 Error@1 44.440
  **Test** Prec@1 64.200 Prec@5 96.540 Error@1 35.800
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 22:04:59] [Epoch=006/040] [Need: 00:55:05] [LR=0.0100] [Best : Accuracy=64.20, Error=35.80]
  Epoch: [006][000/500]   Time 19.281 (19.281)   Data 19.163 (19.163)   Loss 1.1818 (1.1818)   Prec@1 53.000 (53.000)   Prec@5 94.000 (94.000)   [2025-10-22 22:05:19]
  Epoch: [006][100/500]   Time 0.108 (0.295)   Data 0.001 (0.190)   Loss 1.2984 (1.2235)   Prec@1 53.000 (55.990)   Prec@5 96.000 (94.861)   [2025-10-22 22:05:29]
  Epoch: [006][200/500]   Time 0.105 (0.202)   Data 0.000 (0.096)   Loss 1.2591 (1.2098)   Prec@1 53.000 (56.572)   Prec@5 96.000 (94.816)   [2025-10-22 22:05:40]
  Epoch: [006][300/500]   Time 0.106 (0.171)   Data 0.000 (0.064)   Loss 1.1597 (1.2106)   Prec@1 56.000 (56.757)   Prec@5 96.000 (94.698)   [2025-10-22 22:05:51]
  Epoch: [006][400/500]   Time 0.107 (0.155)   Data 0.000 (0.048)   Loss 1.1493 (1.2093)   Prec@1 61.000 (56.833)   Prec@5 95.000 (94.751)   [2025-10-22 22:06:01]
  **Train** Prec@1 56.838 Prec@5 94.738 Error@1 43.162
  **Test** Prec@1 65.800 Prec@5 96.800 Error@1 34.200
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 22:06:34] [Epoch=007/040] [Need: 00:53:14] [LR=0.0100] [Best : Accuracy=65.80, Error=34.20]
  Epoch: [007][000/500]   Time 18.152 (18.152)   Data 18.044 (18.044)   Loss 1.1794 (1.1794)   Prec@1 57.000 (57.000)   Prec@5 98.000 (98.000)   [2025-10-22 22:06:52]
  Epoch: [007][100/500]   Time 0.108 (0.285)   Data 0.001 (0.179)   Loss 1.2460 (1.1797)   Prec@1 51.000 (57.584)   Prec@5 98.000 (95.129)   [2025-10-22 22:07:02]
  Epoch: [007][200/500]   Time 0.108 (0.195)   Data 0.000 (0.090)   Loss 1.1354 (1.1704)   Prec@1 59.000 (58.318)   Prec@5 96.000 (95.259)   [2025-10-22 22:07:13]
  Epoch: [007][300/500]   Time 0.105 (0.166)   Data 0.000 (0.061)   Loss 1.1487 (1.1728)   Prec@1 61.000 (58.415)   Prec@5 93.000 (95.163)   [2025-10-22 22:07:24]
  Epoch: [007][400/500]   Time 0.103 (0.151)   Data 0.000 (0.046)   Loss 1.3904 (1.1759)   Prec@1 47.000 (58.319)   Prec@5 97.000 (95.130)   [2025-10-22 22:07:34]
  **Train** Prec@1 58.532 Prec@5 95.166 Error@1 41.468
  **Test** Prec@1 67.170 Prec@5 97.150 Error@1 32.830
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 22:08:06] [Epoch=008/040] [Need: 00:51:21] [LR=0.0100] [Best : Accuracy=67.17, Error=32.83]
  Epoch: [008][000/500]   Time 17.777 (17.777)   Data 17.665 (17.665)   Loss 1.1901 (1.1901)   Prec@1 62.000 (62.000)   Prec@5 97.000 (97.000)   [2025-10-22 22:08:24]
  Epoch: [008][100/500]   Time 0.104 (0.280)   Data 0.000 (0.175)   Loss 1.1403 (1.1553)   Prec@1 59.000 (59.020)   Prec@5 97.000 (95.426)   [2025-10-22 22:08:35]
  Epoch: [008][200/500]   Time 0.112 (0.193)   Data 0.001 (0.088)   Loss 1.0664 (1.1548)   Prec@1 67.000 (59.254)   Prec@5 94.000 (95.378)   [2025-10-22 22:08:45]
  Epoch: [008][300/500]   Time 0.107 (0.165)   Data 0.001 (0.059)   Loss 1.0988 (1.1510)   Prec@1 61.000 (59.468)   Prec@5 95.000 (95.372)   [2025-10-22 22:08:56]
  Epoch: [008][400/500]   Time 0.104 (0.151)   Data 0.001 (0.045)   Loss 1.0296 (1.1492)   Prec@1 61.000 (59.539)   Prec@5 98.000 (95.237)   [2025-10-22 22:09:07]
  **Train** Prec@1 59.376 Prec@5 95.296 Error@1 40.624
  **Test** Prec@1 67.100 Prec@5 97.220 Error@1 32.900

==>>[2025-10-22 22:09:37] [Epoch=009/040] [Need: 00:49:26] [LR=0.0100] [Best : Accuracy=67.17, Error=32.83]
  Epoch: [009][000/500]   Time 18.524 (18.524)   Data 18.411 (18.411)   Loss 1.0683 (1.0683)   Prec@1 62.000 (62.000)   Prec@5 99.000 (99.000)   [2025-10-22 22:09:56]
  Epoch: [009][100/500]   Time 0.104 (0.289)   Data 0.000 (0.183)   Loss 1.1571 (1.1264)   Prec@1 62.000 (60.426)   Prec@5 93.000 (95.733)   [2025-10-22 22:10:06]
  Epoch: [009][200/500]   Time 0.112 (0.199)   Data 0.001 (0.092)   Loss 1.1496 (1.1277)   Prec@1 59.000 (60.179)   Prec@5 94.000 (95.607)   [2025-10-22 22:10:17]
  Epoch: [009][300/500]   Time 0.105 (0.169)   Data 0.001 (0.062)   Loss 0.9502 (1.1276)   Prec@1 67.000 (60.066)   Prec@5 93.000 (95.621)   [2025-10-22 22:10:28]
  Epoch: [009][400/500]   Time 0.104 (0.154)   Data 0.001 (0.047)   Loss 1.0306 (1.1275)   Prec@1 68.000 (60.187)   Prec@5 94.000 (95.599)   [2025-10-22 22:10:39]
  **Train** Prec@1 60.298 Prec@5 95.590 Error@1 39.702
  **Test** Prec@1 68.050 Prec@5 97.240 Error@1 31.950
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 22:11:09] [Epoch=010/040] [Need: 00:47:40] [LR=0.0100] [Best : Accuracy=68.05, Error=31.95]
  Epoch: [010][000/500]   Time 19.097 (19.097)   Data 18.980 (18.980)   Loss 1.0231 (1.0231)   Prec@1 54.000 (54.000)   Prec@5 97.000 (97.000)   [2025-10-22 22:11:28]
  Epoch: [010][100/500]   Time 0.104 (0.293)   Data 0.001 (0.189)   Loss 1.2076 (1.1176)   Prec@1 60.000 (60.525)   Prec@5 94.000 (95.772)   [2025-10-22 22:11:39]
  Epoch: [010][200/500]   Time 0.102 (0.200)   Data 0.000 (0.095)   Loss 1.1871 (1.1249)   Prec@1 66.000 (60.035)   Prec@5 96.000 (95.637)   [2025-10-22 22:11:49]
  Epoch: [010][300/500]   Time 0.103 (0.168)   Data 0.000 (0.064)   Loss 1.0909 (1.1140)   Prec@1 62.000 (60.571)   Prec@5 96.000 (95.728)   [2025-10-22 22:12:00]
  Epoch: [010][400/500]   Time 0.101 (0.153)   Data 0.001 (0.048)   Loss 1.0533 (1.1072)   Prec@1 58.000 (60.800)   Prec@5 97.000 (95.798)   [2025-10-22 22:12:11]
  **Train** Prec@1 60.936 Prec@5 95.830 Error@1 39.064
  **Test** Prec@1 69.480 Prec@5 97.520 Error@1 30.520
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 22:12:42] [Epoch=011/040] [Need: 00:45:58] [LR=0.0100] [Best : Accuracy=69.48, Error=30.52]
