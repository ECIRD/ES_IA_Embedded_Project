save path : ./save/tinyvgg_quan/randbet_0.1_0.01_10_-1
{'data_path': './dataset', 'arch': 'tinyvgg_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': True, 'clipping_coeff': 0.1, 'learning_rate': 0.01, 'manualSeed': 7303, 'save_path': './save/tinyvgg_quan/randbet_0.1_0.01_10_-1', 'enable_bfa': False, 'resume': '', 'quan_bitwidth': None, 'reset_weight': False, 'evaluate': False, 'n_iter': 30, 'model_only': False, 'random_bfa': False, 'use_cuda': True}
Random Seed: 7303
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'tinyvgg_quan'
=> network :
 TinyVGG(
  (features): Sequential(
    (0): quan_Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Dropout2d(p=0.25, inplace=False)
    (6): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace=True)
    (8): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace=True)
    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (11): Dropout2d(p=0.25, inplace=False)
  )
  (classifier): Sequential(
    (0): quan_Linear(in_features=2048, out_features=10, bias=True)
  )
)
=> do not use any checkpoint for tinyvgg_quan model

==>>[2025-10-22 22:16:32] [Epoch=000/040] [Need: 00:00:00] [LR=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/500]   Time 18.307 (18.307)   Data 17.459 (17.459)   Loss 2.3004 (2.3004)   Prec@1 12.000 (12.000)   Prec@5 54.000 (54.000)   [2025-10-22 22:16:51]
  Epoch: [000][100/500]   Time 0.051 (0.231)   Data 0.001 (0.174)   Loss 2.0103 (2.2203)   Prec@1 22.000 (17.149)   Prec@5 78.000 (62.802)   [2025-10-22 22:16:56]
  Epoch: [000][200/500]   Time 0.054 (0.142)   Data 0.001 (0.088)   Loss 1.7968 (2.1062)   Prec@1 33.000 (22.557)   Prec@5 84.000 (70.811)   [2025-10-22 22:17:01]
  Epoch: [000][300/500]   Time 0.055 (0.113)   Data 0.000 (0.059)   Loss 1.7842 (2.0227)   Prec@1 28.000 (26.013)   Prec@5 83.000 (75.282)   [2025-10-22 22:17:06]
  Epoch: [000][400/500]   Time 0.054 (0.098)   Data 0.001 (0.044)   Loss 1.8416 (1.9629)   Prec@1 29.000 (28.204)   Prec@5 86.000 (78.037)   [2025-10-22 22:17:12]
  **Train** Prec@1 30.034 Prec@5 79.836 Error@1 69.966
  **Test** Prec@1 45.190 Prec@5 91.500 Error@1 54.810
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 22:17:35] [Epoch=001/040] [Need: 00:40:48] [LR=0.0100] [Best : Accuracy=45.19, Error=54.81]
  Epoch: [001][000/500]   Time 17.805 (17.805)   Data 17.624 (17.624)   Loss 1.8113 (1.8113)   Prec@1 37.000 (37.000)   Prec@5 82.000 (82.000)   [2025-10-22 22:17:53]
  Epoch: [001][100/500]   Time 0.051 (0.224)   Data 0.001 (0.175)   Loss 1.5440 (1.6620)   Prec@1 49.000 (39.980)   Prec@5 91.000 (88.386)   [2025-10-22 22:17:58]
  Epoch: [001][200/500]   Time 0.053 (0.138)   Data 0.000 (0.088)   Loss 1.5093 (1.6518)   Prec@1 50.000 (40.119)   Prec@5 90.000 (88.562)   [2025-10-22 22:18:03]
  Epoch: [001][300/500]   Time 0.055 (0.110)   Data 0.001 (0.059)   Loss 1.5043 (1.6327)   Prec@1 45.000 (40.787)   Prec@5 92.000 (89.179)   [2025-10-22 22:18:09]
  Epoch: [001][400/500]   Time 0.055 (0.096)   Data 0.000 (0.045)   Loss 1.5937 (1.6133)   Prec@1 42.000 (41.461)   Prec@5 93.000 (89.516)   [2025-10-22 22:18:14]
  **Train** Prec@1 42.116 Prec@5 89.848 Error@1 57.884
  **Test** Prec@1 53.120 Prec@5 94.120 Error@1 46.880
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 22:18:38] [Epoch=002/040] [Need: 00:39:34] [LR=0.0100] [Best : Accuracy=53.12, Error=46.88]
  Epoch: [002][000/500]   Time 17.695 (17.695)   Data 17.511 (17.511)   Loss 1.5668 (1.5668)   Prec@1 47.000 (47.000)   Prec@5 90.000 (90.000)   [2025-10-22 22:18:55]
  Epoch: [002][100/500]   Time 0.050 (0.224)   Data 0.000 (0.174)   Loss 1.4407 (1.4796)   Prec@1 51.000 (46.465)   Prec@5 93.000 (91.733)   [2025-10-22 22:19:00]
  Epoch: [002][200/500]   Time 0.053 (0.139)   Data 0.000 (0.088)   Loss 1.4475 (1.4842)   Prec@1 44.000 (46.000)   Prec@5 93.000 (91.851)   [2025-10-22 22:19:05]
  Epoch: [002][300/500]   Time 0.057 (0.111)   Data 0.001 (0.059)   Loss 1.3827 (1.4838)   Prec@1 54.000 (46.256)   Prec@5 94.000 (91.794)   [2025-10-22 22:19:11]
  Epoch: [002][400/500]   Time 0.055 (0.097)   Data 0.001 (0.044)   Loss 1.2659 (1.4808)   Prec@1 55.000 (46.374)   Prec@5 96.000 (91.823)   [2025-10-22 22:19:16]
  **Train** Prec@1 46.838 Prec@5 91.922 Error@1 53.162
  **Test** Prec@1 55.740 Prec@5 94.610 Error@1 44.260
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 22:19:40] [Epoch=003/040] [Need: 00:38:33] [LR=0.0100] [Best : Accuracy=55.74, Error=44.26]
  Epoch: [003][000/500]   Time 18.307 (18.307)   Data 18.126 (18.126)   Loss 1.3593 (1.3593)   Prec@1 50.000 (50.000)   Prec@5 93.000 (93.000)   [2025-10-22 22:19:58]
  Epoch: [003][100/500]   Time 0.051 (0.230)   Data 0.001 (0.180)   Loss 1.3429 (1.4085)   Prec@1 56.000 (49.406)   Prec@5 93.000 (92.772)   [2025-10-22 22:20:03]
  Epoch: [003][200/500]   Time 0.055 (0.142)   Data 0.000 (0.091)   Loss 1.2482 (1.3986)   Prec@1 63.000 (49.995)   Prec@5 93.000 (93.070)   [2025-10-22 22:20:09]
  Epoch: [003][300/500]   Time 0.054 (0.113)   Data 0.001 (0.061)   Loss 1.3169 (1.3886)   Prec@1 52.000 (50.336)   Prec@5 95.000 (93.103)   [2025-10-22 22:20:14]
  Epoch: [003][400/500]   Time 0.053 (0.098)   Data 0.000 (0.046)   Loss 1.2647 (1.3832)   Prec@1 56.000 (50.444)   Prec@5 94.000 (93.185)   [2025-10-22 22:20:19]
  **Train** Prec@1 50.870 Prec@5 93.242 Error@1 49.130
  **Test** Prec@1 60.860 Prec@5 95.760 Error@1 39.140
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 22:20:44] [Epoch=004/040] [Need: 00:37:44] [LR=0.0100] [Best : Accuracy=60.86, Error=39.14]
  Epoch: [004][000/500]   Time 22.739 (22.739)   Data 22.555 (22.555)   Loss 1.4813 (1.4813)   Prec@1 50.000 (50.000)   Prec@5 91.000 (91.000)   [2025-10-22 22:21:07]
  Epoch: [004][100/500]   Time 0.057 (0.283)   Data 0.001 (0.224)   Loss 1.1986 (1.3315)   Prec@1 62.000 (51.782)   Prec@5 93.000 (93.634)   [2025-10-22 22:21:13]
  Epoch: [004][200/500]   Time 0.056 (0.171)   Data 0.001 (0.113)   Loss 1.3227 (1.3206)   Prec@1 54.000 (52.502)   Prec@5 94.000 (93.657)   [2025-10-22 22:21:19]
  Epoch: [004][300/500]   Time 0.056 (0.133)   Data 0.000 (0.076)   Loss 1.2889 (1.3220)   Prec@1 52.000 (52.512)   Prec@5 94.000 (93.731)   [2025-10-22 22:21:24]
  Epoch: [004][400/500]   Time 0.052 (0.113)   Data 0.001 (0.057)   Loss 1.3969 (1.3093)   Prec@1 50.000 (52.945)   Prec@5 91.000 (93.870)   [2025-10-22 22:21:30]
  **Train** Prec@1 53.142 Prec@5 94.002 Error@1 46.858
  **Test** Prec@1 63.910 Prec@5 96.530 Error@1 36.090
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 22:21:54] [Epoch=005/040] [Need: 00:37:28] [LR=0.0100] [Best : Accuracy=63.91, Error=36.09]
  Epoch: [005][000/500]   Time 18.331 (18.331)   Data 18.150 (18.150)   Loss 1.3420 (1.3420)   Prec@1 54.000 (54.000)   Prec@5 93.000 (93.000)   [2025-10-22 22:22:12]
  Epoch: [005][100/500]   Time 0.055 (0.231)   Data 0.001 (0.180)   Loss 1.2794 (1.2659)   Prec@1 53.000 (54.663)   Prec@5 92.000 (94.109)   [2025-10-22 22:22:17]
  Epoch: [005][200/500]   Time 0.053 (0.143)   Data 0.001 (0.091)   Loss 1.1725 (1.2629)   Prec@1 58.000 (54.886)   Prec@5 93.000 (94.254)   [2025-10-22 22:22:22]
  Epoch: [005][300/500]   Time 0.054 (0.113)   Data 0.000 (0.061)   Loss 1.1712 (1.2613)   Prec@1 50.000 (55.243)   Prec@5 94.000 (94.309)   [2025-10-22 22:22:28]
  Epoch: [005][400/500]   Time 0.055 (0.099)   Data 0.001 (0.046)   Loss 1.1365 (1.2523)   Prec@1 60.000 (55.511)   Prec@5 96.000 (94.441)   [2025-10-22 22:22:33]
  **Train** Prec@1 55.642 Prec@5 94.532 Error@1 44.358
  **Test** Prec@1 65.310 Prec@5 97.020 Error@1 34.690
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 22:22:57] [Epoch=006/040] [Need: 00:36:20] [LR=0.0100] [Best : Accuracy=65.31, Error=34.69]
  Epoch: [006][000/500]   Time 17.902 (17.902)   Data 17.715 (17.715)   Loss 1.2031 (1.2031)   Prec@1 56.000 (56.000)   Prec@5 96.000 (96.000)   [2025-10-22 22:23:15]
  Epoch: [006][100/500]   Time 0.050 (0.225)   Data 0.000 (0.176)   Loss 1.0206 (1.2143)   Prec@1 66.000 (56.772)   Prec@5 97.000 (94.832)   [2025-10-22 22:23:20]
  Epoch: [006][200/500]   Time 0.056 (0.139)   Data 0.000 (0.089)   Loss 1.1487 (1.2100)   Prec@1 52.000 (56.746)   Prec@5 97.000 (94.766)   [2025-10-22 22:23:25]
  Epoch: [006][300/500]   Time 0.054 (0.111)   Data 0.001 (0.059)   Loss 1.3897 (1.2090)   Prec@1 51.000 (56.907)   Prec@5 90.000 (94.797)   [2025-10-22 22:23:31]
  Epoch: [006][400/500]   Time 0.053 (0.097)   Data 0.000 (0.045)   Loss 1.1689 (1.2090)   Prec@1 60.000 (57.112)   Prec@5 99.000 (94.733)   [2025-10-22 22:23:36]
  **Train** Prec@1 57.394 Prec@5 94.824 Error@1 42.606
  **Test** Prec@1 66.530 Prec@5 96.680 Error@1 33.470
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 22:24:01] [Epoch=007/040] [Need: 00:35:14] [LR=0.0100] [Best : Accuracy=66.53, Error=33.47]
  Epoch: [007][000/500]   Time 18.741 (18.741)   Data 18.557 (18.557)   Loss 1.2572 (1.2572)   Prec@1 57.000 (57.000)   Prec@5 94.000 (94.000)   [2025-10-22 22:24:20]
  Epoch: [007][100/500]   Time 0.049 (0.233)   Data 0.001 (0.184)   Loss 1.1709 (1.1928)   Prec@1 61.000 (57.703)   Prec@5 97.000 (94.842)   [2025-10-22 22:24:25]
  Epoch: [007][200/500]   Time 0.052 (0.143)   Data 0.000 (0.093)   Loss 1.2885 (1.1795)   Prec@1 56.000 (58.249)   Prec@5 96.000 (94.980)   [2025-10-22 22:24:30]
  Epoch: [007][300/500]   Time 0.053 (0.113)   Data 0.001 (0.062)   Loss 1.2484 (1.1789)   Prec@1 51.000 (58.399)   Prec@5 95.000 (95.020)   [2025-10-22 22:24:35]
  Epoch: [007][400/500]   Time 0.056 (0.099)   Data 0.000 (0.047)   Loss 1.1616 (1.1770)   Prec@1 59.000 (58.431)   Prec@5 95.000 (94.968)   [2025-10-22 22:24:41]
  **Train** Prec@1 58.506 Prec@5 95.118 Error@1 41.494
  **Test** Prec@1 66.810 Prec@5 97.060 Error@1 33.190
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 22:25:05] [Epoch=008/040] [Need: 00:34:11] [LR=0.0100] [Best : Accuracy=66.81, Error=33.19]
  Epoch: [008][000/500]   Time 17.837 (17.837)   Data 17.653 (17.653)   Loss 1.1348 (1.1348)   Prec@1 56.000 (56.000)   Prec@5 96.000 (96.000)   [2025-10-22 22:25:23]
  Epoch: [008][100/500]   Time 0.052 (0.225)   Data 0.001 (0.175)   Loss 1.1976 (1.1623)   Prec@1 58.000 (58.881)   Prec@5 95.000 (95.248)   [2025-10-22 22:25:28]
  Epoch: [008][200/500]   Time 0.053 (0.140)   Data 0.000 (0.089)   Loss 1.1788 (1.1558)   Prec@1 63.000 (59.109)   Prec@5 96.000 (95.274)   [2025-10-22 22:25:33]
  Epoch: [008][300/500]   Time 0.059 (0.112)   Data 0.001 (0.059)   Loss 1.0010 (1.1478)   Prec@1 66.000 (59.409)   Prec@5 97.000 (95.296)   [2025-10-22 22:25:39]
  Epoch: [008][400/500]   Time 0.054 (0.098)   Data 0.001 (0.045)   Loss 0.9050 (1.1474)   Prec@1 68.000 (59.387)   Prec@5 100.000 (95.409)   [2025-10-22 22:25:45]
  **Train** Prec@1 59.536 Prec@5 95.342 Error@1 40.464
  **Test** Prec@1 68.230 Prec@5 97.460 Error@1 31.770
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 22:26:09] [Epoch=009/040] [Need: 00:33:05] [LR=0.0100] [Best : Accuracy=68.23, Error=31.77]
  Epoch: [009][000/500]   Time 18.153 (18.153)   Data 17.967 (17.967)   Loss 1.0511 (1.0511)   Prec@1 67.000 (67.000)   Prec@5 97.000 (97.000)   [2025-10-22 22:26:27]
  Epoch: [009][100/500]   Time 0.052 (0.229)   Data 0.001 (0.179)   Loss 1.3099 (1.1275)   Prec@1 51.000 (59.743)   Prec@5 94.000 (95.752)   [2025-10-22 22:26:32]
  Epoch: [009][200/500]   Time 0.053 (0.142)   Data 0.000 (0.090)   Loss 1.0749 (1.1303)   Prec@1 56.000 (60.134)   Prec@5 99.000 (95.821)   [2025-10-22 22:26:37]
  Epoch: [009][300/500]   Time 0.053 (0.113)   Data 0.001 (0.060)   Loss 1.2001 (1.1259)   Prec@1 54.000 (60.226)   Prec@5 97.000 (95.708)   [2025-10-22 22:26:43]
  Epoch: [009][400/500]   Time 0.058 (0.100)   Data 0.001 (0.045)   Loss 1.2584 (1.1252)   Prec@1 51.000 (60.242)   Prec@5 94.000 (95.589)   [2025-10-22 22:26:49]
  **Train** Prec@1 60.252 Prec@5 95.558 Error@1 39.748
  **Test** Prec@1 69.380 Prec@5 97.360 Error@1 30.620
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 22:27:13] [Epoch=010/040] [Need: 00:32:01] [LR=0.0100] [Best : Accuracy=69.38, Error=30.62]
  Epoch: [010][000/500]   Time 17.699 (17.699)   Data 17.517 (17.517)   Loss 1.2636 (1.2636)   Prec@1 58.000 (58.000)   Prec@5 92.000 (92.000)   [2025-10-22 22:27:31]
  Epoch: [010][100/500]   Time 0.052 (0.224)   Data 0.001 (0.174)   Loss 0.9393 (1.0963)   Prec@1 71.000 (61.356)   Prec@5 97.000 (96.050)   [2025-10-22 22:27:36]
  Epoch: [010][200/500]   Time 0.054 (0.139)   Data 0.001 (0.088)   Loss 1.4010 (1.1083)   Prec@1 52.000 (60.831)   Prec@5 91.000 (95.856)   [2025-10-22 22:27:41]
  Epoch: [010][300/500]   Time 0.054 (0.111)   Data 0.001 (0.059)   Loss 1.1159 (1.1104)   Prec@1 57.000 (60.774)   Prec@5 98.000 (95.734)   [2025-10-22 22:27:46]
  Epoch: [010][400/500]   Time 0.055 (0.097)   Data 0.001 (0.044)   Loss 1.0155 (1.0990)   Prec@1 61.000 (61.224)   Prec@5 98.000 (95.875)   [2025-10-22 22:27:52]
  **Train** Prec@1 61.112 Prec@5 95.830 Error@1 38.888
  **Test** Prec@1 68.970 Prec@5 97.400 Error@1 31.030

==>>[2025-10-22 22:28:16] [Epoch=011/040] [Need: 00:30:53] [LR=0.0100] [Best : Accuracy=69.38, Error=30.62]
  Epoch: [011][000/500]   Time 17.704 (17.704)   Data 17.522 (17.522)   Loss 1.1787 (1.1787)   Prec@1 57.000 (57.000)   Prec@5 94.000 (94.000)   [2025-10-22 22:28:33]
  Epoch: [011][100/500]   Time 0.051 (0.222)   Data 0.001 (0.174)   Loss 1.0025 (1.0747)   Prec@1 62.000 (62.455)   Prec@5 95.000 (95.772)   [2025-10-22 22:28:38]
  Epoch: [011][200/500]   Time 0.054 (0.138)   Data 0.000 (0.088)   Loss 1.1530 (1.0872)   Prec@1 60.000 (61.741)   Prec@5 96.000 (95.876)   [2025-10-22 22:28:43]
  Epoch: [011][300/500]   Time 0.056 (0.110)   Data 0.000 (0.059)   Loss 0.9495 (1.0875)   Prec@1 65.000 (61.880)   Prec@5 97.000 (95.884)   [2025-10-22 22:28:49]
  Epoch: [011][400/500]   Time 0.055 (0.096)   Data 0.001 (0.044)   Loss 1.1606 (1.0883)   Prec@1 61.000 (61.863)   Prec@5 95.000 (95.873)   [2025-10-22 22:28:54]
  **Train** Prec@1 61.828 Prec@5 95.848 Error@1 38.172
  **Test** Prec@1 69.790 Prec@5 97.680 Error@1 30.210
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 22:29:18] [Epoch=012/040] [Need: 00:29:46] [LR=0.0100] [Best : Accuracy=69.79, Error=30.21]
  Epoch: [012][000/500]   Time 17.530 (17.530)   Data 17.350 (17.350)   Loss 1.2399 (1.2399)   Prec@1 59.000 (59.000)   Prec@5 93.000 (93.000)   [2025-10-22 22:29:36]
  Epoch: [012][100/500]   Time 0.049 (0.222)   Data 0.000 (0.172)   Loss 1.0796 (1.0753)   Prec@1 61.000 (61.921)   Prec@5 94.000 (96.030)   [2025-10-22 22:29:41]
  Epoch: [012][200/500]   Time 0.056 (0.139)   Data 0.001 (0.087)   Loss 1.2880 (1.0778)   Prec@1 61.000 (62.154)   Prec@5 97.000 (95.900)   [2025-10-22 22:29:46]
  Epoch: [012][300/500]   Time 0.055 (0.111)   Data 0.001 (0.058)   Loss 0.9928 (1.0760)   Prec@1 64.000 (62.110)   Prec@5 95.000 (95.804)   [2025-10-22 22:29:51]
  Epoch: [012][400/500]   Time 0.052 (0.097)   Data 0.000 (0.044)   Loss 0.9506 (1.0759)   Prec@1 69.000 (62.157)   Prec@5 99.000 (95.838)   [2025-10-22 22:29:57]
  **Train** Prec@1 62.240 Prec@5 95.870 Error@1 37.760
  **Test** Prec@1 70.480 Prec@5 97.610 Error@1 29.520
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 22:30:21] [Epoch=013/040] [Need: 00:28:40] [LR=0.0100] [Best : Accuracy=70.48, Error=29.52]
  Epoch: [013][000/500]   Time 17.636 (17.636)   Data 17.449 (17.449)   Loss 1.0319 (1.0319)   Prec@1 66.000 (66.000)   Prec@5 99.000 (99.000)   [2025-10-22 22:30:38]
  Epoch: [013][100/500]   Time 0.051 (0.224)   Data 0.001 (0.173)   Loss 0.9948 (1.0826)   Prec@1 70.000 (61.752)   Prec@5 95.000 (96.069)   [2025-10-22 22:30:43]
  Epoch: [013][200/500]   Time 0.055 (0.139)   Data 0.001 (0.087)   Loss 0.9998 (1.0803)   Prec@1 68.000 (61.871)   Prec@5 95.000 (96.050)   [2025-10-22 22:30:49]
  Epoch: [013][300/500]   Time 0.058 (0.111)   Data 0.000 (0.059)   Loss 1.0633 (1.0745)   Prec@1 62.000 (62.226)   Prec@5 95.000 (96.063)   [2025-10-22 22:30:54]
  Epoch: [013][400/500]   Time 0.056 (0.097)   Data 0.001 (0.044)   Loss 1.0170 (1.0721)   Prec@1 65.000 (62.332)   Prec@5 97.000 (96.125)   [2025-10-22 22:31:00]
  **Train** Prec@1 62.254 Prec@5 96.096 Error@1 37.746
  **Test** Prec@1 70.460 Prec@5 97.500 Error@1 29.540

==>>[2025-10-22 22:31:24] [Epoch=014/040] [Need: 00:27:35] [LR=0.0100] [Best : Accuracy=70.48, Error=29.52]
  Epoch: [014][000/500]   Time 17.906 (17.906)   Data 17.711 (17.711)   Loss 1.0937 (1.0937)   Prec@1 62.000 (62.000)   Prec@5 96.000 (96.000)   [2025-10-22 22:31:42]
  Epoch: [014][100/500]   Time 0.051 (0.227)   Data 0.000 (0.176)   Loss 0.9348 (1.0438)   Prec@1 68.000 (63.584)   Prec@5 98.000 (96.287)   [2025-10-22 22:31:47]
  Epoch: [014][200/500]   Time 0.059 (0.141)   Data 0.000 (0.089)   Loss 1.0775 (1.0541)   Prec@1 61.000 (63.313)   Prec@5 98.000 (96.249)   [2025-10-22 22:31:52]
  Epoch: [014][300/500]   Time 0.057 (0.112)   Data 0.001 (0.059)   Loss 1.1903 (1.0626)   Prec@1 61.000 (62.884)   Prec@5 95.000 (96.203)   [2025-10-22 22:31:57]
  Epoch: [014][400/500]   Time 0.052 (0.098)   Data 0.000 (0.045)   Loss 1.1088 (1.0576)   Prec@1 60.000 (63.020)   Prec@5 96.000 (96.185)   [2025-10-22 22:32:03]
  **Train** Prec@1 62.944 Prec@5 96.226 Error@1 37.056
  **Test** Prec@1 71.700 Prec@5 97.760 Error@1 28.300
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 22:32:27] [Epoch=015/040] [Need: 00:26:31] [LR=0.0100] [Best : Accuracy=71.70, Error=28.30]
  Epoch: [015][000/500]   Time 17.767 (17.767)   Data 17.576 (17.576)   Loss 1.0046 (1.0046)   Prec@1 59.000 (59.000)   Prec@5 96.000 (96.000)   [2025-10-22 22:32:45]
  Epoch: [015][100/500]   Time 0.049 (0.223)   Data 0.001 (0.175)   Loss 1.0349 (1.0441)   Prec@1 64.000 (63.376)   Prec@5 97.000 (96.050)   [2025-10-22 22:32:50]
  Epoch: [015][200/500]   Time 0.053 (0.138)   Data 0.001 (0.088)   Loss 0.9568 (1.0355)   Prec@1 67.000 (63.841)   Prec@5 95.000 (96.209)   [2025-10-22 22:32:55]
  Epoch: [015][300/500]   Time 0.054 (0.110)   Data 0.001 (0.059)   Loss 0.8910 (1.0389)   Prec@1 66.000 (63.641)   Prec@5 98.000 (96.233)   [2025-10-22 22:33:00]
  Epoch: [015][400/500]   Time 0.054 (0.096)   Data 0.000 (0.044)   Loss 0.9347 (1.0419)   Prec@1 65.000 (63.456)   Prec@5 96.000 (96.177)   [2025-10-22 22:33:06]
  **Train** Prec@1 63.464 Prec@5 96.238 Error@1 36.536
  **Test** Prec@1 72.030 Prec@5 97.750 Error@1 27.970
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 22:33:30] [Epoch=016/040] [Need: 00:25:25] [LR=0.0100] [Best : Accuracy=72.03, Error=27.97]
  Epoch: [016][000/500]   Time 17.623 (17.623)   Data 17.439 (17.439)   Loss 0.8858 (0.8858)   Prec@1 67.000 (67.000)   Prec@5 98.000 (98.000)   [2025-10-22 22:33:47]
  Epoch: [016][100/500]   Time 0.049 (0.224)   Data 0.001 (0.173)   Loss 1.1241 (1.0341)   Prec@1 62.000 (64.030)   Prec@5 94.000 (96.356)   [2025-10-22 22:33:52]
  Epoch: [016][200/500]   Time 0.054 (0.139)   Data 0.000 (0.087)   Loss 1.0035 (1.0405)   Prec@1 63.000 (63.647)   Prec@5 98.000 (96.284)   [2025-10-22 22:33:58]
  Epoch: [016][300/500]   Time 0.055 (0.111)   Data 0.000 (0.059)   Loss 1.0493 (1.0398)   Prec@1 66.000 (63.635)   Prec@5 94.000 (96.312)   [2025-10-22 22:34:03]
  Epoch: [016][400/500]   Time 0.056 (0.097)   Data 0.001 (0.044)   Loss 0.9545 (1.0431)   Prec@1 66.000 (63.459)   Prec@5 98.000 (96.234)   [2025-10-22 22:34:08]
  **Train** Prec@1 63.442 Prec@5 96.212 Error@1 36.558
  **Test** Prec@1 71.890 Prec@5 97.640 Error@1 28.110

==>>[2025-10-22 22:34:32] [Epoch=017/040] [Need: 00:24:21] [LR=0.0100] [Best : Accuracy=72.03, Error=27.97]
  Epoch: [017][000/500]   Time 19.684 (19.684)   Data 19.489 (19.489)   Loss 1.1784 (1.1784)   Prec@1 60.000 (60.000)   Prec@5 95.000 (95.000)   [2025-10-22 22:34:52]
  Epoch: [017][100/500]   Time 0.050 (0.243)   Data 0.000 (0.194)   Loss 1.0088 (1.0060)   Prec@1 63.000 (64.990)   Prec@5 98.000 (96.307)   [2025-10-22 22:34:57]
  Epoch: [017][200/500]   Time 0.054 (0.149)   Data 0.001 (0.098)   Loss 0.8514 (1.0247)   Prec@1 76.000 (64.060)   Prec@5 97.000 (96.234)   [2025-10-22 22:35:02]
  Epoch: [017][300/500]   Time 0.053 (0.117)   Data 0.000 (0.065)   Loss 0.8466 (1.0282)   Prec@1 70.000 (64.066)   Prec@5 98.000 (96.209)   [2025-10-22 22:35:08]
  Epoch: [017][400/500]   Time 0.054 (0.101)   Data 0.001 (0.049)   Loss 0.9548 (1.0326)   Prec@1 68.000 (63.850)   Prec@5 97.000 (96.289)   [2025-10-22 22:35:13]
  **Train** Prec@1 63.772 Prec@5 96.306 Error@1 36.228
  **Test** Prec@1 71.570 Prec@5 97.840 Error@1 28.430

==>>[2025-10-22 22:35:37] [Epoch=018/040] [Need: 00:23:18] [LR=0.0100] [Best : Accuracy=72.03, Error=27.97]
  Epoch: [018][000/500]   Time 18.561 (18.561)   Data 18.381 (18.381)   Loss 0.8897 (0.8897)   Prec@1 65.000 (65.000)   Prec@5 96.000 (96.000)   [2025-10-22 22:35:56]
  Epoch: [018][100/500]   Time 0.050 (0.232)   Data 0.001 (0.183)   Loss 0.8301 (1.0164)   Prec@1 63.000 (63.752)   Prec@5 100.000 (96.594)   [2025-10-22 22:36:01]
  Epoch: [018][200/500]   Time 0.056 (0.143)   Data 0.001 (0.092)   Loss 1.0938 (1.0275)   Prec@1 56.000 (63.443)   Prec@5 97.000 (96.398)   [2025-10-22 22:36:06]
  Epoch: [018][300/500]   Time 0.054 (0.114)   Data 0.000 (0.062)   Loss 0.9920 (1.0278)   Prec@1 65.000 (63.621)   Prec@5 96.000 (96.355)   [2025-10-22 22:36:11]
  Epoch: [018][400/500]   Time 0.053 (0.099)   Data 0.000 (0.046)   Loss 1.0719 (1.0267)   Prec@1 65.000 (63.718)   Prec@5 96.000 (96.342)   [2025-10-22 22:36:17]
  **Train** Prec@1 63.742 Prec@5 96.334 Error@1 36.258
  **Test** Prec@1 72.190 Prec@5 97.870 Error@1 27.810
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 22:36:41] [Epoch=019/040] [Need: 00:22:15] [LR=0.0100] [Best : Accuracy=72.19, Error=27.81]
  Epoch: [019][000/500]   Time 17.835 (17.835)   Data 17.646 (17.646)   Loss 0.9510 (0.9510)   Prec@1 66.000 (66.000)   Prec@5 97.000 (97.000)   [2025-10-22 22:36:59]
  Epoch: [019][100/500]   Time 0.050 (0.226)   Data 0.001 (0.175)   Loss 0.9119 (1.0364)   Prec@1 67.000 (63.822)   Prec@5 96.000 (96.327)   [2025-10-22 22:37:04]
  Epoch: [019][200/500]   Time 0.054 (0.140)   Data 0.001 (0.088)   Loss 0.8541 (1.0236)   Prec@1 68.000 (64.348)   Prec@5 98.000 (96.443)   [2025-10-22 22:37:09]
  Epoch: [019][300/500]   Time 0.054 (0.112)   Data 0.001 (0.059)   Loss 1.0786 (1.0257)   Prec@1 58.000 (64.173)   Prec@5 97.000 (96.266)   [2025-10-22 22:37:14]
  Epoch: [019][400/500]   Time 0.055 (0.097)   Data 0.001 (0.045)   Loss 0.9223 (1.0243)   Prec@1 66.000 (64.127)   Prec@5 97.000 (96.332)   [2025-10-22 22:37:20]
  **Train** Prec@1 64.208 Prec@5 96.384 Error@1 35.792
  **Test** Prec@1 70.920 Prec@5 97.750 Error@1 29.080

==>>[2025-10-22 22:37:44] [Epoch=020/040] [Need: 00:21:11] [LR=0.0100] [Best : Accuracy=72.19, Error=27.81]
  Epoch: [020][000/500]   Time 17.559 (17.559)   Data 17.378 (17.378)   Loss 1.1704 (1.1704)   Prec@1 60.000 (60.000)   Prec@5 93.000 (93.000)   [2025-10-22 22:38:01]
  Epoch: [020][100/500]   Time 0.052 (0.223)   Data 0.001 (0.173)   Loss 0.9934 (1.0277)   Prec@1 62.000 (64.139)   Prec@5 96.000 (96.436)   [2025-10-22 22:38:06]
  Epoch: [020][200/500]   Time 0.055 (0.139)   Data 0.001 (0.087)   Loss 1.0406 (1.0242)   Prec@1 61.000 (64.199)   Prec@5 96.000 (96.507)   [2025-10-22 22:38:12]
  Epoch: [020][300/500]   Time 0.053 (0.111)   Data 0.000 (0.058)   Loss 1.1536 (1.0158)   Prec@1 61.000 (64.478)   Prec@5 94.000 (96.468)   [2025-10-22 22:38:17]
  Epoch: [020][400/500]   Time 0.054 (0.097)   Data 0.001 (0.044)   Loss 0.8419 (1.0135)   Prec@1 67.000 (64.631)   Prec@5 99.000 (96.419)   [2025-10-22 22:38:22]
  **Train** Prec@1 64.420 Prec@5 96.392 Error@1 35.580
  **Test** Prec@1 72.790 Prec@5 98.020 Error@1 27.210
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 22:38:47] [Epoch=021/040] [Need: 00:20:07] [LR=0.0100] [Best : Accuracy=72.79, Error=27.21]
  Epoch: [021][000/500]   Time 17.749 (17.749)   Data 17.564 (17.564)   Loss 1.2728 (1.2728)   Prec@1 57.000 (57.000)   Prec@5 92.000 (92.000)   [2025-10-22 22:39:04]
  Epoch: [021][100/500]   Time 0.051 (0.224)   Data 0.001 (0.174)   Loss 0.9044 (0.9982)   Prec@1 68.000 (64.990)   Prec@5 97.000 (96.545)   [2025-10-22 22:39:09]
  Epoch: [021][200/500]   Time 0.053 (0.139)   Data 0.000 (0.088)   Loss 1.1968 (1.0068)   Prec@1 59.000 (64.716)   Prec@5 95.000 (96.502)   [2025-10-22 22:39:15]
  Epoch: [021][300/500]   Time 0.054 (0.111)   Data 0.001 (0.059)   Loss 1.0035 (1.0066)   Prec@1 63.000 (64.591)   Prec@5 98.000 (96.551)   [2025-10-22 22:39:20]
  Epoch: [021][400/500]   Time 0.053 (0.097)   Data 0.000 (0.044)   Loss 1.0499 (1.0101)   Prec@1 62.000 (64.481)   Prec@5 99.000 (96.509)   [2025-10-22 22:39:25]
  **Train** Prec@1 64.468 Prec@5 96.530 Error@1 35.532
  **Test** Prec@1 71.860 Prec@5 97.980 Error@1 28.140

==>>[2025-10-22 22:39:50] [Epoch=022/040] [Need: 00:19:03] [LR=0.0100] [Best : Accuracy=72.79, Error=27.21]
  Epoch: [022][000/500]   Time 17.918 (17.918)   Data 17.728 (17.728)   Loss 0.9748 (0.9748)   Prec@1 69.000 (69.000)   Prec@5 98.000 (98.000)   [2025-10-22 22:40:08]
  Epoch: [022][100/500]   Time 0.051 (0.227)   Data 0.000 (0.176)   Loss 1.0005 (0.9852)   Prec@1 68.000 (65.426)   Prec@5 97.000 (96.624)   [2025-10-22 22:40:13]
  Epoch: [022][200/500]   Time 0.054 (0.140)   Data 0.000 (0.089)   Loss 0.9536 (0.9959)   Prec@1 64.000 (65.224)   Prec@5 99.000 (96.627)   [2025-10-22 22:40:18]
  Epoch: [022][300/500]   Time 0.055 (0.112)   Data 0.001 (0.059)   Loss 0.9245 (1.0044)   Prec@1 63.000 (64.831)   Prec@5 97.000 (96.565)   [2025-10-22 22:40:24]
  Epoch: [022][400/500]   Time 0.059 (0.097)   Data 0.001 (0.045)   Loss 1.2306 (1.0079)   Prec@1 58.000 (64.673)   Prec@5 97.000 (96.584)   [2025-10-22 22:40:29]
  **Train** Prec@1 64.746 Prec@5 96.578 Error@1 35.254
  **Test** Prec@1 72.690 Prec@5 97.930 Error@1 27.310

==>>[2025-10-22 22:40:54] [Epoch=023/040] [Need: 00:18:00] [LR=0.0100] [Best : Accuracy=72.79, Error=27.21]
  Epoch: [023][000/500]   Time 17.791 (17.791)   Data 17.609 (17.609)   Loss 0.9485 (0.9485)   Prec@1 63.000 (63.000)   Prec@5 97.000 (97.000)   [2025-10-22 22:41:12]
  Epoch: [023][100/500]   Time 0.051 (0.226)   Data 0.001 (0.175)   Loss 1.0491 (1.0297)   Prec@1 65.000 (63.475)   Prec@5 95.000 (96.584)   [2025-10-22 22:41:17]
  Epoch: [023][200/500]   Time 0.054 (0.140)   Data 0.001 (0.088)   Loss 0.9657 (1.0054)   Prec@1 70.000 (64.498)   Prec@5 97.000 (96.692)   [2025-10-22 22:41:22]
  Epoch: [023][300/500]   Time 0.052 (0.111)   Data 0.001 (0.059)   Loss 0.9840 (1.0014)   Prec@1 69.000 (64.601)   Prec@5 100.000 (96.645)   [2025-10-22 22:41:28]
  Epoch: [023][400/500]   Time 0.055 (0.097)   Data 0.001 (0.045)   Loss 0.8806 (1.0004)   Prec@1 68.000 (64.825)   Prec@5 97.000 (96.536)   [2025-10-22 22:41:33]
  **Train** Prec@1 64.812 Prec@5 96.474 Error@1 35.188
  **Test** Prec@1 73.130 Prec@5 97.920 Error@1 26.870
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 22:41:57] [Epoch=024/040] [Need: 00:16:56] [LR=0.0100] [Best : Accuracy=73.13, Error=26.87]
  Epoch: [024][000/500]   Time 17.691 (17.691)   Data 17.496 (17.496)   Loss 0.9955 (0.9955)   Prec@1 62.000 (62.000)   Prec@5 98.000 (98.000)   [2025-10-22 22:42:14]
  Epoch: [024][100/500]   Time 0.052 (0.225)   Data 0.001 (0.174)   Loss 1.1219 (1.0054)   Prec@1 61.000 (64.386)   Prec@5 96.000 (96.376)   [2025-10-22 22:42:19]
  Epoch: [024][200/500]   Time 0.056 (0.139)   Data 0.000 (0.088)   Loss 0.8468 (1.0000)   Prec@1 74.000 (64.806)   Prec@5 98.000 (96.413)   [2025-10-22 22:42:25]
  Epoch: [024][300/500]   Time 0.053 (0.111)   Data 0.000 (0.059)   Loss 1.0604 (1.0011)   Prec@1 66.000 (64.983)   Prec@5 92.000 (96.505)   [2025-10-22 22:42:30]
  Epoch: [024][400/500]   Time 0.054 (0.097)   Data 0.000 (0.044)   Loss 0.9034 (0.9965)   Prec@1 65.000 (65.217)   Prec@5 99.000 (96.516)   [2025-10-22 22:42:36]
  **Train** Prec@1 65.226 Prec@5 96.530 Error@1 34.774
  **Test** Prec@1 72.350 Prec@5 97.970 Error@1 27.650

==>>[2025-10-22 22:42:59] [Epoch=025/040] [Need: 00:15:52] [LR=0.0010] [Best : Accuracy=73.13, Error=26.87]
  Epoch: [025][000/500]   Time 17.736 (17.736)   Data 17.550 (17.550)   Loss 0.9465 (0.9465)   Prec@1 70.000 (70.000)   Prec@5 93.000 (93.000)   [2025-10-22 22:43:17]
  Epoch: [025][100/500]   Time 0.049 (0.225)   Data 0.001 (0.174)   Loss 0.8383 (0.9436)   Prec@1 71.000 (67.208)   Prec@5 98.000 (96.911)   [2025-10-22 22:43:22]
  Epoch: [025][200/500]   Time 0.055 (0.140)   Data 0.001 (0.088)   Loss 0.9518 (0.9359)   Prec@1 63.000 (67.095)   Prec@5 95.000 (96.970)   [2025-10-22 22:43:28]
  Epoch: [025][300/500]   Time 0.053 (0.111)   Data 0.001 (0.059)   Loss 1.1035 (0.9302)   Prec@1 65.000 (67.435)   Prec@5 96.000 (97.003)   [2025-10-22 22:43:33]
  Epoch: [025][400/500]   Time 0.053 (0.097)   Data 0.001 (0.044)   Loss 1.0209 (0.9293)   Prec@1 62.000 (67.446)   Prec@5 97.000 (97.032)   [2025-10-22 22:43:38]
  **Train** Prec@1 67.390 Prec@5 96.980 Error@1 32.610
  **Test** Prec@1 74.300 Prec@5 98.160 Error@1 25.700
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 22:44:02] [Epoch=026/040] [Need: 00:14:48] [LR=0.0010] [Best : Accuracy=74.30, Error=25.70]
  Epoch: [026][000/500]   Time 17.650 (17.650)   Data 17.463 (17.463)   Loss 1.1049 (1.1049)   Prec@1 63.000 (63.000)   Prec@5 94.000 (94.000)   [2025-10-22 22:44:20]
  Epoch: [026][100/500]   Time 0.049 (0.224)   Data 0.001 (0.174)   Loss 0.9254 (0.9283)   Prec@1 71.000 (67.614)   Prec@5 97.000 (97.267)   [2025-10-22 22:44:25]
  Epoch: [026][200/500]   Time 0.052 (0.138)   Data 0.000 (0.087)   Loss 1.1345 (0.9301)   Prec@1 66.000 (67.582)   Prec@5 93.000 (97.179)   [2025-10-22 22:44:30]
  Epoch: [026][300/500]   Time 0.055 (0.110)   Data 0.001 (0.059)   Loss 0.9639 (0.9208)   Prec@1 69.000 (68.027)   Prec@5 97.000 (97.249)   [2025-10-22 22:44:36]
  Epoch: [026][400/500]   Time 0.055 (0.096)   Data 0.000 (0.044)   Loss 0.8518 (0.9152)   Prec@1 70.000 (68.100)   Prec@5 98.000 (97.307)   [2025-10-22 22:44:41]
  **Train** Prec@1 67.972 Prec@5 97.248 Error@1 32.028
  **Test** Prec@1 74.730 Prec@5 98.320 Error@1 25.270
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 22:45:05] [Epoch=027/040] [Need: 00:13:44] [LR=0.0010] [Best : Accuracy=74.73, Error=25.27]
  Epoch: [027][000/500]   Time 17.875 (17.875)   Data 17.678 (17.678)   Loss 1.0018 (1.0018)   Prec@1 67.000 (67.000)   Prec@5 94.000 (94.000)   [2025-10-22 22:45:23]
  Epoch: [027][100/500]   Time 0.050 (0.227)   Data 0.000 (0.176)   Loss 1.1333 (0.9100)   Prec@1 60.000 (68.614)   Prec@5 99.000 (97.228)   [2025-10-22 22:45:28]
  Epoch: [027][200/500]   Time 0.056 (0.140)   Data 0.000 (0.089)   Loss 0.8286 (0.9174)   Prec@1 71.000 (68.279)   Prec@5 98.000 (97.109)   [2025-10-22 22:45:34]
  Epoch: [027][300/500]   Time 0.054 (0.112)   Data 0.001 (0.059)   Loss 0.9080 (0.9130)   Prec@1 64.000 (68.236)   Prec@5 95.000 (97.193)   [2025-10-22 22:45:39]
  Epoch: [027][400/500]   Time 0.052 (0.098)   Data 0.001 (0.045)   Loss 0.9986 (0.9113)   Prec@1 64.000 (68.304)   Prec@5 95.000 (97.222)   [2025-10-22 22:45:45]
  **Train** Prec@1 68.334 Prec@5 97.248 Error@1 31.666
  **Test** Prec@1 74.880 Prec@5 98.280 Error@1 25.120
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 22:46:09] [Epoch=028/040] [Need: 00:12:41] [LR=0.0010] [Best : Accuracy=74.88, Error=25.12]
  Epoch: [028][000/500]   Time 17.725 (17.725)   Data 17.540 (17.540)   Loss 0.8501 (0.8501)   Prec@1 68.000 (68.000)   Prec@5 97.000 (97.000)   [2025-10-22 22:46:27]
  Epoch: [028][100/500]   Time 0.051 (0.224)   Data 0.000 (0.174)   Loss 0.8198 (0.9152)   Prec@1 71.000 (68.614)   Prec@5 98.000 (96.911)   [2025-10-22 22:46:32]
  Epoch: [028][200/500]   Time 0.052 (0.139)   Data 0.001 (0.088)   Loss 0.8024 (0.9173)   Prec@1 68.000 (68.532)   Prec@5 100.000 (97.035)   [2025-10-22 22:46:37]
  Epoch: [028][300/500]   Time 0.053 (0.111)   Data 0.000 (0.059)   Loss 0.9203 (0.9077)   Prec@1 64.000 (68.721)   Prec@5 98.000 (97.106)   [2025-10-22 22:46:42]
  Epoch: [028][400/500]   Time 0.052 (0.097)   Data 0.001 (0.044)   Loss 0.9020 (0.9083)   Prec@1 68.000 (68.529)   Prec@5 97.000 (97.162)   [2025-10-22 22:46:48]
  **Train** Prec@1 68.484 Prec@5 97.202 Error@1 31.516
  **Test** Prec@1 74.850 Prec@5 98.260 Error@1 25.150

==>>[2025-10-22 22:47:12] [Epoch=029/040] [Need: 00:11:37] [LR=0.0010] [Best : Accuracy=74.88, Error=25.12]
  Epoch: [029][000/500]   Time 17.800 (17.800)   Data 17.613 (17.613)   Loss 0.7694 (0.7694)   Prec@1 79.000 (79.000)   Prec@5 96.000 (96.000)   [2025-10-22 22:47:30]
  Epoch: [029][100/500]   Time 0.052 (0.225)   Data 0.001 (0.175)   Loss 1.0300 (0.9039)   Prec@1 65.000 (69.040)   Prec@5 95.000 (97.238)   [2025-10-22 22:47:35]
  Epoch: [029][200/500]   Time 0.054 (0.139)   Data 0.001 (0.088)   Loss 0.9331 (0.8966)   Prec@1 67.000 (68.776)   Prec@5 96.000 (97.343)   [2025-10-22 22:47:40]
  Epoch: [029][300/500]   Time 0.057 (0.111)   Data 0.001 (0.059)   Loss 0.9487 (0.8972)   Prec@1 68.000 (68.811)   Prec@5 96.000 (97.332)   [2025-10-22 22:47:45]
  Epoch: [029][400/500]   Time 0.054 (0.097)   Data 0.000 (0.045)   Loss 0.8226 (0.9017)   Prec@1 70.000 (68.514)   Prec@5 97.000 (97.294)   [2025-10-22 22:47:51]
  **Train** Prec@1 68.552 Prec@5 97.252 Error@1 31.448
  **Test** Prec@1 75.220 Prec@5 98.310 Error@1 24.780
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 22:48:15] [Epoch=030/040] [Need: 00:10:34] [LR=0.0010] [Best : Accuracy=75.22, Error=24.78]
  Epoch: [030][000/500]   Time 17.748 (17.748)   Data 17.538 (17.538)   Loss 0.9066 (0.9066)   Prec@1 68.000 (68.000)   Prec@5 99.000 (99.000)   [2025-10-22 22:48:33]
  Epoch: [030][100/500]   Time 0.052 (0.228)   Data 0.000 (0.174)   Loss 1.1044 (0.8888)   Prec@1 62.000 (69.168)   Prec@5 94.000 (97.406)   [2025-10-22 22:48:38]
  Epoch: [030][200/500]   Time 0.054 (0.141)   Data 0.001 (0.088)   Loss 0.9338 (0.9060)   Prec@1 70.000 (68.368)   Prec@5 95.000 (97.269)   [2025-10-22 22:48:43]
  Epoch: [030][300/500]   Time 0.052 (0.112)   Data 0.000 (0.059)   Loss 0.9172 (0.9020)   Prec@1 73.000 (68.678)   Prec@5 95.000 (97.243)   [2025-10-22 22:48:49]
  Epoch: [030][400/500]   Time 0.052 (0.098)   Data 0.001 (0.044)   Loss 0.7899 (0.8997)   Prec@1 75.000 (68.676)   Prec@5 98.000 (97.227)   [2025-10-22 22:48:54]
  **Train** Prec@1 68.750 Prec@5 97.258 Error@1 31.250
  **Test** Prec@1 75.030 Prec@5 98.260 Error@1 24.970

==>>[2025-10-22 22:49:19] [Epoch=031/040] [Need: 00:09:30] [LR=0.0010] [Best : Accuracy=75.22, Error=24.78]
  Epoch: [031][000/500]   Time 17.885 (17.885)   Data 17.705 (17.705)   Loss 0.9062 (0.9062)   Prec@1 68.000 (68.000)   Prec@5 97.000 (97.000)   [2025-10-22 22:49:36]
  Epoch: [031][100/500]   Time 0.054 (0.228)   Data 0.001 (0.176)   Loss 1.0110 (0.9239)   Prec@1 62.000 (67.851)   Prec@5 96.000 (97.069)   [2025-10-22 22:49:42]
  Epoch: [031][200/500]   Time 0.052 (0.141)   Data 0.000 (0.089)   Loss 0.9663 (0.9135)   Prec@1 70.000 (68.318)   Prec@5 95.000 (97.189)   [2025-10-22 22:49:47]
  Epoch: [031][300/500]   Time 0.057 (0.113)   Data 0.001 (0.059)   Loss 0.9008 (0.9057)   Prec@1 67.000 (68.658)   Prec@5 96.000 (97.259)   [2025-10-22 22:49:53]
  Epoch: [031][400/500]   Time 0.059 (0.099)   Data 0.001 (0.045)   Loss 0.6789 (0.9017)   Prec@1 76.000 (68.893)   Prec@5 99.000 (97.257)   [2025-10-22 22:49:58]
  **Train** Prec@1 68.892 Prec@5 97.274 Error@1 31.108
  **Test** Prec@1 75.330 Prec@5 98.270 Error@1 24.670
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 22:50:22] [Epoch=032/040] [Need: 00:08:27] [LR=0.0010] [Best : Accuracy=75.33, Error=24.67]
  Epoch: [032][000/500]   Time 18.394 (18.394)   Data 18.212 (18.212)   Loss 0.9857 (0.9857)   Prec@1 67.000 (67.000)   Prec@5 97.000 (97.000)   [2025-10-22 22:50:41]
  Epoch: [032][100/500]   Time 0.054 (0.238)   Data 0.001 (0.181)   Loss 0.8466 (0.9013)   Prec@1 72.000 (68.782)   Prec@5 98.000 (97.218)   [2025-10-22 22:50:46]
  Epoch: [032][200/500]   Time 0.054 (0.146)   Data 0.000 (0.091)   Loss 0.9116 (0.8932)   Prec@1 66.000 (68.692)   Prec@5 96.000 (97.483)   [2025-10-22 22:50:52]
  Epoch: [032][300/500]   Time 0.053 (0.116)   Data 0.001 (0.061)   Loss 0.7248 (0.8916)   Prec@1 79.000 (68.877)   Prec@5 97.000 (97.445)   [2025-10-22 22:50:57]
  Epoch: [032][400/500]   Time 0.054 (0.100)   Data 0.001 (0.046)   Loss 1.0844 (0.8874)   Prec@1 61.000 (69.072)   Prec@5 97.000 (97.404)   [2025-10-22 22:51:03]
  **Train** Prec@1 68.990 Prec@5 97.362 Error@1 31.010
  **Test** Prec@1 75.130 Prec@5 98.230 Error@1 24.870

==>>[2025-10-22 22:51:27] [Epoch=033/040] [Need: 00:07:24] [LR=0.0010] [Best : Accuracy=75.33, Error=24.67]
  Epoch: [033][000/500]   Time 17.628 (17.628)   Data 17.448 (17.448)   Loss 0.7673 (0.7673)   Prec@1 70.000 (70.000)   Prec@5 100.000 (100.000)   [2025-10-22 22:51:45]
  Epoch: [033][100/500]   Time 0.051 (0.224)   Data 0.000 (0.173)   Loss 1.0735 (0.9026)   Prec@1 61.000 (68.752)   Prec@5 96.000 (97.248)   [2025-10-22 22:51:50]
  Epoch: [033][200/500]   Time 0.054 (0.139)   Data 0.001 (0.087)   Loss 0.7879 (0.9011)   Prec@1 77.000 (68.726)   Prec@5 98.000 (97.164)   [2025-10-22 22:51:55]
  Epoch: [033][300/500]   Time 0.053 (0.111)   Data 0.000 (0.059)   Loss 0.8343 (0.8984)   Prec@1 72.000 (68.960)   Prec@5 97.000 (97.173)   [2025-10-22 22:52:00]
  Epoch: [033][400/500]   Time 0.054 (0.096)   Data 0.001 (0.044)   Loss 1.0435 (0.8969)   Prec@1 60.000 (68.878)   Prec@5 96.000 (97.155)   [2025-10-22 22:52:06]
  **Train** Prec@1 68.886 Prec@5 97.206 Error@1 31.114
  **Test** Prec@1 75.690 Prec@5 98.330 Error@1 24.310
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 22:52:30] [Epoch=034/040] [Need: 00:06:20] [LR=0.0010] [Best : Accuracy=75.69, Error=24.31]
  Epoch: [034][000/500]   Time 17.809 (17.809)   Data 17.623 (17.623)   Loss 0.8654 (0.8654)   Prec@1 69.000 (69.000)   Prec@5 98.000 (98.000)   [2025-10-22 22:52:48]
  Epoch: [034][100/500]   Time 0.050 (0.225)   Data 0.000 (0.175)   Loss 0.7605 (0.9048)   Prec@1 73.000 (68.277)   Prec@5 99.000 (97.228)   [2025-10-22 22:52:52]
  Epoch: [034][200/500]   Time 0.054 (0.139)   Data 0.001 (0.088)   Loss 0.8804 (0.8903)   Prec@1 75.000 (68.891)   Prec@5 97.000 (97.348)   [2025-10-22 22:52:58]
  Epoch: [034][300/500]   Time 0.053 (0.111)   Data 0.001 (0.059)   Loss 0.8726 (0.8875)   Prec@1 68.000 (69.033)   Prec@5 97.000 (97.332)   [2025-10-22 22:53:03]
  Epoch: [034][400/500]   Time 0.055 (0.097)   Data 0.001 (0.045)   Loss 1.1205 (0.8892)   Prec@1 60.000 (69.027)   Prec@5 97.000 (97.327)   [2025-10-22 22:53:09]
  **Train** Prec@1 68.894 Prec@5 97.334 Error@1 31.106
  **Test** Prec@1 75.880 Prec@5 98.390 Error@1 24.120
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 22:53:33] [Epoch=035/040] [Need: 00:05:17] [LR=0.0010] [Best : Accuracy=75.88, Error=24.12]
  Epoch: [035][000/500]   Time 18.331 (18.331)   Data 18.149 (18.149)   Loss 0.8895 (0.8895)   Prec@1 66.000 (66.000)   Prec@5 96.000 (96.000)   [2025-10-22 22:53:51]
  Epoch: [035][100/500]   Time 0.051 (0.231)   Data 0.001 (0.180)   Loss 1.0341 (0.9018)   Prec@1 66.000 (68.416)   Prec@5 93.000 (97.238)   [2025-10-22 22:53:56]
  Epoch: [035][200/500]   Time 0.053 (0.143)   Data 0.000 (0.091)   Loss 0.8654 (0.8822)   Prec@1 69.000 (68.990)   Prec@5 99.000 (97.473)   [2025-10-22 22:54:01]
  Epoch: [035][300/500]   Time 0.054 (0.113)   Data 0.000 (0.061)   Loss 0.8442 (0.8840)   Prec@1 71.000 (68.910)   Prec@5 94.000 (97.442)   [2025-10-22 22:54:07]
  Epoch: [035][400/500]   Time 0.055 (0.099)   Data 0.001 (0.046)   Loss 0.8392 (0.8889)   Prec@1 67.000 (68.820)   Prec@5 97.000 (97.349)   [2025-10-22 22:54:12]
  **Train** Prec@1 68.866 Prec@5 97.394 Error@1 31.134
  **Test** Prec@1 75.560 Prec@5 98.290 Error@1 24.440

==>>[2025-10-22 22:54:36] [Epoch=036/040] [Need: 00:04:13] [LR=0.0010] [Best : Accuracy=75.88, Error=24.12]
  Epoch: [036][000/500]   Time 18.745 (18.745)   Data 18.548 (18.548)   Loss 0.8561 (0.8561)   Prec@1 72.000 (72.000)   Prec@5 97.000 (97.000)   [2025-10-22 22:54:55]
  Epoch: [036][100/500]   Time 0.052 (0.235)   Data 0.001 (0.184)   Loss 1.0150 (0.8671)   Prec@1 67.000 (69.812)   Prec@5 96.000 (97.574)   [2025-10-22 22:55:00]
  Epoch: [036][200/500]   Time 0.054 (0.145)   Data 0.000 (0.093)   Loss 1.0414 (0.8824)   Prec@1 62.000 (69.020)   Prec@5 96.000 (97.562)   [2025-10-22 22:55:05]
  Epoch: [036][300/500]   Time 0.055 (0.115)   Data 0.000 (0.062)   Loss 0.8360 (0.8816)   Prec@1 70.000 (69.186)   Prec@5 98.000 (97.482)   [2025-10-22 22:55:11]
  Epoch: [036][400/500]   Time 0.053 (0.100)   Data 0.001 (0.047)   Loss 0.9467 (0.8885)   Prec@1 65.000 (68.948)   Prec@5 98.000 (97.354)   [2025-10-22 22:55:16]
  **Train** Prec@1 69.106 Prec@5 97.296 Error@1 30.894
  **Test** Prec@1 75.660 Prec@5 98.350 Error@1 24.340

==>>[2025-10-22 22:55:41] [Epoch=037/040] [Need: 00:03:10] [LR=0.0010] [Best : Accuracy=75.88, Error=24.12]
  Epoch: [037][000/500]   Time 18.049 (18.049)   Data 17.856 (17.856)   Loss 0.9394 (0.9394)   Prec@1 71.000 (71.000)   Prec@5 99.000 (99.000)   [2025-10-22 22:55:59]
  Epoch: [037][100/500]   Time 0.051 (0.228)   Data 0.000 (0.177)   Loss 1.0070 (0.8768)   Prec@1 65.000 (69.178)   Prec@5 97.000 (97.485)   [2025-10-22 22:56:04]
  Epoch: [037][200/500]   Time 0.054 (0.141)   Data 0.000 (0.089)   Loss 0.8835 (0.8882)   Prec@1 61.000 (69.015)   Prec@5 96.000 (97.358)   [2025-10-22 22:56:09]
  Epoch: [037][300/500]   Time 0.053 (0.112)   Data 0.001 (0.060)   Loss 1.0625 (0.8898)   Prec@1 64.000 (68.950)   Prec@5 95.000 (97.389)   [2025-10-22 22:56:15]
  Epoch: [037][400/500]   Time 0.053 (0.097)   Data 0.001 (0.045)   Loss 0.8548 (0.8863)   Prec@1 70.000 (69.100)   Prec@5 95.000 (97.384)   [2025-10-22 22:56:20]
  **Train** Prec@1 69.238 Prec@5 97.422 Error@1 30.762
  **Test** Prec@1 75.990 Prec@5 98.300 Error@1 24.010
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 22:56:44] [Epoch=038/040] [Need: 00:02:06] [LR=0.0010] [Best : Accuracy=75.99, Error=24.01]
  Epoch: [038][000/500]   Time 17.739 (17.739)   Data 17.557 (17.557)   Loss 1.0034 (1.0034)   Prec@1 69.000 (69.000)   Prec@5 95.000 (95.000)   [2025-10-22 22:57:02]
  Epoch: [038][100/500]   Time 0.050 (0.224)   Data 0.000 (0.174)   Loss 0.7119 (0.9069)   Prec@1 70.000 (68.485)   Prec@5 99.000 (97.376)   [2025-10-22 22:57:07]
  Epoch: [038][200/500]   Time 0.055 (0.139)   Data 0.001 (0.088)   Loss 1.0057 (0.8879)   Prec@1 62.000 (69.164)   Prec@5 96.000 (97.403)   [2025-10-22 22:57:12]
  Epoch: [038][300/500]   Time 0.056 (0.111)   Data 0.000 (0.059)   Loss 0.8085 (0.8881)   Prec@1 73.000 (69.282)   Prec@5 97.000 (97.439)   [2025-10-22 22:57:18]
  Epoch: [038][400/500]   Time 0.053 (0.097)   Data 0.000 (0.044)   Loss 0.9116 (0.8851)   Prec@1 68.000 (69.224)   Prec@5 97.000 (97.446)   [2025-10-22 22:57:23]
  **Train** Prec@1 69.244 Prec@5 97.438 Error@1 30.756
  **Test** Prec@1 75.580 Prec@5 98.310 Error@1 24.420

==>>[2025-10-22 22:57:47] [Epoch=039/040] [Need: 00:01:03] [LR=0.0010] [Best : Accuracy=75.99, Error=24.01]
  Epoch: [039][000/500]   Time 17.611 (17.611)   Data 17.431 (17.431)   Loss 0.9894 (0.9894)   Prec@1 67.000 (67.000)   Prec@5 98.000 (98.000)   [2025-10-22 22:58:05]
  Epoch: [039][100/500]   Time 0.052 (0.223)   Data 0.001 (0.173)   Loss 0.9603 (0.8722)   Prec@1 68.000 (70.238)   Prec@5 95.000 (97.525)   [2025-10-22 22:58:10]
  Epoch: [039][200/500]   Time 0.055 (0.139)   Data 0.000 (0.087)   Loss 0.8944 (0.8807)   Prec@1 64.000 (69.965)   Prec@5 98.000 (97.348)   [2025-10-22 22:58:15]
  Epoch: [039][300/500]   Time 0.054 (0.110)   Data 0.001 (0.059)   Loss 0.9467 (0.8825)   Prec@1 70.000 (69.684)   Prec@5 97.000 (97.326)   [2025-10-22 22:58:20]
  Epoch: [039][400/500]   Time 0.054 (0.096)   Data 0.001 (0.044)   Loss 0.8137 (0.8783)   Prec@1 73.000 (69.693)   Prec@5 97.000 (97.372)   [2025-10-22 22:58:26]
  **Train** Prec@1 69.528 Prec@5 97.360 Error@1 30.472
  **Test** Prec@1 75.620 Prec@5 98.250 Error@1 24.380
