save path : ./save/tinyvgg_quan/randbet_0.1_0.01_10_-1
{'data_path': './dataset', 'arch': 'tinyvgg_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': True, 'clipping_coeff': 0.1, 'learning_rate': 0.01, 'manualSeed': 4232, 'save_path': './save/tinyvgg_quan/randbet_0.1_0.01_10_-1', 'enable_bfa': False, 'resume': '', 'quan_bitwidth': None, 'reset_weight': False, 'evaluate': False, 'n_iter': 30, 'model_only': False, 'random_bfa': False, 'use_cuda': True}
Random Seed: 4232
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'tinyvgg_quan'
=> network :
 TinyVGG(
  (features): Sequential(
    (0): quan_Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Dropout2d(p=0.25, inplace=False)
    (6): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace=True)
    (8): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace=True)
    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (11): Dropout2d(p=0.25, inplace=False)
  )
  (classifier): Sequential(
    (0): quan_Linear(in_features=2048, out_features=10, bias=True)
  )
)
=> do not use any checkpoint for tinyvgg_quan model

==>>[2025-10-22 22:58:56] [Epoch=000/040] [Need: 00:00:00] [LR=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/500]   Time 18.015 (18.015)   Data 17.622 (17.622)   Loss 2.3086 (2.3086)   Prec@1 7.000 (7.000)   Prec@5 36.000 (36.000)   [2025-10-22 22:59:15]
  Epoch: [000][100/500]   Time 0.050 (0.228)   Data 0.000 (0.175)   Loss 2.1857 (2.2462)   Prec@1 29.000 (14.812)   Prec@5 64.000 (61.119)   [2025-10-22 22:59:19]
  Epoch: [000][200/500]   Time 0.053 (0.140)   Data 0.001 (0.088)   Loss 1.9253 (2.1307)   Prec@1 34.000 (20.940)   Prec@5 80.000 (69.448)   [2025-10-22 22:59:25]
  Epoch: [000][300/500]   Time 0.052 (0.112)   Data 0.000 (0.059)   Loss 1.9315 (2.0411)   Prec@1 26.000 (24.595)   Prec@5 81.000 (74.093)   [2025-10-22 22:59:30]
  Epoch: [000][400/500]   Time 0.054 (0.097)   Data 0.000 (0.045)   Loss 1.9294 (1.9769)   Prec@1 27.000 (27.197)   Prec@5 82.000 (77.030)   [2025-10-22 22:59:36]
  **Train** Prec@1 29.176 Prec@5 79.098 Error@1 70.824
  **Test** Prec@1 43.760 Prec@5 91.020 Error@1 56.240
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 23:00:00] [Epoch=001/040] [Need: 00:41:23] [LR=0.0100] [Best : Accuracy=43.76, Error=56.24]
  Epoch: [001][000/500]   Time 18.055 (18.055)   Data 17.870 (17.870)   Loss 1.6518 (1.6518)   Prec@1 29.000 (29.000)   Prec@5 93.000 (93.000)   [2025-10-22 23:00:18]
  Epoch: [001][100/500]   Time 0.052 (0.227)   Data 0.001 (0.178)   Loss 1.6899 (1.6838)   Prec@1 38.000 (38.663)   Prec@5 89.000 (88.059)   [2025-10-22 23:00:23]
  Epoch: [001][200/500]   Time 0.054 (0.141)   Data 0.001 (0.090)   Loss 1.6360 (1.6631)   Prec@1 37.000 (39.159)   Prec@5 90.000 (88.657)   [2025-10-22 23:00:29]
  Epoch: [001][300/500]   Time 0.054 (0.112)   Data 0.000 (0.060)   Loss 1.5350 (1.6418)   Prec@1 47.000 (39.887)   Prec@5 91.000 (89.159)   [2025-10-22 23:00:34]
  Epoch: [001][400/500]   Time 0.053 (0.098)   Data 0.001 (0.045)   Loss 1.6732 (1.6232)   Prec@1 44.000 (40.643)   Prec@5 86.000 (89.574)   [2025-10-22 23:00:40]
  **Train** Prec@1 41.112 Prec@5 89.910 Error@1 58.888
  **Test** Prec@1 50.590 Prec@5 94.310 Error@1 49.410
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 23:01:04] [Epoch=002/040] [Need: 00:40:16] [LR=0.0100] [Best : Accuracy=50.59, Error=49.41]
  Epoch: [002][000/500]   Time 17.864 (17.864)   Data 17.670 (17.670)   Loss 1.4248 (1.4248)   Prec@1 49.000 (49.000)   Prec@5 95.000 (95.000)   [2025-10-22 23:01:22]
  Epoch: [002][100/500]   Time 0.060 (0.225)   Data 0.001 (0.176)   Loss 1.5950 (1.5217)   Prec@1 39.000 (44.703)   Prec@5 90.000 (91.198)   [2025-10-22 23:01:27]
  Epoch: [002][200/500]   Time 0.055 (0.140)   Data 0.000 (0.088)   Loss 1.4701 (1.5139)   Prec@1 46.000 (45.015)   Prec@5 96.000 (91.358)   [2025-10-22 23:01:32]
  Epoch: [002][300/500]   Time 0.052 (0.111)   Data 0.001 (0.059)   Loss 1.5091 (1.5018)   Prec@1 48.000 (45.485)   Prec@5 92.000 (91.538)   [2025-10-22 23:01:37]
  Epoch: [002][400/500]   Time 0.054 (0.097)   Data 0.000 (0.045)   Loss 1.7168 (1.4903)   Prec@1 45.000 (46.052)   Prec@5 86.000 (91.663)   [2025-10-22 23:01:43]
  **Train** Prec@1 46.312 Prec@5 91.736 Error@1 53.688
  **Test** Prec@1 57.270 Prec@5 95.250 Error@1 42.730
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 23:02:07] [Epoch=003/040] [Need: 00:39:02] [LR=0.0100] [Best : Accuracy=57.27, Error=42.73]
  Epoch: [003][000/500]   Time 17.589 (17.589)   Data 17.396 (17.396)   Loss 1.2092 (1.2092)   Prec@1 57.000 (57.000)   Prec@5 94.000 (94.000)   [2025-10-22 23:02:24]
  Epoch: [003][100/500]   Time 0.054 (0.222)   Data 0.000 (0.173)   Loss 1.4795 (1.4108)   Prec@1 48.000 (49.356)   Prec@5 94.000 (92.495)   [2025-10-22 23:02:29]
  Epoch: [003][200/500]   Time 0.052 (0.138)   Data 0.000 (0.087)   Loss 1.2554 (1.4019)   Prec@1 51.000 (49.647)   Prec@5 93.000 (92.592)   [2025-10-22 23:02:34]
  Epoch: [003][300/500]   Time 0.052 (0.110)   Data 0.000 (0.058)   Loss 1.3343 (1.3903)   Prec@1 54.000 (50.199)   Prec@5 90.000 (92.894)   [2025-10-22 23:02:40]
  Epoch: [003][400/500]   Time 0.054 (0.096)   Data 0.000 (0.044)   Loss 1.3995 (1.3878)   Prec@1 50.000 (50.259)   Prec@5 93.000 (92.925)   [2025-10-22 23:02:45]
  **Train** Prec@1 50.486 Prec@5 93.054 Error@1 49.514
  **Test** Prec@1 60.420 Prec@5 95.670 Error@1 39.580
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 23:03:09] [Epoch=004/040] [Need: 00:37:49] [LR=0.0100] [Best : Accuracy=60.42, Error=39.58]
  Epoch: [004][000/500]   Time 19.651 (19.651)   Data 19.467 (19.467)   Loss 1.3366 (1.3366)   Prec@1 43.000 (43.000)   Prec@5 98.000 (98.000)   [2025-10-22 23:03:28]
  Epoch: [004][100/500]   Time 0.056 (0.245)   Data 0.001 (0.193)   Loss 1.3020 (1.3382)   Prec@1 55.000 (51.277)   Prec@5 93.000 (93.663)   [2025-10-22 23:03:33]
  Epoch: [004][200/500]   Time 0.056 (0.150)   Data 0.001 (0.098)   Loss 1.2425 (1.3204)   Prec@1 51.000 (52.388)   Prec@5 96.000 (93.836)   [2025-10-22 23:03:39]
  Epoch: [004][300/500]   Time 0.054 (0.119)   Data 0.000 (0.065)   Loss 1.3087 (1.3192)   Prec@1 54.000 (52.628)   Prec@5 92.000 (93.781)   [2025-10-22 23:03:44]
  Epoch: [004][400/500]   Time 0.057 (0.103)   Data 0.000 (0.049)   Loss 1.3072 (1.3114)   Prec@1 56.000 (53.025)   Prec@5 95.000 (93.888)   [2025-10-22 23:03:50]
  **Train** Prec@1 53.256 Prec@5 93.912 Error@1 46.744
  **Test** Prec@1 63.380 Prec@5 96.390 Error@1 36.620
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 23:04:14] [Epoch=005/040] [Need: 00:37:00] [LR=0.0100] [Best : Accuracy=63.38, Error=36.62]
  Epoch: [005][000/500]   Time 17.770 (17.770)   Data 17.585 (17.585)   Loss 1.4162 (1.4162)   Prec@1 53.000 (53.000)   Prec@5 92.000 (92.000)   [2025-10-22 23:04:32]
  Epoch: [005][100/500]   Time 0.053 (0.224)   Data 0.001 (0.175)   Loss 1.2961 (1.2646)   Prec@1 53.000 (55.109)   Prec@5 96.000 (94.396)   [2025-10-22 23:04:37]
  Epoch: [005][200/500]   Time 0.053 (0.139)   Data 0.000 (0.088)   Loss 1.5499 (1.2645)   Prec@1 50.000 (54.891)   Prec@5 90.000 (94.383)   [2025-10-22 23:04:42]
  Epoch: [005][300/500]   Time 0.057 (0.111)   Data 0.001 (0.059)   Loss 1.1945 (1.2647)   Prec@1 58.000 (54.870)   Prec@5 92.000 (94.322)   [2025-10-22 23:04:47]
  Epoch: [005][400/500]   Time 0.056 (0.097)   Data 0.001 (0.044)   Loss 1.2387 (1.2598)   Prec@1 54.000 (55.012)   Prec@5 94.000 (94.439)   [2025-10-22 23:04:53]
  **Train** Prec@1 55.312 Prec@5 94.438 Error@1 44.688
  **Test** Prec@1 65.980 Prec@5 96.750 Error@1 34.020
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 23:05:17] [Epoch=006/040] [Need: 00:35:54] [LR=0.0100] [Best : Accuracy=65.98, Error=34.02]
  Epoch: [006][000/500]   Time 17.658 (17.658)   Data 17.477 (17.477)   Loss 1.1770 (1.1770)   Prec@1 56.000 (56.000)   Prec@5 93.000 (93.000)   [2025-10-22 23:05:35]
  Epoch: [006][100/500]   Time 0.050 (0.224)   Data 0.000 (0.174)   Loss 1.2289 (1.2176)   Prec@1 60.000 (57.139)   Prec@5 96.000 (95.000)   [2025-10-22 23:05:40]
  Epoch: [006][200/500]   Time 0.054 (0.140)   Data 0.001 (0.088)   Loss 1.1335 (1.2138)   Prec@1 57.000 (56.896)   Prec@5 97.000 (94.851)   [2025-10-22 23:05:45]
  Epoch: [006][300/500]   Time 0.054 (0.111)   Data 0.000 (0.059)   Loss 1.1094 (1.2104)   Prec@1 57.000 (57.030)   Prec@5 96.000 (94.754)   [2025-10-22 23:05:50]
  Epoch: [006][400/500]   Time 0.055 (0.097)   Data 0.001 (0.044)   Loss 1.2433 (1.2065)   Prec@1 49.000 (57.095)   Prec@5 95.000 (94.858)   [2025-10-22 23:05:56]
  **Train** Prec@1 57.204 Prec@5 94.840 Error@1 42.796
  **Test** Prec@1 67.090 Prec@5 96.840 Error@1 32.910
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 23:06:20] [Epoch=007/040] [Need: 00:34:49] [LR=0.0100] [Best : Accuracy=67.09, Error=32.91]
  Epoch: [007][000/500]   Time 17.559 (17.559)   Data 17.375 (17.375)   Loss 1.2528 (1.2528)   Prec@1 56.000 (56.000)   Prec@5 93.000 (93.000)   [2025-10-22 23:06:37]
  Epoch: [007][100/500]   Time 0.050 (0.222)   Data 0.000 (0.173)   Loss 1.0601 (1.1674)   Prec@1 67.000 (58.554)   Prec@5 96.000 (95.337)   [2025-10-22 23:06:42]
  Epoch: [007][200/500]   Time 0.052 (0.138)   Data 0.001 (0.087)   Loss 1.1123 (1.1804)   Prec@1 60.000 (57.995)   Prec@5 97.000 (95.214)   [2025-10-22 23:06:48]
  Epoch: [007][300/500]   Time 0.053 (0.110)   Data 0.001 (0.058)   Loss 1.0048 (1.1845)   Prec@1 61.000 (57.827)   Prec@5 100.000 (95.153)   [2025-10-22 23:06:53]
  Epoch: [007][400/500]   Time 0.056 (0.096)   Data 0.001 (0.044)   Loss 1.1097 (1.1796)   Prec@1 61.000 (58.130)   Prec@5 96.000 (95.229)   [2025-10-22 23:06:58]
  **Train** Prec@1 58.356 Prec@5 95.284 Error@1 41.644
  **Test** Prec@1 66.410 Prec@5 96.610 Error@1 33.590

==>>[2025-10-22 23:07:22] [Epoch=008/040] [Need: 00:33:42] [LR=0.0100] [Best : Accuracy=67.09, Error=32.91]
  Epoch: [008][000/500]   Time 17.677 (17.677)   Data 17.496 (17.496)   Loss 1.3302 (1.3302)   Prec@1 61.000 (61.000)   Prec@5 93.000 (93.000)   [2025-10-22 23:07:40]
  Epoch: [008][100/500]   Time 0.053 (0.223)   Data 0.001 (0.174)   Loss 1.1540 (1.1614)   Prec@1 62.000 (59.436)   Prec@5 94.000 (95.139)   [2025-10-22 23:07:45]
  Epoch: [008][200/500]   Time 0.054 (0.139)   Data 0.001 (0.088)   Loss 0.9636 (1.1460)   Prec@1 64.000 (59.632)   Prec@5 98.000 (95.388)   [2025-10-22 23:07:50]
  Epoch: [008][300/500]   Time 0.055 (0.111)   Data 0.001 (0.059)   Loss 1.2028 (1.1426)   Prec@1 59.000 (59.814)   Prec@5 95.000 (95.455)   [2025-10-22 23:07:56]
  Epoch: [008][400/500]   Time 0.053 (0.097)   Data 0.000 (0.044)   Loss 1.0197 (1.1435)   Prec@1 68.000 (59.576)   Prec@5 96.000 (95.419)   [2025-10-22 23:08:01]
  **Train** Prec@1 59.676 Prec@5 95.416 Error@1 40.324
  **Test** Prec@1 68.680 Prec@5 97.290 Error@1 31.320
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 23:08:25] [Epoch=009/040] [Need: 00:32:37] [LR=0.0100] [Best : Accuracy=68.68, Error=31.32]
  Epoch: [009][000/500]   Time 17.719 (17.719)   Data 17.536 (17.536)   Loss 1.1068 (1.1068)   Prec@1 60.000 (60.000)   Prec@5 93.000 (93.000)   [2025-10-22 23:08:43]
  Epoch: [009][100/500]   Time 0.054 (0.227)   Data 0.001 (0.174)   Loss 1.1061 (1.1274)   Prec@1 63.000 (59.980)   Prec@5 95.000 (95.921)   [2025-10-22 23:08:48]
  Epoch: [009][200/500]   Time 0.055 (0.141)   Data 0.000 (0.088)   Loss 1.0512 (1.1241)   Prec@1 62.000 (60.269)   Prec@5 97.000 (95.756)   [2025-10-22 23:08:53]
  Epoch: [009][300/500]   Time 0.052 (0.112)   Data 0.000 (0.059)   Loss 1.0992 (1.1235)   Prec@1 64.000 (60.252)   Prec@5 94.000 (95.671)   [2025-10-22 23:08:59]
  Epoch: [009][400/500]   Time 0.055 (0.098)   Data 0.001 (0.044)   Loss 0.8787 (1.1233)   Prec@1 67.000 (60.347)   Prec@5 96.000 (95.656)   [2025-10-22 23:09:04]
  **Train** Prec@1 60.500 Prec@5 95.664 Error@1 39.500
  **Test** Prec@1 69.170 Prec@5 97.510 Error@1 30.830
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 23:09:28] [Epoch=010/040] [Need: 00:31:33] [LR=0.0100] [Best : Accuracy=69.17, Error=30.83]
  Epoch: [010][000/500]   Time 18.674 (18.674)   Data 18.481 (18.481)   Loss 1.1123 (1.1123)   Prec@1 60.000 (60.000)   Prec@5 96.000 (96.000)   [2025-10-22 23:09:47]
  Epoch: [010][100/500]   Time 0.052 (0.237)   Data 0.000 (0.184)   Loss 1.1671 (1.1269)   Prec@1 62.000 (60.673)   Prec@5 95.000 (95.426)   [2025-10-22 23:09:52]
  Epoch: [010][200/500]   Time 0.054 (0.146)   Data 0.000 (0.092)   Loss 1.0501 (1.1117)   Prec@1 59.000 (61.085)   Prec@5 97.000 (95.607)   [2025-10-22 23:09:57]
  Epoch: [010][300/500]   Time 0.055 (0.116)   Data 0.001 (0.062)   Loss 1.0781 (1.1111)   Prec@1 62.000 (61.246)   Prec@5 97.000 (95.578)   [2025-10-22 23:10:03]
  Epoch: [010][400/500]   Time 0.054 (0.101)   Data 0.001 (0.047)   Loss 1.1201 (1.1075)   Prec@1 59.000 (61.337)   Prec@5 95.000 (95.686)   [2025-10-22 23:10:08]
  **Train** Prec@1 61.312 Prec@5 95.720 Error@1 38.688
  **Test** Prec@1 70.700 Prec@5 97.590 Error@1 29.300
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 23:10:32] [Epoch=011/040] [Need: 00:30:33] [LR=0.0100] [Best : Accuracy=70.70, Error=29.30]
  Epoch: [011][000/500]   Time 18.506 (18.506)   Data 18.320 (18.320)   Loss 0.9470 (0.9470)   Prec@1 70.000 (70.000)   Prec@5 98.000 (98.000)   [2025-10-22 23:10:51]
  Epoch: [011][100/500]   Time 0.050 (0.232)   Data 0.000 (0.182)   Loss 1.0607 (1.0923)   Prec@1 65.000 (61.495)   Prec@5 96.000 (96.050)   [2025-10-22 23:10:56]
  Epoch: [011][200/500]   Time 0.055 (0.143)   Data 0.001 (0.092)   Loss 1.0003 (1.0980)   Prec@1 58.000 (61.050)   Prec@5 100.000 (95.836)   [2025-10-22 23:11:01]
  Epoch: [011][300/500]   Time 0.055 (0.114)   Data 0.001 (0.061)   Loss 1.0273 (1.1013)   Prec@1 63.000 (61.010)   Prec@5 96.000 (95.831)   [2025-10-22 23:11:06]
  Epoch: [011][400/500]   Time 0.053 (0.099)   Data 0.001 (0.046)   Loss 1.0229 (1.0970)   Prec@1 64.000 (61.217)   Prec@5 96.000 (95.878)   [2025-10-22 23:11:12]
  **Train** Prec@1 61.356 Prec@5 95.852 Error@1 38.644
  **Test** Prec@1 71.240 Prec@5 97.590 Error@1 28.760
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 23:11:36] [Epoch=012/040] [Need: 00:29:31] [LR=0.0100] [Best : Accuracy=71.24, Error=28.76]
  Epoch: [012][000/500]   Time 17.768 (17.768)   Data 17.577 (17.577)   Loss 1.0661 (1.0661)   Prec@1 66.000 (66.000)   Prec@5 96.000 (96.000)   [2025-10-22 23:11:54]
  Epoch: [012][100/500]   Time 0.051 (0.224)   Data 0.001 (0.175)   Loss 1.1556 (1.0864)   Prec@1 62.000 (61.624)   Prec@5 94.000 (95.921)   [2025-10-22 23:11:58]
  Epoch: [012][200/500]   Time 0.054 (0.139)   Data 0.000 (0.088)   Loss 1.0775 (1.0973)   Prec@1 63.000 (61.443)   Prec@5 97.000 (95.881)   [2025-10-22 23:12:04]
  Epoch: [012][300/500]   Time 0.056 (0.111)   Data 0.001 (0.059)   Loss 1.0907 (1.0888)   Prec@1 63.000 (61.777)   Prec@5 95.000 (95.934)   [2025-10-22 23:12:09]
  Epoch: [012][400/500]   Time 0.054 (0.097)   Data 0.001 (0.044)   Loss 1.2270 (1.0860)   Prec@1 50.000 (61.753)   Prec@5 93.000 (95.890)   [2025-10-22 23:12:15]
  **Train** Prec@1 61.940 Prec@5 95.960 Error@1 38.060
  **Test** Prec@1 71.350 Prec@5 97.790 Error@1 28.650
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 23:12:39] [Epoch=013/040] [Need: 00:28:27] [LR=0.0100] [Best : Accuracy=71.35, Error=28.65]
  Epoch: [013][000/500]   Time 17.819 (17.819)   Data 17.637 (17.637)   Loss 0.9844 (0.9844)   Prec@1 72.000 (72.000)   Prec@5 95.000 (95.000)   [2025-10-22 23:12:56]
  Epoch: [013][100/500]   Time 0.052 (0.224)   Data 0.000 (0.175)   Loss 1.1178 (1.0627)   Prec@1 58.000 (62.772)   Prec@5 94.000 (96.218)   [2025-10-22 23:13:01]
  Epoch: [013][200/500]   Time 0.055 (0.139)   Data 0.001 (0.088)   Loss 1.1410 (1.0737)   Prec@1 61.000 (62.358)   Prec@5 94.000 (96.015)   [2025-10-22 23:13:06]
  Epoch: [013][300/500]   Time 0.054 (0.111)   Data 0.001 (0.059)   Loss 1.0248 (1.0754)   Prec@1 61.000 (62.226)   Prec@5 96.000 (95.950)   [2025-10-22 23:13:12]
  Epoch: [013][400/500]   Time 0.054 (0.097)   Data 0.001 (0.045)   Loss 1.0382 (1.0699)   Prec@1 64.000 (62.374)   Prec@5 98.000 (96.017)   [2025-10-22 23:13:17]
  **Train** Prec@1 62.426 Prec@5 96.026 Error@1 37.574
  **Test** Prec@1 71.180 Prec@5 97.700 Error@1 28.820

==>>[2025-10-22 23:13:41] [Epoch=014/040] [Need: 00:27:22] [LR=0.0100] [Best : Accuracy=71.35, Error=28.65]
  Epoch: [014][000/500]   Time 17.795 (17.795)   Data 17.602 (17.602)   Loss 1.0820 (1.0820)   Prec@1 64.000 (64.000)   Prec@5 96.000 (96.000)   [2025-10-22 23:13:59]
  Epoch: [014][100/500]   Time 0.053 (0.226)   Data 0.001 (0.175)   Loss 1.1187 (1.0525)   Prec@1 57.000 (63.178)   Prec@5 95.000 (96.198)   [2025-10-22 23:14:04]
  Epoch: [014][200/500]   Time 0.054 (0.141)   Data 0.001 (0.088)   Loss 1.2696 (1.0538)   Prec@1 51.000 (63.174)   Prec@5 95.000 (96.244)   [2025-10-22 23:14:09]
  Epoch: [014][300/500]   Time 0.056 (0.112)   Data 0.001 (0.059)   Loss 1.0241 (1.0606)   Prec@1 66.000 (63.050)   Prec@5 98.000 (96.113)   [2025-10-22 23:14:15]
  Epoch: [014][400/500]   Time 0.054 (0.098)   Data 0.001 (0.044)   Loss 1.0916 (1.0640)   Prec@1 66.000 (62.853)   Prec@5 99.000 (96.097)   [2025-10-22 23:14:20]
  **Train** Prec@1 62.760 Prec@5 96.174 Error@1 37.240
  **Test** Prec@1 71.160 Prec@5 97.750 Error@1 28.840

==>>[2025-10-22 23:14:45] [Epoch=015/040] [Need: 00:26:20] [LR=0.0100] [Best : Accuracy=71.35, Error=28.65]
  Epoch: [015][000/500]   Time 18.200 (18.200)   Data 18.008 (18.008)   Loss 1.0963 (1.0963)   Prec@1 64.000 (64.000)   Prec@5 96.000 (96.000)   [2025-10-22 23:15:03]
  Epoch: [015][100/500]   Time 0.051 (0.229)   Data 0.001 (0.179)   Loss 0.8417 (1.0667)   Prec@1 74.000 (62.663)   Prec@5 96.000 (95.980)   [2025-10-22 23:15:08]
  Epoch: [015][200/500]   Time 0.055 (0.142)   Data 0.000 (0.090)   Loss 1.0295 (1.0698)   Prec@1 66.000 (62.612)   Prec@5 96.000 (95.766)   [2025-10-22 23:15:13]
  Epoch: [015][300/500]   Time 0.055 (0.113)   Data 0.001 (0.060)   Loss 1.2553 (1.0630)   Prec@1 64.000 (62.738)   Prec@5 95.000 (95.884)   [2025-10-22 23:15:19]
  Epoch: [015][400/500]   Time 0.053 (0.099)   Data 0.001 (0.045)   Loss 0.8430 (1.0577)   Prec@1 70.000 (62.898)   Prec@5 99.000 (96.015)   [2025-10-22 23:15:24]
  **Train** Prec@1 62.968 Prec@5 96.006 Error@1 37.032
  **Test** Prec@1 71.030 Prec@5 97.520 Error@1 28.970

==>>[2025-10-22 23:15:49] [Epoch=016/040] [Need: 00:25:18] [LR=0.0100] [Best : Accuracy=71.35, Error=28.65]
  Epoch: [016][000/500]   Time 18.047 (18.047)   Data 17.847 (17.847)   Loss 1.2015 (1.2015)   Prec@1 56.000 (56.000)   Prec@5 93.000 (93.000)   [2025-10-22 23:16:07]
  Epoch: [016][100/500]   Time 0.052 (0.229)   Data 0.000 (0.177)   Loss 0.9303 (1.0442)   Prec@1 69.000 (63.446)   Prec@5 98.000 (96.198)   [2025-10-22 23:16:12]
  Epoch: [016][200/500]   Time 0.054 (0.141)   Data 0.001 (0.089)   Loss 1.0043 (1.0435)   Prec@1 71.000 (63.577)   Prec@5 97.000 (96.154)   [2025-10-22 23:16:18]
  Epoch: [016][300/500]   Time 0.058 (0.112)   Data 0.002 (0.060)   Loss 1.2419 (1.0430)   Prec@1 52.000 (63.412)   Prec@5 95.000 (96.183)   [2025-10-22 23:16:23]
  Epoch: [016][400/500]   Time 0.054 (0.098)   Data 0.001 (0.045)   Loss 1.0864 (1.0413)   Prec@1 54.000 (63.449)   Prec@5 98.000 (96.207)   [2025-10-22 23:16:29]
  **Train** Prec@1 63.372 Prec@5 96.174 Error@1 36.628
  **Test** Prec@1 71.790 Prec@5 97.880 Error@1 28.210
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 23:16:53] [Epoch=017/040] [Need: 00:24:15] [LR=0.0100] [Best : Accuracy=71.79, Error=28.21]
  Epoch: [017][000/500]   Time 17.868 (17.868)   Data 17.683 (17.683)   Loss 1.0520 (1.0520)   Prec@1 58.000 (58.000)   Prec@5 97.000 (97.000)   [2025-10-22 23:17:10]
  Epoch: [017][100/500]   Time 0.050 (0.225)   Data 0.001 (0.176)   Loss 1.1218 (1.0392)   Prec@1 60.000 (63.297)   Prec@5 96.000 (96.218)   [2025-10-22 23:17:15]
  Epoch: [017][200/500]   Time 0.054 (0.140)   Data 0.001 (0.089)   Loss 0.9511 (1.0411)   Prec@1 67.000 (63.403)   Prec@5 96.000 (96.249)   [2025-10-22 23:17:21]
  Epoch: [017][300/500]   Time 0.056 (0.111)   Data 0.001 (0.059)   Loss 1.4006 (1.0423)   Prec@1 48.000 (63.551)   Prec@5 92.000 (96.209)   [2025-10-22 23:17:26]
  Epoch: [017][400/500]   Time 0.055 (0.097)   Data 0.001 (0.045)   Loss 1.0134 (1.0389)   Prec@1 66.000 (63.708)   Prec@5 96.000 (96.229)   [2025-10-22 23:17:31]
  **Train** Prec@1 63.774 Prec@5 96.294 Error@1 36.226
  **Test** Prec@1 71.800 Prec@5 97.470 Error@1 28.200
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 23:17:55] [Epoch=018/040] [Need: 00:23:11] [LR=0.0100] [Best : Accuracy=71.80, Error=28.20]
  Epoch: [018][000/500]   Time 17.648 (17.648)   Data 17.469 (17.469)   Loss 0.9452 (0.9452)   Prec@1 67.000 (67.000)   Prec@5 95.000 (95.000)   [2025-10-22 23:18:13]
  Epoch: [018][100/500]   Time 0.051 (0.223)   Data 0.000 (0.174)   Loss 0.8352 (1.0336)   Prec@1 72.000 (63.812)   Prec@5 96.000 (96.356)   [2025-10-22 23:18:18]
  Epoch: [018][200/500]   Time 0.055 (0.139)   Data 0.000 (0.087)   Loss 1.0973 (1.0406)   Prec@1 62.000 (63.463)   Prec@5 96.000 (96.279)   [2025-10-22 23:18:23]
  Epoch: [018][300/500]   Time 0.053 (0.111)   Data 0.001 (0.059)   Loss 1.1800 (1.0359)   Prec@1 55.000 (63.601)   Prec@5 96.000 (96.272)   [2025-10-22 23:18:29]
  Epoch: [018][400/500]   Time 0.054 (0.097)   Data 0.000 (0.044)   Loss 0.9613 (1.0306)   Prec@1 66.000 (63.885)   Prec@5 98.000 (96.357)   [2025-10-22 23:18:34]
  **Train** Prec@1 63.948 Prec@5 96.410 Error@1 36.052
  **Test** Prec@1 71.120 Prec@5 97.900 Error@1 28.880

==>>[2025-10-22 23:18:58] [Epoch=019/040] [Need: 00:22:07] [LR=0.0100] [Best : Accuracy=71.80, Error=28.20]
  Epoch: [019][000/500]   Time 17.931 (17.931)   Data 17.742 (17.742)   Loss 1.0565 (1.0565)   Prec@1 62.000 (62.000)   Prec@5 95.000 (95.000)   [2025-10-22 23:19:16]
  Epoch: [019][100/500]   Time 0.050 (0.227)   Data 0.000 (0.176)   Loss 0.9476 (1.0062)   Prec@1 71.000 (64.970)   Prec@5 96.000 (96.653)   [2025-10-22 23:19:21]
  Epoch: [019][200/500]   Time 0.057 (0.141)   Data 0.000 (0.089)   Loss 0.9275 (1.0080)   Prec@1 66.000 (65.025)   Prec@5 99.000 (96.577)   [2025-10-22 23:19:26]
  Epoch: [019][300/500]   Time 0.053 (0.112)   Data 0.000 (0.059)   Loss 1.0662 (1.0115)   Prec@1 63.000 (64.681)   Prec@5 97.000 (96.512)   [2025-10-22 23:19:32]
  Epoch: [019][400/500]   Time 0.053 (0.097)   Data 0.000 (0.045)   Loss 1.0498 (1.0208)   Prec@1 61.000 (64.299)   Prec@5 96.000 (96.481)   [2025-10-22 23:19:37]
  **Train** Prec@1 64.156 Prec@5 96.456 Error@1 35.844
  **Test** Prec@1 72.710 Prec@5 97.970 Error@1 27.290
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 23:20:02] [Epoch=020/040] [Need: 00:21:05] [LR=0.0100] [Best : Accuracy=72.71, Error=27.29]
  Epoch: [020][000/500]   Time 18.029 (18.029)   Data 17.844 (17.844)   Loss 0.9016 (0.9016)   Prec@1 69.000 (69.000)   Prec@5 97.000 (97.000)   [2025-10-22 23:20:20]
  Epoch: [020][100/500]   Time 0.051 (0.228)   Data 0.000 (0.177)   Loss 1.2169 (0.9987)   Prec@1 58.000 (64.584)   Prec@5 96.000 (96.653)   [2025-10-22 23:20:25]
  Epoch: [020][200/500]   Time 0.054 (0.141)   Data 0.001 (0.089)   Loss 0.9225 (1.0121)   Prec@1 67.000 (64.488)   Prec@5 98.000 (96.483)   [2025-10-22 23:20:30]
  Epoch: [020][300/500]   Time 0.053 (0.112)   Data 0.001 (0.060)   Loss 1.0009 (1.0114)   Prec@1 68.000 (64.415)   Prec@5 97.000 (96.472)   [2025-10-22 23:20:36]
  Epoch: [020][400/500]   Time 0.065 (0.099)   Data 0.000 (0.045)   Loss 1.1531 (1.0161)   Prec@1 61.000 (64.374)   Prec@5 94.000 (96.441)   [2025-10-22 23:20:41]
  **Train** Prec@1 64.264 Prec@5 96.396 Error@1 35.736
  **Test** Prec@1 72.550 Prec@5 97.810 Error@1 27.450

==>>[2025-10-22 23:21:05] [Epoch=021/040] [Need: 00:20:02] [LR=0.0100] [Best : Accuracy=72.71, Error=27.29]
  Epoch: [021][000/500]   Time 17.777 (17.777)   Data 17.571 (17.571)   Loss 0.8895 (0.8895)   Prec@1 70.000 (70.000)   Prec@5 97.000 (97.000)   [2025-10-22 23:21:23]
  Epoch: [021][100/500]   Time 0.051 (0.225)   Data 0.001 (0.175)   Loss 0.9124 (1.0049)   Prec@1 70.000 (64.455)   Prec@5 96.000 (96.376)   [2025-10-22 23:21:28]
  Epoch: [021][200/500]   Time 0.056 (0.140)   Data 0.001 (0.088)   Loss 0.8037 (1.0198)   Prec@1 68.000 (64.124)   Prec@5 97.000 (96.294)   [2025-10-22 23:21:34]
  Epoch: [021][300/500]   Time 0.053 (0.111)   Data 0.001 (0.059)   Loss 0.9219 (1.0175)   Prec@1 73.000 (64.146)   Prec@5 99.000 (96.306)   [2025-10-22 23:21:39]
  Epoch: [021][400/500]   Time 0.056 (0.097)   Data 0.001 (0.044)   Loss 0.8563 (1.0167)   Prec@1 70.000 (64.172)   Prec@5 99.000 (96.309)   [2025-10-22 23:21:44]
  **Train** Prec@1 64.188 Prec@5 96.314 Error@1 35.812
  **Test** Prec@1 71.590 Prec@5 97.910 Error@1 28.410

==>>[2025-10-22 23:22:08] [Epoch=022/040] [Need: 00:18:58] [LR=0.0100] [Best : Accuracy=72.71, Error=27.29]
  Epoch: [022][000/500]   Time 17.567 (17.567)   Data 17.381 (17.381)   Loss 1.0474 (1.0474)   Prec@1 58.000 (58.000)   Prec@5 99.000 (99.000)   [2025-10-22 23:22:26]
  Epoch: [022][100/500]   Time 0.051 (0.222)   Data 0.000 (0.173)   Loss 1.0892 (1.0051)   Prec@1 64.000 (65.030)   Prec@5 94.000 (96.347)   [2025-10-22 23:22:31]
  Epoch: [022][200/500]   Time 0.056 (0.138)   Data 0.001 (0.087)   Loss 0.9682 (1.0129)   Prec@1 67.000 (64.856)   Prec@5 95.000 (96.289)   [2025-10-22 23:22:36]
  Epoch: [022][300/500]   Time 0.057 (0.110)   Data 0.001 (0.058)   Loss 1.1355 (1.0106)   Prec@1 57.000 (64.641)   Prec@5 96.000 (96.472)   [2025-10-22 23:22:41]
  Epoch: [022][400/500]   Time 0.055 (0.097)   Data 0.001 (0.044)   Loss 1.3059 (1.0140)   Prec@1 50.000 (64.551)   Prec@5 97.000 (96.484)   [2025-10-22 23:22:47]
  **Train** Prec@1 64.508 Prec@5 96.476 Error@1 35.492
  **Test** Prec@1 72.570 Prec@5 97.870 Error@1 27.430

==>>[2025-10-22 23:23:11] [Epoch=023/040] [Need: 00:17:54] [LR=0.0100] [Best : Accuracy=72.71, Error=27.29]
  Epoch: [023][000/500]   Time 17.776 (17.776)   Data 17.592 (17.592)   Loss 0.9946 (0.9946)   Prec@1 72.000 (72.000)   Prec@5 95.000 (95.000)   [2025-10-22 23:23:29]
  Epoch: [023][100/500]   Time 0.049 (0.224)   Data 0.000 (0.175)   Loss 1.0315 (1.0088)   Prec@1 64.000 (65.376)   Prec@5 96.000 (96.257)   [2025-10-22 23:23:34]
  Epoch: [023][200/500]   Time 0.053 (0.139)   Data 0.001 (0.088)   Loss 0.9752 (1.0108)   Prec@1 66.000 (65.134)   Prec@5 100.000 (96.343)   [2025-10-22 23:23:39]
  Epoch: [023][300/500]   Time 0.054 (0.111)   Data 0.001 (0.059)   Loss 1.0157 (1.0129)   Prec@1 59.000 (64.880)   Prec@5 99.000 (96.355)   [2025-10-22 23:23:44]
  Epoch: [023][400/500]   Time 0.054 (0.097)   Data 0.001 (0.044)   Loss 1.1005 (1.0078)   Prec@1 60.000 (65.097)   Prec@5 97.000 (96.446)   [2025-10-22 23:23:50]
  **Train** Prec@1 65.150 Prec@5 96.460 Error@1 34.850
  **Test** Prec@1 73.300 Prec@5 97.760 Error@1 26.700
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 23:24:15] [Epoch=024/040] [Need: 00:16:51] [LR=0.0100] [Best : Accuracy=73.30, Error=26.70]
  Epoch: [024][000/500]   Time 17.663 (17.663)   Data 17.464 (17.464)   Loss 0.8171 (0.8171)   Prec@1 75.000 (75.000)   Prec@5 97.000 (97.000)   [2025-10-22 23:24:32]
  Epoch: [024][100/500]   Time 0.051 (0.227)   Data 0.000 (0.174)   Loss 1.0654 (0.9943)   Prec@1 60.000 (65.475)   Prec@5 96.000 (96.485)   [2025-10-22 23:24:37]
  Epoch: [024][200/500]   Time 0.071 (0.142)   Data 0.001 (0.088)   Loss 0.8374 (0.9884)   Prec@1 67.000 (65.458)   Prec@5 98.000 (96.517)   [2025-10-22 23:24:43]
  Epoch: [024][300/500]   Time 0.059 (0.114)   Data 0.001 (0.059)   Loss 1.0056 (0.9904)   Prec@1 66.000 (65.555)   Prec@5 95.000 (96.625)   [2025-10-22 23:24:49]
  Epoch: [024][400/500]   Time 0.052 (0.099)   Data 0.000 (0.044)   Loss 1.0577 (0.9914)   Prec@1 65.000 (65.451)   Prec@5 100.000 (96.646)   [2025-10-22 23:24:54]
  **Train** Prec@1 65.058 Prec@5 96.622 Error@1 34.942
  **Test** Prec@1 72.700 Prec@5 98.090 Error@1 27.300

==>>[2025-10-22 23:25:18] [Epoch=025/040] [Need: 00:15:48] [LR=0.0010] [Best : Accuracy=73.30, Error=26.70]
  Epoch: [025][000/500]   Time 17.742 (17.742)   Data 17.548 (17.548)   Loss 1.1512 (1.1512)   Prec@1 61.000 (61.000)   Prec@5 97.000 (97.000)   [2025-10-22 23:25:36]
  Epoch: [025][100/500]   Time 0.051 (0.228)   Data 0.000 (0.174)   Loss 0.9460 (0.9685)   Prec@1 67.000 (66.158)   Prec@5 99.000 (96.871)   [2025-10-22 23:25:41]
  Epoch: [025][200/500]   Time 0.053 (0.142)   Data 0.001 (0.088)   Loss 1.0755 (0.9499)   Prec@1 58.000 (66.905)   Prec@5 99.000 (96.970)   [2025-10-22 23:25:47]
  Epoch: [025][300/500]   Time 0.055 (0.113)   Data 0.001 (0.059)   Loss 0.7358 (0.9438)   Prec@1 73.000 (67.096)   Prec@5 98.000 (97.153)   [2025-10-22 23:25:52]
  Epoch: [025][400/500]   Time 0.054 (0.098)   Data 0.001 (0.044)   Loss 1.0004 (0.9402)   Prec@1 68.000 (67.125)   Prec@5 98.000 (97.135)   [2025-10-22 23:25:57]
  **Train** Prec@1 67.270 Prec@5 97.140 Error@1 32.730
  **Test** Prec@1 74.500 Prec@5 98.210 Error@1 25.500
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 23:26:21] [Epoch=026/040] [Need: 00:14:45] [LR=0.0010] [Best : Accuracy=74.50, Error=25.50]
  Epoch: [026][000/500]   Time 17.557 (17.557)   Data 17.377 (17.377)   Loss 0.7351 (0.7351)   Prec@1 71.000 (71.000)   Prec@5 100.000 (100.000)   [2025-10-22 23:26:39]
  Epoch: [026][100/500]   Time 0.053 (0.223)   Data 0.000 (0.173)   Loss 0.8141 (0.9206)   Prec@1 75.000 (68.198)   Prec@5 99.000 (97.257)   [2025-10-22 23:26:44]
  Epoch: [026][200/500]   Time 0.055 (0.138)   Data 0.000 (0.087)   Loss 1.1581 (0.9152)   Prec@1 64.000 (68.179)   Prec@5 95.000 (97.284)   [2025-10-22 23:26:49]
  Epoch: [026][300/500]   Time 0.055 (0.110)   Data 0.000 (0.058)   Loss 1.0229 (0.9195)   Prec@1 64.000 (67.973)   Prec@5 97.000 (97.249)   [2025-10-22 23:26:55]
  Epoch: [026][400/500]   Time 0.053 (0.096)   Data 0.000 (0.044)   Loss 0.8579 (0.9122)   Prec@1 69.000 (68.182)   Prec@5 97.000 (97.222)   [2025-10-22 23:27:00]
  **Train** Prec@1 67.944 Prec@5 97.190 Error@1 32.056
  **Test** Prec@1 74.960 Prec@5 98.060 Error@1 25.040
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 23:27:24] [Epoch=027/040] [Need: 00:13:42] [LR=0.0010] [Best : Accuracy=74.96, Error=25.04]
  Epoch: [027][000/500]   Time 17.717 (17.717)   Data 17.526 (17.526)   Loss 0.9291 (0.9291)   Prec@1 64.000 (64.000)   Prec@5 96.000 (96.000)   [2025-10-22 23:27:42]
  Epoch: [027][100/500]   Time 0.051 (0.225)   Data 0.000 (0.174)   Loss 1.0570 (0.9159)   Prec@1 58.000 (67.812)   Prec@5 96.000 (97.238)   [2025-10-22 23:27:47]
  Epoch: [027][200/500]   Time 0.053 (0.140)   Data 0.001 (0.088)   Loss 0.8788 (0.9131)   Prec@1 74.000 (68.159)   Prec@5 97.000 (97.154)   [2025-10-22 23:27:52]
  Epoch: [027][300/500]   Time 0.055 (0.111)   Data 0.001 (0.059)   Loss 1.0476 (0.9178)   Prec@1 66.000 (67.993)   Prec@5 96.000 (97.047)   [2025-10-22 23:27:58]
  Epoch: [027][400/500]   Time 0.055 (0.097)   Data 0.001 (0.044)   Loss 0.8712 (0.9171)   Prec@1 72.000 (68.027)   Prec@5 96.000 (97.047)   [2025-10-22 23:28:03]
  **Train** Prec@1 68.234 Prec@5 97.100 Error@1 31.766
  **Test** Prec@1 74.840 Prec@5 98.180 Error@1 25.160

==>>[2025-10-22 23:28:27] [Epoch=028/040] [Need: 00:12:38] [LR=0.0010] [Best : Accuracy=74.96, Error=25.04]
  Epoch: [028][000/500]   Time 17.621 (17.621)   Data 17.437 (17.437)   Loss 0.7960 (0.7960)   Prec@1 70.000 (70.000)   Prec@5 99.000 (99.000)   [2025-10-22 23:28:45]
  Epoch: [028][100/500]   Time 0.053 (0.223)   Data 0.001 (0.173)   Loss 0.7844 (0.9196)   Prec@1 72.000 (67.574)   Prec@5 98.000 (97.089)   [2025-10-22 23:28:50]
  Epoch: [028][200/500]   Time 0.053 (0.138)   Data 0.000 (0.087)   Loss 0.9202 (0.9145)   Prec@1 67.000 (67.756)   Prec@5 98.000 (97.169)   [2025-10-22 23:28:55]
  Epoch: [028][300/500]   Time 0.055 (0.110)   Data 0.000 (0.059)   Loss 0.8421 (0.9074)   Prec@1 70.000 (67.963)   Prec@5 99.000 (97.269)   [2025-10-22 23:29:00]
  Epoch: [028][400/500]   Time 0.054 (0.096)   Data 0.000 (0.044)   Loss 0.9126 (0.9063)   Prec@1 69.000 (68.095)   Prec@5 98.000 (97.207)   [2025-10-22 23:29:06]
  **Train** Prec@1 68.244 Prec@5 97.230 Error@1 31.756
  **Test** Prec@1 75.060 Prec@5 98.340 Error@1 24.940
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 23:29:30] [Epoch=029/040] [Need: 00:11:35] [LR=0.0010] [Best : Accuracy=75.06, Error=24.94]
  Epoch: [029][000/500]   Time 18.591 (18.591)   Data 18.401 (18.401)   Loss 1.0461 (1.0461)   Prec@1 66.000 (66.000)   Prec@5 97.000 (97.000)   [2025-10-22 23:29:48]
  Epoch: [029][100/500]   Time 0.052 (0.232)   Data 0.001 (0.183)   Loss 0.9967 (0.9057)   Prec@1 64.000 (68.475)   Prec@5 97.000 (97.327)   [2025-10-22 23:29:53]
  Epoch: [029][200/500]   Time 0.055 (0.143)   Data 0.001 (0.092)   Loss 0.9392 (0.9023)   Prec@1 69.000 (68.488)   Prec@5 97.000 (97.383)   [2025-10-22 23:29:58]
  Epoch: [029][300/500]   Time 0.054 (0.114)   Data 0.001 (0.062)   Loss 0.9054 (0.9006)   Prec@1 68.000 (68.478)   Prec@5 97.000 (97.395)   [2025-10-22 23:30:04]
  Epoch: [029][400/500]   Time 0.055 (0.099)   Data 0.000 (0.046)   Loss 0.8958 (0.9047)   Prec@1 69.000 (68.469)   Prec@5 97.000 (97.299)   [2025-10-22 23:30:09]
  **Train** Prec@1 68.458 Prec@5 97.316 Error@1 31.542
  **Test** Prec@1 75.530 Prec@5 98.220 Error@1 24.470
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 23:30:33] [Epoch=030/040] [Need: 00:10:32] [LR=0.0010] [Best : Accuracy=75.53, Error=24.47]
  Epoch: [030][000/500]   Time 18.539 (18.539)   Data 18.349 (18.349)   Loss 1.0467 (1.0467)   Prec@1 65.000 (65.000)   Prec@5 96.000 (96.000)   [2025-10-22 23:30:52]
  Epoch: [030][100/500]   Time 0.049 (0.232)   Data 0.000 (0.182)   Loss 0.8683 (0.9134)   Prec@1 67.000 (67.881)   Prec@5 99.000 (97.198)   [2025-10-22 23:30:57]
  Epoch: [030][200/500]   Time 0.056 (0.143)   Data 0.001 (0.092)   Loss 0.8309 (0.9094)   Prec@1 69.000 (68.169)   Prec@5 97.000 (97.313)   [2025-10-22 23:31:02]
  Epoch: [030][300/500]   Time 0.051 (0.114)   Data 0.001 (0.062)   Loss 0.9385 (0.9050)   Prec@1 63.000 (68.369)   Prec@5 96.000 (97.339)   [2025-10-22 23:31:07]
  Epoch: [030][400/500]   Time 0.055 (0.099)   Data 0.001 (0.046)   Loss 1.0408 (0.8970)   Prec@1 70.000 (68.748)   Prec@5 93.000 (97.344)   [2025-10-22 23:31:13]
  **Train** Prec@1 68.690 Prec@5 97.314 Error@1 31.310
  **Test** Prec@1 75.200 Prec@5 98.210 Error@1 24.800

==>>[2025-10-22 23:31:37] [Epoch=031/040] [Need: 00:09:29] [LR=0.0010] [Best : Accuracy=75.53, Error=24.47]
  Epoch: [031][000/500]   Time 19.258 (19.258)   Data 19.074 (19.074)   Loss 0.9337 (0.9337)   Prec@1 63.000 (63.000)   Prec@5 97.000 (97.000)   [2025-10-22 23:31:56]
  Epoch: [031][100/500]   Time 0.053 (0.240)   Data 0.001 (0.189)   Loss 0.8909 (0.9023)   Prec@1 69.000 (68.307)   Prec@5 99.000 (97.287)   [2025-10-22 23:32:01]
  Epoch: [031][200/500]   Time 0.055 (0.147)   Data 0.000 (0.095)   Loss 1.0790 (0.8922)   Prec@1 66.000 (68.876)   Prec@5 96.000 (97.363)   [2025-10-22 23:32:07]
  Epoch: [031][300/500]   Time 0.055 (0.116)   Data 0.001 (0.064)   Loss 0.7899 (0.8973)   Prec@1 72.000 (68.578)   Prec@5 99.000 (97.369)   [2025-10-22 23:32:12]
  Epoch: [031][400/500]   Time 0.055 (0.101)   Data 0.001 (0.048)   Loss 0.9361 (0.8936)   Prec@1 68.000 (68.938)   Prec@5 99.000 (97.359)   [2025-10-22 23:32:17]
  **Train** Prec@1 68.780 Prec@5 97.306 Error@1 31.220
  **Test** Prec@1 75.540 Prec@5 98.440 Error@1 24.460
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 23:32:42] [Epoch=032/040] [Need: 00:08:26] [LR=0.0010] [Best : Accuracy=75.54, Error=24.46]
  Epoch: [032][000/500]   Time 17.749 (17.749)   Data 17.556 (17.556)   Loss 0.8301 (0.8301)   Prec@1 73.000 (73.000)   Prec@5 98.000 (98.000)   [2025-10-22 23:32:59]
  Epoch: [032][100/500]   Time 0.050 (0.224)   Data 0.001 (0.174)   Loss 0.9608 (0.8873)   Prec@1 68.000 (68.762)   Prec@5 95.000 (97.277)   [2025-10-22 23:33:04]
  Epoch: [032][200/500]   Time 0.054 (0.139)   Data 0.001 (0.088)   Loss 0.9011 (0.8937)   Prec@1 68.000 (68.488)   Prec@5 99.000 (97.318)   [2025-10-22 23:33:10]
  Epoch: [032][300/500]   Time 0.053 (0.111)   Data 0.000 (0.059)   Loss 0.9385 (0.9017)   Prec@1 70.000 (68.525)   Prec@5 99.000 (97.319)   [2025-10-22 23:33:15]
  Epoch: [032][400/500]   Time 0.052 (0.097)   Data 0.001 (0.044)   Loss 0.7554 (0.8956)   Prec@1 77.000 (68.681)   Prec@5 96.000 (97.349)   [2025-10-22 23:33:20]
  **Train** Prec@1 68.748 Prec@5 97.338 Error@1 31.252
  **Test** Prec@1 75.200 Prec@5 98.360 Error@1 24.800

==>>[2025-10-22 23:33:46] [Epoch=033/040] [Need: 00:07:23] [LR=0.0010] [Best : Accuracy=75.54, Error=24.46]
  Epoch: [033][000/500]   Time 18.405 (18.405)   Data 18.220 (18.220)   Loss 0.9712 (0.9712)   Prec@1 65.000 (65.000)   Prec@5 95.000 (95.000)   [2025-10-22 23:34:04]
  Epoch: [033][100/500]   Time 0.053 (0.231)   Data 0.001 (0.181)   Loss 0.9666 (0.8808)   Prec@1 64.000 (69.149)   Prec@5 98.000 (97.406)   [2025-10-22 23:34:09]
  Epoch: [033][200/500]   Time 0.056 (0.143)   Data 0.001 (0.091)   Loss 0.9493 (0.8903)   Prec@1 62.000 (68.925)   Prec@5 99.000 (97.353)   [2025-10-22 23:34:14]
  Epoch: [033][300/500]   Time 0.054 (0.113)   Data 0.001 (0.061)   Loss 0.8104 (0.8851)   Prec@1 70.000 (69.229)   Prec@5 96.000 (97.375)   [2025-10-22 23:34:20]
  Epoch: [033][400/500]   Time 0.054 (0.099)   Data 0.000 (0.046)   Loss 0.9081 (0.8889)   Prec@1 65.000 (69.135)   Prec@5 98.000 (97.357)   [2025-10-22 23:34:25]
  **Train** Prec@1 69.030 Prec@5 97.324 Error@1 30.970
  **Test** Prec@1 75.610 Prec@5 98.410 Error@1 24.390
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 23:34:51] [Epoch=034/040] [Need: 00:06:20] [LR=0.0010] [Best : Accuracy=75.61, Error=24.39]
  Epoch: [034][000/500]   Time 17.631 (17.631)   Data 17.437 (17.437)   Loss 0.8517 (0.8517)   Prec@1 76.000 (76.000)   Prec@5 97.000 (97.000)   [2025-10-22 23:35:08]
  Epoch: [034][100/500]   Time 0.051 (0.224)   Data 0.001 (0.173)   Loss 0.9822 (0.9009)   Prec@1 63.000 (68.970)   Prec@5 98.000 (97.059)   [2025-10-22 23:35:13]
  Epoch: [034][200/500]   Time 0.057 (0.139)   Data 0.001 (0.087)   Loss 1.2411 (0.8998)   Prec@1 60.000 (68.677)   Prec@5 95.000 (97.129)   [2025-10-22 23:35:19]
  Epoch: [034][300/500]   Time 0.054 (0.111)   Data 0.001 (0.059)   Loss 0.8167 (0.8991)   Prec@1 70.000 (68.621)   Prec@5 98.000 (97.116)   [2025-10-22 23:35:24]
  Epoch: [034][400/500]   Time 0.054 (0.097)   Data 0.001 (0.044)   Loss 0.8423 (0.8936)   Prec@1 68.000 (68.910)   Prec@5 98.000 (97.177)   [2025-10-22 23:35:29]
  **Train** Prec@1 68.976 Prec@5 97.230 Error@1 31.024
  **Test** Prec@1 75.510 Prec@5 98.230 Error@1 24.490

==>>[2025-10-22 23:35:54] [Epoch=035/040] [Need: 00:05:16] [LR=0.0010] [Best : Accuracy=75.61, Error=24.39]
  Epoch: [035][000/500]   Time 17.902 (17.902)   Data 17.705 (17.705)   Loss 0.9614 (0.9614)   Prec@1 65.000 (65.000)   Prec@5 99.000 (99.000)   [2025-10-22 23:36:12]
  Epoch: [035][100/500]   Time 0.052 (0.227)   Data 0.001 (0.176)   Loss 0.9998 (0.9109)   Prec@1 64.000 (68.743)   Prec@5 98.000 (96.970)   [2025-10-22 23:36:17]
  Epoch: [035][200/500]   Time 0.054 (0.140)   Data 0.001 (0.089)   Loss 0.8180 (0.9044)   Prec@1 69.000 (68.736)   Prec@5 99.000 (97.109)   [2025-10-22 23:36:22]
  Epoch: [035][300/500]   Time 0.054 (0.111)   Data 0.001 (0.059)   Loss 0.9567 (0.8989)   Prec@1 65.000 (68.827)   Prec@5 99.000 (97.153)   [2025-10-22 23:36:28]
  Epoch: [035][400/500]   Time 0.055 (0.097)   Data 0.000 (0.045)   Loss 0.8142 (0.8962)   Prec@1 76.000 (69.030)   Prec@5 97.000 (97.214)   [2025-10-22 23:36:33]
  **Train** Prec@1 69.158 Prec@5 97.274 Error@1 30.842
  **Test** Prec@1 76.000 Prec@5 98.320 Error@1 24.000
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 23:36:58] [Epoch=036/040] [Need: 00:04:13] [LR=0.0010] [Best : Accuracy=76.00, Error=24.00]
  Epoch: [036][000/500]   Time 18.210 (18.210)   Data 18.027 (18.027)   Loss 0.9054 (0.9054)   Prec@1 66.000 (66.000)   Prec@5 98.000 (98.000)   [2025-10-22 23:37:16]
  Epoch: [036][100/500]   Time 0.051 (0.230)   Data 0.000 (0.179)   Loss 0.8964 (0.8888)   Prec@1 69.000 (69.337)   Prec@5 96.000 (97.505)   [2025-10-22 23:37:21]
  Epoch: [036][200/500]   Time 0.055 (0.142)   Data 0.001 (0.090)   Loss 0.7209 (0.8872)   Prec@1 72.000 (68.975)   Prec@5 100.000 (97.488)   [2025-10-22 23:37:27]
  Epoch: [036][300/500]   Time 0.055 (0.113)   Data 0.001 (0.061)   Loss 0.9016 (0.8844)   Prec@1 70.000 (69.123)   Prec@5 97.000 (97.518)   [2025-10-22 23:37:32]
  Epoch: [036][400/500]   Time 0.054 (0.098)   Data 0.000 (0.046)   Loss 1.0292 (0.8843)   Prec@1 64.000 (69.080)   Prec@5 98.000 (97.449)   [2025-10-22 23:37:38]
  **Train** Prec@1 68.916 Prec@5 97.446 Error@1 31.084
  **Test** Prec@1 75.810 Prec@5 98.300 Error@1 24.190

==>>[2025-10-22 23:38:02] [Epoch=037/040] [Need: 00:03:10] [LR=0.0010] [Best : Accuracy=76.00, Error=24.00]
  Epoch: [037][000/500]   Time 17.589 (17.589)   Data 17.408 (17.408)   Loss 0.8688 (0.8688)   Prec@1 70.000 (70.000)   Prec@5 95.000 (95.000)   [2025-10-22 23:38:19]
  Epoch: [037][100/500]   Time 0.051 (0.222)   Data 0.001 (0.173)   Loss 0.8432 (0.8947)   Prec@1 72.000 (68.604)   Prec@5 99.000 (97.485)   [2025-10-22 23:38:24]
  Epoch: [037][200/500]   Time 0.057 (0.138)   Data 0.000 (0.087)   Loss 0.9452 (0.8949)   Prec@1 70.000 (68.776)   Prec@5 99.000 (97.428)   [2025-10-22 23:38:29]
  Epoch: [037][300/500]   Time 0.055 (0.110)   Data 0.001 (0.058)   Loss 0.9173 (0.8895)   Prec@1 69.000 (68.980)   Prec@5 98.000 (97.442)   [2025-10-22 23:38:35]
  Epoch: [037][400/500]   Time 0.053 (0.096)   Data 0.000 (0.044)   Loss 1.2318 (0.8917)   Prec@1 56.000 (68.968)   Prec@5 96.000 (97.382)   [2025-10-22 23:38:40]
  **Train** Prec@1 68.936 Prec@5 97.352 Error@1 31.064
  **Test** Prec@1 76.270 Prec@5 98.380 Error@1 23.730
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 23:39:04] [Epoch=038/040] [Need: 00:02:06] [LR=0.0010] [Best : Accuracy=76.27, Error=23.73]
  Epoch: [038][000/500]   Time 17.649 (17.649)   Data 17.467 (17.467)   Loss 0.8935 (0.8935)   Prec@1 69.000 (69.000)   Prec@5 98.000 (98.000)   [2025-10-22 23:39:22]
  Epoch: [038][100/500]   Time 0.051 (0.223)   Data 0.000 (0.174)   Loss 0.8131 (0.8706)   Prec@1 71.000 (69.614)   Prec@5 98.000 (97.475)   [2025-10-22 23:39:27]
  Epoch: [038][200/500]   Time 0.057 (0.139)   Data 0.000 (0.088)   Loss 1.0102 (0.8872)   Prec@1 72.000 (69.284)   Prec@5 98.000 (97.303)   [2025-10-22 23:39:32]
  Epoch: [038][300/500]   Time 0.056 (0.111)   Data 0.001 (0.059)   Loss 0.8534 (0.8911)   Prec@1 67.000 (69.166)   Prec@5 97.000 (97.249)   [2025-10-22 23:39:37]
  Epoch: [038][400/500]   Time 0.053 (0.097)   Data 0.001 (0.044)   Loss 0.7590 (0.8887)   Prec@1 74.000 (69.140)   Prec@5 96.000 (97.339)   [2025-10-22 23:39:43]
  **Train** Prec@1 69.172 Prec@5 97.350 Error@1 30.828
  **Test** Prec@1 75.660 Prec@5 98.350 Error@1 24.340

==>>[2025-10-22 23:40:07] [Epoch=039/040] [Need: 00:01:03] [LR=0.0010] [Best : Accuracy=76.27, Error=23.73]
  Epoch: [039][000/500]   Time 17.800 (17.800)   Data 17.614 (17.614)   Loss 0.8395 (0.8395)   Prec@1 69.000 (69.000)   Prec@5 96.000 (96.000)   [2025-10-22 23:40:25]
  Epoch: [039][100/500]   Time 0.054 (0.225)   Data 0.000 (0.175)   Loss 0.6955 (0.8774)   Prec@1 80.000 (69.644)   Prec@5 99.000 (97.297)   [2025-10-22 23:40:30]
  Epoch: [039][200/500]   Time 0.052 (0.139)   Data 0.000 (0.088)   Loss 0.7668 (0.8893)   Prec@1 75.000 (69.050)   Prec@5 100.000 (97.289)   [2025-10-22 23:40:35]
  Epoch: [039][300/500]   Time 0.053 (0.111)   Data 0.001 (0.059)   Loss 0.8852 (0.8861)   Prec@1 63.000 (69.156)   Prec@5 99.000 (97.302)   [2025-10-22 23:40:41]
  Epoch: [039][400/500]   Time 0.056 (0.098)   Data 0.000 (0.045)   Loss 0.7813 (0.8863)   Prec@1 67.000 (69.142)   Prec@5 100.000 (97.304)   [2025-10-22 23:40:46]
  **Train** Prec@1 69.118 Prec@5 97.312 Error@1 30.882
  **Test** Prec@1 75.920 Prec@5 98.420 Error@1 24.080
