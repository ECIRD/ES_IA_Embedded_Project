save path : ./save/tinyvgg_quan/nominal_0.01
{'data_path': './dataset', 'arch': 'tinyvgg_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': False, 'clipping_coeff': 0.0, 'learning_rate': 0.01, 'manualSeed': 350, 'save_path': './save/tinyvgg_quan/nominal_0.01', 'enable_bfa': False, 'resume': '', 'quan_bitwidth': None, 'reset_weight': False, 'evaluate': False, 'n_iter': 30, 'model_only': False, 'random_bfa': False, 'use_cuda': True}
Random Seed: 350
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'tinyvgg_quan'
=> network :
 TinyVGG(
  (features): Sequential(
    (0): quan_Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Dropout2d(p=0.25, inplace=False)
    (6): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace=True)
    (8): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace=True)
    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (11): Dropout2d(p=0.25, inplace=False)
  )
  (classifier): Sequential(
    (0): quan_Linear(in_features=2048, out_features=10, bias=True)
  )
)
=> do not use any checkpoint for tinyvgg_quan model

==>>[2025-10-22 17:28:28] [Epoch=000/040] [Need: 00:00:00] [LR=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/500]   Time 19.090 (19.090)   Data 18.234 (18.234)   Loss 2.3099 (2.3099)   Prec@1 7.000 (7.000)   Prec@5 46.000 (46.000)   [2025-10-22 17:28:47]
  Epoch: [000][100/500]   Time 0.009 (0.206)   Data 0.000 (0.185)   Loss 2.1591 (2.2157)   Prec@1 25.000 (16.574)   Prec@5 67.000 (63.436)   [2025-10-22 17:28:49]
  Epoch: [000][200/500]   Time 0.019 (0.113)   Data 0.001 (0.095)   Loss 1.8746 (2.1137)   Prec@1 32.000 (22.010)   Prec@5 87.000 (70.647)   [2025-10-22 17:28:51]
  Epoch: [000][300/500]   Time 0.021 (0.083)   Data 0.000 (0.064)   Loss 1.7988 (2.0311)   Prec@1 33.000 (25.259)   Prec@5 88.000 (75.047)   [2025-10-22 17:28:53]
  Epoch: [000][400/500]   Time 0.020 (0.068)   Data 0.000 (0.049)   Loss 1.6846 (1.9664)   Prec@1 36.000 (27.791)   Prec@5 86.000 (77.928)   [2025-10-22 17:28:55]
  **Train** Prec@1 29.842 Prec@5 79.880 Error@1 70.158
  **Test** Prec@1 45.920 Prec@5 92.380 Error@1 54.080
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 17:29:20] [Epoch=001/040] [Need: 00:33:25] [LR=0.0100] [Best : Accuracy=45.92, Error=54.08]
  Epoch: [001][000/500]   Time 22.782 (22.782)   Data 22.731 (22.731)   Loss 1.5881 (1.5881)   Prec@1 38.000 (38.000)   Prec@5 89.000 (89.000)   [2025-10-22 17:29:42]
  Epoch: [001][100/500]   Time 0.031 (0.250)   Data 0.007 (0.226)   Loss 1.6974 (1.6432)   Prec@1 38.000 (40.069)   Prec@5 88.000 (89.158)   [2025-10-22 17:29:45]
  Epoch: [001][200/500]   Time 0.026 (0.137)   Data 0.005 (0.115)   Loss 1.4732 (1.6206)   Prec@1 48.000 (40.701)   Prec@5 93.000 (89.726)   [2025-10-22 17:29:47]
  Epoch: [001][300/500]   Time 0.029 (0.099)   Data 0.011 (0.077)   Loss 1.5775 (1.5978)   Prec@1 45.000 (41.508)   Prec@5 94.000 (90.086)   [2025-10-22 17:29:49]
  Epoch: [001][400/500]   Time 0.021 (0.080)   Data 0.001 (0.058)   Loss 1.3624 (1.5816)   Prec@1 49.000 (42.354)   Prec@5 92.000 (90.192)   [2025-10-22 17:29:52]
  **Train** Prec@1 42.880 Prec@5 90.388 Error@1 57.120
  **Test** Prec@1 54.270 Prec@5 94.430 Error@1 45.730
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 17:30:17] [Epoch=002/040] [Need: 00:34:31] [LR=0.0100] [Best : Accuracy=54.27, Error=45.73]
  Epoch: [002][000/500]   Time 19.530 (19.530)   Data 19.486 (19.486)   Loss 1.5118 (1.5118)   Prec@1 42.000 (42.000)   Prec@5 89.000 (89.000)   [2025-10-22 17:30:37]
  Epoch: [002][100/500]   Time 0.011 (0.207)   Data 0.000 (0.193)   Loss 1.3185 (1.4754)   Prec@1 52.000 (46.485)   Prec@5 97.000 (91.832)   [2025-10-22 17:30:38]
  Epoch: [002][200/500]   Time 0.012 (0.109)   Data 0.001 (0.097)   Loss 1.3981 (1.4567)   Prec@1 45.000 (47.184)   Prec@5 90.000 (92.090)   [2025-10-22 17:30:39]
  Epoch: [002][300/500]   Time 0.009 (0.076)   Data 0.000 (0.065)   Loss 1.2846 (1.4447)   Prec@1 53.000 (47.645)   Prec@5 96.000 (92.256)   [2025-10-22 17:30:40]
  Epoch: [002][400/500]   Time 0.010 (0.060)   Data 0.000 (0.049)   Loss 1.4257 (1.4345)   Prec@1 49.000 (48.157)   Prec@5 91.000 (92.317)   [2025-10-22 17:30:41]
  **Train** Prec@1 48.634 Prec@5 92.486 Error@1 51.366
  **Test** Prec@1 58.510 Prec@5 96.070 Error@1 41.490
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 17:31:01] [Epoch=003/040] [Need: 00:31:27] [LR=0.0100] [Best : Accuracy=58.51, Error=41.49]
  Epoch: [003][000/500]   Time 17.883 (17.883)   Data 17.840 (17.840)   Loss 1.2736 (1.2736)   Prec@1 52.000 (52.000)   Prec@5 95.000 (95.000)   [2025-10-22 17:31:19]
  Epoch: [003][100/500]   Time 0.011 (0.189)   Data 0.000 (0.177)   Loss 1.3366 (1.3582)   Prec@1 51.000 (50.881)   Prec@5 92.000 (93.297)   [2025-10-22 17:31:20]
  Epoch: [003][200/500]   Time 0.009 (0.100)   Data 0.000 (0.089)   Loss 1.5016 (1.3503)   Prec@1 46.000 (51.149)   Prec@5 91.000 (93.433)   [2025-10-22 17:31:21]
  Epoch: [003][300/500]   Time 0.011 (0.070)   Data 0.001 (0.059)   Loss 1.3223 (1.3374)   Prec@1 51.000 (51.578)   Prec@5 95.000 (93.502)   [2025-10-22 17:31:22]
  Epoch: [003][400/500]   Time 0.011 (0.055)   Data 0.000 (0.045)   Loss 1.2578 (1.3247)   Prec@1 51.000 (52.234)   Prec@5 96.000 (93.618)   [2025-10-22 17:31:23]
  **Train** Prec@1 52.530 Prec@5 93.730 Error@1 47.470
  **Test** Prec@1 63.620 Prec@5 96.420 Error@1 36.380
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 17:31:43] [Epoch=004/040] [Need: 00:29:13] [LR=0.0100] [Best : Accuracy=63.62, Error=36.38]
  Epoch: [004][000/500]   Time 18.294 (18.294)   Data 18.250 (18.250)   Loss 1.3121 (1.3121)   Prec@1 50.000 (50.000)   Prec@5 96.000 (96.000)   [2025-10-22 17:32:01]
  Epoch: [004][100/500]   Time 0.010 (0.193)   Data 0.000 (0.181)   Loss 1.2436 (1.2714)   Prec@1 58.000 (54.327)   Prec@5 95.000 (94.277)   [2025-10-22 17:32:02]
  Epoch: [004][200/500]   Time 0.011 (0.102)   Data 0.001 (0.091)   Loss 1.2897 (1.2677)   Prec@1 58.000 (54.886)   Prec@5 93.000 (94.343)   [2025-10-22 17:32:03]
  Epoch: [004][300/500]   Time 0.010 (0.072)   Data 0.000 (0.061)   Loss 1.1843 (1.2564)   Prec@1 59.000 (55.272)   Prec@5 95.000 (94.571)   [2025-10-22 17:32:04]
  Epoch: [004][400/500]   Time 0.009 (0.056)   Data 0.000 (0.046)   Loss 1.3357 (1.2465)   Prec@1 53.000 (55.556)   Prec@5 95.000 (94.703)   [2025-10-22 17:32:05]
  **Train** Prec@1 55.768 Prec@5 94.786 Error@1 44.232
  **Test** Prec@1 65.680 Prec@5 96.890 Error@1 34.320
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 17:32:25] [Epoch=005/040] [Need: 00:27:40] [LR=0.0100] [Best : Accuracy=65.68, Error=34.32]
  Epoch: [005][000/500]   Time 17.767 (17.767)   Data 17.723 (17.723)   Loss 1.3277 (1.3277)   Prec@1 48.000 (48.000)   Prec@5 94.000 (94.000)   [2025-10-22 17:32:43]
  Epoch: [005][100/500]   Time 0.011 (0.188)   Data 0.000 (0.176)   Loss 1.0391 (1.2033)   Prec@1 65.000 (57.337)   Prec@5 95.000 (94.851)   [2025-10-22 17:32:44]
  Epoch: [005][200/500]   Time 0.011 (0.100)   Data 0.000 (0.088)   Loss 1.4264 (1.2070)   Prec@1 54.000 (57.383)   Prec@5 95.000 (94.836)   [2025-10-22 17:32:45]
  Epoch: [005][300/500]   Time 0.010 (0.070)   Data 0.001 (0.059)   Loss 0.9426 (1.2046)   Prec@1 65.000 (57.226)   Prec@5 95.000 (94.860)   [2025-10-22 17:32:46]
  Epoch: [005][400/500]   Time 0.011 (0.055)   Data 0.001 (0.044)   Loss 1.0186 (1.1987)   Prec@1 60.000 (57.364)   Prec@5 97.000 (94.920)   [2025-10-22 17:32:47]
  **Train** Prec@1 57.598 Prec@5 95.034 Error@1 42.402
  **Test** Prec@1 64.860 Prec@5 96.690 Error@1 35.140

==>>[2025-10-22 17:33:07] [Epoch=006/040] [Need: 00:26:21] [LR=0.0100] [Best : Accuracy=65.68, Error=34.32]
  Epoch: [006][000/500]   Time 25.637 (25.637)   Data 25.413 (25.413)   Loss 1.1625 (1.1625)   Prec@1 57.000 (57.000)   Prec@5 94.000 (94.000)   [2025-10-22 17:33:33]
  Epoch: [006][100/500]   Time 0.010 (0.268)   Data 0.001 (0.252)   Loss 1.2220 (1.1419)   Prec@1 58.000 (59.653)   Prec@5 93.000 (95.861)   [2025-10-22 17:33:34]
  Epoch: [006][200/500]   Time 0.010 (0.141)   Data 0.000 (0.127)   Loss 1.1553 (1.1659)   Prec@1 57.000 (58.900)   Prec@5 95.000 (95.308)   [2025-10-22 17:33:35]
  Epoch: [006][300/500]   Time 0.012 (0.097)   Data 0.000 (0.085)   Loss 1.3308 (1.1568)   Prec@1 55.000 (58.937)   Prec@5 94.000 (95.412)   [2025-10-22 17:33:36]
  Epoch: [006][400/500]   Time 0.009 (0.076)   Data 0.000 (0.064)   Loss 1.3467 (1.1576)   Prec@1 55.000 (58.893)   Prec@5 93.000 (95.389)   [2025-10-22 17:33:38]
  **Train** Prec@1 58.920 Prec@5 95.322 Error@1 41.080
  **Test** Prec@1 67.850 Prec@5 97.410 Error@1 32.150
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 17:34:03] [Epoch=007/040] [Need: 00:26:19] [LR=0.0100] [Best : Accuracy=67.85, Error=32.15]
  Epoch: [007][000/500]   Time 21.845 (21.845)   Data 21.779 (21.779)   Loss 0.9597 (0.9597)   Prec@1 65.000 (65.000)   Prec@5 98.000 (98.000)   [2025-10-22 17:34:25]
  Epoch: [007][100/500]   Time 0.010 (0.228)   Data 0.000 (0.216)   Loss 0.9646 (1.1317)   Prec@1 66.000 (59.683)   Prec@5 98.000 (95.446)   [2025-10-22 17:34:26]
  Epoch: [007][200/500]   Time 0.010 (0.120)   Data 0.000 (0.109)   Loss 1.1803 (1.1259)   Prec@1 57.000 (60.070)   Prec@5 99.000 (95.701)   [2025-10-22 17:34:27]
  Epoch: [007][300/500]   Time 0.010 (0.083)   Data 0.000 (0.073)   Loss 1.0353 (1.1238)   Prec@1 65.000 (60.189)   Prec@5 97.000 (95.674)   [2025-10-22 17:34:28]
  Epoch: [007][400/500]   Time 0.008 (0.065)   Data 0.000 (0.055)   Loss 1.2034 (1.1245)   Prec@1 61.000 (60.155)   Prec@5 95.000 (95.728)   [2025-10-22 17:34:29]
  **Train** Prec@1 60.152 Prec@5 95.690 Error@1 39.848
  **Test** Prec@1 67.690 Prec@5 97.070 Error@1 32.310

==>>[2025-10-22 17:34:50] [Epoch=008/040] [Need: 00:25:27] [LR=0.0100] [Best : Accuracy=67.85, Error=32.15]
  Epoch: [008][000/500]   Time 17.898 (17.898)   Data 17.854 (17.854)   Loss 1.1933 (1.1933)   Prec@1 64.000 (64.000)   Prec@5 94.000 (94.000)   [2025-10-22 17:35:08]
  Epoch: [008][100/500]   Time 0.009 (0.190)   Data 0.000 (0.177)   Loss 1.0025 (1.1005)   Prec@1 65.000 (61.069)   Prec@5 96.000 (95.950)   [2025-10-22 17:35:09]
  Epoch: [008][200/500]   Time 0.009 (0.100)   Data 0.000 (0.089)   Loss 1.1739 (1.1055)   Prec@1 68.000 (60.622)   Prec@5 93.000 (95.856)   [2025-10-22 17:35:10]
  Epoch: [008][300/500]   Time 0.011 (0.071)   Data 0.001 (0.060)   Loss 1.0873 (1.1008)   Prec@1 62.000 (61.033)   Prec@5 98.000 (95.993)   [2025-10-22 17:35:11]
  Epoch: [008][400/500]   Time 0.009 (0.056)   Data 0.000 (0.045)   Loss 0.9700 (1.1027)   Prec@1 59.000 (60.933)   Prec@5 98.000 (95.938)   [2025-10-22 17:35:12]
  **Train** Prec@1 61.108 Prec@5 95.892 Error@1 38.892
  **Test** Prec@1 70.210 Prec@5 97.510 Error@1 29.790
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 17:35:32] [Epoch=009/040] [Need: 00:24:20] [LR=0.0100] [Best : Accuracy=70.21, Error=29.79]
  Epoch: [009][000/500]   Time 18.188 (18.188)   Data 18.143 (18.143)   Loss 1.2631 (1.2631)   Prec@1 58.000 (58.000)   Prec@5 94.000 (94.000)   [2025-10-22 17:35:50]
  Epoch: [009][100/500]   Time 0.009 (0.193)   Data 0.000 (0.180)   Loss 0.8616 (1.0799)   Prec@1 71.000 (62.158)   Prec@5 99.000 (95.990)   [2025-10-22 17:35:52]
  Epoch: [009][200/500]   Time 0.009 (0.102)   Data 0.000 (0.090)   Loss 0.9606 (1.0771)   Prec@1 63.000 (61.950)   Prec@5 98.000 (95.920)   [2025-10-22 17:35:53]
  Epoch: [009][300/500]   Time 0.010 (0.072)   Data 0.000 (0.060)   Loss 1.1279 (1.0785)   Prec@1 63.000 (61.847)   Prec@5 95.000 (95.917)   [2025-10-22 17:35:54]
  Epoch: [009][400/500]   Time 0.008 (0.056)   Data 0.000 (0.045)   Loss 1.1292 (1.0785)   Prec@1 61.000 (61.905)   Prec@5 97.000 (95.975)   [2025-10-22 17:35:55]
  **Train** Prec@1 61.992 Prec@5 95.926 Error@1 38.008
  **Test** Prec@1 70.370 Prec@5 97.680 Error@1 29.630
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 17:36:15] [Epoch=010/040] [Need: 00:23:21] [LR=0.0100] [Best : Accuracy=70.37, Error=29.63]
  Epoch: [010][000/500]   Time 18.015 (18.015)   Data 17.969 (17.969)   Loss 0.9144 (0.9144)   Prec@1 69.000 (69.000)   Prec@5 99.000 (99.000)   [2025-10-22 17:36:33]
  Epoch: [010][100/500]   Time 0.011 (0.191)   Data 0.000 (0.178)   Loss 0.9378 (1.0587)   Prec@1 69.000 (62.545)   Prec@5 97.000 (96.079)   [2025-10-22 17:36:34]
  Epoch: [010][200/500]   Time 0.012 (0.101)   Data 0.000 (0.090)   Loss 0.7999 (1.0614)   Prec@1 69.000 (62.652)   Prec@5 98.000 (96.174)   [2025-10-22 17:36:36]
  Epoch: [010][300/500]   Time 0.014 (0.071)   Data 0.000 (0.060)   Loss 1.0058 (1.0618)   Prec@1 55.000 (62.362)   Prec@5 99.000 (96.236)   [2025-10-22 17:36:37]
  Epoch: [010][400/500]   Time 0.011 (0.056)   Data 0.000 (0.045)   Loss 1.0775 (1.0595)   Prec@1 63.000 (62.584)   Prec@5 95.000 (96.222)   [2025-10-22 17:36:38]
  **Train** Prec@1 62.440 Prec@5 96.200 Error@1 37.560
  **Test** Prec@1 69.030 Prec@5 97.790 Error@1 30.970

==>>[2025-10-22 17:36:58] [Epoch=011/040] [Need: 00:22:24] [LR=0.0100] [Best : Accuracy=70.37, Error=29.63]
  Epoch: [011][000/500]   Time 18.560 (18.560)   Data 18.513 (18.513)   Loss 1.2408 (1.2408)   Prec@1 61.000 (61.000)   Prec@5 97.000 (97.000)   [2025-10-22 17:37:17]
  Epoch: [011][100/500]   Time 0.011 (0.198)   Data 0.000 (0.183)   Loss 1.0490 (1.0422)   Prec@1 57.000 (63.426)   Prec@5 97.000 (96.683)   [2025-10-22 17:37:18]
  Epoch: [011][200/500]   Time 0.010 (0.104)   Data 0.000 (0.092)   Loss 1.0053 (1.0454)   Prec@1 65.000 (63.373)   Prec@5 99.000 (96.532)   [2025-10-22 17:37:19]
  Epoch: [011][300/500]   Time 0.009 (0.073)   Data 0.000 (0.062)   Loss 1.0604 (1.0416)   Prec@1 66.000 (63.445)   Prec@5 94.000 (96.472)   [2025-10-22 17:37:20]
  Epoch: [011][400/500]   Time 0.014 (0.058)   Data 0.000 (0.046)   Loss 1.0554 (1.0432)   Prec@1 63.000 (63.384)   Prec@5 96.000 (96.416)   [2025-10-22 17:37:21]
  **Train** Prec@1 63.272 Prec@5 96.370 Error@1 36.728
  **Test** Prec@1 71.300 Prec@5 97.880 Error@1 28.700
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 17:37:41] [Epoch=012/040] [Need: 00:21:30] [LR=0.0100] [Best : Accuracy=71.30, Error=28.70]
  Epoch: [012][000/500]   Time 17.903 (17.903)   Data 17.855 (17.855)   Loss 1.0530 (1.0530)   Prec@1 61.000 (61.000)   Prec@5 95.000 (95.000)   [2025-10-22 17:37:59]
  Epoch: [012][100/500]   Time 0.011 (0.190)   Data 0.001 (0.177)   Loss 1.0975 (1.0323)   Prec@1 60.000 (63.376)   Prec@5 98.000 (96.149)   [2025-10-22 17:38:00]
  Epoch: [012][200/500]   Time 0.009 (0.101)   Data 0.000 (0.089)   Loss 0.9655 (1.0318)   Prec@1 70.000 (63.721)   Prec@5 98.000 (96.209)   [2025-10-22 17:38:02]
  Epoch: [012][300/500]   Time 0.008 (0.071)   Data 0.000 (0.060)   Loss 1.2406 (1.0302)   Prec@1 55.000 (63.664)   Prec@5 92.000 (96.262)   [2025-10-22 17:38:03]
  Epoch: [012][400/500]   Time 0.010 (0.056)   Data 0.000 (0.045)   Loss 1.0237 (1.0280)   Prec@1 64.000 (63.820)   Prec@5 96.000 (96.324)   [2025-10-22 17:38:04]
  **Train** Prec@1 63.858 Prec@5 96.330 Error@1 36.142
  **Test** Prec@1 71.410 Prec@5 98.010 Error@1 28.590
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 17:38:24] [Epoch=013/040] [Need: 00:20:37] [LR=0.0100] [Best : Accuracy=71.41, Error=28.59]
  Epoch: [013][000/500]   Time 17.746 (17.746)   Data 17.699 (17.699)   Loss 1.1800 (1.1800)   Prec@1 53.000 (53.000)   Prec@5 98.000 (98.000)   [2025-10-22 17:38:42]
  Epoch: [013][100/500]   Time 0.009 (0.188)   Data 0.000 (0.175)   Loss 1.0922 (1.0187)   Prec@1 60.000 (64.099)   Prec@5 94.000 (96.554)   [2025-10-22 17:38:43]
  Epoch: [013][200/500]   Time 0.011 (0.099)   Data 0.000 (0.088)   Loss 1.0128 (1.0168)   Prec@1 65.000 (64.045)   Prec@5 93.000 (96.522)   [2025-10-22 17:38:44]
  Epoch: [013][300/500]   Time 0.009 (0.070)   Data 0.000 (0.059)   Loss 1.0677 (1.0133)   Prec@1 63.000 (64.176)   Prec@5 96.000 (96.495)   [2025-10-22 17:38:45]
  Epoch: [013][400/500]   Time 0.010 (0.055)   Data 0.000 (0.044)   Loss 1.0960 (1.0182)   Prec@1 63.000 (63.930)   Prec@5 95.000 (96.499)   [2025-10-22 17:38:46]
  **Train** Prec@1 64.090 Prec@5 96.590 Error@1 35.910
  **Test** Prec@1 71.830 Prec@5 97.910 Error@1 28.170
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 17:39:08] [Epoch=014/040] [Need: 00:19:47] [LR=0.0100] [Best : Accuracy=71.83, Error=28.17]
  Epoch: [014][000/500]   Time 19.648 (19.648)   Data 19.603 (19.603)   Loss 1.0499 (1.0499)   Prec@1 68.000 (68.000)   Prec@5 96.000 (96.000)   [2025-10-22 17:39:27]
  Epoch: [014][100/500]   Time 0.012 (0.206)   Data 0.000 (0.194)   Loss 1.0019 (1.0165)   Prec@1 65.000 (64.327)   Prec@5 95.000 (96.624)   [2025-10-22 17:39:29]
  Epoch: [014][200/500]   Time 0.010 (0.109)   Data 0.000 (0.098)   Loss 0.9098 (1.0117)   Prec@1 68.000 (64.453)   Prec@5 97.000 (96.592)   [2025-10-22 17:39:30]
  Epoch: [014][300/500]   Time 0.011 (0.076)   Data 0.000 (0.065)   Loss 1.1185 (1.0142)   Prec@1 67.000 (64.525)   Prec@5 95.000 (96.568)   [2025-10-22 17:39:31]
  Epoch: [014][400/500]   Time 0.009 (0.060)   Data 0.000 (0.049)   Loss 0.8655 (1.0066)   Prec@1 74.000 (64.723)   Prec@5 95.000 (96.594)   [2025-10-22 17:39:32]
  **Train** Prec@1 64.706 Prec@5 96.562 Error@1 35.294
  **Test** Prec@1 72.570 Prec@5 98.030 Error@1 27.430
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 17:39:52] [Epoch=015/040] [Need: 00:18:59] [LR=0.0100] [Best : Accuracy=72.57, Error=27.43]
  Epoch: [015][000/500]   Time 17.497 (17.497)   Data 17.452 (17.452)   Loss 1.0768 (1.0768)   Prec@1 59.000 (59.000)   Prec@5 97.000 (97.000)   [2025-10-22 17:40:09]
  Epoch: [015][100/500]   Time 0.010 (0.186)   Data 0.001 (0.173)   Loss 0.9735 (1.0046)   Prec@1 64.000 (64.673)   Prec@5 99.000 (96.416)   [2025-10-22 17:40:11]
  Epoch: [015][200/500]   Time 0.012 (0.099)   Data 0.000 (0.087)   Loss 0.9138 (0.9917)   Prec@1 64.000 (65.159)   Prec@5 100.000 (96.602)   [2025-10-22 17:40:12]
  Epoch: [015][300/500]   Time 0.012 (0.069)   Data 0.000 (0.058)   Loss 0.8733 (0.9910)   Prec@1 70.000 (65.163)   Prec@5 97.000 (96.645)   [2025-10-22 17:40:13]
  Epoch: [015][400/500]   Time 0.011 (0.055)   Data 0.000 (0.044)   Loss 1.0436 (0.9934)   Prec@1 67.000 (65.080)   Prec@5 97.000 (96.673)   [2025-10-22 17:40:14]
  **Train** Prec@1 65.038 Prec@5 96.664 Error@1 34.962
  **Test** Prec@1 72.590 Prec@5 97.930 Error@1 27.410
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 17:40:33] [Epoch=016/040] [Need: 00:18:08] [LR=0.0100] [Best : Accuracy=72.59, Error=27.41]
  Epoch: [016][000/500]   Time 17.569 (17.569)   Data 17.524 (17.524)   Loss 0.9131 (0.9131)   Prec@1 63.000 (63.000)   Prec@5 96.000 (96.000)   [2025-10-22 17:40:51]
  Epoch: [016][100/500]   Time 0.013 (0.187)   Data 0.000 (0.174)   Loss 0.9247 (0.9774)   Prec@1 72.000 (65.307)   Prec@5 97.000 (96.792)   [2025-10-22 17:40:52]
  Epoch: [016][200/500]   Time 0.009 (0.099)   Data 0.000 (0.087)   Loss 1.0474 (0.9899)   Prec@1 61.000 (64.900)   Prec@5 100.000 (96.741)   [2025-10-22 17:40:53]
  Epoch: [016][300/500]   Time 0.009 (0.070)   Data 0.000 (0.058)   Loss 1.0941 (0.9894)   Prec@1 66.000 (65.033)   Prec@5 95.000 (96.794)   [2025-10-22 17:40:54]
  Epoch: [016][400/500]   Time 0.011 (0.055)   Data 0.000 (0.044)   Loss 0.9814 (0.9869)   Prec@1 65.000 (65.155)   Prec@5 96.000 (96.718)   [2025-10-22 17:40:55]
  **Train** Prec@1 65.120 Prec@5 96.654 Error@1 34.880
  **Test** Prec@1 73.080 Prec@5 98.050 Error@1 26.920
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 17:41:15] [Epoch=017/040] [Need: 00:17:17] [LR=0.0100] [Best : Accuracy=73.08, Error=26.92]
  Epoch: [017][000/500]   Time 17.299 (17.299)   Data 17.250 (17.250)   Loss 1.0531 (1.0531)   Prec@1 68.000 (68.000)   Prec@5 96.000 (96.000)   [2025-10-22 17:41:32]
  Epoch: [017][100/500]   Time 0.011 (0.185)   Data 0.000 (0.171)   Loss 1.0395 (0.9737)   Prec@1 64.000 (65.485)   Prec@5 96.000 (97.079)   [2025-10-22 17:41:34]
  Epoch: [017][200/500]   Time 0.010 (0.098)   Data 0.001 (0.086)   Loss 0.9437 (0.9755)   Prec@1 66.000 (65.378)   Prec@5 99.000 (96.915)   [2025-10-22 17:41:35]
  Epoch: [017][300/500]   Time 0.012 (0.069)   Data 0.000 (0.058)   Loss 0.7463 (0.9764)   Prec@1 76.000 (65.395)   Prec@5 99.000 (96.934)   [2025-10-22 17:41:36]
  Epoch: [017][400/500]   Time 0.008 (0.054)   Data 0.000 (0.043)   Loss 1.0145 (0.9751)   Prec@1 68.000 (65.491)   Prec@5 98.000 (96.915)   [2025-10-22 17:41:37]
  **Train** Prec@1 65.466 Prec@5 96.866 Error@1 34.534
  **Test** Prec@1 73.520 Prec@5 98.140 Error@1 26.480
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 17:41:57] [Epoch=018/040] [Need: 00:16:28] [LR=0.0100] [Best : Accuracy=73.52, Error=26.48]
  Epoch: [018][000/500]   Time 17.405 (17.405)   Data 17.359 (17.359)   Loss 0.8557 (0.8557)   Prec@1 69.000 (69.000)   Prec@5 99.000 (99.000)   [2025-10-22 17:42:14]
  Epoch: [018][100/500]   Time 0.010 (0.185)   Data 0.000 (0.172)   Loss 0.8992 (0.9748)   Prec@1 71.000 (65.010)   Prec@5 98.000 (97.188)   [2025-10-22 17:42:15]
  Epoch: [018][200/500]   Time 0.009 (0.098)   Data 0.000 (0.086)   Loss 1.2098 (0.9726)   Prec@1 64.000 (65.512)   Prec@5 91.000 (96.975)   [2025-10-22 17:42:16]
  Epoch: [018][300/500]   Time 0.009 (0.069)   Data 0.000 (0.058)   Loss 0.9661 (0.9799)   Prec@1 72.000 (65.442)   Prec@5 98.000 (96.880)   [2025-10-22 17:42:17]
  Epoch: [018][400/500]   Time 0.015 (0.054)   Data 0.000 (0.043)   Loss 1.0057 (0.9794)   Prec@1 67.000 (65.549)   Prec@5 96.000 (96.855)   [2025-10-22 17:42:18]
  **Train** Prec@1 65.620 Prec@5 96.854 Error@1 34.380
  **Test** Prec@1 73.720 Prec@5 98.270 Error@1 26.280
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 17:42:38] [Epoch=019/040] [Need: 00:15:39] [LR=0.0100] [Best : Accuracy=73.72, Error=26.28]
  Epoch: [019][000/500]   Time 17.374 (17.374)   Data 17.330 (17.330)   Loss 0.8153 (0.8153)   Prec@1 68.000 (68.000)   Prec@5 100.000 (100.000)   [2025-10-22 17:42:56]
  Epoch: [019][100/500]   Time 0.013 (0.184)   Data 0.000 (0.172)   Loss 0.8545 (0.9603)   Prec@1 68.000 (65.673)   Prec@5 99.000 (97.079)   [2025-10-22 17:42:57]
  Epoch: [019][200/500]   Time 0.012 (0.098)   Data 0.001 (0.086)   Loss 0.8253 (0.9658)   Prec@1 74.000 (65.796)   Prec@5 97.000 (96.801)   [2025-10-22 17:42:58]
  Epoch: [019][300/500]   Time 0.011 (0.069)   Data 0.000 (0.058)   Loss 0.8760 (0.9669)   Prec@1 71.000 (65.914)   Prec@5 96.000 (96.761)   [2025-10-22 17:42:59]
  Epoch: [019][400/500]   Time 0.011 (0.055)   Data 0.000 (0.043)   Loss 0.9963 (0.9660)   Prec@1 65.000 (65.985)   Prec@5 95.000 (96.763)   [2025-10-22 17:43:00]
  **Train** Prec@1 66.056 Prec@5 96.796 Error@1 33.944
  **Test** Prec@1 73.830 Prec@5 98.400 Error@1 26.170
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 17:43:20] [Epoch=020/040] [Need: 00:14:51] [LR=0.0100] [Best : Accuracy=73.83, Error=26.17]
  Epoch: [020][000/500]   Time 18.138 (18.138)   Data 18.089 (18.089)   Loss 0.9085 (0.9085)   Prec@1 68.000 (68.000)   Prec@5 99.000 (99.000)   [2025-10-22 17:43:38]
  Epoch: [020][100/500]   Time 0.012 (0.194)   Data 0.000 (0.179)   Loss 0.9058 (0.9565)   Prec@1 58.000 (66.376)   Prec@5 99.000 (97.000)   [2025-10-22 17:43:39]
  Epoch: [020][200/500]   Time 0.011 (0.104)   Data 0.000 (0.090)   Loss 1.0615 (0.9570)   Prec@1 58.000 (66.299)   Prec@5 96.000 (96.935)   [2025-10-22 17:43:41]
  Epoch: [020][300/500]   Time 0.009 (0.073)   Data 0.000 (0.060)   Loss 0.9077 (0.9571)   Prec@1 65.000 (66.306)   Prec@5 99.000 (96.874)   [2025-10-22 17:43:42]
  Epoch: [020][400/500]   Time 0.009 (0.057)   Data 0.000 (0.045)   Loss 0.9858 (0.9575)   Prec@1 65.000 (66.404)   Prec@5 95.000 (96.786)   [2025-10-22 17:43:43]
  **Train** Prec@1 66.270 Prec@5 96.720 Error@1 33.730
  **Test** Prec@1 74.570 Prec@5 98.350 Error@1 25.430
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 17:44:03] [Epoch=021/040] [Need: 00:14:05] [LR=0.0100] [Best : Accuracy=74.57, Error=25.43]
  Epoch: [021][000/500]   Time 21.858 (21.858)   Data 21.808 (21.808)   Loss 0.9659 (0.9659)   Prec@1 71.000 (71.000)   Prec@5 98.000 (98.000)   [2025-10-22 17:44:25]
  Epoch: [021][100/500]   Time 0.011 (0.230)   Data 0.000 (0.216)   Loss 1.0869 (0.9443)   Prec@1 63.000 (67.307)   Prec@5 97.000 (97.109)   [2025-10-22 17:44:26]
  Epoch: [021][200/500]   Time 0.013 (0.122)   Data 0.000 (0.109)   Loss 0.8304 (0.9444)   Prec@1 70.000 (66.945)   Prec@5 98.000 (97.154)   [2025-10-22 17:44:27]
  Epoch: [021][300/500]   Time 0.013 (0.086)   Data 0.000 (0.073)   Loss 0.8278 (0.9539)   Prec@1 71.000 (66.671)   Prec@5 100.000 (97.093)   [2025-10-22 17:44:29]
  Epoch: [021][400/500]   Time 0.019 (0.068)   Data 0.009 (0.055)   Loss 0.8097 (0.9544)   Prec@1 73.000 (66.733)   Prec@5 98.000 (97.055)   [2025-10-22 17:44:30]
  **Train** Prec@1 66.604 Prec@5 97.052 Error@1 33.396
  **Test** Prec@1 73.470 Prec@5 98.220 Error@1 26.530

==>>[2025-10-22 17:44:57] [Epoch=022/040] [Need: 00:13:28] [LR=0.0100] [Best : Accuracy=74.57, Error=25.43]
  Epoch: [022][000/500]   Time 19.647 (19.647)   Data 19.554 (19.554)   Loss 0.9519 (0.9519)   Prec@1 64.000 (64.000)   Prec@5 95.000 (95.000)   [2025-10-22 17:45:17]
  Epoch: [022][100/500]   Time 0.009 (0.207)   Data 0.000 (0.194)   Loss 1.0838 (0.9583)   Prec@1 63.000 (66.257)   Prec@5 95.000 (97.059)   [2025-10-22 17:45:18]
  Epoch: [022][200/500]   Time 0.008 (0.109)   Data 0.000 (0.097)   Loss 0.6805 (0.9551)   Prec@1 78.000 (66.403)   Prec@5 98.000 (97.025)   [2025-10-22 17:45:19]
  Epoch: [022][300/500]   Time 0.014 (0.077)   Data 0.001 (0.065)   Loss 0.9952 (0.9534)   Prec@1 69.000 (66.522)   Prec@5 96.000 (97.027)   [2025-10-22 17:45:20]
  Epoch: [022][400/500]   Time 0.013 (0.060)   Data 0.001 (0.049)   Loss 0.8808 (0.9528)   Prec@1 64.000 (66.549)   Prec@5 98.000 (97.022)   [2025-10-22 17:45:21]
  **Train** Prec@1 66.462 Prec@5 97.010 Error@1 33.538
  **Test** Prec@1 74.560 Prec@5 98.340 Error@1 25.440

==>>[2025-10-22 17:45:41] [Epoch=023/040] [Need: 00:12:43] [LR=0.0100] [Best : Accuracy=74.57, Error=25.43]
  Epoch: [023][000/500]   Time 17.644 (17.644)   Data 17.597 (17.597)   Loss 0.8228 (0.8228)   Prec@1 69.000 (69.000)   Prec@5 98.000 (98.000)   [2025-10-22 17:45:58]
  Epoch: [023][100/500]   Time 0.013 (0.187)   Data 0.000 (0.174)   Loss 0.9317 (0.9489)   Prec@1 67.000 (66.376)   Prec@5 98.000 (96.881)   [2025-10-22 17:45:59]
  Epoch: [023][200/500]   Time 0.013 (0.100)   Data 0.000 (0.088)   Loss 0.9030 (0.9399)   Prec@1 67.000 (66.726)   Prec@5 97.000 (96.925)   [2025-10-22 17:46:01]
  Epoch: [023][300/500]   Time 0.010 (0.070)   Data 0.000 (0.059)   Loss 0.9456 (0.9417)   Prec@1 65.000 (66.791)   Prec@5 98.000 (97.013)   [2025-10-22 17:46:02]
  Epoch: [023][400/500]   Time 0.010 (0.055)   Data 0.000 (0.044)   Loss 0.8523 (0.9421)   Prec@1 69.000 (66.873)   Prec@5 98.000 (96.953)   [2025-10-22 17:46:03]
  **Train** Prec@1 66.830 Prec@5 96.966 Error@1 33.170
  **Test** Prec@1 74.610 Prec@5 98.260 Error@1 25.390
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 17:46:22] [Epoch=024/040] [Need: 00:11:56] [LR=0.0100] [Best : Accuracy=74.61, Error=25.39]
  Epoch: [024][000/500]   Time 17.450 (17.450)   Data 17.405 (17.405)   Loss 0.9951 (0.9951)   Prec@1 64.000 (64.000)   Prec@5 96.000 (96.000)   [2025-10-22 17:46:40]
  Epoch: [024][100/500]   Time 0.012 (0.186)   Data 0.000 (0.172)   Loss 0.9466 (0.9321)   Prec@1 66.000 (66.861)   Prec@5 98.000 (97.317)   [2025-10-22 17:46:41]
  Epoch: [024][200/500]   Time 0.013 (0.099)   Data 0.000 (0.087)   Loss 0.8811 (0.9470)   Prec@1 71.000 (66.657)   Prec@5 98.000 (97.209)   [2025-10-22 17:46:42]
  Epoch: [024][300/500]   Time 0.010 (0.070)   Data 0.000 (0.058)   Loss 1.0372 (0.9413)   Prec@1 66.000 (66.897)   Prec@5 95.000 (97.243)   [2025-10-22 17:46:43]
  Epoch: [024][400/500]   Time 0.011 (0.055)   Data 0.001 (0.044)   Loss 0.9927 (0.9428)   Prec@1 70.000 (66.963)   Prec@5 97.000 (97.060)   [2025-10-22 17:46:44]
  **Train** Prec@1 67.022 Prec@5 97.034 Error@1 32.978
  **Test** Prec@1 73.710 Prec@5 98.180 Error@1 26.290

==>>[2025-10-22 17:47:04] [Epoch=025/040] [Need: 00:11:09] [LR=0.0010] [Best : Accuracy=74.61, Error=25.39]
  Epoch: [025][000/500]   Time 18.315 (18.315)   Data 18.271 (18.271)   Loss 0.8433 (0.8433)   Prec@1 65.000 (65.000)   Prec@5 99.000 (99.000)   [2025-10-22 17:47:23]
  Epoch: [025][100/500]   Time 0.011 (0.194)   Data 0.001 (0.181)   Loss 0.9408 (0.8958)   Prec@1 67.000 (68.802)   Prec@5 95.000 (97.149)   [2025-10-22 17:47:24]
  Epoch: [025][200/500]   Time 0.009 (0.103)   Data 0.000 (0.091)   Loss 0.8490 (0.8897)   Prec@1 72.000 (68.910)   Prec@5 95.000 (97.294)   [2025-10-22 17:47:25]
  Epoch: [025][300/500]   Time 0.009 (0.072)   Data 0.000 (0.061)   Loss 0.8032 (0.8839)   Prec@1 71.000 (69.362)   Prec@5 98.000 (97.286)   [2025-10-22 17:47:26]
  Epoch: [025][400/500]   Time 0.009 (0.057)   Data 0.001 (0.046)   Loss 0.8300 (0.8767)   Prec@1 72.000 (69.561)   Prec@5 96.000 (97.319)   [2025-10-22 17:47:27]
  **Train** Prec@1 69.364 Prec@5 97.344 Error@1 30.636
  **Test** Prec@1 76.370 Prec@5 98.340 Error@1 23.630
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 17:47:47] [Epoch=026/040] [Need: 00:10:24] [LR=0.0010] [Best : Accuracy=76.37, Error=23.63]
  Epoch: [026][000/500]   Time 17.482 (17.482)   Data 17.435 (17.435)   Loss 0.9011 (0.9011)   Prec@1 68.000 (68.000)   Prec@5 97.000 (97.000)   [2025-10-22 17:48:04]
  Epoch: [026][100/500]   Time 0.009 (0.186)   Data 0.000 (0.173)   Loss 0.6934 (0.8595)   Prec@1 75.000 (69.683)   Prec@5 99.000 (97.525)   [2025-10-22 17:48:06]
  Epoch: [026][200/500]   Time 0.009 (0.098)   Data 0.000 (0.087)   Loss 0.8512 (0.8565)   Prec@1 72.000 (69.980)   Prec@5 96.000 (97.542)   [2025-10-22 17:48:07]
  Epoch: [026][300/500]   Time 0.008 (0.069)   Data 0.000 (0.058)   Loss 0.8633 (0.8586)   Prec@1 69.000 (69.977)   Prec@5 97.000 (97.538)   [2025-10-22 17:48:08]
  Epoch: [026][400/500]   Time 0.009 (0.054)   Data 0.000 (0.044)   Loss 0.8412 (0.8584)   Prec@1 68.000 (70.032)   Prec@5 98.000 (97.484)   [2025-10-22 17:48:09]
  **Train** Prec@1 69.986 Prec@5 97.516 Error@1 30.014
  **Test** Prec@1 76.450 Prec@5 98.490 Error@1 23.550
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 17:48:28] [Epoch=027/040] [Need: 00:09:37] [LR=0.0010] [Best : Accuracy=76.45, Error=23.55]
  Epoch: [027][000/500]   Time 17.785 (17.785)   Data 17.741 (17.741)   Loss 0.8140 (0.8140)   Prec@1 72.000 (72.000)   Prec@5 97.000 (97.000)   [2025-10-22 17:48:46]
  Epoch: [027][100/500]   Time 0.014 (0.188)   Data 0.001 (0.176)   Loss 0.7706 (0.8494)   Prec@1 75.000 (70.218)   Prec@5 97.000 (97.356)   [2025-10-22 17:48:47]
  Epoch: [027][200/500]   Time 0.011 (0.100)   Data 0.000 (0.088)   Loss 0.8365 (0.8570)   Prec@1 68.000 (70.149)   Prec@5 99.000 (97.473)   [2025-10-22 17:48:48]
  Epoch: [027][300/500]   Time 0.009 (0.070)   Data 0.000 (0.059)   Loss 0.8458 (0.8589)   Prec@1 77.000 (70.326)   Prec@5 98.000 (97.472)   [2025-10-22 17:48:50]
  Epoch: [027][400/500]   Time 0.010 (0.055)   Data 0.000 (0.044)   Loss 0.8971 (0.8563)   Prec@1 69.000 (70.289)   Prec@5 96.000 (97.436)   [2025-10-22 17:48:51]
  **Train** Prec@1 70.298 Prec@5 97.482 Error@1 29.702
  **Test** Prec@1 76.730 Prec@5 98.510 Error@1 23.270
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 17:49:10] [Epoch=028/040] [Need: 00:08:52] [LR=0.0010] [Best : Accuracy=76.73, Error=23.27]
  Epoch: [028][000/500]   Time 17.433 (17.433)   Data 17.386 (17.386)   Loss 0.6684 (0.6684)   Prec@1 79.000 (79.000)   Prec@5 99.000 (99.000)   [2025-10-22 17:49:28]
  Epoch: [028][100/500]   Time 0.011 (0.185)   Data 0.000 (0.172)   Loss 0.8889 (0.8407)   Prec@1 69.000 (70.574)   Prec@5 99.000 (97.683)   [2025-10-22 17:49:29]
  Epoch: [028][200/500]   Time 0.009 (0.098)   Data 0.000 (0.087)   Loss 0.8746 (0.8387)   Prec@1 69.000 (70.975)   Prec@5 97.000 (97.597)   [2025-10-22 17:49:30]
  Epoch: [028][300/500]   Time 0.010 (0.069)   Data 0.000 (0.058)   Loss 0.8428 (0.8464)   Prec@1 69.000 (70.488)   Prec@5 97.000 (97.591)   [2025-10-22 17:49:31]
  Epoch: [028][400/500]   Time 0.010 (0.054)   Data 0.000 (0.044)   Loss 0.8588 (0.8455)   Prec@1 62.000 (70.586)   Prec@5 99.000 (97.661)   [2025-10-22 17:49:32]
  **Train** Prec@1 70.608 Prec@5 97.652 Error@1 29.392
  **Test** Prec@1 76.470 Prec@5 98.490 Error@1 23.530

==>>[2025-10-22 17:49:52] [Epoch=029/040] [Need: 00:08:06] [LR=0.0010] [Best : Accuracy=76.73, Error=23.27]
  Epoch: [029][000/500]   Time 18.115 (18.115)   Data 18.071 (18.071)   Loss 0.7621 (0.7621)   Prec@1 81.000 (81.000)   Prec@5 98.000 (98.000)   [2025-10-22 17:50:10]
  Epoch: [029][100/500]   Time 0.009 (0.192)   Data 0.000 (0.179)   Loss 0.9342 (0.8608)   Prec@1 63.000 (70.059)   Prec@5 99.000 (97.624)   [2025-10-22 17:50:11]
  Epoch: [029][200/500]   Time 0.008 (0.102)   Data 0.000 (0.090)   Loss 0.7455 (0.8418)   Prec@1 76.000 (70.617)   Prec@5 99.000 (97.736)   [2025-10-22 17:50:12]
  Epoch: [029][300/500]   Time 0.013 (0.072)   Data 0.000 (0.060)   Loss 1.0172 (0.8399)   Prec@1 68.000 (70.711)   Prec@5 96.000 (97.728)   [2025-10-22 17:50:13]
  Epoch: [029][400/500]   Time 0.013 (0.057)   Data 0.000 (0.045)   Loss 0.8578 (0.8358)   Prec@1 68.000 (70.810)   Prec@5 99.000 (97.751)   [2025-10-22 17:50:14]
  **Train** Prec@1 70.640 Prec@5 97.680 Error@1 29.360
  **Test** Prec@1 76.770 Prec@5 98.480 Error@1 23.230
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 17:50:35] [Epoch=030/040] [Need: 00:07:22] [LR=0.0010] [Best : Accuracy=76.77, Error=23.23]
  Epoch: [030][000/500]   Time 18.667 (18.667)   Data 18.624 (18.624)   Loss 0.8286 (0.8286)   Prec@1 72.000 (72.000)   Prec@5 96.000 (96.000)   [2025-10-22 17:50:54]
  Epoch: [030][100/500]   Time 0.010 (0.198)   Data 0.000 (0.185)   Loss 0.8619 (0.8417)   Prec@1 70.000 (70.545)   Prec@5 97.000 (97.634)   [2025-10-22 17:50:55]
  Epoch: [030][200/500]   Time 0.009 (0.105)   Data 0.000 (0.093)   Loss 0.7583 (0.8412)   Prec@1 72.000 (70.572)   Prec@5 100.000 (97.662)   [2025-10-22 17:50:56]
  Epoch: [030][300/500]   Time 0.013 (0.074)   Data 0.000 (0.062)   Loss 0.7925 (0.8414)   Prec@1 75.000 (70.542)   Prec@5 99.000 (97.681)   [2025-10-22 17:50:58]
  Epoch: [030][400/500]   Time 0.009 (0.058)   Data 0.000 (0.047)   Loss 0.7080 (0.8401)   Prec@1 75.000 (70.701)   Prec@5 100.000 (97.611)   [2025-10-22 17:50:59]
  **Train** Prec@1 70.744 Prec@5 97.554 Error@1 29.256
  **Test** Prec@1 76.920 Prec@5 98.590 Error@1 23.080
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 17:51:20] [Epoch=031/040] [Need: 00:06:38] [LR=0.0010] [Best : Accuracy=76.92, Error=23.08]
  Epoch: [031][000/500]   Time 17.892 (17.892)   Data 17.846 (17.846)   Loss 0.7672 (0.7672)   Prec@1 74.000 (74.000)   Prec@5 98.000 (98.000)   [2025-10-22 17:51:38]
  Epoch: [031][100/500]   Time 0.010 (0.190)   Data 0.000 (0.177)   Loss 0.8614 (0.8400)   Prec@1 73.000 (70.356)   Prec@5 98.000 (98.000)   [2025-10-22 17:51:39]
  Epoch: [031][200/500]   Time 0.017 (0.101)   Data 0.001 (0.089)   Loss 0.9111 (0.8328)   Prec@1 70.000 (70.706)   Prec@5 96.000 (97.726)   [2025-10-22 17:51:40]
  Epoch: [031][300/500]   Time 0.009 (0.071)   Data 0.000 (0.059)   Loss 0.9085 (0.8404)   Prec@1 64.000 (70.595)   Prec@5 100.000 (97.671)   [2025-10-22 17:51:41]
  Epoch: [031][400/500]   Time 0.008 (0.056)   Data 0.000 (0.045)   Loss 0.8599 (0.8403)   Prec@1 67.000 (70.741)   Prec@5 96.000 (97.599)   [2025-10-22 17:51:42]
  **Train** Prec@1 70.832 Prec@5 97.646 Error@1 29.168
  **Test** Prec@1 77.130 Prec@5 98.520 Error@1 22.870
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 17:52:02] [Epoch=032/040] [Need: 00:05:53] [LR=0.0010] [Best : Accuracy=77.13, Error=22.87]
  Epoch: [032][000/500]   Time 17.586 (17.586)   Data 17.542 (17.542)   Loss 0.7682 (0.7682)   Prec@1 74.000 (74.000)   Prec@5 99.000 (99.000)   [2025-10-22 17:52:20]
  Epoch: [032][100/500]   Time 0.009 (0.186)   Data 0.000 (0.174)   Loss 0.6563 (0.8459)   Prec@1 76.000 (70.267)   Prec@5 99.000 (97.673)   [2025-10-22 17:52:21]
  Epoch: [032][200/500]   Time 0.009 (0.098)   Data 0.000 (0.087)   Loss 0.7584 (0.8330)   Prec@1 77.000 (70.866)   Prec@5 96.000 (97.751)   [2025-10-22 17:52:22]
  Epoch: [032][300/500]   Time 0.008 (0.069)   Data 0.000 (0.058)   Loss 0.7713 (0.8314)   Prec@1 72.000 (71.106)   Prec@5 100.000 (97.698)   [2025-10-22 17:52:23]
  Epoch: [032][400/500]   Time 0.015 (0.055)   Data 0.000 (0.044)   Loss 0.7681 (0.8280)   Prec@1 73.000 (71.239)   Prec@5 98.000 (97.698)   [2025-10-22 17:52:24]
  **Train** Prec@1 71.104 Prec@5 97.702 Error@1 28.896
  **Test** Prec@1 77.440 Prec@5 98.630 Error@1 22.560
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 17:52:45] [Epoch=033/040] [Need: 00:05:09] [LR=0.0010] [Best : Accuracy=77.44, Error=22.56]
  Epoch: [033][000/500]   Time 17.453 (17.453)   Data 17.407 (17.407)   Loss 0.8722 (0.8722)   Prec@1 73.000 (73.000)   Prec@5 97.000 (97.000)   [2025-10-22 17:53:03]
  Epoch: [033][100/500]   Time 0.010 (0.185)   Data 0.000 (0.173)   Loss 0.8338 (0.8319)   Prec@1 71.000 (71.000)   Prec@5 97.000 (97.822)   [2025-10-22 17:53:04]
  Epoch: [033][200/500]   Time 0.010 (0.098)   Data 0.000 (0.087)   Loss 1.0169 (0.8401)   Prec@1 67.000 (70.791)   Prec@5 95.000 (97.721)   [2025-10-22 17:53:05]
  Epoch: [033][300/500]   Time 0.010 (0.069)   Data 0.001 (0.058)   Loss 1.0885 (0.8366)   Prec@1 59.000 (70.857)   Prec@5 96.000 (97.704)   [2025-10-22 17:53:06]
  Epoch: [033][400/500]   Time 0.011 (0.054)   Data 0.001 (0.044)   Loss 0.9265 (0.8364)   Prec@1 71.000 (70.873)   Prec@5 97.000 (97.681)   [2025-10-22 17:53:07]
  **Train** Prec@1 70.942 Prec@5 97.730 Error@1 29.058
  **Test** Prec@1 77.570 Prec@5 98.620 Error@1 22.430
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 17:53:28] [Epoch=034/040] [Need: 00:04:24] [LR=0.0010] [Best : Accuracy=77.57, Error=22.43]
  Epoch: [034][000/500]   Time 17.986 (17.986)   Data 17.943 (17.943)   Loss 0.8660 (0.8660)   Prec@1 74.000 (74.000)   Prec@5 97.000 (97.000)   [2025-10-22 17:53:46]
  Epoch: [034][100/500]   Time 0.015 (0.191)   Data 0.001 (0.178)   Loss 0.9928 (0.8253)   Prec@1 64.000 (71.366)   Prec@5 97.000 (97.693)   [2025-10-22 17:53:47]
  Epoch: [034][200/500]   Time 0.010 (0.101)   Data 0.000 (0.089)   Loss 0.8277 (0.8287)   Prec@1 70.000 (71.234)   Prec@5 98.000 (97.632)   [2025-10-22 17:53:48]
  Epoch: [034][300/500]   Time 0.009 (0.071)   Data 0.000 (0.060)   Loss 0.6472 (0.8283)   Prec@1 78.000 (71.007)   Prec@5 100.000 (97.714)   [2025-10-22 17:53:49]
  Epoch: [034][400/500]   Time 0.010 (0.056)   Data 0.000 (0.045)   Loss 0.7142 (0.8305)   Prec@1 73.000 (70.953)   Prec@5 97.000 (97.641)   [2025-10-22 17:53:50]
  **Train** Prec@1 71.056 Prec@5 97.694 Error@1 28.944
  **Test** Prec@1 77.270 Prec@5 98.570 Error@1 22.730

==>>[2025-10-22 17:54:10] [Epoch=035/040] [Need: 00:03:40] [LR=0.0010] [Best : Accuracy=77.57, Error=22.43]
  Epoch: [035][000/500]   Time 17.459 (17.459)   Data 17.414 (17.414)   Loss 0.7579 (0.7579)   Prec@1 71.000 (71.000)   Prec@5 98.000 (98.000)   [2025-10-22 17:54:28]
  Epoch: [035][100/500]   Time 0.008 (0.186)   Data 0.000 (0.173)   Loss 0.7034 (0.8168)   Prec@1 77.000 (71.337)   Prec@5 100.000 (97.634)   [2025-10-22 17:54:29]
  Epoch: [035][200/500]   Time 0.012 (0.099)   Data 0.000 (0.087)   Loss 0.7236 (0.8286)   Prec@1 71.000 (70.970)   Prec@5 100.000 (97.552)   [2025-10-22 17:54:30]
  Epoch: [035][300/500]   Time 0.013 (0.069)   Data 0.000 (0.058)   Loss 0.6999 (0.8234)   Prec@1 72.000 (70.940)   Prec@5 100.000 (97.651)   [2025-10-22 17:54:31]
  Epoch: [035][400/500]   Time 0.011 (0.055)   Data 0.001 (0.044)   Loss 0.8173 (0.8252)   Prec@1 74.000 (70.988)   Prec@5 99.000 (97.693)   [2025-10-22 17:54:32]
  **Train** Prec@1 70.988 Prec@5 97.726 Error@1 29.012
  **Test** Prec@1 76.950 Prec@5 98.550 Error@1 23.050

==>>[2025-10-22 17:54:52] [Epoch=036/040] [Need: 00:02:55] [LR=0.0010] [Best : Accuracy=77.57, Error=22.43]
  Epoch: [036][000/500]   Time 20.773 (20.773)   Data 20.715 (20.715)   Loss 0.9751 (0.9751)   Prec@1 69.000 (69.000)   Prec@5 97.000 (97.000)   [2025-10-22 17:55:12]
  Epoch: [036][100/500]   Time 0.012 (0.222)   Data 0.000 (0.206)   Loss 0.9459 (0.8466)   Prec@1 64.000 (70.911)   Prec@5 99.000 (97.386)   [2025-10-22 17:55:14]
  Epoch: [036][200/500]   Time 0.014 (0.118)   Data 0.000 (0.104)   Loss 1.0506 (0.8288)   Prec@1 68.000 (71.289)   Prec@5 96.000 (97.652)   [2025-10-22 17:55:15]
  Epoch: [036][300/500]   Time 0.014 (0.083)   Data 0.001 (0.069)   Loss 0.9031 (0.8318)   Prec@1 72.000 (71.106)   Prec@5 97.000 (97.651)   [2025-10-22 17:55:17]
  Epoch: [036][400/500]   Time 0.014 (0.066)   Data 0.000 (0.052)   Loss 0.8564 (0.8298)   Prec@1 72.000 (71.160)   Prec@5 97.000 (97.668)   [2025-10-22 17:55:18]
  **Train** Prec@1 71.156 Prec@5 97.654 Error@1 28.844
  **Test** Prec@1 77.230 Prec@5 98.510 Error@1 22.770

==>>[2025-10-22 17:55:49] [Epoch=037/040] [Need: 00:02:12] [LR=0.0010] [Best : Accuracy=77.57, Error=22.43]
  Epoch: [037][000/500]   Time 22.011 (22.011)   Data 21.895 (21.895)   Loss 0.7949 (0.7949)   Prec@1 72.000 (72.000)   Prec@5 99.000 (99.000)   [2025-10-22 17:56:11]
  Epoch: [037][100/500]   Time 0.011 (0.232)   Data 0.001 (0.217)   Loss 0.8510 (0.8185)   Prec@1 69.000 (71.604)   Prec@5 98.000 (97.822)   [2025-10-22 17:56:12]
  Epoch: [037][200/500]   Time 0.010 (0.122)   Data 0.000 (0.109)   Loss 0.7979 (0.8279)   Prec@1 71.000 (71.124)   Prec@5 99.000 (97.806)   [2025-10-22 17:56:13]
  Epoch: [037][300/500]   Time 0.010 (0.085)   Data 0.000 (0.073)   Loss 0.7944 (0.8290)   Prec@1 69.000 (71.000)   Prec@5 99.000 (97.784)   [2025-10-22 17:56:14]
  Epoch: [037][400/500]   Time 0.014 (0.066)   Data 0.001 (0.055)   Loss 0.9259 (0.8285)   Prec@1 70.000 (70.998)   Prec@5 95.000 (97.788)   [2025-10-22 17:56:15]
  **Train** Prec@1 70.964 Prec@5 97.774 Error@1 29.036
  **Test** Prec@1 77.460 Prec@5 98.560 Error@1 22.540

==>>[2025-10-22 17:56:35] [Epoch=038/040] [Need: 00:01:28] [LR=0.0010] [Best : Accuracy=77.57, Error=22.43]
  Epoch: [038][000/500]   Time 17.792 (17.792)   Data 17.746 (17.746)   Loss 0.8438 (0.8438)   Prec@1 68.000 (68.000)   Prec@5 96.000 (96.000)   [2025-10-22 17:56:53]
  Epoch: [038][100/500]   Time 0.009 (0.189)   Data 0.000 (0.176)   Loss 0.7378 (0.8238)   Prec@1 78.000 (71.347)   Prec@5 99.000 (97.832)   [2025-10-22 17:56:54]
  Epoch: [038][200/500]   Time 0.013 (0.100)   Data 0.001 (0.088)   Loss 0.8579 (0.8215)   Prec@1 72.000 (71.587)   Prec@5 100.000 (97.841)   [2025-10-22 17:56:55]
  Epoch: [038][300/500]   Time 0.011 (0.071)   Data 0.000 (0.059)   Loss 0.9736 (0.8288)   Prec@1 66.000 (71.252)   Prec@5 97.000 (97.658)   [2025-10-22 17:56:56]
  Epoch: [038][400/500]   Time 0.014 (0.055)   Data 0.000 (0.044)   Loss 0.7728 (0.8260)   Prec@1 73.000 (71.249)   Prec@5 97.000 (97.663)   [2025-10-22 17:56:57]
  **Train** Prec@1 71.254 Prec@5 97.714 Error@1 28.746
  **Test** Prec@1 77.480 Prec@5 98.530 Error@1 22.520

==>>[2025-10-22 17:57:17] [Epoch=039/040] [Need: 00:00:44] [LR=0.0010] [Best : Accuracy=77.57, Error=22.43]
  Epoch: [039][000/500]   Time 17.404 (17.404)   Data 17.359 (17.359)   Loss 0.8475 (0.8475)   Prec@1 70.000 (70.000)   Prec@5 97.000 (97.000)   [2025-10-22 17:57:35]
  Epoch: [039][100/500]   Time 0.009 (0.185)   Data 0.000 (0.172)   Loss 0.7865 (0.8296)   Prec@1 73.000 (70.861)   Prec@5 97.000 (97.663)   [2025-10-22 17:57:36]
  Epoch: [039][200/500]   Time 0.009 (0.098)   Data 0.000 (0.087)   Loss 0.8403 (0.8245)   Prec@1 72.000 (71.313)   Prec@5 97.000 (97.632)   [2025-10-22 17:57:37]
  Epoch: [039][300/500]   Time 0.011 (0.069)   Data 0.001 (0.058)   Loss 0.8394 (0.8218)   Prec@1 76.000 (71.415)   Prec@5 99.000 (97.698)   [2025-10-22 17:57:38]
  Epoch: [039][400/500]   Time 0.009 (0.055)   Data 0.000 (0.043)   Loss 0.7155 (0.8206)   Prec@1 74.000 (71.424)   Prec@5 99.000 (97.693)   [2025-10-22 17:57:39]
  **Train** Prec@1 71.358 Prec@5 97.680 Error@1 28.642
  **Test** Prec@1 77.400 Prec@5 98.650 Error@1 22.600
