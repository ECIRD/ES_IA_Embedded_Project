save path : ./save/tinyvgg_quan/randbet_0.2_0.01_10_-1
{'data_path': './dataset', 'arch': 'tinyvgg_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': True, 'clipping_coeff': 0.2, 'learning_rate': 0.01, 'manualSeed': 9265, 'save_path': './save/tinyvgg_quan/randbet_0.2_0.01_10_-1', 'enable_bfa': False, 'resume': '', 'quan_bitwidth': None, 'reset_weight': False, 'evaluate': False, 'n_iter': 30, 'model_only': False, 'random_bfa': False, 'use_cuda': True}
Random Seed: 9265
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'tinyvgg_quan'
=> network :
 TinyVGG(
  (features): Sequential(
    (0): quan_Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Dropout2d(p=0.25, inplace=False)
    (6): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace=True)
    (8): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace=True)
    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (11): Dropout2d(p=0.25, inplace=False)
  )
  (classifier): Sequential(
    (0): quan_Linear(in_features=2048, out_features=10, bias=True)
  )
)
=> do not use any checkpoint for tinyvgg_quan model

==>>[2025-10-23 16:50:40] [Epoch=000/040] [Need: 00:00:00] [LR=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/500]   Time 18.481 (18.481)   Data 17.496 (17.496)   Loss 2.3012 (2.3012)   Prec@1 12.000 (12.000)   Prec@5 52.000 (52.000)   [2025-10-23 16:50:58]
  Epoch: [000][100/500]   Time 0.049 (0.235)   Data 0.001 (0.175)   Loss 2.0619 (2.2149)   Prec@1 23.000 (17.277)   Prec@5 69.000 (62.723)   [2025-10-23 16:51:04]
  Epoch: [000][200/500]   Time 0.053 (0.145)   Data 0.000 (0.088)   Loss 1.8307 (2.0968)   Prec@1 31.000 (22.582)   Prec@5 83.000 (71.259)   [2025-10-23 16:51:09]
  Epoch: [000][300/500]   Time 0.052 (0.114)   Data 0.000 (0.059)   Loss 1.8416 (2.0096)   Prec@1 32.000 (25.940)   Prec@5 83.000 (75.714)   [2025-10-23 16:51:14]
  Epoch: [000][400/500]   Time 0.055 (0.099)   Data 0.001 (0.044)   Loss 1.8306 (1.9421)   Prec@1 36.000 (28.571)   Prec@5 82.000 (78.628)   [2025-10-23 16:51:20]
  **Train** Prec@1 30.598 Prec@5 80.600 Error@1 69.402
  **Test** Prec@1 48.270 Prec@5 92.390 Error@1 51.730
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:51:44] [Epoch=001/040] [Need: 00:41:23] [LR=0.0100] [Best : Accuracy=48.27, Error=51.73]
  Epoch: [001][000/500]   Time 17.678 (17.678)   Data 17.496 (17.496)   Loss 1.6792 (1.6792)   Prec@1 41.000 (41.000)   Prec@5 89.000 (89.000)   [2025-10-23 16:52:01]
  Epoch: [001][100/500]   Time 0.050 (0.223)   Data 0.001 (0.174)   Loss 1.6321 (1.6161)   Prec@1 40.000 (41.406)   Prec@5 90.000 (89.535)   [2025-10-23 16:52:06]
  Epoch: [001][200/500]   Time 0.056 (0.138)   Data 0.001 (0.088)   Loss 1.5279 (1.6016)   Prec@1 44.000 (41.915)   Prec@5 91.000 (90.000)   [2025-10-23 16:52:11]
  Epoch: [001][300/500]   Time 0.053 (0.110)   Data 0.001 (0.059)   Loss 1.3895 (1.5888)   Prec@1 52.000 (42.522)   Prec@5 94.000 (90.083)   [2025-10-23 16:52:17]
  Epoch: [001][400/500]   Time 0.053 (0.096)   Data 0.001 (0.044)   Loss 1.6345 (1.5744)   Prec@1 35.000 (43.002)   Prec@5 89.000 (90.287)   [2025-10-23 16:52:22]
  **Train** Prec@1 43.660 Prec@5 90.500 Error@1 56.340
  **Test** Prec@1 53.790 Prec@5 93.840 Error@1 46.210
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:52:46] [Epoch=002/040] [Need: 00:39:54] [LR=0.0100] [Best : Accuracy=53.79, Error=46.21]
  Epoch: [002][000/500]   Time 17.568 (17.568)   Data 17.386 (17.386)   Loss 1.5774 (1.5774)   Prec@1 42.000 (42.000)   Prec@5 89.000 (89.000)   [2025-10-23 16:53:04]
  Epoch: [002][100/500]   Time 0.051 (0.221)   Data 0.001 (0.173)   Loss 1.6204 (1.4776)   Prec@1 45.000 (47.614)   Prec@5 88.000 (91.752)   [2025-10-23 16:53:08]
  Epoch: [002][200/500]   Time 0.053 (0.137)   Data 0.001 (0.087)   Loss 1.5186 (1.4645)   Prec@1 46.000 (47.682)   Prec@5 92.000 (91.975)   [2025-10-23 16:53:14]
  Epoch: [002][300/500]   Time 0.053 (0.110)   Data 0.000 (0.058)   Loss 1.3968 (1.4545)   Prec@1 51.000 (47.701)   Prec@5 92.000 (92.056)   [2025-10-23 16:53:19]
  Epoch: [002][400/500]   Time 0.052 (0.096)   Data 0.001 (0.044)   Loss 1.5097 (1.4460)   Prec@1 45.000 (47.928)   Prec@5 92.000 (92.187)   [2025-10-23 16:53:24]
  **Train** Prec@1 48.214 Prec@5 92.446 Error@1 51.786
  **Test** Prec@1 58.350 Prec@5 95.700 Error@1 41.650
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:53:48] [Epoch=003/040] [Need: 00:38:40] [LR=0.0100] [Best : Accuracy=58.35, Error=41.65]
  Epoch: [003][000/500]   Time 17.526 (17.526)   Data 17.345 (17.345)   Loss 1.4456 (1.4456)   Prec@1 45.000 (45.000)   Prec@5 93.000 (93.000)   [2025-10-23 16:54:06]
  Epoch: [003][100/500]   Time 0.054 (0.222)   Data 0.001 (0.172)   Loss 1.4006 (1.3751)   Prec@1 51.000 (50.802)   Prec@5 94.000 (92.990)   [2025-10-23 16:54:11]
  Epoch: [003][200/500]   Time 0.054 (0.138)   Data 0.000 (0.087)   Loss 1.3109 (1.3708)   Prec@1 55.000 (51.139)   Prec@5 94.000 (93.070)   [2025-10-23 16:54:16]
  Epoch: [003][300/500]   Time 0.052 (0.110)   Data 0.000 (0.058)   Loss 1.2532 (1.3661)   Prec@1 53.000 (50.973)   Prec@5 94.000 (93.159)   [2025-10-23 16:54:21]
  Epoch: [003][400/500]   Time 0.057 (0.096)   Data 0.002 (0.044)   Loss 1.2908 (1.3588)   Prec@1 56.000 (51.219)   Prec@5 95.000 (93.259)   [2025-10-23 16:54:27]
  **Train** Prec@1 51.608 Prec@5 93.444 Error@1 48.392
  **Test** Prec@1 60.990 Prec@5 96.280 Error@1 39.010
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:54:52] [Epoch=004/040] [Need: 00:37:44] [LR=0.0100] [Best : Accuracy=60.99, Error=39.01]
  Epoch: [004][000/500]   Time 17.722 (17.722)   Data 17.535 (17.535)   Loss 1.2438 (1.2438)   Prec@1 50.000 (50.000)   Prec@5 92.000 (92.000)   [2025-10-23 16:55:09]
  Epoch: [004][100/500]   Time 0.051 (0.224)   Data 0.000 (0.174)   Loss 1.3243 (1.2980)   Prec@1 55.000 (53.515)   Prec@5 95.000 (93.931)   [2025-10-23 16:55:14]
  Epoch: [004][200/500]   Time 0.054 (0.139)   Data 0.000 (0.088)   Loss 1.3723 (1.2969)   Prec@1 49.000 (53.607)   Prec@5 94.000 (94.000)   [2025-10-23 16:55:19]
  Epoch: [004][300/500]   Time 0.054 (0.111)   Data 0.000 (0.059)   Loss 1.1797 (1.2909)   Prec@1 60.000 (54.050)   Prec@5 96.000 (93.993)   [2025-10-23 16:55:25]
  Epoch: [004][400/500]   Time 0.055 (0.096)   Data 0.001 (0.044)   Loss 1.3431 (1.2864)   Prec@1 54.000 (54.274)   Prec@5 93.000 (94.012)   [2025-10-23 16:55:30]
  **Train** Prec@1 54.688 Prec@5 94.138 Error@1 45.312
  **Test** Prec@1 63.240 Prec@5 96.630 Error@1 36.760
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:55:55] [Epoch=005/040] [Need: 00:36:46] [LR=0.0100] [Best : Accuracy=63.24, Error=36.76]
  Epoch: [005][000/500]   Time 17.814 (17.814)   Data 17.631 (17.631)   Loss 1.1631 (1.1631)   Prec@1 58.000 (58.000)   Prec@5 94.000 (94.000)   [2025-10-23 16:56:13]
  Epoch: [005][100/500]   Time 0.052 (0.226)   Data 0.000 (0.175)   Loss 1.2176 (1.2432)   Prec@1 55.000 (55.901)   Prec@5 97.000 (94.545)   [2025-10-23 16:56:18]
  Epoch: [005][200/500]   Time 0.054 (0.140)   Data 0.001 (0.088)   Loss 1.1581 (1.2361)   Prec@1 54.000 (55.995)   Prec@5 99.000 (94.592)   [2025-10-23 16:56:23]
  Epoch: [005][300/500]   Time 0.055 (0.112)   Data 0.001 (0.059)   Loss 1.1809 (1.2285)   Prec@1 58.000 (56.189)   Prec@5 95.000 (94.601)   [2025-10-23 16:56:29]
  Epoch: [005][400/500]   Time 0.057 (0.097)   Data 0.001 (0.045)   Loss 1.1362 (1.2208)   Prec@1 58.000 (56.441)   Prec@5 97.000 (94.686)   [2025-10-23 16:56:34]
  **Train** Prec@1 56.680 Prec@5 94.678 Error@1 43.320
  **Test** Prec@1 65.360 Prec@5 96.970 Error@1 34.640
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:56:58] [Epoch=006/040] [Need: 00:35:44] [LR=0.0100] [Best : Accuracy=65.36, Error=34.64]
  Epoch: [006][000/500]   Time 17.829 (17.829)   Data 17.648 (17.648)   Loss 1.1568 (1.1568)   Prec@1 62.000 (62.000)   Prec@5 97.000 (97.000)   [2025-10-23 16:57:16]
  Epoch: [006][100/500]   Time 0.054 (0.224)   Data 0.001 (0.175)   Loss 1.2009 (1.1889)   Prec@1 57.000 (57.743)   Prec@5 94.000 (94.891)   [2025-10-23 16:57:21]
  Epoch: [006][200/500]   Time 0.054 (0.139)   Data 0.001 (0.088)   Loss 1.2265 (1.1881)   Prec@1 53.000 (57.796)   Prec@5 98.000 (94.935)   [2025-10-23 16:57:26]
  Epoch: [006][300/500]   Time 0.053 (0.111)   Data 0.001 (0.059)   Loss 1.1215 (1.1734)   Prec@1 55.000 (58.316)   Prec@5 98.000 (95.140)   [2025-10-23 16:57:32]
  Epoch: [006][400/500]   Time 0.053 (0.097)   Data 0.001 (0.045)   Loss 1.2816 (1.1751)   Prec@1 51.000 (58.377)   Prec@5 91.000 (95.135)   [2025-10-23 16:57:37]
  **Train** Prec@1 58.654 Prec@5 95.190 Error@1 41.346
  **Test** Prec@1 67.040 Prec@5 97.080 Error@1 32.960
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:58:01] [Epoch=007/040] [Need: 00:34:40] [LR=0.0100] [Best : Accuracy=67.04, Error=32.96]
  Epoch: [007][000/500]   Time 17.644 (17.644)   Data 17.458 (17.458)   Loss 1.0711 (1.0711)   Prec@1 62.000 (62.000)   Prec@5 97.000 (97.000)   [2025-10-23 16:58:19]
  Epoch: [007][100/500]   Time 0.049 (0.223)   Data 0.001 (0.174)   Loss 1.2021 (1.1643)   Prec@1 55.000 (58.446)   Prec@5 96.000 (95.267)   [2025-10-23 16:58:24]
  Epoch: [007][200/500]   Time 0.054 (0.139)   Data 0.000 (0.088)   Loss 1.1701 (1.1439)   Prec@1 60.000 (59.483)   Prec@5 96.000 (95.264)   [2025-10-23 16:58:29]
  Epoch: [007][300/500]   Time 0.056 (0.110)   Data 0.001 (0.059)   Loss 1.2155 (1.1368)   Prec@1 58.000 (59.774)   Prec@5 95.000 (95.452)   [2025-10-23 16:58:34]
  Epoch: [007][400/500]   Time 0.056 (0.097)   Data 0.001 (0.044)   Loss 1.1989 (1.1356)   Prec@1 55.000 (59.840)   Prec@5 97.000 (95.481)   [2025-10-23 16:58:40]
  **Train** Prec@1 60.102 Prec@5 95.498 Error@1 39.898
  **Test** Prec@1 69.010 Prec@5 97.500 Error@1 30.990
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:59:04] [Epoch=008/040] [Need: 00:33:35] [LR=0.0100] [Best : Accuracy=69.01, Error=30.99]
  Epoch: [008][000/500]   Time 17.716 (17.716)   Data 17.533 (17.533)   Loss 1.0135 (1.0135)   Prec@1 63.000 (63.000)   Prec@5 98.000 (98.000)   [2025-10-23 16:59:22]
  Epoch: [008][100/500]   Time 0.051 (0.224)   Data 0.000 (0.174)   Loss 1.1405 (1.1181)   Prec@1 56.000 (59.980)   Prec@5 98.000 (95.653)   [2025-10-23 16:59:27]
  Epoch: [008][200/500]   Time 0.054 (0.139)   Data 0.000 (0.088)   Loss 1.0421 (1.1065)   Prec@1 59.000 (60.811)   Prec@5 96.000 (95.667)   [2025-10-23 16:59:32]
  Epoch: [008][300/500]   Time 0.053 (0.111)   Data 0.001 (0.059)   Loss 1.0129 (1.1043)   Prec@1 58.000 (60.897)   Prec@5 99.000 (95.714)   [2025-10-23 16:59:37]
  Epoch: [008][400/500]   Time 0.053 (0.097)   Data 0.000 (0.044)   Loss 1.0433 (1.1006)   Prec@1 62.000 (61.072)   Prec@5 98.000 (95.810)   [2025-10-23 16:59:43]
  **Train** Prec@1 60.946 Prec@5 95.784 Error@1 39.054
  **Test** Prec@1 69.330 Prec@5 97.370 Error@1 30.670
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 17:00:07] [Epoch=009/040] [Need: 00:32:34] [LR=0.0100] [Best : Accuracy=69.33, Error=30.67]
  Epoch: [009][000/500]   Time 17.854 (17.854)   Data 17.668 (17.668)   Loss 1.0652 (1.0652)   Prec@1 61.000 (61.000)   Prec@5 97.000 (97.000)   [2025-10-23 17:00:25]
  Epoch: [009][100/500]   Time 0.053 (0.225)   Data 0.001 (0.175)   Loss 1.1903 (1.0840)   Prec@1 57.000 (62.040)   Prec@5 94.000 (95.881)   [2025-10-23 17:00:30]
  Epoch: [009][200/500]   Time 0.054 (0.140)   Data 0.001 (0.088)   Loss 1.1243 (1.0800)   Prec@1 64.000 (62.075)   Prec@5 96.000 (95.940)   [2025-10-23 17:00:35]
  Epoch: [009][300/500]   Time 0.054 (0.112)   Data 0.001 (0.059)   Loss 1.2645 (1.0819)   Prec@1 59.000 (62.010)   Prec@5 92.000 (95.950)   [2025-10-23 17:00:41]
  Epoch: [009][400/500]   Time 0.052 (0.098)   Data 0.001 (0.045)   Loss 1.0590 (1.0788)   Prec@1 65.000 (62.005)   Prec@5 97.000 (95.895)   [2025-10-23 17:00:47]
  **Train** Prec@1 61.954 Prec@5 95.904 Error@1 38.046
  **Test** Prec@1 69.240 Prec@5 97.790 Error@1 30.760

==>>[2025-10-23 17:01:10] [Epoch=010/040] [Need: 00:31:30] [LR=0.0100] [Best : Accuracy=69.33, Error=30.67]
  Epoch: [010][000/500]   Time 17.650 (17.650)   Data 17.457 (17.457)   Loss 1.0346 (1.0346)   Prec@1 63.000 (63.000)   Prec@5 96.000 (96.000)   [2025-10-23 17:01:28]
  Epoch: [010][100/500]   Time 0.055 (0.224)   Data 0.001 (0.173)   Loss 1.1335 (1.0801)   Prec@1 53.000 (62.327)   Prec@5 96.000 (95.812)   [2025-10-23 17:01:33]
  Epoch: [010][200/500]   Time 0.056 (0.138)   Data 0.001 (0.088)   Loss 1.1078 (1.0712)   Prec@1 66.000 (62.224)   Prec@5 97.000 (96.030)   [2025-10-23 17:01:38]
  Epoch: [010][300/500]   Time 0.053 (0.111)   Data 0.001 (0.059)   Loss 1.1498 (1.0643)   Prec@1 57.000 (62.299)   Prec@5 95.000 (96.120)   [2025-10-23 17:01:44]
  Epoch: [010][400/500]   Time 0.054 (0.096)   Data 0.001 (0.044)   Loss 1.0370 (1.0588)   Prec@1 59.000 (62.466)   Prec@5 96.000 (96.157)   [2025-10-23 17:01:49]
  **Train** Prec@1 62.356 Prec@5 96.158 Error@1 37.644
  **Test** Prec@1 70.630 Prec@5 97.690 Error@1 29.370
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 17:02:13] [Epoch=011/040] [Need: 00:30:26] [LR=0.0100] [Best : Accuracy=70.63, Error=29.37]
  Epoch: [011][000/500]   Time 17.315 (17.315)   Data 17.129 (17.129)   Loss 0.9311 (0.9311)   Prec@1 61.000 (61.000)   Prec@5 98.000 (98.000)   [2025-10-23 17:02:30]
  Epoch: [011][100/500]   Time 0.054 (0.220)   Data 0.000 (0.170)   Loss 1.3798 (1.0436)   Prec@1 59.000 (63.376)   Prec@5 93.000 (96.228)   [2025-10-23 17:02:35]
  Epoch: [011][200/500]   Time 0.055 (0.137)   Data 0.001 (0.086)   Loss 1.2447 (1.0332)   Prec@1 57.000 (63.468)   Prec@5 96.000 (96.448)   [2025-10-23 17:02:40]
  Epoch: [011][300/500]   Time 0.053 (0.110)   Data 0.000 (0.058)   Loss 1.0962 (1.0400)   Prec@1 63.000 (63.249)   Prec@5 93.000 (96.292)   [2025-10-23 17:02:46]
  Epoch: [011][400/500]   Time 0.053 (0.096)   Data 0.001 (0.043)   Loss 0.8563 (1.0395)   Prec@1 68.000 (63.147)   Prec@5 99.000 (96.302)   [2025-10-23 17:02:51]
  **Train** Prec@1 63.308 Prec@5 96.374 Error@1 36.692
  **Test** Prec@1 71.580 Prec@5 97.780 Error@1 28.420
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 17:03:15] [Epoch=012/040] [Need: 00:29:21] [LR=0.0100] [Best : Accuracy=71.58, Error=28.42]
  Epoch: [012][000/500]   Time 17.606 (17.606)   Data 17.421 (17.421)   Loss 0.9895 (0.9895)   Prec@1 63.000 (63.000)   Prec@5 97.000 (97.000)   [2025-10-23 17:03:32]
  Epoch: [012][100/500]   Time 0.051 (0.222)   Data 0.001 (0.173)   Loss 0.9898 (1.0333)   Prec@1 64.000 (63.644)   Prec@5 99.000 (96.406)   [2025-10-23 17:03:37]
  Epoch: [012][200/500]   Time 0.051 (0.138)   Data 0.000 (0.087)   Loss 1.0436 (1.0320)   Prec@1 59.000 (63.428)   Prec@5 98.000 (96.323)   [2025-10-23 17:03:43]
  Epoch: [012][300/500]   Time 0.057 (0.110)   Data 0.001 (0.058)   Loss 0.8999 (1.0290)   Prec@1 70.000 (63.518)   Prec@5 98.000 (96.372)   [2025-10-23 17:03:48]
  Epoch: [012][400/500]   Time 0.053 (0.096)   Data 0.001 (0.044)   Loss 0.9706 (1.0271)   Prec@1 67.000 (63.599)   Prec@5 97.000 (96.511)   [2025-10-23 17:03:53]
  **Train** Prec@1 63.692 Prec@5 96.494 Error@1 36.308
  **Test** Prec@1 71.920 Prec@5 97.850 Error@1 28.080
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 17:04:18] [Epoch=013/040] [Need: 00:28:18] [LR=0.0100] [Best : Accuracy=71.92, Error=28.08]
  Epoch: [013][000/500]   Time 17.608 (17.608)   Data 17.422 (17.422)   Loss 0.9567 (0.9567)   Prec@1 66.000 (66.000)   Prec@5 97.000 (97.000)   [2025-10-23 17:04:35]
  Epoch: [013][100/500]   Time 0.050 (0.223)   Data 0.001 (0.173)   Loss 1.1186 (1.0111)   Prec@1 63.000 (64.416)   Prec@5 94.000 (96.644)   [2025-10-23 17:04:40]
  Epoch: [013][200/500]   Time 0.053 (0.140)   Data 0.000 (0.087)   Loss 1.1278 (1.0196)   Prec@1 63.000 (64.318)   Prec@5 95.000 (96.622)   [2025-10-23 17:04:46]
  Epoch: [013][300/500]   Time 0.053 (0.112)   Data 0.001 (0.058)   Loss 1.1487 (1.0211)   Prec@1 62.000 (64.163)   Prec@5 95.000 (96.475)   [2025-10-23 17:04:51]
  Epoch: [013][400/500]   Time 0.055 (0.098)   Data 0.000 (0.044)   Loss 0.9648 (1.0169)   Prec@1 66.000 (64.372)   Prec@5 96.000 (96.494)   [2025-10-23 17:04:57]
  **Train** Prec@1 64.504 Prec@5 96.516 Error@1 35.496
  **Test** Prec@1 73.330 Prec@5 97.970 Error@1 26.670
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 17:05:21] [Epoch=014/040] [Need: 00:27:15] [LR=0.0100] [Best : Accuracy=73.33, Error=26.67]
  Epoch: [014][000/500]   Time 17.794 (17.794)   Data 17.608 (17.608)   Loss 0.9100 (0.9100)   Prec@1 66.000 (66.000)   Prec@5 98.000 (98.000)   [2025-10-23 17:05:39]
  Epoch: [014][100/500]   Time 0.052 (0.227)   Data 0.000 (0.175)   Loss 0.8636 (1.0083)   Prec@1 69.000 (64.554)   Prec@5 97.000 (96.475)   [2025-10-23 17:05:44]
  Epoch: [014][200/500]   Time 0.055 (0.141)   Data 0.001 (0.088)   Loss 1.0100 (0.9970)   Prec@1 66.000 (64.910)   Prec@5 96.000 (96.532)   [2025-10-23 17:05:49]
  Epoch: [014][300/500]   Time 0.053 (0.112)   Data 0.000 (0.059)   Loss 0.9524 (0.9973)   Prec@1 63.000 (64.887)   Prec@5 97.000 (96.578)   [2025-10-23 17:05:55]
  Epoch: [014][400/500]   Time 0.054 (0.098)   Data 0.001 (0.045)   Loss 1.0103 (1.0019)   Prec@1 67.000 (64.838)   Prec@5 96.000 (96.536)   [2025-10-23 17:06:00]
  **Train** Prec@1 64.826 Prec@5 96.560 Error@1 35.174
  **Test** Prec@1 72.060 Prec@5 97.880 Error@1 27.940

==>>[2025-10-23 17:06:24] [Epoch=015/040] [Need: 00:26:12] [LR=0.0100] [Best : Accuracy=73.33, Error=26.67]
  Epoch: [015][000/500]   Time 17.458 (17.458)   Data 17.269 (17.269)   Loss 0.9489 (0.9489)   Prec@1 69.000 (69.000)   Prec@5 96.000 (96.000)   [2025-10-23 17:06:41]
  Epoch: [015][100/500]   Time 0.049 (0.221)   Data 0.000 (0.172)   Loss 1.0934 (1.0039)   Prec@1 56.000 (64.455)   Prec@5 96.000 (96.485)   [2025-10-23 17:06:46]
  Epoch: [015][200/500]   Time 0.055 (0.137)   Data 0.001 (0.087)   Loss 1.0883 (1.0012)   Prec@1 64.000 (64.403)   Prec@5 95.000 (96.592)   [2025-10-23 17:06:51]
  Epoch: [015][300/500]   Time 0.052 (0.109)   Data 0.000 (0.058)   Loss 0.9999 (0.9977)   Prec@1 59.000 (64.648)   Prec@5 97.000 (96.661)   [2025-10-23 17:06:57]
  Epoch: [015][400/500]   Time 0.053 (0.096)   Data 0.001 (0.044)   Loss 1.1437 (0.9972)   Prec@1 59.000 (64.643)   Prec@5 94.000 (96.658)   [2025-10-23 17:07:02]
  **Train** Prec@1 64.856 Prec@5 96.676 Error@1 35.144
  **Test** Prec@1 72.530 Prec@5 98.250 Error@1 27.470

==>>[2025-10-23 17:07:26] [Epoch=016/040] [Need: 00:25:08] [LR=0.0100] [Best : Accuracy=73.33, Error=26.67]
  Epoch: [016][000/500]   Time 17.470 (17.470)   Data 17.290 (17.290)   Loss 0.9899 (0.9899)   Prec@1 62.000 (62.000)   Prec@5 96.000 (96.000)   [2025-10-23 17:07:43]
  Epoch: [016][100/500]   Time 0.051 (0.222)   Data 0.000 (0.172)   Loss 0.8677 (0.9782)   Prec@1 70.000 (65.693)   Prec@5 100.000 (96.901)   [2025-10-23 17:07:48]
  Epoch: [016][200/500]   Time 0.055 (0.138)   Data 0.001 (0.087)   Loss 0.9170 (0.9852)   Prec@1 68.000 (65.244)   Prec@5 97.000 (96.736)   [2025-10-23 17:07:53]
  Epoch: [016][300/500]   Time 0.055 (0.110)   Data 0.001 (0.058)   Loss 1.0209 (0.9816)   Prec@1 68.000 (65.402)   Prec@5 97.000 (96.774)   [2025-10-23 17:07:59]
  Epoch: [016][400/500]   Time 0.055 (0.096)   Data 0.000 (0.044)   Loss 0.9655 (0.9813)   Prec@1 64.000 (65.501)   Prec@5 98.000 (96.798)   [2025-10-23 17:08:04]
  **Train** Prec@1 65.638 Prec@5 96.812 Error@1 34.362
  **Test** Prec@1 73.500 Prec@5 97.840 Error@1 26.500
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 17:08:28] [Epoch=017/040] [Need: 00:24:05] [LR=0.0100] [Best : Accuracy=73.50, Error=26.50]
  Epoch: [017][000/500]   Time 17.578 (17.578)   Data 17.394 (17.394)   Loss 1.1902 (1.1902)   Prec@1 61.000 (61.000)   Prec@5 92.000 (92.000)   [2025-10-23 17:08:46]
  Epoch: [017][100/500]   Time 0.050 (0.222)   Data 0.000 (0.173)   Loss 0.9551 (0.9671)   Prec@1 66.000 (66.356)   Prec@5 95.000 (96.703)   [2025-10-23 17:08:51]
  Epoch: [017][200/500]   Time 0.056 (0.137)   Data 0.001 (0.087)   Loss 0.7686 (0.9655)   Prec@1 74.000 (66.388)   Prec@5 100.000 (96.662)   [2025-10-23 17:08:56]
  Epoch: [017][300/500]   Time 0.055 (0.110)   Data 0.001 (0.058)   Loss 1.2651 (0.9712)   Prec@1 55.000 (66.017)   Prec@5 94.000 (96.817)   [2025-10-23 17:09:01]
  Epoch: [017][400/500]   Time 0.053 (0.096)   Data 0.000 (0.044)   Loss 0.9672 (0.9778)   Prec@1 64.000 (65.751)   Prec@5 97.000 (96.723)   [2025-10-23 17:09:07]
  **Train** Prec@1 65.766 Prec@5 96.702 Error@1 34.234
  **Test** Prec@1 73.090 Prec@5 97.880 Error@1 26.910

==>>[2025-10-23 17:09:31] [Epoch=018/040] [Need: 00:23:01] [LR=0.0100] [Best : Accuracy=73.50, Error=26.50]
  Epoch: [018][000/500]   Time 18.563 (18.563)   Data 18.365 (18.365)   Loss 1.0107 (1.0107)   Prec@1 61.000 (61.000)   Prec@5 98.000 (98.000)   [2025-10-23 17:09:49]
  Epoch: [018][100/500]   Time 0.049 (0.233)   Data 0.000 (0.182)   Loss 1.0369 (0.9753)   Prec@1 60.000 (65.822)   Prec@5 95.000 (96.525)   [2025-10-23 17:09:54]
  Epoch: [018][200/500]   Time 0.054 (0.143)   Data 0.001 (0.092)   Loss 1.0126 (0.9792)   Prec@1 61.000 (65.716)   Prec@5 96.000 (96.711)   [2025-10-23 17:09:59]
  Epoch: [018][300/500]   Time 0.057 (0.113)   Data 0.000 (0.062)   Loss 0.8590 (0.9756)   Prec@1 70.000 (65.751)   Prec@5 96.000 (96.771)   [2025-10-23 17:10:05]
  Epoch: [018][400/500]   Time 0.055 (0.099)   Data 0.001 (0.046)   Loss 0.9142 (0.9680)   Prec@1 65.000 (66.012)   Prec@5 99.000 (96.878)   [2025-10-23 17:10:10]
  **Train** Prec@1 66.018 Prec@5 96.850 Error@1 33.982
  **Test** Prec@1 74.590 Prec@5 98.000 Error@1 25.410
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 17:10:34] [Epoch=019/040] [Need: 00:21:59] [LR=0.0100] [Best : Accuracy=74.59, Error=25.41]
  Epoch: [019][000/500]   Time 18.162 (18.162)   Data 17.981 (17.981)   Loss 0.9747 (0.9747)   Prec@1 68.000 (68.000)   Prec@5 97.000 (97.000)   [2025-10-23 17:10:52]
  Epoch: [019][100/500]   Time 0.050 (0.228)   Data 0.001 (0.179)   Loss 0.8641 (0.9520)   Prec@1 68.000 (67.277)   Prec@5 97.000 (96.792)   [2025-10-23 17:10:57]
  Epoch: [019][200/500]   Time 0.054 (0.141)   Data 0.000 (0.090)   Loss 0.8094 (0.9565)   Prec@1 70.000 (66.866)   Prec@5 99.000 (96.881)   [2025-10-23 17:11:02]
  Epoch: [019][300/500]   Time 0.053 (0.112)   Data 0.001 (0.060)   Loss 0.8937 (0.9538)   Prec@1 68.000 (66.794)   Prec@5 99.000 (96.914)   [2025-10-23 17:11:07]
  Epoch: [019][400/500]   Time 0.056 (0.098)   Data 0.001 (0.045)   Loss 1.0921 (0.9526)   Prec@1 61.000 (66.566)   Prec@5 97.000 (96.978)   [2025-10-23 17:11:13]
  **Train** Prec@1 66.440 Prec@5 96.966 Error@1 33.560
  **Test** Prec@1 73.500 Prec@5 98.250 Error@1 26.500

==>>[2025-10-23 17:11:37] [Epoch=020/040] [Need: 00:20:56] [LR=0.0100] [Best : Accuracy=74.59, Error=25.41]
  Epoch: [020][000/500]   Time 18.018 (18.018)   Data 17.836 (17.836)   Loss 0.9806 (0.9806)   Prec@1 70.000 (70.000)   Prec@5 98.000 (98.000)   [2025-10-23 17:11:55]
  Epoch: [020][100/500]   Time 0.050 (0.228)   Data 0.001 (0.177)   Loss 0.8554 (0.9642)   Prec@1 73.000 (66.337)   Prec@5 99.000 (96.683)   [2025-10-23 17:11:59]
  Epoch: [020][200/500]   Time 0.053 (0.140)   Data 0.000 (0.089)   Loss 1.0840 (0.9547)   Prec@1 69.000 (66.741)   Prec@5 94.000 (96.841)   [2025-10-23 17:12:05]
  Epoch: [020][300/500]   Time 0.056 (0.111)   Data 0.000 (0.060)   Loss 1.0348 (0.9615)   Prec@1 60.000 (66.432)   Prec@5 94.000 (96.917)   [2025-10-23 17:12:10]
  Epoch: [020][400/500]   Time 0.053 (0.097)   Data 0.001 (0.045)   Loss 0.8543 (0.9584)   Prec@1 70.000 (66.406)   Prec@5 99.000 (96.920)   [2025-10-23 17:12:16]
  **Train** Prec@1 66.686 Prec@5 96.960 Error@1 33.314
  **Test** Prec@1 72.700 Prec@5 98.050 Error@1 27.300

==>>[2025-10-23 17:12:39] [Epoch=021/040] [Need: 00:19:53] [LR=0.0100] [Best : Accuracy=74.59, Error=25.41]
  Epoch: [021][000/500]   Time 17.656 (17.656)   Data 17.474 (17.474)   Loss 1.0124 (1.0124)   Prec@1 65.000 (65.000)   Prec@5 96.000 (96.000)   [2025-10-23 17:12:57]
  Epoch: [021][100/500]   Time 0.049 (0.223)   Data 0.001 (0.174)   Loss 1.0292 (0.9659)   Prec@1 58.000 (66.584)   Prec@5 98.000 (96.604)   [2025-10-23 17:13:02]
  Epoch: [021][200/500]   Time 0.054 (0.138)   Data 0.000 (0.088)   Loss 0.7751 (0.9594)   Prec@1 73.000 (66.587)   Prec@5 98.000 (96.766)   [2025-10-23 17:13:07]
  Epoch: [021][300/500]   Time 0.053 (0.110)   Data 0.000 (0.059)   Loss 0.9609 (0.9571)   Prec@1 68.000 (66.601)   Prec@5 97.000 (96.728)   [2025-10-23 17:13:12]
  Epoch: [021][400/500]   Time 0.053 (0.096)   Data 0.001 (0.044)   Loss 1.2560 (0.9545)   Prec@1 55.000 (66.646)   Prec@5 95.000 (96.843)   [2025-10-23 17:13:18]
  **Train** Prec@1 66.696 Prec@5 96.862 Error@1 33.304
  **Test** Prec@1 74.400 Prec@5 98.230 Error@1 25.600

==>>[2025-10-23 17:13:41] [Epoch=022/040] [Need: 00:18:50] [LR=0.0100] [Best : Accuracy=74.59, Error=25.41]
  Epoch: [022][000/500]   Time 17.414 (17.414)   Data 17.231 (17.231)   Loss 0.9771 (0.9771)   Prec@1 62.000 (62.000)   Prec@5 96.000 (96.000)   [2025-10-23 17:13:59]
  Epoch: [022][100/500]   Time 0.052 (0.221)   Data 0.000 (0.171)   Loss 0.7096 (0.9360)   Prec@1 78.000 (67.515)   Prec@5 98.000 (96.911)   [2025-10-23 17:14:04]
  Epoch: [022][200/500]   Time 0.056 (0.137)   Data 0.001 (0.086)   Loss 0.8557 (0.9321)   Prec@1 69.000 (67.313)   Prec@5 96.000 (96.856)   [2025-10-23 17:14:09]
  Epoch: [022][300/500]   Time 0.052 (0.110)   Data 0.001 (0.058)   Loss 1.1391 (0.9363)   Prec@1 59.000 (67.229)   Prec@5 96.000 (96.877)   [2025-10-23 17:14:14]
  Epoch: [022][400/500]   Time 0.053 (0.096)   Data 0.001 (0.044)   Loss 0.6546 (0.9404)   Prec@1 75.000 (67.120)   Prec@5 99.000 (96.873)   [2025-10-23 17:14:20]
  **Train** Prec@1 67.230 Prec@5 96.902 Error@1 32.770
  **Test** Prec@1 74.810 Prec@5 98.110 Error@1 25.190
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 17:14:44] [Epoch=023/040] [Need: 00:17:46] [LR=0.0100] [Best : Accuracy=74.81, Error=25.19]
  Epoch: [023][000/500]   Time 17.627 (17.627)   Data 17.444 (17.444)   Loss 1.0011 (1.0011)   Prec@1 67.000 (67.000)   Prec@5 98.000 (98.000)   [2025-10-23 17:15:01]
  Epoch: [023][100/500]   Time 0.051 (0.222)   Data 0.001 (0.173)   Loss 0.9420 (0.9281)   Prec@1 66.000 (67.139)   Prec@5 98.000 (97.297)   [2025-10-23 17:15:06]
  Epoch: [023][200/500]   Time 0.056 (0.138)   Data 0.001 (0.087)   Loss 1.1216 (0.9451)   Prec@1 59.000 (66.657)   Prec@5 95.000 (97.000)   [2025-10-23 17:15:11]
  Epoch: [023][300/500]   Time 0.052 (0.110)   Data 0.001 (0.059)   Loss 1.1211 (0.9405)   Prec@1 59.000 (66.973)   Prec@5 98.000 (97.096)   [2025-10-23 17:15:17]
  Epoch: [023][400/500]   Time 0.057 (0.096)   Data 0.002 (0.044)   Loss 1.0707 (0.9404)   Prec@1 61.000 (66.898)   Prec@5 95.000 (97.075)   [2025-10-23 17:15:22]
  **Train** Prec@1 67.052 Prec@5 97.130 Error@1 32.948
  **Test** Prec@1 74.400 Prec@5 98.210 Error@1 25.600

==>>[2025-10-23 17:15:46] [Epoch=024/040] [Need: 00:16:44] [LR=0.0100] [Best : Accuracy=74.81, Error=25.19]
  Epoch: [024][000/500]   Time 17.558 (17.558)   Data 17.375 (17.375)   Loss 0.8871 (0.8871)   Prec@1 69.000 (69.000)   Prec@5 97.000 (97.000)   [2025-10-23 17:16:04]
  Epoch: [024][100/500]   Time 0.052 (0.223)   Data 0.001 (0.173)   Loss 1.0463 (0.9446)   Prec@1 60.000 (66.752)   Prec@5 97.000 (97.188)   [2025-10-23 17:16:09]
  Epoch: [024][200/500]   Time 0.051 (0.138)   Data 0.001 (0.087)   Loss 1.0611 (0.9248)   Prec@1 63.000 (67.493)   Prec@5 94.000 (97.179)   [2025-10-23 17:16:14]
  Epoch: [024][300/500]   Time 0.052 (0.110)   Data 0.000 (0.058)   Loss 0.6886 (0.9287)   Prec@1 75.000 (67.382)   Prec@5 99.000 (97.110)   [2025-10-23 17:16:19]
  Epoch: [024][400/500]   Time 0.056 (0.096)   Data 0.001 (0.044)   Loss 0.9568 (0.9321)   Prec@1 72.000 (67.322)   Prec@5 93.000 (97.095)   [2025-10-23 17:16:25]
  **Train** Prec@1 67.344 Prec@5 97.124 Error@1 32.656
  **Test** Prec@1 75.610 Prec@5 98.360 Error@1 24.390
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 17:16:48] [Epoch=025/040] [Need: 00:15:41] [LR=0.0010] [Best : Accuracy=75.61, Error=24.39]
  Epoch: [025][000/500]   Time 17.354 (17.354)   Data 17.167 (17.167)   Loss 0.8483 (0.8483)   Prec@1 74.000 (74.000)   Prec@5 97.000 (97.000)   [2025-10-23 17:17:06]
  Epoch: [025][100/500]   Time 0.052 (0.220)   Data 0.000 (0.171)   Loss 0.9483 (0.9133)   Prec@1 68.000 (68.079)   Prec@5 99.000 (97.089)   [2025-10-23 17:17:11]
  Epoch: [025][200/500]   Time 0.056 (0.137)   Data 0.001 (0.086)   Loss 0.7829 (0.9011)   Prec@1 73.000 (68.652)   Prec@5 99.000 (97.214)   [2025-10-23 17:17:16]
  Epoch: [025][300/500]   Time 0.054 (0.110)   Data 0.001 (0.058)   Loss 0.9955 (0.8839)   Prec@1 70.000 (69.249)   Prec@5 97.000 (97.352)   [2025-10-23 17:17:22]
  Epoch: [025][400/500]   Time 0.053 (0.096)   Data 0.001 (0.043)   Loss 0.7033 (0.8756)   Prec@1 78.000 (69.556)   Prec@5 98.000 (97.404)   [2025-10-23 17:17:27]
  **Train** Prec@1 69.690 Prec@5 97.460 Error@1 30.310
  **Test** Prec@1 75.970 Prec@5 98.450 Error@1 24.030
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 17:17:51] [Epoch=026/040] [Need: 00:14:38] [LR=0.0010] [Best : Accuracy=75.97, Error=24.03]
  Epoch: [026][000/500]   Time 17.361 (17.361)   Data 17.170 (17.170)   Loss 0.8621 (0.8621)   Prec@1 67.000 (67.000)   Prec@5 97.000 (97.000)   [2025-10-23 17:18:08]
  Epoch: [026][100/500]   Time 0.052 (0.219)   Data 0.000 (0.171)   Loss 0.8487 (0.8528)   Prec@1 70.000 (70.119)   Prec@5 99.000 (97.713)   [2025-10-23 17:18:13]
  Epoch: [026][200/500]   Time 0.053 (0.136)   Data 0.000 (0.086)   Loss 0.9127 (0.8600)   Prec@1 67.000 (69.955)   Prec@5 95.000 (97.577)   [2025-10-23 17:18:18]
  Epoch: [026][300/500]   Time 0.053 (0.109)   Data 0.001 (0.058)   Loss 0.8398 (0.8568)   Prec@1 75.000 (70.083)   Prec@5 99.000 (97.635)   [2025-10-23 17:18:24]
  Epoch: [026][400/500]   Time 0.054 (0.095)   Data 0.001 (0.043)   Loss 0.8239 (0.8541)   Prec@1 72.000 (70.212)   Prec@5 99.000 (97.591)   [2025-10-23 17:18:29]
  **Train** Prec@1 70.130 Prec@5 97.538 Error@1 29.870
  **Test** Prec@1 76.530 Prec@5 98.360 Error@1 23.470
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 17:18:53] [Epoch=027/040] [Need: 00:13:34] [LR=0.0010] [Best : Accuracy=76.53, Error=23.47]
  Epoch: [027][000/500]   Time 17.818 (17.818)   Data 17.615 (17.615)   Loss 0.8829 (0.8829)   Prec@1 66.000 (66.000)   Prec@5 97.000 (97.000)   [2025-10-23 17:19:10]
  Epoch: [027][100/500]   Time 0.051 (0.224)   Data 0.000 (0.175)   Loss 0.7795 (0.8733)   Prec@1 74.000 (69.317)   Prec@5 99.000 (97.535)   [2025-10-23 17:19:15]
  Epoch: [027][200/500]   Time 0.053 (0.139)   Data 0.001 (0.088)   Loss 0.9394 (0.8513)   Prec@1 69.000 (70.075)   Prec@5 97.000 (97.706)   [2025-10-23 17:19:21]
  Epoch: [027][300/500]   Time 0.054 (0.110)   Data 0.000 (0.059)   Loss 0.8243 (0.8485)   Prec@1 71.000 (70.169)   Prec@5 96.000 (97.754)   [2025-10-23 17:19:26]
  Epoch: [027][400/500]   Time 0.053 (0.096)   Data 0.000 (0.045)   Loss 0.9394 (0.8472)   Prec@1 72.000 (70.299)   Prec@5 98.000 (97.743)   [2025-10-23 17:19:31]
  **Train** Prec@1 70.372 Prec@5 97.736 Error@1 29.628
  **Test** Prec@1 76.240 Prec@5 98.370 Error@1 23.760

==>>[2025-10-23 17:19:56] [Epoch=028/040] [Need: 00:12:32] [LR=0.0010] [Best : Accuracy=76.53, Error=23.47]
  Epoch: [028][000/500]   Time 17.755 (17.755)   Data 17.572 (17.572)   Loss 0.7324 (0.7324)   Prec@1 73.000 (73.000)   Prec@5 99.000 (99.000)   [2025-10-23 17:20:14]
  Epoch: [028][100/500]   Time 0.052 (0.224)   Data 0.001 (0.174)   Loss 0.7775 (0.8269)   Prec@1 73.000 (71.525)   Prec@5 99.000 (97.663)   [2025-10-23 17:20:19]
  Epoch: [028][200/500]   Time 0.053 (0.139)   Data 0.001 (0.088)   Loss 0.7114 (0.8330)   Prec@1 75.000 (71.080)   Prec@5 100.000 (97.657)   [2025-10-23 17:20:24]
  Epoch: [028][300/500]   Time 0.054 (0.111)   Data 0.001 (0.059)   Loss 0.8226 (0.8362)   Prec@1 70.000 (70.834)   Prec@5 99.000 (97.701)   [2025-10-23 17:20:29]
  Epoch: [028][400/500]   Time 0.056 (0.097)   Data 0.001 (0.044)   Loss 0.8308 (0.8391)   Prec@1 69.000 (70.746)   Prec@5 99.000 (97.641)   [2025-10-23 17:20:35]
  **Train** Prec@1 70.698 Prec@5 97.628 Error@1 29.302
  **Test** Prec@1 77.260 Prec@5 98.450 Error@1 22.740
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 17:20:59] [Epoch=029/040] [Need: 00:11:29] [LR=0.0010] [Best : Accuracy=77.26, Error=22.74]
  Epoch: [029][000/500]   Time 17.504 (17.504)   Data 17.318 (17.318)   Loss 0.8511 (0.8511)   Prec@1 69.000 (69.000)   Prec@5 99.000 (99.000)   [2025-10-23 17:21:16]
  Epoch: [029][100/500]   Time 0.051 (0.222)   Data 0.000 (0.172)   Loss 0.7777 (0.8461)   Prec@1 67.000 (70.228)   Prec@5 99.000 (97.485)   [2025-10-23 17:21:21]
  Epoch: [029][200/500]   Time 0.053 (0.138)   Data 0.000 (0.087)   Loss 0.8133 (0.8448)   Prec@1 72.000 (70.025)   Prec@5 100.000 (97.662)   [2025-10-23 17:21:27]
  Epoch: [029][300/500]   Time 0.057 (0.110)   Data 0.001 (0.058)   Loss 0.7873 (0.8450)   Prec@1 75.000 (70.299)   Prec@5 98.000 (97.625)   [2025-10-23 17:21:32]
  Epoch: [029][400/500]   Time 0.053 (0.096)   Data 0.001 (0.044)   Loss 0.8685 (0.8456)   Prec@1 66.000 (70.297)   Prec@5 97.000 (97.611)   [2025-10-23 17:21:37]
  **Train** Prec@1 70.464 Prec@5 97.606 Error@1 29.536
  **Test** Prec@1 76.740 Prec@5 98.380 Error@1 23.260

==>>[2025-10-23 17:22:01] [Epoch=030/040] [Need: 00:10:26] [LR=0.0010] [Best : Accuracy=77.26, Error=22.74]
  Epoch: [030][000/500]   Time 17.503 (17.503)   Data 17.318 (17.318)   Loss 0.8797 (0.8797)   Prec@1 65.000 (65.000)   Prec@5 96.000 (96.000)   [2025-10-23 17:22:18]
  Epoch: [030][100/500]   Time 0.052 (0.222)   Data 0.002 (0.172)   Loss 0.8514 (0.8449)   Prec@1 77.000 (70.495)   Prec@5 97.000 (97.564)   [2025-10-23 17:22:23]
  Epoch: [030][200/500]   Time 0.053 (0.138)   Data 0.000 (0.087)   Loss 0.9300 (0.8455)   Prec@1 67.000 (70.154)   Prec@5 99.000 (97.692)   [2025-10-23 17:22:29]
  Epoch: [030][300/500]   Time 0.057 (0.110)   Data 0.000 (0.058)   Loss 0.7002 (0.8417)   Prec@1 75.000 (70.249)   Prec@5 99.000 (97.754)   [2025-10-23 17:22:34]
  Epoch: [030][400/500]   Time 0.054 (0.096)   Data 0.001 (0.044)   Loss 0.7393 (0.8363)   Prec@1 69.000 (70.534)   Prec@5 100.000 (97.743)   [2025-10-23 17:22:39]
  **Train** Prec@1 70.504 Prec@5 97.742 Error@1 29.496
  **Test** Prec@1 77.040 Prec@5 98.410 Error@1 22.960

==>>[2025-10-23 17:23:04] [Epoch=031/040] [Need: 00:09:24] [LR=0.0010] [Best : Accuracy=77.26, Error=22.74]
  Epoch: [031][000/500]   Time 17.405 (17.405)   Data 17.218 (17.218)   Loss 0.8197 (0.8197)   Prec@1 71.000 (71.000)   Prec@5 93.000 (93.000)   [2025-10-23 17:23:21]
  Epoch: [031][100/500]   Time 0.052 (0.221)   Data 0.001 (0.171)   Loss 0.8133 (0.8575)   Prec@1 70.000 (70.277)   Prec@5 98.000 (97.426)   [2025-10-23 17:23:26]
  Epoch: [031][200/500]   Time 0.052 (0.137)   Data 0.001 (0.086)   Loss 0.7191 (0.8475)   Prec@1 73.000 (70.517)   Prec@5 100.000 (97.572)   [2025-10-23 17:23:31]
  Epoch: [031][300/500]   Time 0.053 (0.110)   Data 0.000 (0.058)   Loss 0.9858 (0.8445)   Prec@1 66.000 (70.488)   Prec@5 95.000 (97.598)   [2025-10-23 17:23:36]
  Epoch: [031][400/500]   Time 0.054 (0.095)   Data 0.002 (0.044)   Loss 0.7510 (0.8385)   Prec@1 77.000 (70.713)   Prec@5 99.000 (97.641)   [2025-10-23 17:23:42]
  **Train** Prec@1 70.822 Prec@5 97.702 Error@1 29.178
  **Test** Prec@1 76.870 Prec@5 98.410 Error@1 23.130

==>>[2025-10-23 17:24:06] [Epoch=032/040] [Need: 00:08:21] [LR=0.0010] [Best : Accuracy=77.26, Error=22.74]
  Epoch: [032][000/500]   Time 17.760 (17.760)   Data 17.577 (17.577)   Loss 0.9239 (0.9239)   Prec@1 68.000 (68.000)   Prec@5 97.000 (97.000)   [2025-10-23 17:24:24]
  Epoch: [032][100/500]   Time 0.052 (0.224)   Data 0.001 (0.175)   Loss 0.8240 (0.8308)   Prec@1 66.000 (70.772)   Prec@5 99.000 (97.921)   [2025-10-23 17:24:29]
  Epoch: [032][200/500]   Time 0.053 (0.139)   Data 0.000 (0.088)   Loss 0.7109 (0.8391)   Prec@1 77.000 (70.627)   Prec@5 99.000 (97.711)   [2025-10-23 17:24:34]
  Epoch: [032][300/500]   Time 0.055 (0.111)   Data 0.001 (0.059)   Loss 0.9710 (0.8333)   Prec@1 64.000 (71.020)   Prec@5 97.000 (97.734)   [2025-10-23 17:24:40]
  Epoch: [032][400/500]   Time 0.052 (0.096)   Data 0.001 (0.044)   Loss 0.9790 (0.8331)   Prec@1 62.000 (70.955)   Prec@5 95.000 (97.728)   [2025-10-23 17:24:45]
  **Train** Prec@1 70.928 Prec@5 97.740 Error@1 29.072
  **Test** Prec@1 77.080 Prec@5 98.470 Error@1 22.920

==>>[2025-10-23 17:25:11] [Epoch=033/040] [Need: 00:07:19] [LR=0.0010] [Best : Accuracy=77.26, Error=22.74]
  Epoch: [033][000/500]   Time 19.305 (19.305)   Data 19.115 (19.115)   Loss 0.8062 (0.8062)   Prec@1 74.000 (74.000)   Prec@5 98.000 (98.000)   [2025-10-23 17:25:30]
  Epoch: [033][100/500]   Time 0.050 (0.240)   Data 0.000 (0.190)   Loss 0.7123 (0.8322)   Prec@1 77.000 (71.386)   Prec@5 98.000 (97.564)   [2025-10-23 17:25:35]
  Epoch: [033][200/500]   Time 0.056 (0.147)   Data 0.001 (0.096)   Loss 0.8344 (0.8380)   Prec@1 70.000 (70.841)   Prec@5 97.000 (97.612)   [2025-10-23 17:25:40]
  Epoch: [033][300/500]   Time 0.055 (0.116)   Data 0.001 (0.064)   Loss 0.9299 (0.8314)   Prec@1 70.000 (70.963)   Prec@5 100.000 (97.684)   [2025-10-23 17:25:46]
  Epoch: [033][400/500]   Time 0.056 (0.101)   Data 0.001 (0.048)   Loss 0.8301 (0.8313)   Prec@1 71.000 (70.948)   Prec@5 97.000 (97.698)   [2025-10-23 17:25:51]
  **Train** Prec@1 70.984 Prec@5 97.704 Error@1 29.016
  **Test** Prec@1 77.030 Prec@5 98.360 Error@1 22.970

==>>[2025-10-23 17:26:15] [Epoch=034/040] [Need: 00:06:16] [LR=0.0010] [Best : Accuracy=77.26, Error=22.74]
  Epoch: [034][000/500]   Time 17.381 (17.381)   Data 17.195 (17.195)   Loss 0.6956 (0.6956)   Prec@1 70.000 (70.000)   Prec@5 100.000 (100.000)   [2025-10-23 17:26:32]
  Epoch: [034][100/500]   Time 0.050 (0.220)   Data 0.000 (0.171)   Loss 0.9347 (0.8313)   Prec@1 69.000 (70.752)   Prec@5 95.000 (97.505)   [2025-10-23 17:26:37]
  Epoch: [034][200/500]   Time 0.056 (0.137)   Data 0.001 (0.086)   Loss 0.8133 (0.8257)   Prec@1 76.000 (71.030)   Prec@5 97.000 (97.662)   [2025-10-23 17:26:42]
  Epoch: [034][300/500]   Time 0.054 (0.109)   Data 0.000 (0.058)   Loss 0.9652 (0.8267)   Prec@1 64.000 (71.100)   Prec@5 97.000 (97.648)   [2025-10-23 17:26:48]
  Epoch: [034][400/500]   Time 0.059 (0.095)   Data 0.001 (0.043)   Loss 0.8968 (0.8271)   Prec@1 62.000 (70.938)   Prec@5 98.000 (97.726)   [2025-10-23 17:26:53]
  **Train** Prec@1 70.708 Prec@5 97.712 Error@1 29.292
  **Test** Prec@1 77.070 Prec@5 98.470 Error@1 22.930

==>>[2025-10-23 17:27:17] [Epoch=035/040] [Need: 00:05:13] [LR=0.0010] [Best : Accuracy=77.26, Error=22.74]
  Epoch: [035][000/500]   Time 17.411 (17.411)   Data 17.224 (17.224)   Loss 0.8313 (0.8313)   Prec@1 72.000 (72.000)   Prec@5 95.000 (95.000)   [2025-10-23 17:27:34]
  Epoch: [035][100/500]   Time 0.052 (0.221)   Data 0.000 (0.171)   Loss 0.9206 (0.8359)   Prec@1 69.000 (70.663)   Prec@5 95.000 (97.683)   [2025-10-23 17:27:39]
  Epoch: [035][200/500]   Time 0.052 (0.137)   Data 0.001 (0.086)   Loss 0.6759 (0.8270)   Prec@1 79.000 (70.920)   Prec@5 98.000 (97.741)   [2025-10-23 17:27:44]
  Epoch: [035][300/500]   Time 0.052 (0.109)   Data 0.000 (0.058)   Loss 1.0157 (0.8271)   Prec@1 62.000 (70.970)   Prec@5 98.000 (97.751)   [2025-10-23 17:27:50]
  Epoch: [035][400/500]   Time 0.054 (0.096)   Data 0.001 (0.044)   Loss 0.8783 (0.8287)   Prec@1 73.000 (71.095)   Prec@5 97.000 (97.731)   [2025-10-23 17:27:55]
  **Train** Prec@1 71.198 Prec@5 97.806 Error@1 28.802
  **Test** Prec@1 77.370 Prec@5 98.470 Error@1 22.630
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 17:28:19] [Epoch=036/040] [Need: 00:04:11] [LR=0.0010] [Best : Accuracy=77.37, Error=22.63]
  Epoch: [036][000/500]   Time 17.469 (17.469)   Data 17.278 (17.278)   Loss 0.6981 (0.6981)   Prec@1 79.000 (79.000)   Prec@5 99.000 (99.000)   [2025-10-23 17:28:36]
  Epoch: [036][100/500]   Time 0.052 (0.222)   Data 0.001 (0.172)   Loss 1.0001 (0.8247)   Prec@1 67.000 (71.495)   Prec@5 98.000 (97.762)   [2025-10-23 17:28:41]
  Epoch: [036][200/500]   Time 0.053 (0.138)   Data 0.001 (0.087)   Loss 0.7947 (0.8282)   Prec@1 73.000 (71.289)   Prec@5 96.000 (97.652)   [2025-10-23 17:28:47]
  Epoch: [036][300/500]   Time 0.054 (0.110)   Data 0.001 (0.058)   Loss 0.8412 (0.8303)   Prec@1 67.000 (70.983)   Prec@5 98.000 (97.711)   [2025-10-23 17:28:52]
  Epoch: [036][400/500]   Time 0.051 (0.096)   Data 0.000 (0.044)   Loss 0.7159 (0.8256)   Prec@1 80.000 (71.105)   Prec@5 98.000 (97.736)   [2025-10-23 17:28:57]
  **Train** Prec@1 71.102 Prec@5 97.732 Error@1 28.898
  **Test** Prec@1 77.380 Prec@5 98.490 Error@1 22.620
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 17:29:22] [Epoch=037/040] [Need: 00:03:08] [LR=0.0010] [Best : Accuracy=77.38, Error=22.62]
  Epoch: [037][000/500]   Time 17.411 (17.411)   Data 17.222 (17.222)   Loss 0.7683 (0.7683)   Prec@1 72.000 (72.000)   Prec@5 99.000 (99.000)   [2025-10-23 17:29:39]
  Epoch: [037][100/500]   Time 0.051 (0.223)   Data 0.001 (0.171)   Loss 0.8918 (0.8178)   Prec@1 71.000 (71.317)   Prec@5 99.000 (98.129)   [2025-10-23 17:29:44]
  Epoch: [037][200/500]   Time 0.053 (0.139)   Data 0.001 (0.086)   Loss 0.8810 (0.8211)   Prec@1 68.000 (71.343)   Prec@5 98.000 (97.821)   [2025-10-23 17:29:50]
  Epoch: [037][300/500]   Time 0.055 (0.111)   Data 0.001 (0.058)   Loss 0.7414 (0.8269)   Prec@1 74.000 (71.282)   Prec@5 99.000 (97.761)   [2025-10-23 17:29:55]
  Epoch: [037][400/500]   Time 0.055 (0.097)   Data 0.000 (0.044)   Loss 0.8706 (0.8302)   Prec@1 71.000 (71.207)   Prec@5 97.000 (97.766)   [2025-10-23 17:30:00]
  **Train** Prec@1 71.260 Prec@5 97.756 Error@1 28.740
  **Test** Prec@1 76.970 Prec@5 98.410 Error@1 23.030

==>>[2025-10-23 17:30:24] [Epoch=038/040] [Need: 00:02:05] [LR=0.0010] [Best : Accuracy=77.38, Error=22.62]
  Epoch: [038][000/500]   Time 17.849 (17.849)   Data 17.662 (17.662)   Loss 0.9112 (0.9112)   Prec@1 68.000 (68.000)   Prec@5 97.000 (97.000)   [2025-10-23 17:30:42]
  Epoch: [038][100/500]   Time 0.053 (0.225)   Data 0.001 (0.175)   Loss 0.9348 (0.8191)   Prec@1 67.000 (71.485)   Prec@5 98.000 (97.703)   [2025-10-23 17:30:47]
  Epoch: [038][200/500]   Time 0.052 (0.140)   Data 0.001 (0.088)   Loss 0.7665 (0.8188)   Prec@1 71.000 (71.224)   Prec@5 97.000 (97.786)   [2025-10-23 17:30:52]
  Epoch: [038][300/500]   Time 0.053 (0.111)   Data 0.001 (0.059)   Loss 0.6678 (0.8194)   Prec@1 78.000 (71.189)   Prec@5 98.000 (97.867)   [2025-10-23 17:30:58]
  Epoch: [038][400/500]   Time 0.055 (0.097)   Data 0.000 (0.045)   Loss 0.9169 (0.8194)   Prec@1 67.000 (71.219)   Prec@5 98.000 (97.900)   [2025-10-23 17:31:03]
  **Train** Prec@1 71.240 Prec@5 97.886 Error@1 28.760
  **Test** Prec@1 77.570 Prec@5 98.500 Error@1 22.430
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 17:31:27] [Epoch=039/040] [Need: 00:01:02] [LR=0.0010] [Best : Accuracy=77.57, Error=22.43]
  Epoch: [039][000/500]   Time 17.490 (17.490)   Data 17.302 (17.302)   Loss 0.8094 (0.8094)   Prec@1 71.000 (71.000)   Prec@5 98.000 (98.000)   [2025-10-23 17:31:44]
  Epoch: [039][100/500]   Time 0.051 (0.222)   Data 0.000 (0.172)   Loss 0.9148 (0.8135)   Prec@1 69.000 (71.673)   Prec@5 98.000 (97.614)   [2025-10-23 17:31:49]
  Epoch: [039][200/500]   Time 0.052 (0.138)   Data 0.001 (0.087)   Loss 1.0030 (0.8198)   Prec@1 67.000 (71.522)   Prec@5 97.000 (97.632)   [2025-10-23 17:31:54]
  Epoch: [039][300/500]   Time 0.052 (0.110)   Data 0.000 (0.058)   Loss 0.7797 (0.8127)   Prec@1 76.000 (71.748)   Prec@5 99.000 (97.691)   [2025-10-23 17:32:00]
  Epoch: [039][400/500]   Time 0.053 (0.096)   Data 0.001 (0.044)   Loss 0.8438 (0.8207)   Prec@1 72.000 (71.529)   Prec@5 99.000 (97.741)   [2025-10-23 17:32:05]
  **Train** Prec@1 71.540 Prec@5 97.708 Error@1 28.460
  **Test** Prec@1 76.910 Prec@5 98.400 Error@1 23.090
