save path : ./save/tinyvgg_quan/randbet_0.2_0.01_10_-1
{'data_path': './dataset', 'arch': 'tinyvgg_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': True, 'clipping_coeff': 0.2, 'learning_rate': 0.01, 'manualSeed': 7082, 'save_path': './save/tinyvgg_quan/randbet_0.2_0.01_10_-1', 'enable_bfa': False, 'resume': '', 'quan_bitwidth': None, 'reset_weight': False, 'evaluate': False, 'n_iter': 30, 'model_only': False, 'random_bfa': False, 'use_cuda': True}
Random Seed: 7082
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'tinyvgg_quan'
=> network :
 TinyVGG(
  (features): Sequential(
    (0): quan_Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Dropout2d(p=0.25, inplace=False)
    (6): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace=True)
    (8): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace=True)
    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (11): Dropout2d(p=0.25, inplace=False)
  )
  (classifier): Sequential(
    (0): quan_Linear(in_features=2048, out_features=10, bias=True)
  )
)
=> do not use any checkpoint for tinyvgg_quan model

==>>[2025-10-23 17:32:36] [Epoch=000/040] [Need: 00:00:00] [LR=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/500]   Time 17.724 (17.724)   Data 17.335 (17.335)   Loss 2.3023 (2.3023)   Prec@1 10.000 (10.000)   Prec@5 50.000 (50.000)   [2025-10-23 17:32:54]
  Epoch: [000][100/500]   Time 0.053 (0.226)   Data 0.001 (0.173)   Loss 2.0325 (2.2142)   Prec@1 25.000 (16.624)   Prec@5 74.000 (62.772)   [2025-10-23 17:32:59]
  Epoch: [000][200/500]   Time 0.052 (0.140)   Data 0.001 (0.087)   Loss 1.7660 (2.0798)   Prec@1 38.000 (22.896)   Prec@5 86.000 (71.781)   [2025-10-23 17:33:04]
  Epoch: [000][300/500]   Time 0.053 (0.112)   Data 0.000 (0.058)   Loss 1.7761 (1.9898)   Prec@1 34.000 (26.661)   Prec@5 90.000 (76.402)   [2025-10-23 17:33:09]
  Epoch: [000][400/500]   Time 0.053 (0.098)   Data 0.000 (0.044)   Loss 1.6977 (1.9219)   Prec@1 37.000 (29.504)   Prec@5 92.000 (79.292)   [2025-10-23 17:33:15]
  **Train** Prec@1 31.422 Prec@5 81.106 Error@1 68.578
  **Test** Prec@1 46.600 Prec@5 92.380 Error@1 53.400
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 17:33:39] [Epoch=001/040] [Need: 00:40:53] [LR=0.0100] [Best : Accuracy=46.60, Error=53.40]
  Epoch: [001][000/500]   Time 17.571 (17.571)   Data 17.387 (17.387)   Loss 1.5798 (1.5798)   Prec@1 44.000 (44.000)   Prec@5 89.000 (89.000)   [2025-10-23 17:33:56]
  Epoch: [001][100/500]   Time 0.052 (0.223)   Data 0.001 (0.173)   Loss 1.5845 (1.6159)   Prec@1 40.000 (40.950)   Prec@5 92.000 (89.386)   [2025-10-23 17:34:01]
  Epoch: [001][200/500]   Time 0.058 (0.139)   Data 0.000 (0.087)   Loss 1.5729 (1.6067)   Prec@1 41.000 (41.582)   Prec@5 92.000 (89.791)   [2025-10-23 17:34:07]
  Epoch: [001][300/500]   Time 0.054 (0.111)   Data 0.000 (0.058)   Loss 1.4440 (1.5890)   Prec@1 45.000 (42.173)   Prec@5 92.000 (90.143)   [2025-10-23 17:34:12]
  Epoch: [001][400/500]   Time 0.056 (0.097)   Data 0.001 (0.044)   Loss 1.4097 (1.5714)   Prec@1 48.000 (42.880)   Prec@5 95.000 (90.421)   [2025-10-23 17:34:18]
  **Train** Prec@1 43.372 Prec@5 90.572 Error@1 56.628
  **Test** Prec@1 53.360 Prec@5 94.600 Error@1 46.640
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 17:34:41] [Epoch=002/040] [Need: 00:39:44] [LR=0.0100] [Best : Accuracy=53.36, Error=46.64]
  Epoch: [002][000/500]   Time 18.324 (18.324)   Data 18.137 (18.137)   Loss 1.3128 (1.3128)   Prec@1 59.000 (59.000)   Prec@5 91.000 (91.000)   [2025-10-23 17:35:00]
  Epoch: [002][100/500]   Time 0.051 (0.231)   Data 0.001 (0.180)   Loss 1.4058 (1.4994)   Prec@1 52.000 (45.762)   Prec@5 98.000 (91.723)   [2025-10-23 17:35:05]
  Epoch: [002][200/500]   Time 0.056 (0.143)   Data 0.001 (0.091)   Loss 1.5204 (1.4675)   Prec@1 44.000 (46.587)   Prec@5 93.000 (91.920)   [2025-10-23 17:35:10]
  Epoch: [002][300/500]   Time 0.054 (0.113)   Data 0.000 (0.061)   Loss 1.4067 (1.4616)   Prec@1 43.000 (47.010)   Prec@5 93.000 (92.033)   [2025-10-23 17:35:16]
  Epoch: [002][400/500]   Time 0.055 (0.099)   Data 0.001 (0.046)   Loss 1.2595 (1.4499)   Prec@1 57.000 (47.511)   Prec@5 95.000 (92.229)   [2025-10-23 17:35:21]
  **Train** Prec@1 47.896 Prec@5 92.302 Error@1 52.104
  **Test** Prec@1 57.750 Prec@5 95.130 Error@1 42.250
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 17:35:46] [Epoch=003/040] [Need: 00:38:59] [LR=0.0100] [Best : Accuracy=57.75, Error=42.25]
  Epoch: [003][000/500]   Time 17.439 (17.439)   Data 17.253 (17.253)   Loss 1.4512 (1.4512)   Prec@1 49.000 (49.000)   Prec@5 93.000 (93.000)   [2025-10-23 17:36:03]
  Epoch: [003][100/500]   Time 0.054 (0.221)   Data 0.001 (0.171)   Loss 1.3395 (1.3929)   Prec@1 46.000 (49.901)   Prec@5 95.000 (92.911)   [2025-10-23 17:36:08]
  Epoch: [003][200/500]   Time 0.053 (0.137)   Data 0.000 (0.086)   Loss 1.4180 (1.3897)   Prec@1 54.000 (50.294)   Prec@5 95.000 (92.711)   [2025-10-23 17:36:13]
  Epoch: [003][300/500]   Time 0.054 (0.110)   Data 0.001 (0.058)   Loss 1.3401 (1.3687)   Prec@1 56.000 (50.767)   Prec@5 91.000 (93.196)   [2025-10-23 17:36:19]
  Epoch: [003][400/500]   Time 0.057 (0.096)   Data 0.001 (0.044)   Loss 1.2040 (1.3621)   Prec@1 59.000 (50.963)   Prec@5 92.000 (93.242)   [2025-10-23 17:36:24]
  **Train** Prec@1 51.094 Prec@5 93.278 Error@1 48.906
  **Test** Prec@1 61.500 Prec@5 96.050 Error@1 38.500
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 17:36:48] [Epoch=004/040] [Need: 00:37:45] [LR=0.0100] [Best : Accuracy=61.50, Error=38.50]
  Epoch: [004][000/500]   Time 17.362 (17.362)   Data 17.171 (17.171)   Loss 1.2785 (1.2785)   Prec@1 54.000 (54.000)   Prec@5 95.000 (95.000)   [2025-10-23 17:37:05]
  Epoch: [004][100/500]   Time 0.051 (0.220)   Data 0.001 (0.171)   Loss 1.3751 (1.3012)   Prec@1 49.000 (52.861)   Prec@5 95.000 (94.149)   [2025-10-23 17:37:10]
  Epoch: [004][200/500]   Time 0.053 (0.137)   Data 0.000 (0.086)   Loss 1.1406 (1.2959)   Prec@1 59.000 (53.104)   Prec@5 94.000 (94.015)   [2025-10-23 17:37:15]
  Epoch: [004][300/500]   Time 0.054 (0.110)   Data 0.001 (0.058)   Loss 1.4005 (1.2911)   Prec@1 48.000 (53.389)   Prec@5 93.000 (93.953)   [2025-10-23 17:37:21]
  Epoch: [004][400/500]   Time 0.055 (0.096)   Data 0.001 (0.043)   Loss 1.2400 (1.2910)   Prec@1 59.000 (53.618)   Prec@5 95.000 (93.995)   [2025-10-23 17:37:26]
  **Train** Prec@1 53.800 Prec@5 94.056 Error@1 46.200
  **Test** Prec@1 62.480 Prec@5 96.300 Error@1 37.520
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 17:37:50] [Epoch=005/040] [Need: 00:36:37] [LR=0.0100] [Best : Accuracy=62.48, Error=37.52]
  Epoch: [005][000/500]   Time 17.411 (17.411)   Data 17.227 (17.227)   Loss 1.2346 (1.2346)   Prec@1 52.000 (52.000)   Prec@5 93.000 (93.000)   [2025-10-23 17:38:07]
  Epoch: [005][100/500]   Time 0.053 (0.220)   Data 0.001 (0.171)   Loss 1.2567 (1.2610)   Prec@1 59.000 (55.465)   Prec@5 94.000 (93.891)   [2025-10-23 17:38:12]
  Epoch: [005][200/500]   Time 0.057 (0.137)   Data 0.001 (0.086)   Loss 1.1437 (1.2503)   Prec@1 60.000 (55.582)   Prec@5 93.000 (94.279)   [2025-10-23 17:38:17]
  Epoch: [005][300/500]   Time 0.056 (0.110)   Data 0.001 (0.058)   Loss 1.2754 (1.2483)   Prec@1 50.000 (55.777)   Prec@5 94.000 (94.405)   [2025-10-23 17:38:23]
  Epoch: [005][400/500]   Time 0.055 (0.096)   Data 0.000 (0.044)   Loss 1.2342 (1.2390)   Prec@1 55.000 (56.022)   Prec@5 92.000 (94.596)   [2025-10-23 17:38:28]
  **Train** Prec@1 56.224 Prec@5 94.674 Error@1 43.776
  **Test** Prec@1 66.020 Prec@5 96.840 Error@1 33.980
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 17:38:52] [Epoch=006/040] [Need: 00:35:31] [LR=0.0100] [Best : Accuracy=66.02, Error=33.98]
  Epoch: [006][000/500]   Time 17.600 (17.600)   Data 17.413 (17.413)   Loss 1.3304 (1.3304)   Prec@1 52.000 (52.000)   Prec@5 95.000 (95.000)   [2025-10-23 17:39:10]
  Epoch: [006][100/500]   Time 0.053 (0.224)   Data 0.000 (0.173)   Loss 1.2348 (1.2038)   Prec@1 65.000 (57.495)   Prec@5 92.000 (94.733)   [2025-10-23 17:39:15]
  Epoch: [006][200/500]   Time 0.053 (0.139)   Data 0.000 (0.087)   Loss 1.1644 (1.1944)   Prec@1 61.000 (57.940)   Prec@5 92.000 (94.736)   [2025-10-23 17:39:20]
  Epoch: [006][300/500]   Time 0.055 (0.111)   Data 0.000 (0.058)   Loss 1.3813 (1.1910)   Prec@1 52.000 (57.817)   Prec@5 90.000 (94.784)   [2025-10-23 17:39:25]
  Epoch: [006][400/500]   Time 0.053 (0.097)   Data 0.001 (0.044)   Loss 1.1624 (1.1868)   Prec@1 55.000 (58.037)   Prec@5 99.000 (94.788)   [2025-10-23 17:39:31]
  **Train** Prec@1 58.214 Prec@5 94.934 Error@1 41.786
  **Test** Prec@1 66.700 Prec@5 97.040 Error@1 33.300
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 17:39:55] [Epoch=007/040] [Need: 00:34:32] [LR=0.0100] [Best : Accuracy=66.70, Error=33.30]
  Epoch: [007][000/500]   Time 17.690 (17.690)   Data 17.495 (17.495)   Loss 1.0766 (1.0766)   Prec@1 59.000 (59.000)   Prec@5 96.000 (96.000)   [2025-10-23 17:40:13]
  Epoch: [007][100/500]   Time 0.053 (0.224)   Data 0.001 (0.174)   Loss 1.2308 (1.1603)   Prec@1 55.000 (58.782)   Prec@5 95.000 (95.228)   [2025-10-23 17:40:18]
  Epoch: [007][200/500]   Time 0.052 (0.139)   Data 0.000 (0.088)   Loss 1.0499 (1.1582)   Prec@1 59.000 (59.119)   Prec@5 94.000 (95.204)   [2025-10-23 17:40:23]
  Epoch: [007][300/500]   Time 0.054 (0.111)   Data 0.001 (0.059)   Loss 1.1523 (1.1593)   Prec@1 61.000 (59.013)   Prec@5 97.000 (95.169)   [2025-10-23 17:40:29]
  Epoch: [007][400/500]   Time 0.053 (0.097)   Data 0.000 (0.044)   Loss 0.9107 (1.1521)   Prec@1 65.000 (59.207)   Prec@5 100.000 (95.284)   [2025-10-23 17:40:34]
  **Train** Prec@1 59.486 Prec@5 95.378 Error@1 40.514
  **Test** Prec@1 68.540 Prec@5 97.500 Error@1 31.460
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 17:40:58] [Epoch=008/040] [Need: 00:33:29] [LR=0.0100] [Best : Accuracy=68.54, Error=31.46]
  Epoch: [008][000/500]   Time 17.473 (17.473)   Data 17.287 (17.287)   Loss 1.3094 (1.3094)   Prec@1 51.000 (51.000)   Prec@5 92.000 (92.000)   [2025-10-23 17:41:16]
  Epoch: [008][100/500]   Time 0.049 (0.222)   Data 0.001 (0.172)   Loss 1.2189 (1.1203)   Prec@1 57.000 (59.891)   Prec@5 94.000 (95.802)   [2025-10-23 17:41:21]
  Epoch: [008][200/500]   Time 0.055 (0.138)   Data 0.002 (0.087)   Loss 1.0546 (1.1200)   Prec@1 59.000 (60.109)   Prec@5 97.000 (95.687)   [2025-10-23 17:41:26]
  Epoch: [008][300/500]   Time 0.054 (0.110)   Data 0.001 (0.058)   Loss 1.3584 (1.1251)   Prec@1 53.000 (60.090)   Prec@5 92.000 (95.615)   [2025-10-23 17:41:31]
  Epoch: [008][400/500]   Time 0.054 (0.096)   Data 0.000 (0.044)   Loss 0.9476 (1.1171)   Prec@1 68.000 (60.451)   Prec@5 96.000 (95.688)   [2025-10-23 17:41:37]
  **Train** Prec@1 60.628 Prec@5 95.680 Error@1 39.372
  **Test** Prec@1 69.560 Prec@5 97.320 Error@1 30.440
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 17:42:01] [Epoch=009/040] [Need: 00:32:24] [LR=0.0100] [Best : Accuracy=69.56, Error=30.44]
  Epoch: [009][000/500]   Time 17.311 (17.311)   Data 17.119 (17.119)   Loss 1.1596 (1.1596)   Prec@1 60.000 (60.000)   Prec@5 98.000 (98.000)   [2025-10-23 17:42:18]
  Epoch: [009][100/500]   Time 0.051 (0.220)   Data 0.001 (0.170)   Loss 1.2024 (1.1017)   Prec@1 61.000 (60.644)   Prec@5 95.000 (96.000)   [2025-10-23 17:42:23]
  Epoch: [009][200/500]   Time 0.055 (0.137)   Data 0.001 (0.086)   Loss 0.9851 (1.0957)   Prec@1 64.000 (60.975)   Prec@5 95.000 (95.851)   [2025-10-23 17:42:28]
  Epoch: [009][300/500]   Time 0.055 (0.109)   Data 0.001 (0.057)   Loss 1.0936 (1.0904)   Prec@1 62.000 (61.292)   Prec@5 95.000 (95.867)   [2025-10-23 17:42:33]
  Epoch: [009][400/500]   Time 0.054 (0.096)   Data 0.001 (0.043)   Loss 1.1391 (1.0926)   Prec@1 62.000 (61.389)   Prec@5 93.000 (95.791)   [2025-10-23 17:42:39]
  **Train** Prec@1 61.498 Prec@5 95.814 Error@1 38.502
  **Test** Prec@1 68.250 Prec@5 97.350 Error@1 31.750

==>>[2025-10-23 17:43:03] [Epoch=010/040] [Need: 00:31:22] [LR=0.0100] [Best : Accuracy=69.56, Error=30.44]
  Epoch: [010][000/500]   Time 17.484 (17.484)   Data 17.294 (17.294)   Loss 1.1388 (1.1388)   Prec@1 63.000 (63.000)   Prec@5 97.000 (97.000)   [2025-10-23 17:43:21]
  Epoch: [010][100/500]   Time 0.050 (0.222)   Data 0.000 (0.172)   Loss 1.2994 (1.0706)   Prec@1 57.000 (62.386)   Prec@5 93.000 (95.950)   [2025-10-23 17:43:26]
  Epoch: [010][200/500]   Time 0.056 (0.138)   Data 0.001 (0.087)   Loss 0.9473 (1.0769)   Prec@1 68.000 (61.980)   Prec@5 97.000 (95.945)   [2025-10-23 17:43:31]
  Epoch: [010][300/500]   Time 0.053 (0.110)   Data 0.000 (0.058)   Loss 0.9652 (1.0675)   Prec@1 66.000 (62.548)   Prec@5 99.000 (95.910)   [2025-10-23 17:43:37]
  Epoch: [010][400/500]   Time 0.053 (0.096)   Data 0.000 (0.044)   Loss 1.3416 (1.0724)   Prec@1 51.000 (62.362)   Prec@5 94.000 (95.893)   [2025-10-23 17:43:42]
  **Train** Prec@1 62.414 Prec@5 95.894 Error@1 37.586
  **Test** Prec@1 70.310 Prec@5 97.750 Error@1 29.690
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 17:44:06] [Epoch=011/040] [Need: 00:30:18] [LR=0.0100] [Best : Accuracy=70.31, Error=29.69]
  Epoch: [011][000/500]   Time 18.508 (18.508)   Data 18.315 (18.315)   Loss 1.1625 (1.1625)   Prec@1 61.000 (61.000)   Prec@5 95.000 (95.000)   [2025-10-23 17:44:24]
  Epoch: [011][100/500]   Time 0.050 (0.231)   Data 0.000 (0.182)   Loss 1.0823 (1.0427)   Prec@1 60.000 (63.396)   Prec@5 97.000 (96.356)   [2025-10-23 17:44:29]
  Epoch: [011][200/500]   Time 0.053 (0.143)   Data 0.001 (0.092)   Loss 1.0478 (1.0523)   Prec@1 56.000 (62.856)   Prec@5 96.000 (96.109)   [2025-10-23 17:44:34]
  Epoch: [011][300/500]   Time 0.054 (0.113)   Data 0.000 (0.061)   Loss 1.0781 (1.0523)   Prec@1 66.000 (62.841)   Prec@5 93.000 (96.130)   [2025-10-23 17:44:40]
  Epoch: [011][400/500]   Time 0.055 (0.099)   Data 0.001 (0.046)   Loss 1.0229 (1.0571)   Prec@1 61.000 (62.783)   Prec@5 95.000 (96.072)   [2025-10-23 17:44:46]
  **Train** Prec@1 62.738 Prec@5 96.122 Error@1 37.262
  **Test** Prec@1 70.210 Prec@5 97.500 Error@1 29.790

==>>[2025-10-23 17:45:10] [Epoch=012/040] [Need: 00:29:18] [LR=0.0100] [Best : Accuracy=70.31, Error=29.69]
  Epoch: [012][000/500]   Time 17.834 (17.834)   Data 17.645 (17.645)   Loss 1.0503 (1.0503)   Prec@1 59.000 (59.000)   Prec@5 93.000 (93.000)   [2025-10-23 17:45:28]
  Epoch: [012][100/500]   Time 0.050 (0.226)   Data 0.001 (0.175)   Loss 0.8430 (1.0259)   Prec@1 71.000 (63.891)   Prec@5 98.000 (96.475)   [2025-10-23 17:45:33]
  Epoch: [012][200/500]   Time 0.055 (0.141)   Data 0.001 (0.088)   Loss 0.8772 (1.0298)   Prec@1 71.000 (63.915)   Prec@5 98.000 (96.418)   [2025-10-23 17:45:38]
  Epoch: [012][300/500]   Time 0.055 (0.112)   Data 0.001 (0.059)   Loss 1.0462 (1.0315)   Prec@1 65.000 (63.963)   Prec@5 98.000 (96.322)   [2025-10-23 17:45:44]
  Epoch: [012][400/500]   Time 0.054 (0.098)   Data 0.001 (0.045)   Loss 1.1203 (1.0346)   Prec@1 60.000 (63.850)   Prec@5 96.000 (96.289)   [2025-10-23 17:45:49]
  **Train** Prec@1 63.668 Prec@5 96.312 Error@1 36.332
  **Test** Prec@1 71.810 Prec@5 97.630 Error@1 28.190
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 17:46:13] [Epoch=013/040] [Need: 00:28:16] [LR=0.0100] [Best : Accuracy=71.81, Error=28.19]
  Epoch: [013][000/500]   Time 17.478 (17.478)   Data 17.257 (17.257)   Loss 1.0684 (1.0684)   Prec@1 62.000 (62.000)   Prec@5 95.000 (95.000)   [2025-10-23 17:46:30]
  Epoch: [013][100/500]   Time 0.053 (0.222)   Data 0.001 (0.171)   Loss 1.1285 (1.0387)   Prec@1 61.000 (63.822)   Prec@5 95.000 (96.168)   [2025-10-23 17:46:35]
  Epoch: [013][200/500]   Time 0.055 (0.138)   Data 0.000 (0.086)   Loss 1.1006 (1.0354)   Prec@1 64.000 (63.552)   Prec@5 97.000 (96.289)   [2025-10-23 17:46:40]
  Epoch: [013][300/500]   Time 0.054 (0.110)   Data 0.001 (0.058)   Loss 1.0298 (1.0273)   Prec@1 69.000 (63.824)   Prec@5 93.000 (96.306)   [2025-10-23 17:46:46]
  Epoch: [013][400/500]   Time 0.053 (0.096)   Data 0.001 (0.044)   Loss 0.9221 (1.0281)   Prec@1 69.000 (63.815)   Prec@5 98.000 (96.264)   [2025-10-23 17:46:51]
  **Train** Prec@1 63.654 Prec@5 96.232 Error@1 36.346
  **Test** Prec@1 72.700 Prec@5 97.860 Error@1 27.300
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 17:47:15] [Epoch=014/040] [Need: 00:27:12] [LR=0.0100] [Best : Accuracy=72.70, Error=27.30]
  Epoch: [014][000/500]   Time 18.069 (18.069)   Data 17.866 (17.866)   Loss 0.9375 (0.9375)   Prec@1 66.000 (66.000)   Prec@5 97.000 (97.000)   [2025-10-23 17:47:33]
  Epoch: [014][100/500]   Time 0.048 (0.227)   Data 0.001 (0.177)   Loss 1.0055 (1.0109)   Prec@1 59.000 (64.703)   Prec@5 97.000 (96.317)   [2025-10-23 17:47:38]
  Epoch: [014][200/500]   Time 0.055 (0.140)   Data 0.001 (0.089)   Loss 0.9976 (1.0092)   Prec@1 63.000 (64.507)   Prec@5 97.000 (96.562)   [2025-10-23 17:47:43]
  Epoch: [014][300/500]   Time 0.053 (0.112)   Data 0.000 (0.060)   Loss 1.0608 (1.0062)   Prec@1 58.000 (64.691)   Prec@5 97.000 (96.598)   [2025-10-23 17:47:49]
  Epoch: [014][400/500]   Time 0.054 (0.097)   Data 0.001 (0.045)   Loss 0.9612 (1.0117)   Prec@1 66.000 (64.524)   Prec@5 98.000 (96.514)   [2025-10-23 17:47:54]
  **Train** Prec@1 64.492 Prec@5 96.486 Error@1 35.508
  **Test** Prec@1 72.440 Prec@5 97.860 Error@1 27.560

==>>[2025-10-23 17:48:18] [Epoch=015/040] [Need: 00:26:09] [LR=0.0100] [Best : Accuracy=72.70, Error=27.30]
  Epoch: [015][000/500]   Time 17.343 (17.343)   Data 17.152 (17.152)   Loss 0.9829 (0.9829)   Prec@1 68.000 (68.000)   Prec@5 95.000 (95.000)   [2025-10-23 17:48:35]
  Epoch: [015][100/500]   Time 0.051 (0.221)   Data 0.000 (0.170)   Loss 1.0762 (1.0320)   Prec@1 69.000 (63.743)   Prec@5 97.000 (96.307)   [2025-10-23 17:48:40]
  Epoch: [015][200/500]   Time 0.056 (0.138)   Data 0.001 (0.086)   Loss 0.9699 (1.0185)   Prec@1 69.000 (64.075)   Prec@5 95.000 (96.383)   [2025-10-23 17:48:45]
  Epoch: [015][300/500]   Time 0.055 (0.110)   Data 0.001 (0.058)   Loss 0.8497 (1.0081)   Prec@1 67.000 (64.548)   Prec@5 98.000 (96.455)   [2025-10-23 17:48:51]
  Epoch: [015][400/500]   Time 0.054 (0.096)   Data 0.001 (0.043)   Loss 1.0288 (1.0066)   Prec@1 65.000 (64.711)   Prec@5 96.000 (96.496)   [2025-10-23 17:48:56]
  **Train** Prec@1 64.670 Prec@5 96.566 Error@1 35.330
  **Test** Prec@1 71.480 Prec@5 97.710 Error@1 28.520

==>>[2025-10-23 17:49:20] [Epoch=016/040] [Need: 00:25:06] [LR=0.0100] [Best : Accuracy=72.70, Error=27.30]
  Epoch: [016][000/500]   Time 17.677 (17.677)   Data 17.468 (17.468)   Loss 1.1244 (1.1244)   Prec@1 59.000 (59.000)   Prec@5 95.000 (95.000)   [2025-10-23 17:49:38]
  Epoch: [016][100/500]   Time 0.052 (0.229)   Data 0.000 (0.174)   Loss 1.0490 (0.9990)   Prec@1 58.000 (64.782)   Prec@5 94.000 (96.426)   [2025-10-23 17:49:43]
  Epoch: [016][200/500]   Time 0.054 (0.143)   Data 0.000 (0.088)   Loss 1.1326 (1.0032)   Prec@1 59.000 (64.846)   Prec@5 97.000 (96.637)   [2025-10-23 17:49:49]
  Epoch: [016][300/500]   Time 0.055 (0.114)   Data 0.000 (0.059)   Loss 0.8616 (1.0028)   Prec@1 64.000 (64.837)   Prec@5 99.000 (96.595)   [2025-10-23 17:49:55]
  Epoch: [016][400/500]   Time 0.053 (0.099)   Data 0.000 (0.044)   Loss 0.9147 (0.9980)   Prec@1 71.000 (65.080)   Prec@5 97.000 (96.576)   [2025-10-23 17:50:00]
  **Train** Prec@1 64.948 Prec@5 96.622 Error@1 35.052
  **Test** Prec@1 72.940 Prec@5 98.020 Error@1 27.060
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 17:50:24] [Epoch=017/040] [Need: 00:24:05] [LR=0.0100] [Best : Accuracy=72.94, Error=27.06]
  Epoch: [017][000/500]   Time 18.055 (18.055)   Data 17.829 (17.829)   Loss 0.9626 (0.9626)   Prec@1 65.000 (65.000)   Prec@5 99.000 (99.000)   [2025-10-23 17:50:42]
  Epoch: [017][100/500]   Time 0.054 (0.233)   Data 0.001 (0.177)   Loss 1.1267 (0.9941)   Prec@1 62.000 (65.000)   Prec@5 95.000 (96.653)   [2025-10-23 17:50:48]
  Epoch: [017][200/500]   Time 0.056 (0.144)   Data 0.001 (0.089)   Loss 0.9499 (0.9960)   Prec@1 67.000 (64.945)   Prec@5 95.000 (96.438)   [2025-10-23 17:50:53]
  Epoch: [017][300/500]   Time 0.056 (0.114)   Data 0.000 (0.060)   Loss 0.9128 (0.9945)   Prec@1 70.000 (64.997)   Prec@5 99.000 (96.472)   [2025-10-23 17:50:59]
  Epoch: [017][400/500]   Time 0.055 (0.099)   Data 0.001 (0.045)   Loss 0.9516 (0.9927)   Prec@1 69.000 (65.135)   Prec@5 99.000 (96.569)   [2025-10-23 17:51:04]
  **Train** Prec@1 65.228 Prec@5 96.568 Error@1 34.772
  **Test** Prec@1 72.840 Prec@5 97.970 Error@1 27.160

==>>[2025-10-23 17:51:28] [Epoch=018/040] [Need: 00:23:03] [LR=0.0100] [Best : Accuracy=72.94, Error=27.06]
  Epoch: [018][000/500]   Time 17.575 (17.575)   Data 17.378 (17.378)   Loss 1.1875 (1.1875)   Prec@1 53.000 (53.000)   Prec@5 96.000 (96.000)   [2025-10-23 17:51:45]
  Epoch: [018][100/500]   Time 0.050 (0.223)   Data 0.000 (0.173)   Loss 0.8712 (0.9996)   Prec@1 68.000 (64.812)   Prec@5 98.000 (96.307)   [2025-10-23 17:51:50]
  Epoch: [018][200/500]   Time 0.054 (0.138)   Data 0.001 (0.087)   Loss 0.8505 (0.9895)   Prec@1 67.000 (65.418)   Prec@5 97.000 (96.463)   [2025-10-23 17:51:56]
  Epoch: [018][300/500]   Time 0.055 (0.110)   Data 0.001 (0.058)   Loss 1.0350 (0.9935)   Prec@1 60.000 (65.146)   Prec@5 96.000 (96.455)   [2025-10-23 17:52:01]
  Epoch: [018][400/500]   Time 0.053 (0.096)   Data 0.001 (0.044)   Loss 1.0660 (0.9890)   Prec@1 60.000 (65.237)   Prec@5 97.000 (96.504)   [2025-10-23 17:52:06]
  **Train** Prec@1 65.490 Prec@5 96.580 Error@1 34.510
  **Test** Prec@1 73.640 Prec@5 98.170 Error@1 26.360
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 17:52:31] [Epoch=019/040] [Need: 00:22:00] [LR=0.0100] [Best : Accuracy=73.64, Error=26.36]
  Epoch: [019][000/500]   Time 17.468 (17.468)   Data 17.285 (17.285)   Loss 0.8324 (0.8324)   Prec@1 67.000 (67.000)   Prec@5 98.000 (98.000)   [2025-10-23 17:52:48]
  Epoch: [019][100/500]   Time 0.051 (0.222)   Data 0.001 (0.172)   Loss 0.9102 (0.9550)   Prec@1 64.000 (66.723)   Prec@5 100.000 (97.099)   [2025-10-23 17:52:53]
  Epoch: [019][200/500]   Time 0.055 (0.138)   Data 0.001 (0.087)   Loss 0.9378 (0.9565)   Prec@1 63.000 (66.567)   Prec@5 99.000 (96.990)   [2025-10-23 17:52:58]
  Epoch: [019][300/500]   Time 0.054 (0.110)   Data 0.000 (0.058)   Loss 0.9762 (0.9624)   Prec@1 67.000 (66.372)   Prec@5 98.000 (96.904)   [2025-10-23 17:53:04]
  Epoch: [019][400/500]   Time 0.054 (0.096)   Data 0.000 (0.044)   Loss 0.9100 (0.9697)   Prec@1 69.000 (66.060)   Prec@5 99.000 (96.870)   [2025-10-23 17:53:09]
  **Train** Prec@1 66.216 Prec@5 96.862 Error@1 33.784
  **Test** Prec@1 72.100 Prec@5 97.940 Error@1 27.900

==>>[2025-10-23 17:53:33] [Epoch=020/040] [Need: 00:20:57] [LR=0.0100] [Best : Accuracy=73.64, Error=26.36]
  Epoch: [020][000/500]   Time 17.299 (17.299)   Data 17.115 (17.115)   Loss 0.7862 (0.7862)   Prec@1 73.000 (73.000)   Prec@5 100.000 (100.000)   [2025-10-23 17:53:51]
  Epoch: [020][100/500]   Time 0.049 (0.220)   Data 0.000 (0.170)   Loss 0.8915 (0.9721)   Prec@1 69.000 (65.703)   Prec@5 97.000 (96.772)   [2025-10-23 17:53:55]
  Epoch: [020][200/500]   Time 0.053 (0.137)   Data 0.001 (0.086)   Loss 0.8839 (0.9748)   Prec@1 70.000 (65.915)   Prec@5 95.000 (96.796)   [2025-10-23 17:54:01]
  Epoch: [020][300/500]   Time 0.052 (0.109)   Data 0.000 (0.057)   Loss 1.0054 (0.9698)   Prec@1 64.000 (66.179)   Prec@5 96.000 (96.767)   [2025-10-23 17:54:06]
  Epoch: [020][400/500]   Time 0.055 (0.095)   Data 0.001 (0.043)   Loss 0.8558 (0.9683)   Prec@1 74.000 (66.209)   Prec@5 98.000 (96.828)   [2025-10-23 17:54:12]
  **Train** Prec@1 66.340 Prec@5 96.850 Error@1 33.660
  **Test** Prec@1 74.140 Prec@5 98.260 Error@1 25.860
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 17:54:35] [Epoch=021/040] [Need: 00:19:53] [LR=0.0100] [Best : Accuracy=74.14, Error=25.86]
  Epoch: [021][000/500]   Time 18.641 (18.641)   Data 18.456 (18.456)   Loss 0.9438 (0.9438)   Prec@1 63.000 (63.000)   Prec@5 99.000 (99.000)   [2025-10-23 17:54:54]
  Epoch: [021][100/500]   Time 0.051 (0.234)   Data 0.001 (0.183)   Loss 0.8662 (0.9627)   Prec@1 73.000 (65.723)   Prec@5 98.000 (97.188)   [2025-10-23 17:54:59]
  Epoch: [021][200/500]   Time 0.055 (0.144)   Data 0.000 (0.092)   Loss 0.8931 (0.9658)   Prec@1 74.000 (65.851)   Prec@5 94.000 (97.025)   [2025-10-23 17:55:04]
  Epoch: [021][300/500]   Time 0.056 (0.115)   Data 0.000 (0.062)   Loss 1.0117 (0.9664)   Prec@1 69.000 (65.894)   Prec@5 96.000 (97.003)   [2025-10-23 17:55:10]
  Epoch: [021][400/500]   Time 0.052 (0.100)   Data 0.001 (0.047)   Loss 0.8560 (0.9636)   Prec@1 71.000 (66.117)   Prec@5 98.000 (96.960)   [2025-10-23 17:55:15]
  **Train** Prec@1 66.272 Prec@5 96.962 Error@1 33.728
  **Test** Prec@1 74.180 Prec@5 98.270 Error@1 25.820
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 17:55:39] [Epoch=022/040] [Need: 00:18:52] [LR=0.0100] [Best : Accuracy=74.18, Error=25.82]
  Epoch: [022][000/500]   Time 17.695 (17.695)   Data 17.506 (17.506)   Loss 1.1587 (1.1587)   Prec@1 61.000 (61.000)   Prec@5 93.000 (93.000)   [2025-10-23 17:55:57]
  Epoch: [022][100/500]   Time 0.049 (0.225)   Data 0.001 (0.174)   Loss 1.1717 (0.9777)   Prec@1 59.000 (66.000)   Prec@5 96.000 (96.604)   [2025-10-23 17:56:02]
  Epoch: [022][200/500]   Time 0.056 (0.140)   Data 0.000 (0.088)   Loss 0.8964 (0.9721)   Prec@1 66.000 (65.761)   Prec@5 99.000 (96.756)   [2025-10-23 17:56:08]
  Epoch: [022][300/500]   Time 0.057 (0.111)   Data 0.001 (0.059)   Loss 0.9964 (0.9647)   Prec@1 66.000 (66.166)   Prec@5 97.000 (96.847)   [2025-10-23 17:56:13]
  Epoch: [022][400/500]   Time 0.053 (0.097)   Data 0.000 (0.044)   Loss 0.9610 (0.9618)   Prec@1 61.000 (66.187)   Prec@5 100.000 (96.858)   [2025-10-23 17:56:18]
  **Train** Prec@1 66.238 Prec@5 96.916 Error@1 33.762
  **Test** Prec@1 74.070 Prec@5 98.220 Error@1 25.930

==>>[2025-10-23 17:56:42] [Epoch=023/040] [Need: 00:17:49] [LR=0.0100] [Best : Accuracy=74.18, Error=25.82]
  Epoch: [023][000/500]   Time 17.450 (17.450)   Data 17.268 (17.268)   Loss 0.9200 (0.9200)   Prec@1 72.000 (72.000)   Prec@5 98.000 (98.000)   [2025-10-23 17:57:00]
  Epoch: [023][100/500]   Time 0.051 (0.221)   Data 0.001 (0.172)   Loss 0.9181 (0.9486)   Prec@1 68.000 (66.693)   Prec@5 93.000 (96.970)   [2025-10-23 17:57:05]
  Epoch: [023][200/500]   Time 0.053 (0.137)   Data 0.001 (0.086)   Loss 1.0043 (0.9483)   Prec@1 70.000 (66.900)   Prec@5 93.000 (96.915)   [2025-10-23 17:57:10]
  Epoch: [023][300/500]   Time 0.054 (0.109)   Data 0.000 (0.058)   Loss 1.0183 (0.9530)   Prec@1 60.000 (66.777)   Prec@5 97.000 (96.791)   [2025-10-23 17:57:15]
  Epoch: [023][400/500]   Time 0.053 (0.096)   Data 0.000 (0.044)   Loss 1.0660 (0.9541)   Prec@1 66.000 (66.703)   Prec@5 97.000 (96.796)   [2025-10-23 17:57:21]
  **Train** Prec@1 66.864 Prec@5 96.824 Error@1 33.136
  **Test** Prec@1 73.720 Prec@5 98.350 Error@1 26.280

==>>[2025-10-23 17:57:44] [Epoch=024/040] [Need: 00:16:45] [LR=0.0100] [Best : Accuracy=74.18, Error=25.82]
  Epoch: [024][000/500]   Time 17.259 (17.259)   Data 17.076 (17.076)   Loss 1.0452 (1.0452)   Prec@1 68.000 (68.000)   Prec@5 96.000 (96.000)   [2025-10-23 17:58:01]
  Epoch: [024][100/500]   Time 0.053 (0.220)   Data 0.000 (0.170)   Loss 0.9880 (0.9434)   Prec@1 66.000 (66.545)   Prec@5 95.000 (97.139)   [2025-10-23 17:58:06]
  Epoch: [024][200/500]   Time 0.053 (0.137)   Data 0.000 (0.086)   Loss 0.9237 (0.9393)   Prec@1 70.000 (66.761)   Prec@5 97.000 (97.060)   [2025-10-23 17:58:12]
  Epoch: [024][300/500]   Time 0.054 (0.109)   Data 0.001 (0.057)   Loss 1.0478 (0.9445)   Prec@1 66.000 (66.767)   Prec@5 97.000 (96.917)   [2025-10-23 17:58:17]
  Epoch: [024][400/500]   Time 0.053 (0.096)   Data 0.000 (0.043)   Loss 1.0385 (0.9476)   Prec@1 64.000 (66.791)   Prec@5 96.000 (96.860)   [2025-10-23 17:58:22]
  **Train** Prec@1 67.032 Prec@5 96.914 Error@1 32.968
  **Test** Prec@1 74.010 Prec@5 98.230 Error@1 25.990

==>>[2025-10-23 17:58:46] [Epoch=025/040] [Need: 00:15:42] [LR=0.0010] [Best : Accuracy=74.18, Error=25.82]
  Epoch: [025][000/500]   Time 17.299 (17.299)   Data 17.118 (17.118)   Loss 1.0032 (1.0032)   Prec@1 66.000 (66.000)   Prec@5 98.000 (98.000)   [2025-10-23 17:59:03]
  Epoch: [025][100/500]   Time 0.051 (0.221)   Data 0.001 (0.170)   Loss 0.9382 (0.9053)   Prec@1 64.000 (68.317)   Prec@5 97.000 (97.446)   [2025-10-23 17:59:08]
  Epoch: [025][200/500]   Time 0.060 (0.138)   Data 0.001 (0.086)   Loss 0.7917 (0.9047)   Prec@1 74.000 (68.294)   Prec@5 99.000 (97.373)   [2025-10-23 17:59:14]
  Epoch: [025][300/500]   Time 0.053 (0.110)   Data 0.001 (0.057)   Loss 0.6257 (0.8929)   Prec@1 76.000 (68.668)   Prec@5 100.000 (97.352)   [2025-10-23 17:59:19]
  Epoch: [025][400/500]   Time 0.052 (0.097)   Data 0.001 (0.043)   Loss 0.8856 (0.8843)   Prec@1 71.000 (69.195)   Prec@5 97.000 (97.404)   [2025-10-23 17:59:25]
  **Train** Prec@1 69.272 Prec@5 97.378 Error@1 30.728
  **Test** Prec@1 76.200 Prec@5 98.390 Error@1 23.800
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 17:59:49] [Epoch=026/040] [Need: 00:14:39] [LR=0.0010] [Best : Accuracy=76.20, Error=23.80]
  Epoch: [026][000/500]   Time 17.461 (17.461)   Data 17.275 (17.275)   Loss 0.6339 (0.6339)   Prec@1 76.000 (76.000)   Prec@5 100.000 (100.000)   [2025-10-23 18:00:07]
  Epoch: [026][100/500]   Time 0.049 (0.222)   Data 0.000 (0.172)   Loss 0.6682 (0.8760)   Prec@1 78.000 (69.356)   Prec@5 98.000 (97.238)   [2025-10-23 18:00:12]
  Epoch: [026][200/500]   Time 0.056 (0.138)   Data 0.001 (0.087)   Loss 0.9526 (0.8741)   Prec@1 71.000 (69.383)   Prec@5 97.000 (97.299)   [2025-10-23 18:00:17]
  Epoch: [026][300/500]   Time 0.054 (0.110)   Data 0.001 (0.058)   Loss 0.8911 (0.8732)   Prec@1 68.000 (69.289)   Prec@5 99.000 (97.336)   [2025-10-23 18:00:23]
  Epoch: [026][400/500]   Time 0.054 (0.096)   Data 0.000 (0.044)   Loss 0.7948 (0.8707)   Prec@1 72.000 (69.564)   Prec@5 98.000 (97.347)   [2025-10-23 18:00:28]
  **Train** Prec@1 69.678 Prec@5 97.352 Error@1 30.322
  **Test** Prec@1 76.490 Prec@5 98.410 Error@1 23.510
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 18:00:53] [Epoch=027/040] [Need: 00:13:36] [LR=0.0010] [Best : Accuracy=76.49, Error=23.51]
  Epoch: [027][000/500]   Time 17.535 (17.535)   Data 17.352 (17.352)   Loss 1.0153 (1.0153)   Prec@1 63.000 (63.000)   Prec@5 93.000 (93.000)   [2025-10-23 18:01:10]
  Epoch: [027][100/500]   Time 0.052 (0.222)   Data 0.001 (0.172)   Loss 0.9584 (0.8540)   Prec@1 57.000 (69.634)   Prec@5 99.000 (97.743)   [2025-10-23 18:01:15]
  Epoch: [027][200/500]   Time 0.055 (0.138)   Data 0.000 (0.087)   Loss 0.8318 (0.8539)   Prec@1 70.000 (70.104)   Prec@5 97.000 (97.597)   [2025-10-23 18:01:20]
  Epoch: [027][300/500]   Time 0.055 (0.110)   Data 0.000 (0.058)   Loss 0.9479 (0.8580)   Prec@1 67.000 (69.957)   Prec@5 97.000 (97.545)   [2025-10-23 18:01:26]
  Epoch: [027][400/500]   Time 0.054 (0.096)   Data 0.001 (0.044)   Loss 0.8514 (0.8597)   Prec@1 66.000 (69.940)   Prec@5 99.000 (97.461)   [2025-10-23 18:01:31]
  **Train** Prec@1 69.948 Prec@5 97.522 Error@1 30.052
  **Test** Prec@1 76.360 Prec@5 98.450 Error@1 23.640

==>>[2025-10-23 18:01:55] [Epoch=028/040] [Need: 00:12:33] [LR=0.0010] [Best : Accuracy=76.49, Error=23.51]
  Epoch: [028][000/500]   Time 18.367 (18.367)   Data 18.181 (18.181)   Loss 0.9166 (0.9166)   Prec@1 67.000 (67.000)   Prec@5 97.000 (97.000)   [2025-10-23 18:02:13]
  Epoch: [028][100/500]   Time 0.051 (0.231)   Data 0.001 (0.181)   Loss 0.8238 (0.8453)   Prec@1 73.000 (70.287)   Prec@5 99.000 (97.554)   [2025-10-23 18:02:18]
  Epoch: [028][200/500]   Time 0.051 (0.143)   Data 0.000 (0.091)   Loss 0.9083 (0.8538)   Prec@1 70.000 (69.945)   Prec@5 95.000 (97.587)   [2025-10-23 18:02:24]
  Epoch: [028][300/500]   Time 0.056 (0.113)   Data 0.001 (0.061)   Loss 0.8157 (0.8579)   Prec@1 73.000 (69.970)   Prec@5 98.000 (97.482)   [2025-10-23 18:02:29]
  Epoch: [028][400/500]   Time 0.053 (0.099)   Data 0.000 (0.046)   Loss 0.8899 (0.8547)   Prec@1 66.000 (70.192)   Prec@5 98.000 (97.519)   [2025-10-23 18:02:34]
  **Train** Prec@1 70.156 Prec@5 97.554 Error@1 29.844
  **Test** Prec@1 76.630 Prec@5 98.570 Error@1 23.370
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 18:02:58] [Epoch=029/040] [Need: 00:11:31] [LR=0.0010] [Best : Accuracy=76.63, Error=23.37]
  Epoch: [029][000/500]   Time 17.390 (17.390)   Data 17.206 (17.206)   Loss 0.9275 (0.9275)   Prec@1 63.000 (63.000)   Prec@5 99.000 (99.000)   [2025-10-23 18:03:16]
  Epoch: [029][100/500]   Time 0.051 (0.221)   Data 0.000 (0.171)   Loss 0.9425 (0.8508)   Prec@1 74.000 (70.406)   Prec@5 97.000 (97.436)   [2025-10-23 18:03:21]
  Epoch: [029][200/500]   Time 0.055 (0.137)   Data 0.000 (0.086)   Loss 1.0356 (0.8483)   Prec@1 62.000 (70.612)   Prec@5 98.000 (97.498)   [2025-10-23 18:03:26]
  Epoch: [029][300/500]   Time 0.053 (0.110)   Data 0.000 (0.058)   Loss 0.9074 (0.8570)   Prec@1 66.000 (70.176)   Prec@5 97.000 (97.442)   [2025-10-23 18:03:31]
  Epoch: [029][400/500]   Time 0.053 (0.096)   Data 0.001 (0.044)   Loss 0.9179 (0.8546)   Prec@1 68.000 (70.165)   Prec@5 99.000 (97.511)   [2025-10-23 18:03:37]
  **Train** Prec@1 70.256 Prec@5 97.514 Error@1 29.744
  **Test** Prec@1 76.440 Prec@5 98.460 Error@1 23.560

==>>[2025-10-23 18:04:01] [Epoch=030/040] [Need: 00:10:28] [LR=0.0010] [Best : Accuracy=76.63, Error=23.37]
  Epoch: [030][000/500]   Time 17.612 (17.612)   Data 17.428 (17.428)   Loss 0.9374 (0.9374)   Prec@1 69.000 (69.000)   Prec@5 96.000 (96.000)   [2025-10-23 18:04:18]
  Epoch: [030][100/500]   Time 0.049 (0.222)   Data 0.000 (0.173)   Loss 0.8170 (0.8391)   Prec@1 75.000 (70.426)   Prec@5 98.000 (97.634)   [2025-10-23 18:04:23]
  Epoch: [030][200/500]   Time 0.053 (0.138)   Data 0.000 (0.087)   Loss 0.6749 (0.8495)   Prec@1 80.000 (70.507)   Prec@5 98.000 (97.522)   [2025-10-23 18:04:28]
  Epoch: [030][300/500]   Time 0.052 (0.110)   Data 0.000 (0.059)   Loss 0.9366 (0.8525)   Prec@1 65.000 (70.375)   Prec@5 98.000 (97.578)   [2025-10-23 18:04:34]
  Epoch: [030][400/500]   Time 0.054 (0.096)   Data 0.000 (0.044)   Loss 0.7694 (0.8556)   Prec@1 78.000 (70.319)   Prec@5 96.000 (97.586)   [2025-10-23 18:04:39]
  **Train** Prec@1 70.512 Prec@5 97.582 Error@1 29.488
  **Test** Prec@1 76.560 Prec@5 98.490 Error@1 23.440

==>>[2025-10-23 18:05:03] [Epoch=031/040] [Need: 00:09:25] [LR=0.0010] [Best : Accuracy=76.63, Error=23.37]
  Epoch: [031][000/500]   Time 17.884 (17.884)   Data 17.701 (17.701)   Loss 0.8544 (0.8544)   Prec@1 69.000 (69.000)   Prec@5 99.000 (99.000)   [2025-10-23 18:05:21]
  Epoch: [031][100/500]   Time 0.051 (0.226)   Data 0.001 (0.176)   Loss 0.8945 (0.8371)   Prec@1 73.000 (70.733)   Prec@5 97.000 (97.812)   [2025-10-23 18:05:26]
  Epoch: [031][200/500]   Time 0.055 (0.140)   Data 0.001 (0.089)   Loss 0.8446 (0.8517)   Prec@1 72.000 (70.184)   Prec@5 97.000 (97.532)   [2025-10-23 18:05:32]
  Epoch: [031][300/500]   Time 0.060 (0.112)   Data 0.000 (0.059)   Loss 0.8007 (0.8463)   Prec@1 71.000 (70.435)   Prec@5 97.000 (97.598)   [2025-10-23 18:05:37]
  Epoch: [031][400/500]   Time 0.073 (0.098)   Data 0.001 (0.045)   Loss 1.2132 (0.8462)   Prec@1 59.000 (70.471)   Prec@5 95.000 (97.569)   [2025-10-23 18:05:43]
  **Train** Prec@1 70.514 Prec@5 97.640 Error@1 29.486
  **Test** Prec@1 76.840 Prec@5 98.580 Error@1 23.160
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 18:06:07] [Epoch=032/040] [Need: 00:08:22] [LR=0.0010] [Best : Accuracy=76.84, Error=23.16]
  Epoch: [032][000/500]   Time 17.539 (17.539)   Data 17.356 (17.356)   Loss 0.8637 (0.8637)   Prec@1 69.000 (69.000)   Prec@5 98.000 (98.000)   [2025-10-23 18:06:24]
  Epoch: [032][100/500]   Time 0.050 (0.222)   Data 0.001 (0.172)   Loss 0.7860 (0.8389)   Prec@1 71.000 (70.188)   Prec@5 99.000 (97.822)   [2025-10-23 18:06:29]
  Epoch: [032][200/500]   Time 0.054 (0.138)   Data 0.000 (0.087)   Loss 0.8943 (0.8381)   Prec@1 68.000 (70.527)   Prec@5 100.000 (97.746)   [2025-10-23 18:06:34]
  Epoch: [032][300/500]   Time 0.054 (0.110)   Data 0.001 (0.058)   Loss 0.9530 (0.8382)   Prec@1 64.000 (70.684)   Prec@5 97.000 (97.741)   [2025-10-23 18:06:40]
  Epoch: [032][400/500]   Time 0.054 (0.096)   Data 0.001 (0.044)   Loss 0.9168 (0.8419)   Prec@1 68.000 (70.636)   Prec@5 95.000 (97.636)   [2025-10-23 18:06:45]
  **Train** Prec@1 70.696 Prec@5 97.648 Error@1 29.304
  **Test** Prec@1 76.960 Prec@5 98.550 Error@1 23.040
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 18:07:09] [Epoch=033/040] [Need: 00:07:19] [LR=0.0010] [Best : Accuracy=76.96, Error=23.04]
  Epoch: [033][000/500]   Time 17.532 (17.532)   Data 17.347 (17.347)   Loss 0.9850 (0.9850)   Prec@1 67.000 (67.000)   Prec@5 95.000 (95.000)   [2025-10-23 18:07:26]
  Epoch: [033][100/500]   Time 0.052 (0.223)   Data 0.001 (0.172)   Loss 0.5740 (0.8310)   Prec@1 78.000 (71.000)   Prec@5 98.000 (97.752)   [2025-10-23 18:07:31]
  Epoch: [033][200/500]   Time 0.054 (0.138)   Data 0.001 (0.087)   Loss 0.9649 (0.8339)   Prec@1 70.000 (71.075)   Prec@5 98.000 (97.726)   [2025-10-23 18:07:36]
  Epoch: [033][300/500]   Time 0.053 (0.110)   Data 0.000 (0.058)   Loss 0.9988 (0.8384)   Prec@1 67.000 (70.967)   Prec@5 96.000 (97.611)   [2025-10-23 18:07:42]
  Epoch: [033][400/500]   Time 0.058 (0.096)   Data 0.001 (0.044)   Loss 0.8082 (0.8405)   Prec@1 71.000 (70.928)   Prec@5 99.000 (97.626)   [2025-10-23 18:07:47]
  **Train** Prec@1 70.846 Prec@5 97.598 Error@1 29.154
  **Test** Prec@1 77.080 Prec@5 98.490 Error@1 22.920
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 18:08:11] [Epoch=034/040] [Need: 00:06:16] [LR=0.0010] [Best : Accuracy=77.08, Error=22.92]
  Epoch: [034][000/500]   Time 17.510 (17.510)   Data 17.299 (17.299)   Loss 0.7984 (0.7984)   Prec@1 68.000 (68.000)   Prec@5 99.000 (99.000)   [2025-10-23 18:08:29]
  Epoch: [034][100/500]   Time 0.054 (0.223)   Data 0.001 (0.172)   Loss 0.8804 (0.8347)   Prec@1 71.000 (71.455)   Prec@5 99.000 (97.634)   [2025-10-23 18:08:34]
  Epoch: [034][200/500]   Time 0.055 (0.139)   Data 0.001 (0.087)   Loss 0.7505 (0.8383)   Prec@1 74.000 (71.398)   Prec@5 98.000 (97.527)   [2025-10-23 18:08:39]
  Epoch: [034][300/500]   Time 0.054 (0.110)   Data 0.001 (0.058)   Loss 0.7404 (0.8435)   Prec@1 76.000 (71.070)   Prec@5 99.000 (97.565)   [2025-10-23 18:08:44]
  Epoch: [034][400/500]   Time 0.054 (0.096)   Data 0.001 (0.044)   Loss 0.9517 (0.8423)   Prec@1 66.000 (71.077)   Prec@5 98.000 (97.581)   [2025-10-23 18:08:50]
  **Train** Prec@1 71.076 Prec@5 97.594 Error@1 28.924
  **Test** Prec@1 76.960 Prec@5 98.500 Error@1 23.040

==>>[2025-10-23 18:09:13] [Epoch=035/040] [Need: 00:05:13] [LR=0.0010] [Best : Accuracy=77.08, Error=22.92]
  Epoch: [035][000/500]   Time 17.566 (17.566)   Data 17.382 (17.382)   Loss 0.7873 (0.7873)   Prec@1 74.000 (74.000)   Prec@5 98.000 (98.000)   [2025-10-23 18:09:31]
  Epoch: [035][100/500]   Time 0.049 (0.222)   Data 0.000 (0.173)   Loss 0.9646 (0.8352)   Prec@1 64.000 (71.168)   Prec@5 93.000 (97.653)   [2025-10-23 18:09:36]
  Epoch: [035][200/500]   Time 0.054 (0.138)   Data 0.000 (0.087)   Loss 0.9364 (0.8275)   Prec@1 72.000 (71.229)   Prec@5 97.000 (97.697)   [2025-10-23 18:09:41]
  Epoch: [035][300/500]   Time 0.054 (0.111)   Data 0.001 (0.058)   Loss 0.8778 (0.8318)   Prec@1 70.000 (71.199)   Prec@5 98.000 (97.741)   [2025-10-23 18:09:47]
  Epoch: [035][400/500]   Time 0.054 (0.097)   Data 0.001 (0.044)   Loss 0.8948 (0.8412)   Prec@1 68.000 (70.783)   Prec@5 98.000 (97.656)   [2025-10-23 18:09:52]
  **Train** Prec@1 70.736 Prec@5 97.648 Error@1 29.264
  **Test** Prec@1 76.780 Prec@5 98.420 Error@1 23.220

==>>[2025-10-23 18:10:16] [Epoch=036/040] [Need: 00:04:11] [LR=0.0010] [Best : Accuracy=77.08, Error=22.92]
  Epoch: [036][000/500]   Time 17.296 (17.296)   Data 17.112 (17.112)   Loss 0.8742 (0.8742)   Prec@1 65.000 (65.000)   Prec@5 97.000 (97.000)   [2025-10-23 18:10:33]
  Epoch: [036][100/500]   Time 0.052 (0.219)   Data 0.000 (0.170)   Loss 0.9139 (0.8218)   Prec@1 77.000 (71.129)   Prec@5 94.000 (97.772)   [2025-10-23 18:10:38]
  Epoch: [036][200/500]   Time 0.054 (0.136)   Data 0.000 (0.086)   Loss 0.7202 (0.8350)   Prec@1 74.000 (70.846)   Prec@5 98.000 (97.662)   [2025-10-23 18:10:43]
  Epoch: [036][300/500]   Time 0.054 (0.109)   Data 0.000 (0.057)   Loss 0.8687 (0.8315)   Prec@1 69.000 (71.070)   Prec@5 98.000 (97.681)   [2025-10-23 18:10:49]
  Epoch: [036][400/500]   Time 0.055 (0.095)   Data 0.000 (0.043)   Loss 0.8846 (0.8358)   Prec@1 70.000 (70.885)   Prec@5 97.000 (97.681)   [2025-10-23 18:10:54]
  **Train** Prec@1 70.886 Prec@5 97.668 Error@1 29.114
  **Test** Prec@1 77.490 Prec@5 98.600 Error@1 22.510
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 18:11:18] [Epoch=037/040] [Need: 00:03:08] [LR=0.0010] [Best : Accuracy=77.49, Error=22.51]
  Epoch: [037][000/500]   Time 17.298 (17.298)   Data 17.113 (17.113)   Loss 0.7758 (0.7758)   Prec@1 73.000 (73.000)   Prec@5 97.000 (97.000)   [2025-10-23 18:11:35]
  Epoch: [037][100/500]   Time 0.051 (0.220)   Data 0.000 (0.170)   Loss 0.6775 (0.8363)   Prec@1 73.000 (70.743)   Prec@5 100.000 (97.752)   [2025-10-23 18:11:40]
  Epoch: [037][200/500]   Time 0.055 (0.137)   Data 0.001 (0.086)   Loss 0.7481 (0.8380)   Prec@1 75.000 (70.871)   Prec@5 100.000 (97.761)   [2025-10-23 18:11:45]
  Epoch: [037][300/500]   Time 0.055 (0.110)   Data 0.001 (0.057)   Loss 0.7920 (0.8384)   Prec@1 72.000 (70.744)   Prec@5 97.000 (97.744)   [2025-10-23 18:11:51]
  Epoch: [037][400/500]   Time 0.054 (0.096)   Data 0.000 (0.043)   Loss 0.7983 (0.8373)   Prec@1 68.000 (70.875)   Prec@5 98.000 (97.708)   [2025-10-23 18:11:56]
  **Train** Prec@1 70.856 Prec@5 97.706 Error@1 29.144
  **Test** Prec@1 77.340 Prec@5 98.510 Error@1 22.660

==>>[2025-10-23 18:12:20] [Epoch=038/040] [Need: 00:02:05] [LR=0.0010] [Best : Accuracy=77.49, Error=22.51]
  Epoch: [038][000/500]   Time 17.469 (17.469)   Data 17.286 (17.286)   Loss 0.7471 (0.7471)   Prec@1 75.000 (75.000)   Prec@5 97.000 (97.000)   [2025-10-23 18:12:37]
  Epoch: [038][100/500]   Time 0.049 (0.222)   Data 0.000 (0.172)   Loss 0.7276 (0.8483)   Prec@1 73.000 (70.723)   Prec@5 99.000 (97.505)   [2025-10-23 18:12:42]
  Epoch: [038][200/500]   Time 0.056 (0.138)   Data 0.001 (0.087)   Loss 0.8466 (0.8423)   Prec@1 68.000 (70.801)   Prec@5 100.000 (97.652)   [2025-10-23 18:12:48]
  Epoch: [038][300/500]   Time 0.054 (0.110)   Data 0.001 (0.058)   Loss 0.7736 (0.8328)   Prec@1 73.000 (70.960)   Prec@5 97.000 (97.701)   [2025-10-23 18:12:53]
  Epoch: [038][400/500]   Time 0.055 (0.096)   Data 0.001 (0.044)   Loss 1.0719 (0.8395)   Prec@1 61.000 (70.683)   Prec@5 97.000 (97.611)   [2025-10-23 18:12:58]
  **Train** Prec@1 70.806 Prec@5 97.648 Error@1 29.194
  **Test** Prec@1 77.510 Prec@5 98.510 Error@1 22.490
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 18:13:22] [Epoch=039/040] [Need: 00:01:02] [LR=0.0010] [Best : Accuracy=77.51, Error=22.49]
  Epoch: [039][000/500]   Time 17.262 (17.262)   Data 17.079 (17.079)   Loss 0.6613 (0.6613)   Prec@1 76.000 (76.000)   Prec@5 99.000 (99.000)   [2025-10-23 18:13:40]
  Epoch: [039][100/500]   Time 0.054 (0.219)   Data 0.000 (0.170)   Loss 1.0019 (0.8222)   Prec@1 66.000 (71.475)   Prec@5 96.000 (97.970)   [2025-10-23 18:13:45]
  Epoch: [039][200/500]   Time 0.055 (0.137)   Data 0.000 (0.086)   Loss 0.9372 (0.8194)   Prec@1 69.000 (71.547)   Prec@5 96.000 (97.856)   [2025-10-23 18:13:50]
  Epoch: [039][300/500]   Time 0.051 (0.109)   Data 0.000 (0.057)   Loss 0.7443 (0.8245)   Prec@1 75.000 (71.326)   Prec@5 99.000 (97.817)   [2025-10-23 18:13:55]
  Epoch: [039][400/500]   Time 0.056 (0.095)   Data 0.001 (0.043)   Loss 0.8349 (0.8274)   Prec@1 75.000 (71.279)   Prec@5 98.000 (97.748)   [2025-10-23 18:14:01]
  **Train** Prec@1 71.234 Prec@5 97.768 Error@1 28.766
  **Test** Prec@1 77.070 Prec@5 98.540 Error@1 22.930
