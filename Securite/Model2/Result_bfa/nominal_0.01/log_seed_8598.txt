save path : ./save/tinyvgg_quan/nominal_0.01
{'data_path': './dataset', 'arch': 'tinyvgg_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 0, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': False, 'clipping_coeff': 0.0, 'learning_rate': 0.01, 'manualSeed': 8598, 'save_path': './save/tinyvgg_quan/nominal_0.01', 'enable_bfa': False, 'resume': '', 'quan_bitwidth': None, 'reset_weight': False, 'evaluate': False, 'n_iter': 30, 'model_only': False, 'random_bfa': False, 'use_cuda': False}
Random Seed: 8598
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.5.1
cudnn  version : None
=> creating model 'tinyvgg_quan'
=> network :
 TinyVGG(
  (features): Sequential(
    (0): quan_Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Dropout2d(p=0.25, inplace=False)
    (6): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace=True)
    (8): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace=True)
    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (11): Dropout2d(p=0.25, inplace=False)
  )
  (classifier): Sequential(
    (0): quan_Linear(in_features=2048, out_features=10, bias=True)
  )
)
=> do not use any checkpoint for tinyvgg_quan model

==>>[2025-10-22 14:52:18] [Epoch=000/040] [Need: 00:00:00] [LR=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/500]   Time 21.421 (21.421)   Data 21.339 (21.339)   Loss 2.3000 (2.3000)   Prec@1 13.000 (13.000)   Prec@5 55.000 (55.000)   [2025-10-22 14:52:40]
  Epoch: [000][100/500]   Time 0.058 (0.269)   Data 0.000 (0.212)   Loss 1.9637 (2.2065)   Prec@1 32.000 (17.297)   Prec@5 78.000 (63.495)   [2025-10-22 14:52:45]
  Epoch: [000][200/500]   Time 0.059 (0.164)   Data 0.001 (0.107)   Loss 1.9868 (2.1025)   Prec@1 26.000 (22.239)   Prec@5 77.000 (71.184)   [2025-10-22 14:52:51]
  Epoch: [000][300/500]   Time 0.054 (0.128)   Data 0.001 (0.071)   Loss 1.8223 (2.0218)   Prec@1 39.000 (25.771)   Prec@5 81.000 (75.282)   [2025-10-22 14:52:57]
  Epoch: [000][400/500]   Time 0.054 (0.110)   Data 0.001 (0.054)   Loss 1.6068 (1.9562)   Prec@1 46.000 (28.282)   Prec@5 87.000 (78.065)   [2025-10-22 14:53:02]
  **Train** Prec@1 30.194 Prec@5 79.966 Error@1 69.806
  **Test** Prec@1 45.250 Prec@5 91.500 Error@1 54.750
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 14:53:31] [Epoch=001/040] [Need: 00:47:16] [LR=0.0100] [Best : Accuracy=45.25, Error=54.75]
  Epoch: [001][000/500]   Time 20.515 (20.515)   Data 20.437 (20.437)   Loss 1.7020 (1.7020)   Prec@1 34.000 (34.000)   Prec@5 92.000 (92.000)   [2025-10-22 14:53:51]
  Epoch: [001][100/500]   Time 0.054 (0.257)   Data 0.000 (0.203)   Loss 1.4661 (1.6573)   Prec@1 45.000 (39.614)   Prec@5 93.000 (88.970)   [2025-10-22 14:53:57]
  Epoch: [001][200/500]   Time 0.053 (0.156)   Data 0.001 (0.102)   Loss 1.6130 (1.6387)   Prec@1 44.000 (40.468)   Prec@5 90.000 (89.279)   [2025-10-22 14:54:02]
  Epoch: [001][300/500]   Time 0.053 (0.122)   Data 0.001 (0.068)   Loss 1.7334 (1.6250)   Prec@1 34.000 (40.857)   Prec@5 86.000 (89.482)   [2025-10-22 14:54:08]
  Epoch: [001][400/500]   Time 0.053 (0.105)   Data 0.000 (0.052)   Loss 1.4761 (1.6104)   Prec@1 50.000 (41.319)   Prec@5 93.000 (89.731)   [2025-10-22 14:54:13]
  **Train** Prec@1 41.988 Prec@5 89.970 Error@1 58.012
  **Test** Prec@1 53.260 Prec@5 94.390 Error@1 46.740
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 14:54:42] [Epoch=002/040] [Need: 00:45:22] [LR=0.0100] [Best : Accuracy=53.26, Error=46.74]
  Epoch: [002][000/500]   Time 20.936 (20.936)   Data 20.863 (20.863)   Loss 1.4063 (1.4063)   Prec@1 53.000 (53.000)   Prec@5 91.000 (91.000)   [2025-10-22 14:55:02]
  Epoch: [002][100/500]   Time 0.053 (0.261)   Data 0.001 (0.207)   Loss 1.5371 (1.5068)   Prec@1 46.000 (45.307)   Prec@5 92.000 (91.347)   [2025-10-22 14:55:08]
  Epoch: [002][200/500]   Time 0.053 (0.158)   Data 0.000 (0.104)   Loss 1.5166 (1.4905)   Prec@1 43.000 (46.075)   Prec@5 87.000 (91.408)   [2025-10-22 14:55:13]
  Epoch: [002][300/500]   Time 0.052 (0.123)   Data 0.001 (0.070)   Loss 1.5136 (1.4825)   Prec@1 45.000 (46.465)   Prec@5 90.000 (91.518)   [2025-10-22 14:55:19]
  Epoch: [002][400/500]   Time 0.055 (0.105)   Data 0.001 (0.053)   Loss 1.4913 (1.4720)   Prec@1 44.000 (46.748)   Prec@5 94.000 (91.663)   [2025-10-22 14:55:24]
  **Train** Prec@1 47.126 Prec@5 91.788 Error@1 52.874
  **Test** Prec@1 57.090 Prec@5 95.310 Error@1 42.910
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 14:55:53] [Epoch=003/040] [Need: 00:44:03] [LR=0.0100] [Best : Accuracy=57.09, Error=42.91]
  Epoch: [003][000/500]   Time 20.779 (20.779)   Data 20.697 (20.697)   Loss 1.6184 (1.6184)   Prec@1 45.000 (45.000)   Prec@5 93.000 (93.000)   [2025-10-22 14:56:13]
  Epoch: [003][100/500]   Time 0.054 (0.258)   Data 0.000 (0.205)   Loss 1.4113 (1.3966)   Prec@1 44.000 (49.653)   Prec@5 94.000 (92.970)   [2025-10-22 14:56:19]
  Epoch: [003][200/500]   Time 0.056 (0.156)   Data 0.001 (0.104)   Loss 1.3519 (1.3862)   Prec@1 50.000 (50.090)   Prec@5 91.000 (92.836)   [2025-10-22 14:56:24]
  Epoch: [003][300/500]   Time 0.054 (0.122)   Data 0.000 (0.069)   Loss 1.3304 (1.3760)   Prec@1 51.000 (50.608)   Prec@5 94.000 (92.854)   [2025-10-22 14:56:29]
  Epoch: [003][400/500]   Time 0.051 (0.105)   Data 0.000 (0.052)   Loss 1.3368 (1.3721)   Prec@1 48.000 (50.850)   Prec@5 93.000 (92.938)   [2025-10-22 14:56:35]
  **Train** Prec@1 51.150 Prec@5 93.050 Error@1 48.850
  **Test** Prec@1 61.910 Prec@5 96.170 Error@1 38.090
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 14:57:03] [Epoch=004/040] [Need: 00:42:43] [LR=0.0100] [Best : Accuracy=61.91, Error=38.09]
  Epoch: [004][000/500]   Time 20.746 (20.746)   Data 20.673 (20.673)   Loss 1.2558 (1.2558)   Prec@1 57.000 (57.000)   Prec@5 89.000 (89.000)   [2025-10-22 14:57:24]
  Epoch: [004][100/500]   Time 0.057 (0.263)   Data 0.002 (0.205)   Loss 1.3509 (1.3051)   Prec@1 54.000 (53.990)   Prec@5 92.000 (93.802)   [2025-10-22 14:57:30]
  Epoch: [004][200/500]   Time 0.405 (0.236)   Data 0.009 (0.104)   Loss 1.1774 (1.2984)   Prec@1 58.000 (54.020)   Prec@5 96.000 (93.881)   [2025-10-22 14:57:51]
  Epoch: [004][300/500]   Time 0.538 (0.307)   Data 0.002 (0.071)   Loss 1.3641 (1.2922)   Prec@1 57.000 (54.465)   Prec@5 92.000 (93.993)   [2025-10-22 14:58:36]
  Epoch: [004][400/500]   Time 0.381 (0.326)   Data 0.002 (0.054)   Loss 1.2291 (1.2815)   Prec@1 53.000 (54.663)   Prec@5 97.000 (94.132)   [2025-10-22 14:59:14]
  **Train** Prec@1 54.880 Prec@5 94.176 Error@1 45.120
  **Test** Prec@1 64.530 Prec@5 96.830 Error@1 35.470
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 15:00:31] [Epoch=005/040] [Need: 00:57:21] [LR=0.0100] [Best : Accuracy=64.53, Error=35.47]
  Epoch: [005][000/500]   Time 30.498 (30.498)   Data 30.018 (30.018)   Loss 1.1469 (1.1469)   Prec@1 56.000 (56.000)   Prec@5 96.000 (96.000)   [2025-10-22 15:01:01]
  Epoch: [005][100/500]   Time 0.470 (0.711)   Data 0.003 (0.301)   Loss 1.0523 (1.2341)   Prec@1 62.000 (56.030)   Prec@5 99.000 (94.406)   [2025-10-22 15:01:43]
  Epoch: [005][200/500]   Time 0.313 (0.539)   Data 0.002 (0.153)   Loss 1.0185 (1.2230)   Prec@1 59.000 (56.507)   Prec@5 97.000 (94.662)   [2025-10-22 15:02:19]
  Epoch: [005][300/500]   Time 0.365 (0.470)   Data 0.001 (0.103)   Loss 1.1223 (1.2193)   Prec@1 61.000 (56.615)   Prec@5 96.000 (94.728)   [2025-10-22 15:02:52]
  Epoch: [005][400/500]   Time 0.076 (0.382)   Data 0.001 (0.078)   Loss 1.2627 (1.2138)   Prec@1 53.000 (56.890)   Prec@5 95.000 (94.768)   [2025-10-22 15:03:04]
  **Train** Prec@1 57.166 Prec@5 94.806 Error@1 42.834
  **Test** Prec@1 67.480 Prec@5 97.230 Error@1 32.520
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 15:03:35] [Epoch=006/040] [Need: 01:03:53] [LR=0.0100] [Best : Accuracy=67.48, Error=32.52]
  Epoch: [006][000/500]   Time 21.740 (21.740)   Data 21.655 (21.655)   Loss 1.1344 (1.1344)   Prec@1 63.000 (63.000)   Prec@5 96.000 (96.000)   [2025-10-22 15:03:56]
  Epoch: [006][100/500]   Time 0.055 (0.269)   Data 0.001 (0.215)   Loss 1.2053 (1.1746)   Prec@1 57.000 (59.119)   Prec@5 97.000 (94.950)   [2025-10-22 15:04:02]
  Epoch: [006][200/500]   Time 0.053 (0.162)   Data 0.000 (0.108)   Loss 1.1259 (1.1684)   Prec@1 53.000 (58.701)   Prec@5 97.000 (95.010)   [2025-10-22 15:04:07]
  Epoch: [006][300/500]   Time 0.053 (0.126)   Data 0.001 (0.073)   Loss 1.0655 (1.1722)   Prec@1 58.000 (58.618)   Prec@5 97.000 (95.047)   [2025-10-22 15:04:13]
  Epoch: [006][400/500]   Time 0.058 (0.109)   Data 0.001 (0.055)   Loss 1.1847 (1.1657)   Prec@1 57.000 (58.713)   Prec@5 93.000 (95.165)   [2025-10-22 15:04:18]
  **Train** Prec@1 58.878 Prec@5 95.188 Error@1 41.122
  **Test** Prec@1 67.830 Prec@5 97.120 Error@1 32.170
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 15:05:22] [Epoch=007/040] [Need: 01:01:33] [LR=0.0100] [Best : Accuracy=67.83, Error=32.17]
  Epoch: [007][000/500]   Time 22.382 (22.382)   Data 22.300 (22.300)   Loss 1.0495 (1.0495)   Prec@1 61.000 (61.000)   Prec@5 99.000 (99.000)   [2025-10-22 15:05:44]
  Epoch: [007][100/500]   Time 0.062 (0.280)   Data 0.000 (0.221)   Loss 1.2904 (1.1337)   Prec@1 52.000 (59.822)   Prec@5 97.000 (95.614)   [2025-10-22 15:05:50]
  Epoch: [007][200/500]   Time 0.058 (0.170)   Data 0.000 (0.112)   Loss 1.2526 (1.1169)   Prec@1 60.000 (60.264)   Prec@5 95.000 (95.736)   [2025-10-22 15:05:56]
  Epoch: [007][300/500]   Time 0.056 (0.134)   Data 0.001 (0.075)   Loss 1.0858 (1.1205)   Prec@1 60.000 (60.103)   Prec@5 98.000 (95.601)   [2025-10-22 15:06:02]
  Epoch: [007][400/500]   Time 0.052 (0.114)   Data 0.000 (0.056)   Loss 1.0501 (1.1182)   Prec@1 60.000 (60.147)   Prec@5 96.000 (95.663)   [2025-10-22 15:06:08]
  **Train** Prec@1 60.260 Prec@5 95.694 Error@1 39.740
  **Test** Prec@1 69.220 Prec@5 97.650 Error@1 30.780
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 15:06:38] [Epoch=008/040] [Need: 00:57:17] [LR=0.0100] [Best : Accuracy=69.22, Error=30.78]
  Epoch: [008][000/500]   Time 20.855 (20.855)   Data 20.779 (20.779)   Loss 0.9828 (0.9828)   Prec@1 59.000 (59.000)   Prec@5 98.000 (98.000)   [2025-10-22 15:06:59]
  Epoch: [008][100/500]   Time 0.052 (0.260)   Data 0.000 (0.206)   Loss 1.2411 (1.0940)   Prec@1 49.000 (61.248)   Prec@5 95.000 (96.149)   [2025-10-22 15:07:04]
  Epoch: [008][200/500]   Time 0.055 (0.158)   Data 0.000 (0.104)   Loss 1.1928 (1.0923)   Prec@1 53.000 (61.537)   Prec@5 96.000 (95.891)   [2025-10-22 15:07:09]
  Epoch: [008][300/500]   Time 0.053 (0.123)   Data 0.000 (0.070)   Loss 1.1709 (1.0917)   Prec@1 60.000 (61.571)   Prec@5 97.000 (95.953)   [2025-10-22 15:07:15]
  Epoch: [008][400/500]   Time 0.054 (0.106)   Data 0.001 (0.052)   Loss 0.9761 (1.0948)   Prec@1 64.000 (61.444)   Prec@5 99.000 (95.895)   [2025-10-22 15:07:20]
  **Train** Prec@1 61.692 Prec@5 95.872 Error@1 38.308
  **Test** Prec@1 70.370 Prec@5 97.700 Error@1 29.630
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 15:07:49] [Epoch=009/040] [Need: 00:53:25] [LR=0.0100] [Best : Accuracy=70.37, Error=29.63]
  Epoch: [009][000/500]   Time 20.911 (20.911)   Data 20.838 (20.838)   Loss 1.0550 (1.0550)   Prec@1 63.000 (63.000)   Prec@5 94.000 (94.000)   [2025-10-22 15:08:10]
  Epoch: [009][100/500]   Time 0.054 (0.260)   Data 0.001 (0.207)   Loss 1.1388 (1.0779)   Prec@1 58.000 (62.079)   Prec@5 95.000 (95.881)   [2025-10-22 15:08:15]
  Epoch: [009][200/500]   Time 0.053 (0.157)   Data 0.000 (0.104)   Loss 0.7343 (1.0706)   Prec@1 74.000 (62.363)   Prec@5 100.000 (96.075)   [2025-10-22 15:08:20]
  Epoch: [009][300/500]   Time 0.055 (0.123)   Data 0.000 (0.070)   Loss 1.0580 (1.0638)   Prec@1 61.000 (62.618)   Prec@5 95.000 (96.100)   [2025-10-22 15:08:26]
  Epoch: [009][400/500]   Time 0.052 (0.106)   Data 0.001 (0.053)   Loss 1.1779 (1.0658)   Prec@1 57.000 (62.596)   Prec@5 94.000 (96.070)   [2025-10-22 15:08:31]
  **Train** Prec@1 62.362 Prec@5 96.124 Error@1 37.638
  **Test** Prec@1 70.070 Prec@5 97.560 Error@1 29.930

==>>[2025-10-22 15:09:00] [Epoch=010/040] [Need: 00:50:05] [LR=0.0100] [Best : Accuracy=70.37, Error=29.63]
  Epoch: [010][000/500]   Time 20.778 (20.778)   Data 20.699 (20.699)   Loss 1.0808 (1.0808)   Prec@1 60.000 (60.000)   Prec@5 97.000 (97.000)   [2025-10-22 15:09:21]
  Epoch: [010][100/500]   Time 0.052 (0.259)   Data 0.001 (0.205)   Loss 1.2432 (1.0554)   Prec@1 57.000 (62.594)   Prec@5 92.000 (96.198)   [2025-10-22 15:09:26]
  Epoch: [010][200/500]   Time 0.054 (0.157)   Data 0.000 (0.103)   Loss 1.0910 (1.0547)   Prec@1 63.000 (62.577)   Prec@5 94.000 (96.219)   [2025-10-22 15:09:31]
  Epoch: [010][300/500]   Time 0.053 (0.122)   Data 0.000 (0.069)   Loss 1.0583 (1.0543)   Prec@1 64.000 (62.625)   Prec@5 97.000 (96.206)   [2025-10-22 15:09:37]
  Epoch: [010][400/500]   Time 0.054 (0.106)   Data 0.000 (0.052)   Loss 0.9280 (1.0520)   Prec@1 62.000 (62.663)   Prec@5 99.000 (96.249)   [2025-10-22 15:09:42]
  **Train** Prec@1 62.586 Prec@5 96.174 Error@1 37.414
  **Test** Prec@1 70.850 Prec@5 97.730 Error@1 29.150
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 15:10:11] [Epoch=011/040] [Need: 00:47:08] [LR=0.0100] [Best : Accuracy=70.85, Error=29.15]
  Epoch: [011][000/500]   Time 20.949 (20.949)   Data 20.874 (20.874)   Loss 0.8127 (0.8127)   Prec@1 70.000 (70.000)   Prec@5 99.000 (99.000)   [2025-10-22 15:10:32]
  Epoch: [011][100/500]   Time 0.069 (0.264)   Data 0.001 (0.207)   Loss 1.1152 (1.0401)   Prec@1 62.000 (63.356)   Prec@5 93.000 (96.436)   [2025-10-22 15:10:38]
  Epoch: [011][200/500]   Time 0.066 (0.164)   Data 0.001 (0.104)   Loss 1.1959 (1.0405)   Prec@1 60.000 (63.239)   Prec@5 94.000 (96.418)   [2025-10-22 15:10:44]
  Epoch: [011][300/500]   Time 0.062 (0.131)   Data 0.001 (0.070)   Loss 1.0105 (1.0324)   Prec@1 69.000 (63.548)   Prec@5 95.000 (96.422)   [2025-10-22 15:10:50]
  Epoch: [011][400/500]   Time 0.065 (0.115)   Data 0.001 (0.053)   Loss 0.8890 (1.0335)   Prec@1 69.000 (63.494)   Prec@5 98.000 (96.379)   [2025-10-22 15:10:57]
  **Train** Prec@1 63.402 Prec@5 96.354 Error@1 36.598
  **Test** Prec@1 72.550 Prec@5 98.090 Error@1 27.450
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 15:11:27] [Epoch=012/040] [Need: 00:44:40] [LR=0.0100] [Best : Accuracy=72.55, Error=27.45]
  Epoch: [012][000/500]   Time 21.077 (21.077)   Data 21.008 (21.008)   Loss 0.9945 (0.9945)   Prec@1 66.000 (66.000)   Prec@5 98.000 (98.000)   [2025-10-22 15:11:48]
  Epoch: [012][100/500]   Time 0.061 (0.272)   Data 0.001 (0.209)   Loss 1.0838 (1.0238)   Prec@1 63.000 (64.574)   Prec@5 96.000 (96.782)   [2025-10-22 15:11:54]
  Epoch: [012][200/500]   Time 0.061 (0.169)   Data 0.001 (0.105)   Loss 0.9561 (1.0235)   Prec@1 70.000 (63.905)   Prec@5 98.000 (96.716)   [2025-10-22 15:12:01]
  Epoch: [012][300/500]   Time 0.070 (0.134)   Data 0.001 (0.070)   Loss 0.9757 (1.0197)   Prec@1 65.000 (64.000)   Prec@5 95.000 (96.635)   [2025-10-22 15:12:07]
  Epoch: [012][400/500]   Time 0.070 (0.117)   Data 0.000 (0.053)   Loss 1.0597 (1.0260)   Prec@1 63.000 (63.860)   Prec@5 97.000 (96.499)   [2025-10-22 15:12:14]
  **Train** Prec@1 63.912 Prec@5 96.420 Error@1 36.088
  **Test** Prec@1 72.240 Prec@5 97.880 Error@1 27.760

==>>[2025-10-22 15:12:44] [Epoch=013/040] [Need: 00:42:25] [LR=0.0100] [Best : Accuracy=72.55, Error=27.45]
  Epoch: [013][000/500]   Time 20.960 (20.960)   Data 20.869 (20.869)   Loss 1.1968 (1.1968)   Prec@1 58.000 (58.000)   Prec@5 93.000 (93.000)   [2025-10-22 15:13:05]
  Epoch: [013][100/500]   Time 0.069 (0.275)   Data 0.000 (0.207)   Loss 1.1865 (1.0104)   Prec@1 58.000 (64.634)   Prec@5 96.000 (96.248)   [2025-10-22 15:13:12]
  Epoch: [013][200/500]   Time 0.063 (0.172)   Data 0.001 (0.104)   Loss 0.9102 (1.0116)   Prec@1 69.000 (64.527)   Prec@5 99.000 (96.393)   [2025-10-22 15:13:18]
  Epoch: [013][300/500]   Time 0.067 (0.137)   Data 0.000 (0.070)   Loss 1.0836 (1.0116)   Prec@1 65.000 (64.395)   Prec@5 95.000 (96.415)   [2025-10-22 15:13:25]
  Epoch: [013][400/500]   Time 0.071 (0.120)   Data 0.001 (0.053)   Loss 0.9352 (1.0114)   Prec@1 70.000 (64.259)   Prec@5 96.000 (96.471)   [2025-10-22 15:13:32]
  **Train** Prec@1 64.442 Prec@5 96.462 Error@1 35.558
  **Test** Prec@1 72.990 Prec@5 98.140 Error@1 27.010
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 15:14:02] [Epoch=014/040] [Need: 00:40:22] [LR=0.0100] [Best : Accuracy=72.99, Error=27.01]
  Epoch: [014][000/500]   Time 21.384 (21.384)   Data 21.291 (21.291)   Loss 0.9463 (0.9463)   Prec@1 66.000 (66.000)   Prec@5 96.000 (96.000)   [2025-10-22 15:14:24]
  Epoch: [014][100/500]   Time 0.066 (0.276)   Data 0.001 (0.212)   Loss 1.0231 (1.0051)   Prec@1 65.000 (65.297)   Prec@5 98.000 (96.574)   [2025-10-22 15:14:30]
  Epoch: [014][200/500]   Time 0.061 (0.171)   Data 0.001 (0.107)   Loss 1.2522 (1.0025)   Prec@1 53.000 (65.030)   Prec@5 95.000 (96.502)   [2025-10-22 15:14:37]
  Epoch: [014][300/500]   Time 0.064 (0.136)   Data 0.000 (0.071)   Loss 0.8987 (1.0055)   Prec@1 68.000 (65.000)   Prec@5 94.000 (96.462)   [2025-10-22 15:14:43]
  Epoch: [014][400/500]   Time 0.061 (0.118)   Data 0.000 (0.054)   Loss 0.8965 (1.0032)   Prec@1 68.000 (65.097)   Prec@5 96.000 (96.494)   [2025-10-22 15:14:50]
  **Train** Prec@1 65.112 Prec@5 96.540 Error@1 34.888
  **Test** Prec@1 72.860 Prec@5 97.790 Error@1 27.140

==>>[2025-10-22 15:15:20] [Epoch=015/040] [Need: 00:38:23] [LR=0.0100] [Best : Accuracy=72.99, Error=27.01]
  Epoch: [015][000/500]   Time 21.265 (21.265)   Data 21.183 (21.183)   Loss 0.8964 (0.8964)   Prec@1 67.000 (67.000)   Prec@5 97.000 (97.000)   [2025-10-22 15:15:41]
  Epoch: [015][100/500]   Time 0.070 (0.275)   Data 0.001 (0.210)   Loss 0.7974 (0.9991)   Prec@1 74.000 (64.644)   Prec@5 100.000 (96.604)   [2025-10-22 15:15:48]
  Epoch: [015][200/500]   Time 0.062 (0.171)   Data 0.001 (0.106)   Loss 0.8855 (0.9936)   Prec@1 68.000 (65.055)   Prec@5 99.000 (96.687)   [2025-10-22 15:15:55]
  Epoch: [015][300/500]   Time 0.062 (0.136)   Data 0.001 (0.071)   Loss 1.1494 (0.9939)   Prec@1 61.000 (64.987)   Prec@5 98.000 (96.691)   [2025-10-22 15:16:01]
  Epoch: [015][400/500]   Time 0.075 (0.119)   Data 0.001 (0.053)   Loss 1.0126 (0.9912)   Prec@1 69.000 (65.072)   Prec@5 96.000 (96.693)   [2025-10-22 15:16:08]
  **Train** Prec@1 65.114 Prec@5 96.666 Error@1 34.886
  **Test** Prec@1 73.390 Prec@5 98.160 Error@1 26.610
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 15:16:38] [Epoch=016/040] [Need: 00:36:29] [LR=0.0100] [Best : Accuracy=73.39, Error=26.61]
  Epoch: [016][000/500]   Time 20.810 (20.810)   Data 20.731 (20.731)   Loss 0.9694 (0.9694)   Prec@1 64.000 (64.000)   Prec@5 94.000 (94.000)   [2025-10-22 15:16:59]
  Epoch: [016][100/500]   Time 0.067 (0.270)   Data 0.001 (0.206)   Loss 0.9812 (0.9736)   Prec@1 68.000 (65.960)   Prec@5 99.000 (96.733)   [2025-10-22 15:17:05]
  Epoch: [016][200/500]   Time 0.064 (0.167)   Data 0.000 (0.104)   Loss 0.9699 (0.9856)   Prec@1 64.000 (65.448)   Prec@5 95.000 (96.677)   [2025-10-22 15:17:11]
  Epoch: [016][300/500]   Time 0.061 (0.133)   Data 0.001 (0.070)   Loss 1.0926 (0.9809)   Prec@1 66.000 (65.741)   Prec@5 94.000 (96.681)   [2025-10-22 15:17:18]
  Epoch: [016][400/500]   Time 0.064 (0.116)   Data 0.001 (0.052)   Loss 0.7769 (0.9817)   Prec@1 70.000 (65.631)   Prec@5 98.000 (96.703)   [2025-10-22 15:17:24]
  **Train** Prec@1 65.638 Prec@5 96.728 Error@1 34.362
  **Test** Prec@1 73.860 Prec@5 98.110 Error@1 26.140
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 15:17:54] [Epoch=017/040] [Need: 00:34:37] [LR=0.0100] [Best : Accuracy=73.86, Error=26.14]
  Epoch: [017][000/500]   Time 20.780 (20.780)   Data 20.695 (20.695)   Loss 0.8559 (0.8559)   Prec@1 69.000 (69.000)   Prec@5 98.000 (98.000)   [2025-10-22 15:18:15]
  Epoch: [017][100/500]   Time 0.068 (0.269)   Data 0.001 (0.205)   Loss 0.9754 (0.9740)   Prec@1 68.000 (66.208)   Prec@5 98.000 (96.822)   [2025-10-22 15:18:21]
  Epoch: [017][200/500]   Time 0.066 (0.166)   Data 0.001 (0.104)   Loss 0.8020 (0.9684)   Prec@1 72.000 (66.294)   Prec@5 99.000 (96.841)   [2025-10-22 15:18:27]
  Epoch: [017][300/500]   Time 0.061 (0.132)   Data 0.001 (0.069)   Loss 1.0443 (0.9696)   Prec@1 64.000 (66.060)   Prec@5 96.000 (96.874)   [2025-10-22 15:18:34]
  Epoch: [017][400/500]   Time 0.062 (0.116)   Data 0.000 (0.052)   Loss 1.0988 (0.9739)   Prec@1 64.000 (65.833)   Prec@5 94.000 (96.833)   [2025-10-22 15:18:40]
  **Train** Prec@1 65.796 Prec@5 96.834 Error@1 34.204
  **Test** Prec@1 74.380 Prec@5 97.930 Error@1 25.620
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 15:19:10] [Epoch=018/040] [Need: 00:32:50] [LR=0.0100] [Best : Accuracy=74.38, Error=25.62]
  Epoch: [018][000/500]   Time 21.116 (21.116)   Data 21.034 (21.034)   Loss 1.1837 (1.1837)   Prec@1 56.000 (56.000)   Prec@5 95.000 (95.000)   [2025-10-22 15:19:31]
  Epoch: [018][100/500]   Time 0.061 (0.273)   Data 0.000 (0.209)   Loss 1.2531 (0.9906)   Prec@1 63.000 (65.376)   Prec@5 97.000 (96.505)   [2025-10-22 15:19:38]
  Epoch: [018][200/500]   Time 0.066 (0.169)   Data 0.001 (0.105)   Loss 0.8781 (0.9830)   Prec@1 70.000 (65.662)   Prec@5 97.000 (96.642)   [2025-10-22 15:19:44]
  Epoch: [018][300/500]   Time 0.065 (0.134)   Data 0.001 (0.071)   Loss 1.0884 (0.9786)   Prec@1 64.000 (65.668)   Prec@5 95.000 (96.694)   [2025-10-22 15:19:51]
  Epoch: [018][400/500]   Time 0.063 (0.117)   Data 0.001 (0.053)   Loss 0.9145 (0.9678)   Prec@1 71.000 (65.880)   Prec@5 95.000 (96.823)   [2025-10-22 15:19:57]
  **Train** Prec@1 66.014 Prec@5 96.818 Error@1 33.986
  **Test** Prec@1 74.840 Prec@5 98.210 Error@1 25.160
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 15:20:27] [Epoch=019/040] [Need: 00:31:06] [LR=0.0100] [Best : Accuracy=74.84, Error=25.16]
  Epoch: [019][000/500]   Time 21.024 (21.024)   Data 20.942 (20.942)   Loss 1.0037 (1.0037)   Prec@1 67.000 (67.000)   Prec@5 97.000 (97.000)   [2025-10-22 15:20:48]
  Epoch: [019][100/500]   Time 0.069 (0.272)   Data 0.001 (0.208)   Loss 0.9625 (0.9637)   Prec@1 63.000 (66.287)   Prec@5 98.000 (96.624)   [2025-10-22 15:20:54]
  Epoch: [019][200/500]   Time 0.066 (0.169)   Data 0.001 (0.105)   Loss 0.9467 (0.9581)   Prec@1 67.000 (66.617)   Prec@5 99.000 (96.746)   [2025-10-22 15:21:01]
  Epoch: [019][300/500]   Time 0.069 (0.135)   Data 0.001 (0.070)   Loss 0.9551 (0.9622)   Prec@1 70.000 (66.306)   Prec@5 96.000 (96.738)   [2025-10-22 15:21:08]
  Epoch: [019][400/500]   Time 0.065 (0.118)   Data 0.001 (0.053)   Loss 0.8890 (0.9641)   Prec@1 70.000 (66.147)   Prec@5 97.000 (96.773)   [2025-10-22 15:21:14]
  **Train** Prec@1 66.192 Prec@5 96.770 Error@1 33.808
  **Test** Prec@1 74.370 Prec@5 98.370 Error@1 25.630

==>>[2025-10-22 15:21:45] [Epoch=020/040] [Need: 00:29:26] [LR=0.0100] [Best : Accuracy=74.84, Error=25.16]
  Epoch: [020][000/500]   Time 24.082 (24.082)   Data 24.009 (24.009)   Loss 1.0912 (1.0912)   Prec@1 62.000 (62.000)   Prec@5 94.000 (94.000)   [2025-10-22 15:22:09]
  Epoch: [020][100/500]   Time 0.063 (0.314)   Data 0.000 (0.239)   Loss 1.1872 (0.9648)   Prec@1 65.000 (66.198)   Prec@5 92.000 (96.842)   [2025-10-22 15:22:17]
  Epoch: [020][200/500]   Time 0.062 (0.191)   Data 0.001 (0.120)   Loss 1.0036 (0.9617)   Prec@1 64.000 (66.413)   Prec@5 97.000 (96.945)   [2025-10-22 15:22:23]
  Epoch: [020][300/500]   Time 0.063 (0.149)   Data 0.001 (0.081)   Loss 1.2209 (0.9620)   Prec@1 62.000 (66.439)   Prec@5 94.000 (96.927)   [2025-10-22 15:22:30]
  Epoch: [020][400/500]   Time 0.066 (0.128)   Data 0.001 (0.061)   Loss 1.1211 (0.9567)   Prec@1 60.000 (66.494)   Prec@5 95.000 (96.948)   [2025-10-22 15:22:36]
  **Train** Prec@1 66.584 Prec@5 96.900 Error@1 33.416
  **Test** Prec@1 75.120 Prec@5 98.180 Error@1 24.880
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 15:23:06] [Epoch=021/040] [Need: 00:27:51] [LR=0.0100] [Best : Accuracy=75.12, Error=24.88]
  Epoch: [021][000/500]   Time 21.024 (21.024)   Data 20.937 (20.937)   Loss 1.1020 (1.1020)   Prec@1 61.000 (61.000)   Prec@5 96.000 (96.000)   [2025-10-22 15:23:27]
  Epoch: [021][100/500]   Time 0.067 (0.274)   Data 0.001 (0.208)   Loss 0.9750 (0.9534)   Prec@1 63.000 (66.168)   Prec@5 98.000 (96.901)   [2025-10-22 15:23:34]
  Epoch: [021][200/500]   Time 0.066 (0.171)   Data 0.001 (0.105)   Loss 1.0113 (0.9518)   Prec@1 65.000 (66.418)   Prec@5 97.000 (97.040)   [2025-10-22 15:23:41]
  Epoch: [021][300/500]   Time 0.063 (0.136)   Data 0.001 (0.070)   Loss 0.8349 (0.9538)   Prec@1 69.000 (66.233)   Prec@5 100.000 (97.007)   [2025-10-22 15:23:47]
  Epoch: [021][400/500]   Time 0.070 (0.119)   Data 0.001 (0.053)   Loss 0.8773 (0.9479)   Prec@1 71.000 (66.731)   Prec@5 100.000 (97.052)   [2025-10-22 15:23:54]
  **Train** Prec@1 66.714 Prec@5 97.044 Error@1 33.286
  **Test** Prec@1 74.770 Prec@5 98.380 Error@1 25.230

==>>[2025-10-22 15:24:23] [Epoch=022/040] [Need: 00:26:15] [LR=0.0100] [Best : Accuracy=75.12, Error=24.88]
  Epoch: [022][000/500]   Time 20.917 (20.917)   Data 20.832 (20.832)   Loss 1.0430 (1.0430)   Prec@1 65.000 (65.000)   Prec@5 98.000 (98.000)   [2025-10-22 15:24:44]
  Epoch: [022][100/500]   Time 0.063 (0.273)   Data 0.000 (0.207)   Loss 1.1570 (0.9244)   Prec@1 65.000 (67.733)   Prec@5 95.000 (97.129)   [2025-10-22 15:24:51]
  Epoch: [022][200/500]   Time 0.069 (0.169)   Data 0.001 (0.104)   Loss 1.0024 (0.9390)   Prec@1 65.000 (67.353)   Prec@5 98.000 (96.985)   [2025-10-22 15:24:57]
  Epoch: [022][300/500]   Time 0.069 (0.135)   Data 0.000 (0.070)   Loss 0.9865 (0.9366)   Prec@1 68.000 (67.429)   Prec@5 96.000 (97.037)   [2025-10-22 15:25:04]
  Epoch: [022][400/500]   Time 0.062 (0.117)   Data 0.000 (0.053)   Loss 0.7728 (0.9400)   Prec@1 79.000 (67.449)   Prec@5 98.000 (97.022)   [2025-10-22 15:25:11]
  **Train** Prec@1 67.434 Prec@5 97.050 Error@1 32.566
  **Test** Prec@1 75.340 Prec@5 98.440 Error@1 24.660
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 15:25:41] [Epoch=023/040] [Need: 00:24:39] [LR=0.0100] [Best : Accuracy=75.34, Error=24.66]
  Epoch: [023][000/500]   Time 20.647 (20.647)   Data 20.562 (20.562)   Loss 0.9186 (0.9186)   Prec@1 67.000 (67.000)   Prec@5 95.000 (95.000)   [2025-10-22 15:26:01]
  Epoch: [023][100/500]   Time 0.074 (0.271)   Data 0.001 (0.204)   Loss 0.6627 (0.9548)   Prec@1 78.000 (66.356)   Prec@5 97.000 (96.960)   [2025-10-22 15:26:08]
  Epoch: [023][200/500]   Time 0.063 (0.169)   Data 0.001 (0.103)   Loss 0.9692 (0.9390)   Prec@1 71.000 (67.219)   Prec@5 96.000 (96.940)   [2025-10-22 15:26:14]
  Epoch: [023][300/500]   Time 0.067 (0.135)   Data 0.001 (0.069)   Loss 0.9055 (0.9385)   Prec@1 70.000 (67.219)   Prec@5 98.000 (96.997)   [2025-10-22 15:26:21]
  Epoch: [023][400/500]   Time 0.063 (0.118)   Data 0.000 (0.052)   Loss 0.9444 (0.9324)   Prec@1 69.000 (67.399)   Prec@5 97.000 (97.040)   [2025-10-22 15:26:28]
  **Train** Prec@1 67.322 Prec@5 97.064 Error@1 32.678
  **Test** Prec@1 75.600 Prec@5 98.330 Error@1 24.400
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 15:26:58] [Epoch=024/040] [Need: 00:23:06] [LR=0.0100] [Best : Accuracy=75.60, Error=24.40]
  Epoch: [024][000/500]   Time 20.454 (20.454)   Data 20.366 (20.366)   Loss 0.7590 (0.7590)   Prec@1 74.000 (74.000)   Prec@5 100.000 (100.000)   [2025-10-22 15:27:18]
  Epoch: [024][100/500]   Time 0.066 (0.268)   Data 0.001 (0.202)   Loss 1.1079 (0.9406)   Prec@1 60.000 (67.426)   Prec@5 92.000 (97.020)   [2025-10-22 15:27:25]
  Epoch: [024][200/500]   Time 0.067 (0.168)   Data 0.001 (0.102)   Loss 0.9823 (0.9417)   Prec@1 62.000 (67.373)   Prec@5 97.000 (97.030)   [2025-10-22 15:27:32]
  Epoch: [024][300/500]   Time 0.064 (0.135)   Data 0.001 (0.068)   Loss 0.8675 (0.9354)   Prec@1 67.000 (67.292)   Prec@5 97.000 (97.126)   [2025-10-22 15:27:38]
  Epoch: [024][400/500]   Time 0.064 (0.117)   Data 0.001 (0.051)   Loss 1.0019 (0.9305)   Prec@1 66.000 (67.419)   Prec@5 96.000 (97.165)   [2025-10-22 15:27:45]
  **Train** Prec@1 67.430 Prec@5 97.178 Error@1 32.570
  **Test** Prec@1 74.870 Prec@5 98.160 Error@1 25.130

==>>[2025-10-22 15:28:15] [Epoch=025/040] [Need: 00:21:33] [LR=0.0010] [Best : Accuracy=75.60, Error=24.40]
  Epoch: [025][000/500]   Time 20.657 (20.657)   Data 20.575 (20.575)   Loss 1.0256 (1.0256)   Prec@1 60.000 (60.000)   Prec@5 97.000 (97.000)   [2025-10-22 15:28:35]
  Epoch: [025][100/500]   Time 0.070 (0.271)   Data 0.001 (0.204)   Loss 0.8164 (0.8896)   Prec@1 67.000 (68.723)   Prec@5 97.000 (97.386)   [2025-10-22 15:28:42]
  Epoch: [025][200/500]   Time 0.067 (0.169)   Data 0.001 (0.103)   Loss 0.7748 (0.8737)   Prec@1 73.000 (69.199)   Prec@5 99.000 (97.577)   [2025-10-22 15:28:49]
  Epoch: [025][300/500]   Time 0.062 (0.135)   Data 0.001 (0.069)   Loss 1.1152 (0.8727)   Prec@1 56.000 (69.415)   Prec@5 96.000 (97.468)   [2025-10-22 15:28:55]
  Epoch: [025][400/500]   Time 0.068 (0.117)   Data 0.001 (0.052)   Loss 0.8776 (0.8743)   Prec@1 68.000 (69.389)   Prec@5 98.000 (97.459)   [2025-10-22 15:29:02]
  **Train** Prec@1 69.404 Prec@5 97.478 Error@1 30.596
  **Test** Prec@1 76.580 Prec@5 98.410 Error@1 23.420
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 15:29:32] [Epoch=026/040] [Need: 00:20:02] [LR=0.0010] [Best : Accuracy=76.58, Error=23.42]
  Epoch: [026][000/500]   Time 20.865 (20.865)   Data 20.778 (20.778)   Loss 0.9241 (0.9241)   Prec@1 67.000 (67.000)   Prec@5 98.000 (98.000)   [2025-10-22 15:29:53]
  Epoch: [026][100/500]   Time 0.064 (0.274)   Data 0.001 (0.206)   Loss 0.9514 (0.8415)   Prec@1 64.000 (70.347)   Prec@5 98.000 (97.574)   [2025-10-22 15:29:59]
  Epoch: [026][200/500]   Time 0.065 (0.170)   Data 0.001 (0.104)   Loss 0.9270 (0.8468)   Prec@1 65.000 (70.338)   Prec@5 97.000 (97.672)   [2025-10-22 15:30:06]
  Epoch: [026][300/500]   Time 0.063 (0.135)   Data 0.001 (0.070)   Loss 0.9976 (0.8541)   Prec@1 66.000 (70.023)   Prec@5 93.000 (97.605)   [2025-10-22 15:30:12]
  Epoch: [026][400/500]   Time 0.063 (0.118)   Data 0.001 (0.053)   Loss 0.7294 (0.8515)   Prec@1 71.000 (70.249)   Prec@5 100.000 (97.581)   [2025-10-22 15:30:19]
  **Train** Prec@1 70.270 Prec@5 97.560 Error@1 29.730
  **Test** Prec@1 76.850 Prec@5 98.560 Error@1 23.150
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 15:30:49] [Epoch=027/040] [Need: 00:18:32] [LR=0.0010] [Best : Accuracy=76.85, Error=23.15]
  Epoch: [027][000/500]   Time 20.880 (20.880)   Data 20.788 (20.788)   Loss 0.7450 (0.7450)   Prec@1 79.000 (79.000)   Prec@5 98.000 (98.000)   [2025-10-22 15:31:10]
  Epoch: [027][100/500]   Time 0.072 (0.273)   Data 0.000 (0.207)   Loss 0.8721 (0.8455)   Prec@1 70.000 (70.455)   Prec@5 99.000 (97.485)   [2025-10-22 15:31:16]
  Epoch: [027][200/500]   Time 0.069 (0.171)   Data 0.001 (0.104)   Loss 0.6994 (0.8523)   Prec@1 70.000 (70.174)   Prec@5 99.000 (97.612)   [2025-10-22 15:31:23]
  Epoch: [027][300/500]   Time 0.069 (0.137)   Data 0.000 (0.070)   Loss 0.6718 (0.8507)   Prec@1 80.000 (70.319)   Prec@5 99.000 (97.571)   [2025-10-22 15:31:30]
  Epoch: [027][400/500]   Time 0.068 (0.120)   Data 0.001 (0.053)   Loss 0.7661 (0.8463)   Prec@1 74.000 (70.571)   Prec@5 98.000 (97.531)   [2025-10-22 15:31:37]
  **Train** Prec@1 70.464 Prec@5 97.604 Error@1 29.536
  **Test** Prec@1 77.020 Prec@5 98.500 Error@1 22.980
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 15:32:07] [Epoch=028/040] [Need: 00:17:03] [LR=0.0010] [Best : Accuracy=77.02, Error=22.98]
  Epoch: [028][000/500]   Time 20.632 (20.632)   Data 20.557 (20.557)   Loss 0.9730 (0.9730)   Prec@1 68.000 (68.000)   Prec@5 94.000 (94.000)   [2025-10-22 15:32:28]
  Epoch: [028][100/500]   Time 0.078 (0.270)   Data 0.001 (0.204)   Loss 0.7497 (0.8447)   Prec@1 75.000 (70.307)   Prec@5 99.000 (97.624)   [2025-10-22 15:32:34]
  Epoch: [028][200/500]   Time 0.067 (0.169)   Data 0.001 (0.103)   Loss 1.0756 (0.8524)   Prec@1 63.000 (70.080)   Prec@5 97.000 (97.657)   [2025-10-22 15:32:41]
  Epoch: [028][300/500]   Time 0.061 (0.134)   Data 0.001 (0.069)   Loss 0.6462 (0.8410)   Prec@1 80.000 (70.611)   Prec@5 99.000 (97.631)   [2025-10-22 15:32:48]
  Epoch: [028][400/500]   Time 0.067 (0.117)   Data 0.001 (0.052)   Loss 0.9935 (0.8408)   Prec@1 62.000 (70.626)   Prec@5 99.000 (97.681)   [2025-10-22 15:32:54]
  **Train** Prec@1 70.700 Prec@5 97.680 Error@1 29.300
  **Test** Prec@1 76.720 Prec@5 98.430 Error@1 23.280

==>>[2025-10-22 15:33:24] [Epoch=029/040] [Need: 00:15:35] [LR=0.0010] [Best : Accuracy=77.02, Error=22.98]
  Epoch: [029][000/500]   Time 20.734 (20.734)   Data 20.655 (20.655)   Loss 0.8022 (0.8022)   Prec@1 72.000 (72.000)   Prec@5 98.000 (98.000)   [2025-10-22 15:33:45]
  Epoch: [029][100/500]   Time 0.059 (0.271)   Data 0.000 (0.205)   Loss 0.7956 (0.8263)   Prec@1 70.000 (70.832)   Prec@5 98.000 (97.653)   [2025-10-22 15:33:52]
  Epoch: [029][200/500]   Time 0.068 (0.169)   Data 0.001 (0.103)   Loss 0.8700 (0.8359)   Prec@1 70.000 (70.791)   Prec@5 98.000 (97.771)   [2025-10-22 15:33:58]
  Epoch: [029][300/500]   Time 0.065 (0.135)   Data 0.000 (0.069)   Loss 0.9010 (0.8350)   Prec@1 69.000 (70.847)   Prec@5 97.000 (97.731)   [2025-10-22 15:34:05]
  Epoch: [029][400/500]   Time 0.067 (0.118)   Data 0.000 (0.052)   Loss 0.7863 (0.8342)   Prec@1 67.000 (70.820)   Prec@5 97.000 (97.776)   [2025-10-22 15:34:12]
  **Train** Prec@1 70.840 Prec@5 97.786 Error@1 29.160
  **Test** Prec@1 77.130 Prec@5 98.460 Error@1 22.870
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 15:34:42] [Epoch=030/040] [Need: 00:14:07] [LR=0.0010] [Best : Accuracy=77.13, Error=22.87]
  Epoch: [030][000/500]   Time 20.874 (20.874)   Data 20.792 (20.792)   Loss 0.7972 (0.7972)   Prec@1 72.000 (72.000)   Prec@5 98.000 (98.000)   [2025-10-22 15:35:03]
  Epoch: [030][100/500]   Time 0.064 (0.270)   Data 0.001 (0.206)   Loss 0.8605 (0.8301)   Prec@1 68.000 (70.752)   Prec@5 96.000 (97.683)   [2025-10-22 15:35:09]
  Epoch: [030][200/500]   Time 0.067 (0.168)   Data 0.000 (0.104)   Loss 0.7080 (0.8240)   Prec@1 74.000 (70.940)   Prec@5 99.000 (97.801)   [2025-10-22 15:35:15]
  Epoch: [030][300/500]   Time 0.071 (0.134)   Data 0.001 (0.070)   Loss 0.7183 (0.8313)   Prec@1 75.000 (70.854)   Prec@5 99.000 (97.751)   [2025-10-22 15:35:22]
  Epoch: [030][400/500]   Time 0.061 (0.117)   Data 0.001 (0.053)   Loss 0.6455 (0.8314)   Prec@1 76.000 (70.830)   Prec@5 99.000 (97.736)   [2025-10-22 15:35:29]
  **Train** Prec@1 70.976 Prec@5 97.770 Error@1 29.024
  **Test** Prec@1 77.060 Prec@5 98.540 Error@1 22.940

==>>[2025-10-22 15:35:59] [Epoch=031/040] [Need: 00:12:40] [LR=0.0010] [Best : Accuracy=77.13, Error=22.87]
  Epoch: [031][000/500]   Time 20.802 (20.802)   Data 20.727 (20.727)   Loss 0.6738 (0.6738)   Prec@1 79.000 (79.000)   Prec@5 97.000 (97.000)   [2025-10-22 15:36:19]
  Epoch: [031][100/500]   Time 0.066 (0.271)   Data 0.001 (0.206)   Loss 0.6664 (0.8180)   Prec@1 75.000 (71.871)   Prec@5 97.000 (97.822)   [2025-10-22 15:36:26]
  Epoch: [031][200/500]   Time 0.064 (0.168)   Data 0.001 (0.104)   Loss 0.9156 (0.8314)   Prec@1 71.000 (71.338)   Prec@5 99.000 (97.776)   [2025-10-22 15:36:32]
  Epoch: [031][300/500]   Time 0.061 (0.134)   Data 0.001 (0.070)   Loss 0.8381 (0.8382)   Prec@1 75.000 (70.960)   Prec@5 99.000 (97.748)   [2025-10-22 15:36:39]
  Epoch: [031][400/500]   Time 0.062 (0.116)   Data 0.001 (0.052)   Loss 0.8413 (0.8315)   Prec@1 71.000 (71.162)   Prec@5 95.000 (97.798)   [2025-10-22 15:36:45]
  **Train** Prec@1 71.170 Prec@5 97.812 Error@1 28.830
  **Test** Prec@1 77.200 Prec@5 98.560 Error@1 22.800
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 15:37:15] [Epoch=032/040] [Need: 00:11:14] [LR=0.0010] [Best : Accuracy=77.20, Error=22.80]
  Epoch: [032][000/500]   Time 20.472 (20.472)   Data 20.387 (20.387)   Loss 0.7572 (0.7572)   Prec@1 75.000 (75.000)   Prec@5 97.000 (97.000)   [2025-10-22 15:37:36]
  Epoch: [032][100/500]   Time 0.068 (0.268)   Data 0.001 (0.202)   Loss 0.9042 (0.8148)   Prec@1 67.000 (71.228)   Prec@5 98.000 (98.050)   [2025-10-22 15:37:42]
  Epoch: [032][200/500]   Time 0.064 (0.167)   Data 0.001 (0.102)   Loss 0.8400 (0.8229)   Prec@1 69.000 (71.214)   Prec@5 96.000 (97.736)   [2025-10-22 15:37:49]
  Epoch: [032][300/500]   Time 0.071 (0.133)   Data 0.001 (0.068)   Loss 0.7036 (0.8273)   Prec@1 74.000 (71.073)   Prec@5 98.000 (97.701)   [2025-10-22 15:37:55]
  Epoch: [032][400/500]   Time 0.066 (0.117)   Data 0.001 (0.052)   Loss 0.8003 (0.8247)   Prec@1 73.000 (71.229)   Prec@5 97.000 (97.723)   [2025-10-22 15:38:02]
  **Train** Prec@1 71.164 Prec@5 97.692 Error@1 28.836
  **Test** Prec@1 77.110 Prec@5 98.490 Error@1 22.890

==>>[2025-10-22 15:38:32] [Epoch=033/040] [Need: 00:09:48] [LR=0.0010] [Best : Accuracy=77.20, Error=22.80]
  Epoch: [033][000/500]   Time 20.658 (20.658)   Data 20.585 (20.585)   Loss 0.7479 (0.7479)   Prec@1 73.000 (73.000)   Prec@5 100.000 (100.000)   [2025-10-22 15:38:53]
  Epoch: [033][100/500]   Time 0.072 (0.273)   Data 0.001 (0.205)   Loss 0.7456 (0.8113)   Prec@1 73.000 (71.545)   Prec@5 100.000 (97.990)   [2025-10-22 15:39:00]
  Epoch: [033][200/500]   Time 0.068 (0.172)   Data 0.001 (0.103)   Loss 0.7447 (0.8198)   Prec@1 72.000 (71.527)   Prec@5 97.000 (97.881)   [2025-10-22 15:39:07]
  Epoch: [033][300/500]   Time 0.070 (0.137)   Data 0.001 (0.069)   Loss 1.0487 (0.8246)   Prec@1 65.000 (71.379)   Prec@5 97.000 (97.867)   [2025-10-22 15:39:14]
  Epoch: [033][400/500]   Time 0.064 (0.120)   Data 0.001 (0.052)   Loss 0.6974 (0.8243)   Prec@1 76.000 (71.471)   Prec@5 98.000 (97.813)   [2025-10-22 15:39:20]
  **Train** Prec@1 71.394 Prec@5 97.870 Error@1 28.606
  **Test** Prec@1 77.590 Prec@5 98.580 Error@1 22.410
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 15:39:50] [Epoch=034/040] [Need: 00:08:23] [LR=0.0010] [Best : Accuracy=77.59, Error=22.41]
  Epoch: [034][000/500]   Time 22.840 (22.840)   Data 22.744 (22.744)   Loss 0.7604 (0.7604)   Prec@1 71.000 (71.000)   Prec@5 98.000 (98.000)   [2025-10-22 15:40:13]
  Epoch: [034][100/500]   Time 0.063 (0.294)   Data 0.000 (0.226)   Loss 0.9255 (0.8129)   Prec@1 72.000 (71.604)   Prec@5 96.000 (97.693)   [2025-10-22 15:40:20]
  Epoch: [034][200/500]   Time 0.066 (0.181)   Data 0.001 (0.114)   Loss 0.5990 (0.8167)   Prec@1 79.000 (71.766)   Prec@5 99.000 (97.692)   [2025-10-22 15:40:27]
  Epoch: [034][300/500]   Time 0.065 (0.144)   Data 0.001 (0.076)   Loss 0.8021 (0.8249)   Prec@1 70.000 (71.385)   Prec@5 100.000 (97.724)   [2025-10-22 15:40:34]
  Epoch: [034][400/500]   Time 0.069 (0.126)   Data 0.001 (0.058)   Loss 0.7662 (0.8222)   Prec@1 75.000 (71.389)   Prec@5 99.000 (97.748)   [2025-10-22 15:40:41]
  **Train** Prec@1 71.210 Prec@5 97.748 Error@1 28.790
  **Test** Prec@1 77.580 Prec@5 98.590 Error@1 22.420

==>>[2025-10-22 15:41:38] [Epoch=035/040] [Need: 00:07:02] [LR=0.0010] [Best : Accuracy=77.59, Error=22.41]
  Epoch: [035][000/500]   Time 27.098 (27.098)   Data 26.691 (26.691)   Loss 0.9004 (0.9004)   Prec@1 71.000 (71.000)   Prec@5 98.000 (98.000)   [2025-10-22 15:42:05]
  Epoch: [035][100/500]   Time 0.056 (0.491)   Data 0.001 (0.267)   Loss 0.7673 (0.8173)   Prec@1 72.000 (71.475)   Prec@5 98.000 (97.752)   [2025-10-22 15:42:28]
  Epoch: [035][200/500]   Time 0.054 (0.273)   Data 0.001 (0.134)   Loss 0.7723 (0.8215)   Prec@1 74.000 (71.204)   Prec@5 98.000 (97.816)   [2025-10-22 15:42:33]
  Epoch: [035][300/500]   Time 0.056 (0.201)   Data 0.000 (0.090)   Loss 1.0936 (0.8231)   Prec@1 65.000 (71.166)   Prec@5 94.000 (97.787)   [2025-10-22 15:42:38]
  Epoch: [035][400/500]   Time 0.055 (0.165)   Data 0.000 (0.068)   Loss 0.7952 (0.8239)   Prec@1 72.000 (71.162)   Prec@5 97.000 (97.773)   [2025-10-22 15:42:44]
  **Train** Prec@1 70.994 Prec@5 97.768 Error@1 29.006
  **Test** Prec@1 77.330 Prec@5 98.560 Error@1 22.670

==>>[2025-10-22 15:43:19] [Epoch=036/040] [Need: 00:05:40] [LR=0.0010] [Best : Accuracy=77.59, Error=22.41]
  Epoch: [036][000/500]   Time 33.003 (33.003)   Data 32.917 (32.917)   Loss 0.8100 (0.8100)   Prec@1 72.000 (72.000)   Prec@5 98.000 (98.000)   [2025-10-22 15:43:52]
  Epoch: [036][100/500]   Time 0.066 (0.403)   Data 0.000 (0.327)   Loss 0.8744 (0.8256)   Prec@1 70.000 (71.713)   Prec@5 97.000 (97.861)   [2025-10-22 15:44:00]
  Epoch: [036][200/500]   Time 0.089 (0.242)   Data 0.001 (0.165)   Loss 0.7348 (0.8196)   Prec@1 74.000 (71.677)   Prec@5 98.000 (97.886)   [2025-10-22 15:44:08]
  Epoch: [036][300/500]   Time 0.082 (0.187)   Data 0.001 (0.110)   Loss 0.8872 (0.8197)   Prec@1 67.000 (71.475)   Prec@5 96.000 (97.791)   [2025-10-22 15:44:16]
  Epoch: [036][400/500]   Time 0.067 (0.159)   Data 0.001 (0.083)   Loss 0.8257 (0.8189)   Prec@1 69.000 (71.551)   Prec@5 98.000 (97.813)   [2025-10-22 15:44:23]
  **Train** Prec@1 71.378 Prec@5 97.780 Error@1 28.622
  **Test** Prec@1 77.730 Prec@5 98.570 Error@1 22.270
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 15:45:07] [Epoch=037/040] [Need: 00:04:16] [LR=0.0010] [Best : Accuracy=77.73, Error=22.27]
  Epoch: [037][000/500]   Time 38.398 (38.398)   Data 38.256 (38.256)   Loss 0.8390 (0.8390)   Prec@1 73.000 (73.000)   Prec@5 96.000 (96.000)   [2025-10-22 15:45:45]
  Epoch: [037][100/500]   Time 0.135 (0.517)   Data 0.001 (0.380)   Loss 0.7183 (0.8097)   Prec@1 80.000 (71.842)   Prec@5 99.000 (97.762)   [2025-10-22 15:45:59]
  Epoch: [037][200/500]   Time 0.148 (0.329)   Data 0.001 (0.192)   Loss 0.6857 (0.8181)   Prec@1 76.000 (71.502)   Prec@5 98.000 (97.721)   [2025-10-22 15:46:13]
  Epoch: [037][300/500]   Time 0.132 (0.267)   Data 0.001 (0.128)   Loss 0.9570 (0.8200)   Prec@1 66.000 (71.571)   Prec@5 99.000 (97.711)   [2025-10-22 15:46:27]
  Epoch: [037][400/500]   Time 0.141 (0.235)   Data 0.001 (0.097)   Loss 0.7173 (0.8154)   Prec@1 77.000 (71.701)   Prec@5 98.000 (97.753)   [2025-10-22 15:46:41]
  **Train** Prec@1 71.574 Prec@5 97.740 Error@1 28.426
  **Test** Prec@1 77.740 Prec@5 98.560 Error@1 22.260
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 15:47:38] [Epoch=038/040] [Need: 00:02:54] [LR=0.0010] [Best : Accuracy=77.74, Error=22.26]
  Epoch: [038][000/500]   Time 37.874 (37.874)   Data 37.712 (37.712)   Loss 0.9939 (0.9939)   Prec@1 66.000 (66.000)   Prec@5 95.000 (95.000)   [2025-10-22 15:48:16]
  Epoch: [038][100/500]   Time 0.142 (0.518)   Data 0.001 (0.375)   Loss 0.6729 (0.8196)   Prec@1 77.000 (71.782)   Prec@5 99.000 (97.723)   [2025-10-22 15:48:30]
  Epoch: [038][200/500]   Time 0.119 (0.333)   Data 0.001 (0.189)   Loss 0.7392 (0.8127)   Prec@1 72.000 (71.746)   Prec@5 98.000 (97.821)   [2025-10-22 15:48:45]
  Epoch: [038][300/500]   Time 0.067 (0.260)   Data 0.000 (0.127)   Loss 1.0880 (0.8142)   Prec@1 66.000 (71.645)   Prec@5 97.000 (97.854)   [2025-10-22 15:48:56]
  Epoch: [038][400/500]   Time 0.055 (0.210)   Data 0.001 (0.095)   Loss 0.7979 (0.8139)   Prec@1 73.000 (71.731)   Prec@5 100.000 (97.828)   [2025-10-22 15:49:02]
  **Train** Prec@1 71.692 Prec@5 97.732 Error@1 28.308
  **Test** Prec@1 77.760 Prec@5 98.590 Error@1 22.240
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 15:49:31] [Epoch=039/040] [Need: 00:01:28] [LR=0.0010] [Best : Accuracy=77.76, Error=22.24]
  Epoch: [039][000/500]   Time 21.072 (21.072)   Data 20.990 (20.990)   Loss 0.9626 (0.9626)   Prec@1 69.000 (69.000)   Prec@5 95.000 (95.000)   [2025-10-22 15:49:52]
  Epoch: [039][100/500]   Time 0.052 (0.263)   Data 0.000 (0.208)   Loss 0.9375 (0.8137)   Prec@1 69.000 (71.693)   Prec@5 95.000 (97.931)   [2025-10-22 15:49:57]
  Epoch: [039][200/500]   Time 0.053 (0.159)   Data 0.001 (0.105)   Loss 0.9156 (0.8209)   Prec@1 74.000 (71.204)   Prec@5 95.000 (97.945)   [2025-10-22 15:50:03]
  Epoch: [039][300/500]   Time 0.052 (0.124)   Data 0.001 (0.070)   Loss 1.0091 (0.8204)   Prec@1 67.000 (71.133)   Prec@5 94.000 (97.900)   [2025-10-22 15:50:08]
  Epoch: [039][400/500]   Time 0.056 (0.106)   Data 0.000 (0.053)   Loss 0.8392 (0.8177)   Prec@1 73.000 (71.334)   Prec@5 99.000 (97.865)   [2025-10-22 15:50:13]
  **Train** Prec@1 71.388 Prec@5 97.850 Error@1 28.612
  **Test** Prec@1 77.520 Prec@5 98.600 Error@1 22.480
