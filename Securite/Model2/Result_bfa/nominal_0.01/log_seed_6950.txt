save path : ./save/tinyvgg_quan/nominal_0.01
{'data_path': './dataset', 'arch': 'tinyvgg_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 1, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': False, 'clipping_coeff': 0.0, 'learning_rate': 0.01, 'manualSeed': 6950, 'save_path': './save/tinyvgg_quan/nominal_0.01', 'enable_bfa': False, 'resume': '', 'quan_bitwidth': None, 'reset_weight': False, 'evaluate': False, 'n_iter': 30, 'model_only': False, 'random_bfa': False, 'use_cuda': False}
Random Seed: 6950
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'tinyvgg_quan'
=> network :
 TinyVGG(
  (features): Sequential(
    (0): quan_Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Dropout2d(p=0.25, inplace=False)
    (6): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace=True)
    (8): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace=True)
    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (11): Dropout2d(p=0.25, inplace=False)
  )
  (classifier): Sequential(
    (0): quan_Linear(in_features=2048, out_features=10, bias=True)
  )
)
=> do not use any checkpoint for tinyvgg_quan model

==>>[2025-10-22 17:58:06] [Epoch=000/040] [Need: 00:00:00] [LR=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/500]   Time 17.567 (17.567)   Data 17.396 (17.396)   Loss 2.3048 (2.3048)   Prec@1 12.000 (12.000)   Prec@5 53.000 (53.000)   [2025-10-22 17:58:24]
  Epoch: [000][100/500]   Time 0.062 (0.235)   Data 0.001 (0.173)   Loss 1.9751 (2.1605)   Prec@1 28.000 (20.079)   Prec@5 83.000 (66.911)   [2025-10-22 17:58:30]
  Epoch: [000][200/500]   Time 0.059 (0.148)   Data 0.001 (0.087)   Loss 1.7309 (2.0489)   Prec@1 36.000 (25.060)   Prec@5 88.000 (74.149)   [2025-10-22 17:58:36]
  Epoch: [000][300/500]   Time 0.065 (0.119)   Data 0.001 (0.058)   Loss 1.6032 (1.9682)   Prec@1 48.000 (28.150)   Prec@5 89.000 (77.950)   [2025-10-22 17:58:42]
  Epoch: [000][400/500]   Time 0.059 (0.105)   Data 0.001 (0.044)   Loss 1.5982 (1.9090)   Prec@1 38.000 (30.264)   Prec@5 93.000 (80.307)   [2025-10-22 17:58:48]
  **Train** Prec@1 32.082 Prec@5 82.076 Error@1 67.918
  **Test** Prec@1 46.730 Prec@5 92.600 Error@1 53.270
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 17:59:14] [Epoch=001/040] [Need: 00:44:11] [LR=0.0100] [Best : Accuracy=46.73, Error=53.27]
  Epoch: [001][000/500]   Time 17.580 (17.580)   Data 17.506 (17.506)   Loss 1.7052 (1.7052)   Prec@1 35.000 (35.000)   Prec@5 92.000 (92.000)   [2025-10-22 17:59:32]
  Epoch: [001][100/500]   Time 0.061 (0.234)   Data 0.001 (0.174)   Loss 1.5970 (1.6288)   Prec@1 42.000 (40.228)   Prec@5 91.000 (89.465)   [2025-10-22 17:59:38]
  Epoch: [001][200/500]   Time 0.063 (0.148)   Data 0.001 (0.088)   Loss 1.5260 (1.6048)   Prec@1 46.000 (41.607)   Prec@5 90.000 (89.786)   [2025-10-22 17:59:44]
  Epoch: [001][300/500]   Time 0.063 (0.119)   Data 0.000 (0.059)   Loss 1.5271 (1.5852)   Prec@1 51.000 (42.528)   Prec@5 91.000 (90.153)   [2025-10-22 17:59:50]
  Epoch: [001][400/500]   Time 0.063 (0.104)   Data 0.001 (0.044)   Loss 1.8607 (1.5690)   Prec@1 33.000 (43.102)   Prec@5 87.000 (90.434)   [2025-10-22 17:59:56]
  **Train** Prec@1 43.748 Prec@5 90.758 Error@1 56.252
  **Test** Prec@1 55.650 Prec@5 94.750 Error@1 44.350
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 18:00:23] [Epoch=002/040] [Need: 00:43:06] [LR=0.0100] [Best : Accuracy=55.65, Error=44.35]
  Epoch: [002][000/500]   Time 17.397 (17.397)   Data 17.328 (17.328)   Loss 1.6316 (1.6316)   Prec@1 43.000 (43.000)   Prec@5 90.000 (90.000)   [2025-10-22 18:00:40]
  Epoch: [002][100/500]   Time 0.060 (0.232)   Data 0.000 (0.172)   Loss 1.5153 (1.4512)   Prec@1 46.000 (47.980)   Prec@5 92.000 (92.040)   [2025-10-22 18:00:46]
  Epoch: [002][200/500]   Time 0.061 (0.147)   Data 0.000 (0.087)   Loss 1.4403 (1.4386)   Prec@1 50.000 (48.542)   Prec@5 94.000 (92.274)   [2025-10-22 18:00:52]
  Epoch: [002][300/500]   Time 0.059 (0.117)   Data 0.000 (0.058)   Loss 1.4651 (1.4308)   Prec@1 46.000 (48.993)   Prec@5 96.000 (92.375)   [2025-10-22 18:00:58]
  Epoch: [002][400/500]   Time 0.059 (0.103)   Data 0.000 (0.044)   Loss 1.3559 (1.4188)   Prec@1 50.000 (49.209)   Prec@5 93.000 (92.521)   [2025-10-22 18:01:04]
  **Train** Prec@1 49.602 Prec@5 92.734 Error@1 50.398
  **Test** Prec@1 60.210 Prec@5 96.170 Error@1 39.790
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 18:01:30] [Epoch=003/040] [Need: 00:41:49] [LR=0.0100] [Best : Accuracy=60.21, Error=39.79]
  Epoch: [003][000/500]   Time 17.372 (17.372)   Data 17.301 (17.301)   Loss 1.5792 (1.5792)   Prec@1 50.000 (50.000)   Prec@5 94.000 (94.000)   [2025-10-22 18:01:47]
  Epoch: [003][100/500]   Time 0.064 (0.231)   Data 0.001 (0.172)   Loss 1.3688 (1.3492)   Prec@1 48.000 (51.545)   Prec@5 94.000 (93.158)   [2025-10-22 18:01:53]
  Epoch: [003][200/500]   Time 0.056 (0.146)   Data 0.001 (0.087)   Loss 1.2705 (1.3396)   Prec@1 53.000 (51.900)   Prec@5 96.000 (93.478)   [2025-10-22 18:01:59]
  Epoch: [003][300/500]   Time 0.061 (0.117)   Data 0.001 (0.058)   Loss 1.1669 (1.3361)   Prec@1 56.000 (52.043)   Prec@5 96.000 (93.578)   [2025-10-22 18:02:05]
  Epoch: [003][400/500]   Time 0.058 (0.103)   Data 0.001 (0.044)   Loss 1.3167 (1.3250)   Prec@1 53.000 (52.456)   Prec@5 90.000 (93.718)   [2025-10-22 18:02:11]
  **Train** Prec@1 52.694 Prec@5 93.864 Error@1 47.306
  **Test** Prec@1 63.050 Prec@5 96.380 Error@1 36.950
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 18:02:37] [Epoch=004/040] [Need: 00:40:33] [LR=0.0100] [Best : Accuracy=63.05, Error=36.95]
  Epoch: [004][000/500]   Time 17.381 (17.381)   Data 17.309 (17.309)   Loss 1.1158 (1.1158)   Prec@1 67.000 (67.000)   Prec@5 95.000 (95.000)   [2025-10-22 18:02:54]
  Epoch: [004][100/500]   Time 0.058 (0.230)   Data 0.001 (0.172)   Loss 1.2470 (1.2699)   Prec@1 55.000 (54.743)   Prec@5 95.000 (94.396)   [2025-10-22 18:03:00]
  Epoch: [004][200/500]   Time 0.055 (0.146)   Data 0.000 (0.087)   Loss 1.2143 (1.2578)   Prec@1 56.000 (55.080)   Prec@5 97.000 (94.502)   [2025-10-22 18:03:06]
  Epoch: [004][300/500]   Time 0.060 (0.117)   Data 0.001 (0.058)   Loss 1.3895 (1.2504)   Prec@1 52.000 (55.389)   Prec@5 94.000 (94.688)   [2025-10-22 18:03:12]
  Epoch: [004][400/500]   Time 0.061 (0.103)   Data 0.000 (0.044)   Loss 1.2926 (1.2497)   Prec@1 57.000 (55.399)   Prec@5 92.000 (94.641)   [2025-10-22 18:03:18]
  **Train** Prec@1 55.490 Prec@5 94.710 Error@1 44.510
  **Test** Prec@1 64.700 Prec@5 96.770 Error@1 35.300
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 18:03:44] [Epoch=005/040] [Need: 00:39:21] [LR=0.0100] [Best : Accuracy=64.70, Error=35.30]
  Epoch: [005][000/500]   Time 17.419 (17.419)   Data 17.347 (17.347)   Loss 1.2547 (1.2547)   Prec@1 58.000 (58.000)   Prec@5 92.000 (92.000)   [2025-10-22 18:04:01]
  Epoch: [005][100/500]   Time 0.061 (0.231)   Data 0.001 (0.172)   Loss 1.2442 (1.2030)   Prec@1 53.000 (57.416)   Prec@5 92.000 (95.040)   [2025-10-22 18:04:07]
  Epoch: [005][200/500]   Time 0.058 (0.145)   Data 0.001 (0.087)   Loss 1.2136 (1.2043)   Prec@1 63.000 (57.184)   Prec@5 93.000 (94.940)   [2025-10-22 18:04:13]
  Epoch: [005][300/500]   Time 0.059 (0.117)   Data 0.000 (0.058)   Loss 1.1487 (1.1985)   Prec@1 52.000 (57.392)   Prec@5 97.000 (95.037)   [2025-10-22 18:04:19]
  Epoch: [005][400/500]   Time 0.058 (0.102)   Data 0.001 (0.044)   Loss 1.0463 (1.1951)   Prec@1 63.000 (57.596)   Prec@5 96.000 (95.042)   [2025-10-22 18:04:25]
  **Train** Prec@1 57.660 Prec@5 95.034 Error@1 42.340
  **Test** Prec@1 67.580 Prec@5 97.390 Error@1 32.420
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 18:04:51] [Epoch=006/040] [Need: 00:38:11] [LR=0.0100] [Best : Accuracy=67.58, Error=32.42]
  Epoch: [006][000/500]   Time 17.257 (17.257)   Data 17.185 (17.185)   Loss 1.1118 (1.1118)   Prec@1 62.000 (62.000)   Prec@5 97.000 (97.000)   [2025-10-22 18:05:08]
  Epoch: [006][100/500]   Time 0.067 (0.232)   Data 0.001 (0.171)   Loss 1.1901 (1.1773)   Prec@1 58.000 (58.307)   Prec@5 92.000 (95.347)   [2025-10-22 18:05:14]
  Epoch: [006][200/500]   Time 0.061 (0.147)   Data 0.001 (0.086)   Loss 1.2166 (1.1620)   Prec@1 59.000 (58.826)   Prec@5 96.000 (95.522)   [2025-10-22 18:05:20]
  Epoch: [006][300/500]   Time 0.061 (0.119)   Data 0.000 (0.058)   Loss 1.0856 (1.1593)   Prec@1 65.000 (58.993)   Prec@5 96.000 (95.405)   [2025-10-22 18:05:27]
  Epoch: [006][400/500]   Time 0.058 (0.104)   Data 0.000 (0.044)   Loss 0.9913 (1.1560)   Prec@1 60.000 (58.960)   Prec@5 100.000 (95.434)   [2025-10-22 18:05:33]
  **Train** Prec@1 58.966 Prec@5 95.370 Error@1 41.034
  **Test** Prec@1 68.650 Prec@5 97.400 Error@1 31.350
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 18:05:59] [Epoch=007/040] [Need: 00:37:06] [LR=0.0100] [Best : Accuracy=68.65, Error=31.35]
  Epoch: [007][000/500]   Time 17.448 (17.448)   Data 17.380 (17.380)   Loss 1.1282 (1.1282)   Prec@1 53.000 (53.000)   Prec@5 98.000 (98.000)   [2025-10-22 18:06:16]
  Epoch: [007][100/500]   Time 0.058 (0.232)   Data 0.001 (0.173)   Loss 1.1820 (1.1358)   Prec@1 56.000 (59.109)   Prec@5 97.000 (95.515)   [2025-10-22 18:06:22]
  Epoch: [007][200/500]   Time 0.058 (0.146)   Data 0.001 (0.087)   Loss 1.0421 (1.1329)   Prec@1 64.000 (59.597)   Prec@5 99.000 (95.632)   [2025-10-22 18:06:28]
  Epoch: [007][300/500]   Time 0.059 (0.118)   Data 0.001 (0.058)   Loss 1.1138 (1.1260)   Prec@1 59.000 (59.983)   Prec@5 98.000 (95.791)   [2025-10-22 18:06:34]
  Epoch: [007][400/500]   Time 0.060 (0.104)   Data 0.000 (0.044)   Loss 0.9513 (1.1288)   Prec@1 63.000 (59.995)   Prec@5 96.000 (95.613)   [2025-10-22 18:06:40]
  **Train** Prec@1 60.314 Prec@5 95.570 Error@1 39.686
  **Test** Prec@1 68.730 Prec@5 97.480 Error@1 31.270
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 18:07:06] [Epoch=008/040] [Need: 00:35:59] [LR=0.0100] [Best : Accuracy=68.73, Error=31.27]
  Epoch: [008][000/500]   Time 17.460 (17.460)   Data 17.388 (17.388)   Loss 1.1585 (1.1585)   Prec@1 57.000 (57.000)   Prec@5 96.000 (96.000)   [2025-10-22 18:07:24]
  Epoch: [008][100/500]   Time 0.065 (0.232)   Data 0.000 (0.173)   Loss 1.2419 (1.1029)   Prec@1 62.000 (60.901)   Prec@5 91.000 (95.693)   [2025-10-22 18:07:30]
  Epoch: [008][200/500]   Time 0.062 (0.146)   Data 0.001 (0.087)   Loss 1.1757 (1.0948)   Prec@1 61.000 (61.030)   Prec@5 98.000 (95.940)   [2025-10-22 18:07:36]
  Epoch: [008][300/500]   Time 0.058 (0.117)   Data 0.000 (0.058)   Loss 1.0817 (1.0975)   Prec@1 61.000 (61.223)   Prec@5 98.000 (95.867)   [2025-10-22 18:07:42]
  Epoch: [008][400/500]   Time 0.059 (0.103)   Data 0.001 (0.044)   Loss 1.2922 (1.0994)   Prec@1 56.000 (61.242)   Prec@5 92.000 (95.835)   [2025-10-22 18:07:48]
  **Train** Prec@1 61.330 Prec@5 95.834 Error@1 38.670
  **Test** Prec@1 69.780 Prec@5 97.590 Error@1 30.220
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 18:08:13] [Epoch=009/040] [Need: 00:34:50] [LR=0.0100] [Best : Accuracy=69.78, Error=30.22]
  Epoch: [009][000/500]   Time 17.177 (17.177)   Data 17.107 (17.107)   Loss 1.1024 (1.1024)   Prec@1 60.000 (60.000)   Prec@5 95.000 (95.000)   [2025-10-22 18:08:31]
  Epoch: [009][100/500]   Time 0.061 (0.229)   Data 0.001 (0.170)   Loss 1.1778 (1.0610)   Prec@1 65.000 (62.564)   Prec@5 92.000 (96.119)   [2025-10-22 18:08:36]
  Epoch: [009][200/500]   Time 0.059 (0.144)   Data 0.000 (0.086)   Loss 0.9339 (1.0714)   Prec@1 68.000 (62.299)   Prec@5 99.000 (96.214)   [2025-10-22 18:08:42]
  Epoch: [009][300/500]   Time 0.058 (0.116)   Data 0.000 (0.057)   Loss 1.0344 (1.0684)   Prec@1 60.000 (62.302)   Prec@5 98.000 (96.269)   [2025-10-22 18:08:48]
  Epoch: [009][400/500]   Time 0.061 (0.102)   Data 0.000 (0.043)   Loss 1.1797 (1.0708)   Prec@1 60.000 (62.347)   Prec@5 92.000 (96.125)   [2025-10-22 18:08:54]
  **Train** Prec@1 62.358 Prec@5 96.048 Error@1 37.642
  **Test** Prec@1 69.640 Prec@5 97.350 Error@1 30.360

==>>[2025-10-22 18:09:20] [Epoch=010/040] [Need: 00:33:40] [LR=0.0100] [Best : Accuracy=69.78, Error=30.22]
  Epoch: [010][000/500]   Time 17.187 (17.187)   Data 17.115 (17.115)   Loss 0.9486 (0.9486)   Prec@1 69.000 (69.000)   Prec@5 97.000 (97.000)   [2025-10-22 18:09:37]
  Epoch: [010][100/500]   Time 0.060 (0.230)   Data 0.000 (0.170)   Loss 1.2149 (1.0439)   Prec@1 62.000 (63.158)   Prec@5 96.000 (96.356)   [2025-10-22 18:09:43]
  Epoch: [010][200/500]   Time 0.058 (0.145)   Data 0.001 (0.086)   Loss 1.0210 (1.0487)   Prec@1 62.000 (63.060)   Prec@5 98.000 (96.249)   [2025-10-22 18:09:49]
  Epoch: [010][300/500]   Time 0.059 (0.116)   Data 0.001 (0.057)   Loss 0.9263 (1.0541)   Prec@1 70.000 (63.076)   Prec@5 96.000 (96.123)   [2025-10-22 18:09:55]
  Epoch: [010][400/500]   Time 0.063 (0.102)   Data 0.001 (0.043)   Loss 0.9942 (1.0515)   Prec@1 64.000 (63.157)   Prec@5 97.000 (96.157)   [2025-10-22 18:10:01]
  **Train** Prec@1 63.056 Prec@5 96.162 Error@1 36.944
  **Test** Prec@1 70.720 Prec@5 97.780 Error@1 29.280
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 18:10:26] [Epoch=011/040] [Need: 00:32:31] [LR=0.0100] [Best : Accuracy=70.72, Error=29.28]
  Epoch: [011][000/500]   Time 17.483 (17.483)   Data 17.412 (17.412)   Loss 1.1442 (1.1442)   Prec@1 58.000 (58.000)   Prec@5 97.000 (97.000)   [2025-10-22 18:10:44]
  Epoch: [011][100/500]   Time 0.059 (0.236)   Data 0.000 (0.173)   Loss 0.9437 (1.0368)   Prec@1 59.000 (62.871)   Prec@5 98.000 (96.238)   [2025-10-22 18:10:50]
  Epoch: [011][200/500]   Time 0.056 (0.148)   Data 0.000 (0.087)   Loss 0.8283 (1.0349)   Prec@1 73.000 (63.055)   Prec@5 99.000 (96.373)   [2025-10-22 18:10:56]
  Epoch: [011][300/500]   Time 0.058 (0.119)   Data 0.000 (0.058)   Loss 1.2984 (1.0429)   Prec@1 55.000 (62.937)   Prec@5 96.000 (96.419)   [2025-10-22 18:11:02]
  Epoch: [011][400/500]   Time 0.058 (0.104)   Data 0.001 (0.044)   Loss 1.0380 (1.0419)   Prec@1 59.000 (63.020)   Prec@5 93.000 (96.377)   [2025-10-22 18:11:08]
  **Train** Prec@1 63.232 Prec@5 96.350 Error@1 36.768
  **Test** Prec@1 72.280 Prec@5 98.030 Error@1 27.720
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 18:11:34] [Epoch=012/040] [Need: 00:31:24] [LR=0.0100] [Best : Accuracy=72.28, Error=27.72]
  Epoch: [012][000/500]   Time 17.460 (17.460)   Data 17.390 (17.390)   Loss 1.0052 (1.0052)   Prec@1 67.000 (67.000)   Prec@5 97.000 (97.000)   [2025-10-22 18:11:51]
  Epoch: [012][100/500]   Time 0.057 (0.232)   Data 0.001 (0.173)   Loss 1.0740 (1.0313)   Prec@1 67.000 (63.535)   Prec@5 94.000 (96.347)   [2025-10-22 18:11:57]
  Epoch: [012][200/500]   Time 0.060 (0.146)   Data 0.001 (0.087)   Loss 1.0890 (1.0325)   Prec@1 58.000 (63.468)   Prec@5 97.000 (96.299)   [2025-10-22 18:12:03]
  Epoch: [012][300/500]   Time 0.056 (0.117)   Data 0.000 (0.058)   Loss 0.9804 (1.0210)   Prec@1 67.000 (64.020)   Prec@5 98.000 (96.462)   [2025-10-22 18:12:09]
  Epoch: [012][400/500]   Time 0.059 (0.102)   Data 0.000 (0.044)   Loss 1.0269 (1.0226)   Prec@1 60.000 (63.923)   Prec@5 97.000 (96.476)   [2025-10-22 18:12:15]
  **Train** Prec@1 63.986 Prec@5 96.468 Error@1 36.014
  **Test** Prec@1 71.350 Prec@5 97.790 Error@1 28.650

==>>[2025-10-22 18:12:41] [Epoch=013/040] [Need: 00:30:16] [LR=0.0100] [Best : Accuracy=72.28, Error=27.72]
  Epoch: [013][000/500]   Time 17.514 (17.514)   Data 17.439 (17.439)   Loss 1.1153 (1.1153)   Prec@1 64.000 (64.000)   Prec@5 95.000 (95.000)   [2025-10-22 18:12:58]
  Epoch: [013][100/500]   Time 0.059 (0.233)   Data 0.000 (0.173)   Loss 1.1518 (1.0190)   Prec@1 57.000 (64.139)   Prec@5 98.000 (96.446)   [2025-10-22 18:13:04]
  Epoch: [013][200/500]   Time 0.059 (0.147)   Data 0.000 (0.087)   Loss 0.9839 (1.0128)   Prec@1 67.000 (64.388)   Prec@5 96.000 (96.582)   [2025-10-22 18:13:10]
  Epoch: [013][300/500]   Time 0.064 (0.118)   Data 0.000 (0.059)   Loss 1.0872 (1.0170)   Prec@1 69.000 (64.246)   Prec@5 93.000 (96.455)   [2025-10-22 18:13:16]
  Epoch: [013][400/500]   Time 0.057 (0.103)   Data 0.000 (0.044)   Loss 1.0131 (1.0145)   Prec@1 66.000 (64.334)   Prec@5 98.000 (96.449)   [2025-10-22 18:13:22]
  **Train** Prec@1 64.282 Prec@5 96.412 Error@1 35.718
  **Test** Prec@1 72.460 Prec@5 98.090 Error@1 27.540
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 18:13:48] [Epoch=014/040] [Need: 00:29:08] [LR=0.0100] [Best : Accuracy=72.46, Error=27.54]
  Epoch: [014][000/500]   Time 17.206 (17.206)   Data 17.130 (17.130)   Loss 1.1726 (1.1726)   Prec@1 62.000 (62.000)   Prec@5 93.000 (93.000)   [2025-10-22 18:14:05]
  Epoch: [014][100/500]   Time 0.060 (0.229)   Data 0.000 (0.170)   Loss 0.9979 (0.9904)   Prec@1 63.000 (65.040)   Prec@5 99.000 (96.673)   [2025-10-22 18:14:11]
  Epoch: [014][200/500]   Time 0.058 (0.144)   Data 0.000 (0.086)   Loss 0.8649 (1.0070)   Prec@1 73.000 (64.557)   Prec@5 97.000 (96.552)   [2025-10-22 18:14:17]
  Epoch: [014][300/500]   Time 0.061 (0.116)   Data 0.001 (0.057)   Loss 1.1862 (1.0080)   Prec@1 57.000 (64.741)   Prec@5 98.000 (96.475)   [2025-10-22 18:14:23]
  Epoch: [014][400/500]   Time 0.062 (0.102)   Data 0.001 (0.043)   Loss 0.8910 (1.0021)   Prec@1 65.000 (64.955)   Prec@5 97.000 (96.534)   [2025-10-22 18:14:29]
  **Train** Prec@1 64.964 Prec@5 96.486 Error@1 35.036
  **Test** Prec@1 72.520 Prec@5 98.010 Error@1 27.480
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 18:14:55] [Epoch=015/040] [Need: 00:28:00] [LR=0.0100] [Best : Accuracy=72.52, Error=27.48]
  Epoch: [015][000/500]   Time 17.242 (17.242)   Data 17.170 (17.170)   Loss 1.1644 (1.1644)   Prec@1 59.000 (59.000)   Prec@5 95.000 (95.000)   [2025-10-22 18:15:12]
  Epoch: [015][100/500]   Time 0.060 (0.228)   Data 0.000 (0.171)   Loss 1.1013 (0.9990)   Prec@1 54.000 (65.149)   Prec@5 95.000 (96.545)   [2025-10-22 18:15:18]
  Epoch: [015][200/500]   Time 0.063 (0.144)   Data 0.001 (0.086)   Loss 0.8540 (0.9892)   Prec@1 71.000 (65.299)   Prec@5 95.000 (96.786)   [2025-10-22 18:15:24]
  Epoch: [015][300/500]   Time 0.060 (0.116)   Data 0.001 (0.058)   Loss 0.9974 (0.9840)   Prec@1 63.000 (65.282)   Prec@5 99.000 (96.850)   [2025-10-22 18:15:30]
  Epoch: [015][400/500]   Time 0.059 (0.102)   Data 0.001 (0.043)   Loss 1.0019 (0.9865)   Prec@1 64.000 (65.274)   Prec@5 98.000 (96.818)   [2025-10-22 18:15:36]
  **Train** Prec@1 65.258 Prec@5 96.766 Error@1 34.742
  **Test** Prec@1 72.690 Prec@5 97.810 Error@1 27.310
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 18:16:02] [Epoch=016/040] [Need: 00:26:52] [LR=0.0100] [Best : Accuracy=72.69, Error=27.31]
  Epoch: [016][000/500]   Time 17.386 (17.386)   Data 17.313 (17.313)   Loss 0.8107 (0.8107)   Prec@1 71.000 (71.000)   Prec@5 97.000 (97.000)   [2025-10-22 18:16:19]
  Epoch: [016][100/500]   Time 0.057 (0.232)   Data 0.001 (0.172)   Loss 0.9354 (0.9683)   Prec@1 67.000 (65.832)   Prec@5 98.000 (96.941)   [2025-10-22 18:16:25]
  Epoch: [016][200/500]   Time 0.059 (0.146)   Data 0.000 (0.087)   Loss 0.9832 (0.9631)   Prec@1 67.000 (66.129)   Prec@5 96.000 (96.970)   [2025-10-22 18:16:31]
  Epoch: [016][300/500]   Time 0.061 (0.117)   Data 0.001 (0.058)   Loss 1.0710 (0.9651)   Prec@1 61.000 (66.073)   Prec@5 96.000 (96.953)   [2025-10-22 18:16:37]
  Epoch: [016][400/500]   Time 0.062 (0.103)   Data 0.000 (0.044)   Loss 0.9977 (0.9710)   Prec@1 61.000 (65.888)   Prec@5 97.000 (96.855)   [2025-10-22 18:16:43]
  **Train** Prec@1 65.870 Prec@5 96.834 Error@1 34.130
  **Test** Prec@1 73.160 Prec@5 98.090 Error@1 26.840
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 18:17:09] [Epoch=017/040] [Need: 00:25:45] [LR=0.0100] [Best : Accuracy=73.16, Error=26.84]
  Epoch: [017][000/500]   Time 17.439 (17.439)   Data 17.365 (17.365)   Loss 0.9413 (0.9413)   Prec@1 70.000 (70.000)   Prec@5 97.000 (97.000)   [2025-10-22 18:17:26]
  Epoch: [017][100/500]   Time 0.061 (0.233)   Data 0.001 (0.173)   Loss 0.9640 (0.9880)   Prec@1 73.000 (66.297)   Prec@5 98.000 (96.554)   [2025-10-22 18:17:32]
  Epoch: [017][200/500]   Time 0.063 (0.147)   Data 0.001 (0.087)   Loss 0.9855 (0.9751)   Prec@1 63.000 (66.209)   Prec@5 95.000 (96.622)   [2025-10-22 18:17:38]
  Epoch: [017][300/500]   Time 0.057 (0.118)   Data 0.000 (0.058)   Loss 1.0444 (0.9732)   Prec@1 60.000 (66.010)   Prec@5 96.000 (96.708)   [2025-10-22 18:17:44]
  Epoch: [017][400/500]   Time 0.059 (0.103)   Data 0.000 (0.044)   Loss 0.7991 (0.9755)   Prec@1 73.000 (65.878)   Prec@5 99.000 (96.726)   [2025-10-22 18:17:50]
  **Train** Prec@1 65.974 Prec@5 96.720 Error@1 34.026
  **Test** Prec@1 73.990 Prec@5 98.020 Error@1 26.010
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 18:18:16] [Epoch=018/040] [Need: 00:24:38] [LR=0.0100] [Best : Accuracy=73.99, Error=26.01]
  Epoch: [018][000/500]   Time 17.341 (17.341)   Data 17.270 (17.270)   Loss 0.9783 (0.9783)   Prec@1 66.000 (66.000)   Prec@5 99.000 (99.000)   [2025-10-22 18:18:33]
  Epoch: [018][100/500]   Time 0.058 (0.233)   Data 0.000 (0.172)   Loss 0.8925 (0.9780)   Prec@1 69.000 (65.673)   Prec@5 96.000 (96.792)   [2025-10-22 18:18:39]
  Epoch: [018][200/500]   Time 0.060 (0.146)   Data 0.001 (0.087)   Loss 1.0295 (0.9686)   Prec@1 62.000 (65.915)   Prec@5 95.000 (96.776)   [2025-10-22 18:18:45]
  Epoch: [018][300/500]   Time 0.063 (0.117)   Data 0.001 (0.058)   Loss 0.9735 (0.9657)   Prec@1 63.000 (65.924)   Prec@5 97.000 (96.821)   [2025-10-22 18:18:51]
  Epoch: [018][400/500]   Time 0.056 (0.103)   Data 0.000 (0.044)   Loss 1.0068 (0.9672)   Prec@1 67.000 (65.875)   Prec@5 95.000 (96.783)   [2025-10-22 18:18:57]
  **Train** Prec@1 65.964 Prec@5 96.844 Error@1 34.036
  **Test** Prec@1 73.770 Prec@5 98.250 Error@1 26.230

==>>[2025-10-22 18:19:23] [Epoch=019/040] [Need: 00:23:31] [LR=0.0100] [Best : Accuracy=73.99, Error=26.01]
  Epoch: [019][000/500]   Time 17.507 (17.507)   Data 17.434 (17.434)   Loss 0.8222 (0.8222)   Prec@1 70.000 (70.000)   Prec@5 98.000 (98.000)   [2025-10-22 18:19:41]
  Epoch: [019][100/500]   Time 0.055 (0.231)   Data 0.000 (0.173)   Loss 0.9240 (0.9431)   Prec@1 63.000 (66.693)   Prec@5 100.000 (97.119)   [2025-10-22 18:19:46]
  Epoch: [019][200/500]   Time 0.060 (0.146)   Data 0.001 (0.087)   Loss 0.9751 (0.9555)   Prec@1 64.000 (66.333)   Prec@5 96.000 (96.846)   [2025-10-22 18:19:52]
  Epoch: [019][300/500]   Time 0.057 (0.117)   Data 0.001 (0.059)   Loss 0.8556 (0.9536)   Prec@1 69.000 (66.449)   Prec@5 97.000 (96.787)   [2025-10-22 18:19:58]
  Epoch: [019][400/500]   Time 0.059 (0.103)   Data 0.000 (0.044)   Loss 0.9459 (0.9529)   Prec@1 70.000 (66.653)   Prec@5 95.000 (96.763)   [2025-10-22 18:20:04]
  **Train** Prec@1 66.492 Prec@5 96.786 Error@1 33.508
  **Test** Prec@1 73.650 Prec@5 98.170 Error@1 26.350

==>>[2025-10-22 18:20:30] [Epoch=020/040] [Need: 00:22:23] [LR=0.0100] [Best : Accuracy=73.99, Error=26.01]
  Epoch: [020][000/500]   Time 17.410 (17.410)   Data 17.340 (17.340)   Loss 0.9227 (0.9227)   Prec@1 68.000 (68.000)   Prec@5 96.000 (96.000)   [2025-10-22 18:20:48]
  Epoch: [020][100/500]   Time 0.060 (0.230)   Data 0.001 (0.172)   Loss 1.1157 (0.9396)   Prec@1 61.000 (67.723)   Prec@5 94.000 (96.911)   [2025-10-22 18:20:53]
  Epoch: [020][200/500]   Time 0.056 (0.145)   Data 0.001 (0.087)   Loss 1.0140 (0.9417)   Prec@1 63.000 (67.159)   Prec@5 96.000 (96.965)   [2025-10-22 18:20:59]
  Epoch: [020][300/500]   Time 0.058 (0.116)   Data 0.001 (0.058)   Loss 0.7358 (0.9445)   Prec@1 71.000 (67.033)   Prec@5 100.000 (97.066)   [2025-10-22 18:21:05]
  Epoch: [020][400/500]   Time 0.061 (0.102)   Data 0.001 (0.044)   Loss 0.7655 (0.9475)   Prec@1 75.000 (66.963)   Prec@5 99.000 (97.027)   [2025-10-22 18:21:11]
  **Train** Prec@1 67.022 Prec@5 97.008 Error@1 32.978
  **Test** Prec@1 73.840 Prec@5 97.940 Error@1 26.160

==>>[2025-10-22 18:21:37] [Epoch=021/040] [Need: 00:21:16] [LR=0.0100] [Best : Accuracy=73.99, Error=26.01]
  Epoch: [021][000/500]   Time 17.303 (17.303)   Data 17.231 (17.231)   Loss 0.9784 (0.9784)   Prec@1 69.000 (69.000)   Prec@5 94.000 (94.000)   [2025-10-22 18:21:54]
  Epoch: [021][100/500]   Time 0.057 (0.231)   Data 0.000 (0.171)   Loss 0.8670 (0.9634)   Prec@1 73.000 (66.624)   Prec@5 100.000 (96.851)   [2025-10-22 18:22:00]
  Epoch: [021][200/500]   Time 0.060 (0.146)   Data 0.001 (0.086)   Loss 0.8803 (0.9496)   Prec@1 68.000 (66.940)   Prec@5 98.000 (96.871)   [2025-10-22 18:22:06]
  Epoch: [021][300/500]   Time 0.061 (0.118)   Data 0.001 (0.058)   Loss 1.0390 (0.9455)   Prec@1 68.000 (67.140)   Prec@5 95.000 (96.924)   [2025-10-22 18:22:12]
  Epoch: [021][400/500]   Time 0.062 (0.104)   Data 0.001 (0.044)   Loss 0.7567 (0.9474)   Prec@1 73.000 (67.000)   Prec@5 99.000 (96.915)   [2025-10-22 18:22:18]
  **Train** Prec@1 67.114 Prec@5 96.922 Error@1 32.886
  **Test** Prec@1 73.420 Prec@5 98.130 Error@1 26.580

==>>[2025-10-22 18:22:44] [Epoch=022/040] [Need: 00:20:09] [LR=0.0100] [Best : Accuracy=73.99, Error=26.01]
  Epoch: [022][000/500]   Time 17.151 (17.151)   Data 17.078 (17.078)   Loss 0.8826 (0.8826)   Prec@1 71.000 (71.000)   Prec@5 98.000 (98.000)   [2025-10-22 18:23:01]
  Epoch: [022][100/500]   Time 0.061 (0.230)   Data 0.001 (0.170)   Loss 0.9801 (0.9409)   Prec@1 65.000 (66.842)   Prec@5 97.000 (97.089)   [2025-10-22 18:23:08]
  Epoch: [022][200/500]   Time 0.060 (0.146)   Data 0.000 (0.086)   Loss 1.0490 (0.9369)   Prec@1 63.000 (66.861)   Prec@5 94.000 (97.249)   [2025-10-22 18:23:14]
  Epoch: [022][300/500]   Time 0.057 (0.117)   Data 0.000 (0.057)   Loss 0.8974 (0.9415)   Prec@1 74.000 (66.787)   Prec@5 96.000 (97.113)   [2025-10-22 18:23:20]
  Epoch: [022][400/500]   Time 0.060 (0.103)   Data 0.001 (0.043)   Loss 0.8429 (0.9379)   Prec@1 69.000 (67.017)   Prec@5 100.000 (97.115)   [2025-10-22 18:23:25]
  **Train** Prec@1 67.028 Prec@5 97.110 Error@1 32.972
  **Test** Prec@1 75.560 Prec@5 98.310 Error@1 24.440
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 18:23:51] [Epoch=023/040] [Need: 00:19:01] [LR=0.0100] [Best : Accuracy=75.56, Error=24.44]
  Epoch: [023][000/500]   Time 17.274 (17.274)   Data 17.200 (17.200)   Loss 0.8342 (0.8342)   Prec@1 73.000 (73.000)   Prec@5 96.000 (96.000)   [2025-10-22 18:24:08]
  Epoch: [023][100/500]   Time 0.057 (0.229)   Data 0.000 (0.171)   Loss 1.1917 (0.9173)   Prec@1 57.000 (67.554)   Prec@5 95.000 (97.059)   [2025-10-22 18:24:14]
  Epoch: [023][200/500]   Time 0.056 (0.144)   Data 0.001 (0.086)   Loss 0.7631 (0.9235)   Prec@1 80.000 (67.478)   Prec@5 97.000 (97.154)   [2025-10-22 18:24:20]
  Epoch: [023][300/500]   Time 0.058 (0.115)   Data 0.001 (0.058)   Loss 0.7750 (0.9276)   Prec@1 78.000 (67.488)   Prec@5 94.000 (97.133)   [2025-10-22 18:24:26]
  Epoch: [023][400/500]   Time 0.057 (0.101)   Data 0.001 (0.044)   Loss 0.7724 (0.9292)   Prec@1 77.000 (67.491)   Prec@5 98.000 (97.112)   [2025-10-22 18:24:32]
  **Train** Prec@1 67.508 Prec@5 97.076 Error@1 32.492
  **Test** Prec@1 73.880 Prec@5 98.120 Error@1 26.120

==>>[2025-10-22 18:24:58] [Epoch=024/040] [Need: 00:17:54] [LR=0.0100] [Best : Accuracy=75.56, Error=24.44]
  Epoch: [024][000/500]   Time 17.529 (17.529)   Data 17.458 (17.458)   Loss 1.1503 (1.1503)   Prec@1 57.000 (57.000)   Prec@5 95.000 (95.000)   [2025-10-22 18:25:15]
  Epoch: [024][100/500]   Time 0.060 (0.234)   Data 0.000 (0.174)   Loss 1.1368 (0.9163)   Prec@1 65.000 (68.050)   Prec@5 95.000 (97.208)   [2025-10-22 18:25:22]
  Epoch: [024][200/500]   Time 0.061 (0.149)   Data 0.000 (0.088)   Loss 0.8765 (0.9241)   Prec@1 68.000 (67.547)   Prec@5 97.000 (97.109)   [2025-10-22 18:25:28]
  Epoch: [024][300/500]   Time 0.063 (0.120)   Data 0.001 (0.059)   Loss 0.8348 (0.9279)   Prec@1 66.000 (67.319)   Prec@5 97.000 (97.136)   [2025-10-22 18:25:34]
  Epoch: [024][400/500]   Time 0.061 (0.105)   Data 0.000 (0.044)   Loss 0.9079 (0.9273)   Prec@1 66.000 (67.369)   Prec@5 96.000 (97.107)   [2025-10-22 18:25:40]
  **Train** Prec@1 67.392 Prec@5 97.122 Error@1 32.608
  **Test** Prec@1 74.300 Prec@5 98.310 Error@1 25.700

==>>[2025-10-22 18:26:06] [Epoch=025/040] [Need: 00:16:47] [LR=0.0010] [Best : Accuracy=75.56, Error=24.44]
  Epoch: [025][000/500]   Time 17.270 (17.270)   Data 17.200 (17.200)   Loss 0.8280 (0.8280)   Prec@1 69.000 (69.000)   Prec@5 99.000 (99.000)   [2025-10-22 18:26:23]
  Epoch: [025][100/500]   Time 0.062 (0.231)   Data 0.001 (0.171)   Loss 0.7188 (0.8818)   Prec@1 72.000 (69.059)   Prec@5 99.000 (97.416)   [2025-10-22 18:26:29]
  Epoch: [025][200/500]   Time 0.059 (0.147)   Data 0.001 (0.086)   Loss 0.9052 (0.8705)   Prec@1 67.000 (69.507)   Prec@5 97.000 (97.458)   [2025-10-22 18:26:35]
  Epoch: [025][300/500]   Time 0.062 (0.118)   Data 0.001 (0.058)   Loss 0.9278 (0.8715)   Prec@1 68.000 (69.435)   Prec@5 97.000 (97.502)   [2025-10-22 18:26:42]
  Epoch: [025][400/500]   Time 0.059 (0.104)   Data 0.001 (0.044)   Loss 0.6432 (0.8715)   Prec@1 78.000 (69.596)   Prec@5 97.000 (97.456)   [2025-10-22 18:26:48]
  **Train** Prec@1 69.880 Prec@5 97.516 Error@1 30.120
  **Test** Prec@1 76.280 Prec@5 98.480 Error@1 23.720
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 18:27:14] [Epoch=026/040] [Need: 00:15:40] [LR=0.0010] [Best : Accuracy=76.28, Error=23.72]
  Epoch: [026][000/500]   Time 17.360 (17.360)   Data 17.285 (17.285)   Loss 1.0496 (1.0496)   Prec@1 63.000 (63.000)   Prec@5 96.000 (96.000)   [2025-10-22 18:27:31]
  Epoch: [026][100/500]   Time 0.062 (0.231)   Data 0.001 (0.172)   Loss 0.8263 (0.8505)   Prec@1 72.000 (70.386)   Prec@5 96.000 (97.584)   [2025-10-22 18:27:37]
  Epoch: [026][200/500]   Time 0.061 (0.147)   Data 0.000 (0.087)   Loss 0.9502 (0.8552)   Prec@1 65.000 (70.249)   Prec@5 97.000 (97.448)   [2025-10-22 18:27:43]
  Epoch: [026][300/500]   Time 0.061 (0.117)   Data 0.000 (0.058)   Loss 0.9181 (0.8532)   Prec@1 63.000 (70.233)   Prec@5 100.000 (97.508)   [2025-10-22 18:27:49]
  Epoch: [026][400/500]   Time 0.059 (0.103)   Data 0.001 (0.044)   Loss 0.8167 (0.8527)   Prec@1 72.000 (70.197)   Prec@5 100.000 (97.519)   [2025-10-22 18:27:55]
  **Train** Prec@1 70.192 Prec@5 97.582 Error@1 29.808
  **Test** Prec@1 76.760 Prec@5 98.490 Error@1 23.240
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 18:28:21] [Epoch=027/040] [Need: 00:14:33] [LR=0.0010] [Best : Accuracy=76.76, Error=23.24]
  Epoch: [027][000/500]   Time 17.449 (17.449)   Data 17.376 (17.376)   Loss 0.9649 (0.9649)   Prec@1 71.000 (71.000)   Prec@5 98.000 (98.000)   [2025-10-22 18:28:38]
  Epoch: [027][100/500]   Time 0.054 (0.231)   Data 0.000 (0.173)   Loss 0.8845 (0.8485)   Prec@1 72.000 (70.525)   Prec@5 98.000 (97.485)   [2025-10-22 18:28:44]
  Epoch: [027][200/500]   Time 0.062 (0.145)   Data 0.001 (0.087)   Loss 0.7416 (0.8396)   Prec@1 73.000 (70.617)   Prec@5 99.000 (97.607)   [2025-10-22 18:28:50]
  Epoch: [027][300/500]   Time 0.057 (0.116)   Data 0.001 (0.058)   Loss 0.7383 (0.8370)   Prec@1 71.000 (70.631)   Prec@5 99.000 (97.654)   [2025-10-22 18:28:56]
  Epoch: [027][400/500]   Time 0.059 (0.102)   Data 0.000 (0.044)   Loss 0.8438 (0.8395)   Prec@1 74.000 (70.656)   Prec@5 99.000 (97.596)   [2025-10-22 18:29:02]
  **Train** Prec@1 70.702 Prec@5 97.584 Error@1 29.298
  **Test** Prec@1 76.370 Prec@5 98.340 Error@1 23.630

==>>[2025-10-22 18:29:27] [Epoch=028/040] [Need: 00:13:26] [LR=0.0010] [Best : Accuracy=76.76, Error=23.24]
  Epoch: [028][000/500]   Time 17.159 (17.159)   Data 17.087 (17.087)   Loss 0.9957 (0.9957)   Prec@1 66.000 (66.000)   Prec@5 98.000 (98.000)   [2025-10-22 18:29:44]
  Epoch: [028][100/500]   Time 0.061 (0.228)   Data 0.000 (0.170)   Loss 0.9765 (0.8203)   Prec@1 67.000 (71.614)   Prec@5 98.000 (97.644)   [2025-10-22 18:29:50]
  Epoch: [028][200/500]   Time 0.060 (0.144)   Data 0.000 (0.086)   Loss 0.7253 (0.8298)   Prec@1 73.000 (71.358)   Prec@5 100.000 (97.602)   [2025-10-22 18:29:56]
  Epoch: [028][300/500]   Time 0.059 (0.116)   Data 0.001 (0.057)   Loss 0.8741 (0.8304)   Prec@1 70.000 (71.336)   Prec@5 98.000 (97.724)   [2025-10-22 18:30:02]
  Epoch: [028][400/500]   Time 0.058 (0.102)   Data 0.000 (0.043)   Loss 0.7743 (0.8282)   Prec@1 70.000 (71.424)   Prec@5 99.000 (97.728)   [2025-10-22 18:30:08]
  **Train** Prec@1 71.186 Prec@5 97.700 Error@1 28.814
  **Test** Prec@1 76.600 Prec@5 98.390 Error@1 23.400

==>>[2025-10-22 18:30:34] [Epoch=029/040] [Need: 00:12:18] [LR=0.0010] [Best : Accuracy=76.76, Error=23.24]
  Epoch: [029][000/500]   Time 17.969 (17.969)   Data 17.899 (17.899)   Loss 0.7265 (0.7265)   Prec@1 72.000 (72.000)   Prec@5 100.000 (100.000)   [2025-10-22 18:30:52]
  Epoch: [029][100/500]   Time 0.058 (0.237)   Data 0.000 (0.178)   Loss 0.7969 (0.8320)   Prec@1 74.000 (70.901)   Prec@5 98.000 (97.812)   [2025-10-22 18:30:58]
  Epoch: [029][200/500]   Time 0.065 (0.149)   Data 0.001 (0.090)   Loss 0.9178 (0.8270)   Prec@1 69.000 (71.035)   Prec@5 98.000 (97.756)   [2025-10-22 18:31:04]
  Epoch: [029][300/500]   Time 0.056 (0.119)   Data 0.001 (0.060)   Loss 0.8907 (0.8327)   Prec@1 69.000 (70.907)   Prec@5 99.000 (97.787)   [2025-10-22 18:31:10]
  Epoch: [029][400/500]   Time 0.057 (0.104)   Data 0.000 (0.045)   Loss 0.7927 (0.8315)   Prec@1 75.000 (70.895)   Prec@5 97.000 (97.786)   [2025-10-22 18:31:16]
  **Train** Prec@1 70.956 Prec@5 97.746 Error@1 29.044
  **Test** Prec@1 76.820 Prec@5 98.410 Error@1 23.180
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 18:31:42] [Epoch=030/040] [Need: 00:11:11] [LR=0.0010] [Best : Accuracy=76.82, Error=23.18]
  Epoch: [030][000/500]   Time 17.570 (17.570)   Data 17.501 (17.501)   Loss 0.7989 (0.7989)   Prec@1 69.000 (69.000)   Prec@5 98.000 (98.000)   [2025-10-22 18:31:59]
  Epoch: [030][100/500]   Time 0.058 (0.233)   Data 0.000 (0.174)   Loss 0.8584 (0.8281)   Prec@1 70.000 (71.099)   Prec@5 96.000 (97.723)   [2025-10-22 18:32:05]
  Epoch: [030][200/500]   Time 0.059 (0.146)   Data 0.001 (0.088)   Loss 0.9447 (0.8335)   Prec@1 70.000 (71.000)   Prec@5 98.000 (97.617)   [2025-10-22 18:32:11]
  Epoch: [030][300/500]   Time 0.058 (0.117)   Data 0.000 (0.059)   Loss 0.7121 (0.8326)   Prec@1 71.000 (70.897)   Prec@5 99.000 (97.638)   [2025-10-22 18:32:17]
  Epoch: [030][400/500]   Time 0.057 (0.103)   Data 0.000 (0.044)   Loss 0.8610 (0.8291)   Prec@1 72.000 (71.092)   Prec@5 96.000 (97.611)   [2025-10-22 18:32:23]
  **Train** Prec@1 71.150 Prec@5 97.632 Error@1 28.850
  **Test** Prec@1 77.040 Prec@5 98.430 Error@1 22.960
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 18:32:49] [Epoch=031/040] [Need: 00:10:04] [LR=0.0010] [Best : Accuracy=77.04, Error=22.96]
  Epoch: [031][000/500]   Time 17.459 (17.459)   Data 17.384 (17.384)   Loss 0.8571 (0.8571)   Prec@1 66.000 (66.000)   Prec@5 96.000 (96.000)   [2025-10-22 18:33:06]
  Epoch: [031][100/500]   Time 0.056 (0.231)   Data 0.001 (0.173)   Loss 1.0261 (0.8231)   Prec@1 65.000 (71.109)   Prec@5 97.000 (97.743)   [2025-10-22 18:33:12]
  Epoch: [031][200/500]   Time 0.058 (0.145)   Data 0.001 (0.087)   Loss 1.0194 (0.8301)   Prec@1 70.000 (71.149)   Prec@5 96.000 (97.677)   [2025-10-22 18:33:18]
  Epoch: [031][300/500]   Time 0.058 (0.117)   Data 0.001 (0.058)   Loss 0.9390 (0.8238)   Prec@1 64.000 (71.249)   Prec@5 97.000 (97.777)   [2025-10-22 18:33:24]
  Epoch: [031][400/500]   Time 0.058 (0.102)   Data 0.001 (0.044)   Loss 0.6659 (0.8227)   Prec@1 80.000 (71.287)   Prec@5 98.000 (97.805)   [2025-10-22 18:33:30]
  **Train** Prec@1 71.126 Prec@5 97.736 Error@1 28.874
  **Test** Prec@1 76.820 Prec@5 98.490 Error@1 23.180

==>>[2025-10-22 18:33:55] [Epoch=032/040] [Need: 00:08:57] [LR=0.0010] [Best : Accuracy=77.04, Error=22.96]
  Epoch: [032][000/500]   Time 17.232 (17.232)   Data 17.163 (17.163)   Loss 0.7333 (0.7333)   Prec@1 70.000 (70.000)   Prec@5 96.000 (96.000)   [2025-10-22 18:34:13]
  Epoch: [032][100/500]   Time 0.059 (0.230)   Data 0.000 (0.171)   Loss 0.8861 (0.8169)   Prec@1 66.000 (71.069)   Prec@5 96.000 (97.842)   [2025-10-22 18:34:19]
  Epoch: [032][200/500]   Time 0.063 (0.146)   Data 0.001 (0.086)   Loss 0.9598 (0.8156)   Prec@1 72.000 (71.373)   Prec@5 93.000 (97.841)   [2025-10-22 18:34:25]
  Epoch: [032][300/500]   Time 0.061 (0.117)   Data 0.000 (0.058)   Loss 0.8099 (0.8219)   Prec@1 70.000 (71.339)   Prec@5 98.000 (97.837)   [2025-10-22 18:34:31]
  Epoch: [032][400/500]   Time 0.061 (0.102)   Data 0.001 (0.043)   Loss 0.7915 (0.8245)   Prec@1 75.000 (71.202)   Prec@5 98.000 (97.800)   [2025-10-22 18:34:37]
  **Train** Prec@1 71.166 Prec@5 97.798 Error@1 28.834
  **Test** Prec@1 77.080 Prec@5 98.380 Error@1 22.920
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 18:35:02] [Epoch=033/040] [Need: 00:07:50] [LR=0.0010] [Best : Accuracy=77.08, Error=22.92]
  Epoch: [033][000/500]   Time 17.317 (17.317)   Data 17.245 (17.245)   Loss 0.9449 (0.9449)   Prec@1 65.000 (65.000)   Prec@5 96.000 (96.000)   [2025-10-22 18:35:20]
  Epoch: [033][100/500]   Time 0.056 (0.231)   Data 0.000 (0.171)   Loss 0.8056 (0.8374)   Prec@1 69.000 (71.030)   Prec@5 99.000 (97.584)   [2025-10-22 18:35:26]
  Epoch: [033][200/500]   Time 0.055 (0.145)   Data 0.001 (0.086)   Loss 0.7054 (0.8262)   Prec@1 80.000 (71.284)   Prec@5 97.000 (97.672)   [2025-10-22 18:35:32]
  Epoch: [033][300/500]   Time 0.060 (0.117)   Data 0.001 (0.058)   Loss 0.7174 (0.8259)   Prec@1 69.000 (71.296)   Prec@5 99.000 (97.674)   [2025-10-22 18:35:37]
  Epoch: [033][400/500]   Time 0.065 (0.103)   Data 0.000 (0.044)   Loss 0.8350 (0.8239)   Prec@1 72.000 (71.426)   Prec@5 97.000 (97.743)   [2025-10-22 18:35:43]
  **Train** Prec@1 71.440 Prec@5 97.808 Error@1 28.560
  **Test** Prec@1 76.960 Prec@5 98.480 Error@1 23.040

==>>[2025-10-22 18:36:09] [Epoch=034/040] [Need: 00:06:42] [LR=0.0010] [Best : Accuracy=77.08, Error=22.92]
  Epoch: [034][000/500]   Time 17.650 (17.650)   Data 17.582 (17.582)   Loss 0.6912 (0.6912)   Prec@1 77.000 (77.000)   Prec@5 99.000 (99.000)   [2025-10-22 18:36:27]
  Epoch: [034][100/500]   Time 0.056 (0.234)   Data 0.001 (0.175)   Loss 0.9674 (0.8048)   Prec@1 69.000 (71.960)   Prec@5 97.000 (97.752)   [2025-10-22 18:36:33]
  Epoch: [034][200/500]   Time 0.062 (0.147)   Data 0.000 (0.088)   Loss 0.7236 (0.8146)   Prec@1 77.000 (71.607)   Prec@5 97.000 (97.761)   [2025-10-22 18:36:39]
  Epoch: [034][300/500]   Time 0.058 (0.118)   Data 0.001 (0.059)   Loss 1.0552 (0.8198)   Prec@1 60.000 (71.442)   Prec@5 95.000 (97.678)   [2025-10-22 18:36:45]
  Epoch: [034][400/500]   Time 0.063 (0.103)   Data 0.001 (0.044)   Loss 0.8825 (0.8205)   Prec@1 66.000 (71.419)   Prec@5 99.000 (97.738)   [2025-10-22 18:36:51]
  **Train** Prec@1 71.466 Prec@5 97.750 Error@1 28.534
  **Test** Prec@1 77.180 Prec@5 98.530 Error@1 22.820
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 18:37:16] [Epoch=035/040] [Need: 00:05:35] [LR=0.0010] [Best : Accuracy=77.18, Error=22.82]
  Epoch: [035][000/500]   Time 17.342 (17.342)   Data 17.268 (17.268)   Loss 0.8863 (0.8863)   Prec@1 68.000 (68.000)   Prec@5 97.000 (97.000)   [2025-10-22 18:37:34]
  Epoch: [035][100/500]   Time 0.056 (0.229)   Data 0.001 (0.172)   Loss 0.8909 (0.8051)   Prec@1 67.000 (71.525)   Prec@5 98.000 (98.129)   [2025-10-22 18:37:40]
  Epoch: [035][200/500]   Time 0.056 (0.144)   Data 0.001 (0.086)   Loss 1.0079 (0.8201)   Prec@1 66.000 (71.348)   Prec@5 96.000 (97.816)   [2025-10-22 18:37:45]
  Epoch: [035][300/500]   Time 0.054 (0.115)   Data 0.001 (0.058)   Loss 0.8372 (0.8188)   Prec@1 71.000 (71.462)   Prec@5 97.000 (97.867)   [2025-10-22 18:37:51]
  Epoch: [035][400/500]   Time 0.059 (0.101)   Data 0.001 (0.044)   Loss 0.6984 (0.8219)   Prec@1 76.000 (71.362)   Prec@5 97.000 (97.791)   [2025-10-22 18:37:57]
  **Train** Prec@1 71.322 Prec@5 97.790 Error@1 28.678
  **Test** Prec@1 76.920 Prec@5 98.570 Error@1 23.080

==>>[2025-10-22 18:38:23] [Epoch=036/040] [Need: 00:04:28] [LR=0.0010] [Best : Accuracy=77.18, Error=22.82]
  Epoch: [036][000/500]   Time 17.369 (17.369)   Data 17.294 (17.294)   Loss 0.9046 (0.9046)   Prec@1 67.000 (67.000)   Prec@5 95.000 (95.000)   [2025-10-22 18:38:40]
  Epoch: [036][100/500]   Time 0.057 (0.230)   Data 0.000 (0.172)   Loss 0.7963 (0.8072)   Prec@1 72.000 (72.218)   Prec@5 99.000 (97.743)   [2025-10-22 18:38:46]
  Epoch: [036][200/500]   Time 0.062 (0.145)   Data 0.000 (0.087)   Loss 0.7974 (0.8031)   Prec@1 71.000 (72.070)   Prec@5 99.000 (97.846)   [2025-10-22 18:38:52]
  Epoch: [036][300/500]   Time 0.065 (0.117)   Data 0.001 (0.058)   Loss 0.8971 (0.8059)   Prec@1 69.000 (72.003)   Prec@5 98.000 (97.817)   [2025-10-22 18:38:58]
  Epoch: [036][400/500]   Time 0.061 (0.102)   Data 0.001 (0.044)   Loss 0.7680 (0.8088)   Prec@1 74.000 (71.753)   Prec@5 98.000 (97.823)   [2025-10-22 18:39:04]
  **Train** Prec@1 71.502 Prec@5 97.738 Error@1 28.498
  **Test** Prec@1 77.250 Prec@5 98.510 Error@1 22.750
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 18:39:30] [Epoch=037/040] [Need: 00:03:21] [LR=0.0010] [Best : Accuracy=77.25, Error=22.75]
  Epoch: [037][000/500]   Time 17.304 (17.304)   Data 17.232 (17.232)   Loss 0.7792 (0.7792)   Prec@1 72.000 (72.000)   Prec@5 96.000 (96.000)   [2025-10-22 18:39:47]
  Epoch: [037][100/500]   Time 0.059 (0.229)   Data 0.000 (0.171)   Loss 0.8936 (0.8196)   Prec@1 73.000 (71.208)   Prec@5 97.000 (97.802)   [2025-10-22 18:39:53]
  Epoch: [037][200/500]   Time 0.059 (0.144)   Data 0.001 (0.086)   Loss 0.6658 (0.8155)   Prec@1 74.000 (71.353)   Prec@5 99.000 (97.896)   [2025-10-22 18:39:59]
  Epoch: [037][300/500]   Time 0.061 (0.116)   Data 0.000 (0.058)   Loss 0.6239 (0.8125)   Prec@1 77.000 (71.578)   Prec@5 99.000 (97.944)   [2025-10-22 18:40:05]
  Epoch: [037][400/500]   Time 0.060 (0.101)   Data 0.001 (0.044)   Loss 0.7157 (0.8144)   Prec@1 77.000 (71.531)   Prec@5 98.000 (97.915)   [2025-10-22 18:40:10]
  **Train** Prec@1 71.702 Prec@5 97.816 Error@1 28.298
  **Test** Prec@1 77.290 Prec@5 98.490 Error@1 22.710
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 18:40:36] [Epoch=038/040] [Need: 00:02:14] [LR=0.0010] [Best : Accuracy=77.29, Error=22.71]
  Epoch: [038][000/500]   Time 17.338 (17.338)   Data 17.265 (17.265)   Loss 0.7730 (0.7730)   Prec@1 75.000 (75.000)   Prec@5 98.000 (98.000)   [2025-10-22 18:40:54]
  Epoch: [038][100/500]   Time 0.057 (0.230)   Data 0.000 (0.172)   Loss 0.7833 (0.8085)   Prec@1 72.000 (72.079)   Prec@5 99.000 (97.832)   [2025-10-22 18:41:00]
  Epoch: [038][200/500]   Time 0.060 (0.145)   Data 0.001 (0.086)   Loss 0.8090 (0.8092)   Prec@1 74.000 (71.905)   Prec@5 98.000 (97.826)   [2025-10-22 18:41:05]
  Epoch: [038][300/500]   Time 0.064 (0.116)   Data 0.001 (0.058)   Loss 0.7143 (0.8135)   Prec@1 78.000 (71.744)   Prec@5 100.000 (97.784)   [2025-10-22 18:41:11]
  Epoch: [038][400/500]   Time 0.058 (0.102)   Data 0.001 (0.044)   Loss 0.7803 (0.8136)   Prec@1 73.000 (71.606)   Prec@5 98.000 (97.835)   [2025-10-22 18:41:17]
  **Train** Prec@1 71.652 Prec@5 97.820 Error@1 28.348
  **Test** Prec@1 77.510 Prec@5 98.480 Error@1 22.490
=> Obtain best accuracy, and update the best model

==>>[2025-10-22 18:41:43] [Epoch=039/040] [Need: 00:01:07] [LR=0.0010] [Best : Accuracy=77.51, Error=22.49]
  Epoch: [039][000/500]   Time 17.266 (17.266)   Data 17.194 (17.194)   Loss 0.8141 (0.8141)   Prec@1 70.000 (70.000)   Prec@5 96.000 (96.000)   [2025-10-22 18:42:00]
  Epoch: [039][100/500]   Time 0.056 (0.229)   Data 0.001 (0.171)   Loss 0.8117 (0.8143)   Prec@1 71.000 (71.683)   Prec@5 98.000 (97.683)   [2025-10-22 18:42:06]
  Epoch: [039][200/500]   Time 0.057 (0.144)   Data 0.000 (0.086)   Loss 0.9954 (0.8162)   Prec@1 66.000 (71.453)   Prec@5 94.000 (97.697)   [2025-10-22 18:42:12]
  Epoch: [039][300/500]   Time 0.061 (0.116)   Data 0.001 (0.058)   Loss 0.8609 (0.8133)   Prec@1 74.000 (71.651)   Prec@5 98.000 (97.684)   [2025-10-22 18:42:18]
  Epoch: [039][400/500]   Time 0.060 (0.101)   Data 0.001 (0.044)   Loss 0.6594 (0.8155)   Prec@1 75.000 (71.608)   Prec@5 100.000 (97.711)   [2025-10-22 18:42:23]
  **Train** Prec@1 71.796 Prec@5 97.730 Error@1 28.204
  **Test** Prec@1 77.480 Prec@5 98.520 Error@1 22.520
