save path : ./save/tinyvgg_quan/clipping_0.2_0.01
{'data_path': './dataset', 'arch': 'tinyvgg_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': False, 'clipping_coeff': 0.2, 'learning_rate': 0.01, 'manualSeed': 1372, 'save_path': './save/tinyvgg_quan/clipping_0.2_0.01', 'enable_bfa': False, 'resume': '', 'quan_bitwidth': None, 'reset_weight': False, 'evaluate': False, 'n_iter': 30, 'model_only': False, 'random_bfa': False, 'use_cuda': True}
Random Seed: 1372
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'tinyvgg_quan'
=> network :
 TinyVGG(
  (features): Sequential(
    (0): quan_Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Dropout2d(p=0.25, inplace=False)
    (6): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace=True)
    (8): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace=True)
    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (11): Dropout2d(p=0.25, inplace=False)
  )
  (classifier): Sequential(
    (0): quan_Linear(in_features=2048, out_features=10, bias=True)
  )
)
=> do not use any checkpoint for tinyvgg_quan model

==>>[2025-10-23 16:18:05] [Epoch=000/040] [Need: 00:00:00] [LR=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/500]   Time 18.192 (18.192)   Data 17.326 (17.326)   Loss 2.3042 (2.3042)   Prec@1 14.000 (14.000)   Prec@5 48.000 (48.000)   [2025-10-23 16:18:23]
  Epoch: [000][100/500]   Time 0.010 (0.193)   Data 0.000 (0.172)   Loss 2.0389 (2.2162)   Prec@1 26.000 (17.119)   Prec@5 82.000 (63.366)   [2025-10-23 16:18:25]
  Epoch: [000][200/500]   Time 0.013 (0.102)   Data 0.000 (0.086)   Loss 1.9065 (2.1147)   Prec@1 29.000 (21.801)   Prec@5 78.000 (70.587)   [2025-10-23 16:18:26]
  Epoch: [000][300/500]   Time 0.011 (0.072)   Data 0.000 (0.058)   Loss 1.6877 (2.0363)   Prec@1 32.000 (25.126)   Prec@5 89.000 (74.678)   [2025-10-23 16:18:27]
  Epoch: [000][400/500]   Time 0.011 (0.057)   Data 0.000 (0.043)   Loss 1.8578 (1.9681)   Prec@1 30.000 (27.845)   Prec@5 87.000 (77.678)   [2025-10-23 16:18:28]
  **Train** Prec@1 30.014 Prec@5 79.672 Error@1 69.986
  **Test** Prec@1 46.250 Prec@5 91.470 Error@1 53.750
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:18:47] [Epoch=001/040] [Need: 00:27:29] [LR=0.0100] [Best : Accuracy=46.25, Error=53.75]
  Epoch: [001][000/500]   Time 17.512 (17.512)   Data 17.467 (17.467)   Loss 1.6246 (1.6246)   Prec@1 40.000 (40.000)   Prec@5 91.000 (91.000)   [2025-10-23 16:19:05]
  Epoch: [001][100/500]   Time 0.011 (0.185)   Data 0.000 (0.173)   Loss 1.6458 (1.6452)   Prec@1 40.000 (40.802)   Prec@5 90.000 (88.733)   [2025-10-23 16:19:06]
  Epoch: [001][200/500]   Time 0.009 (0.099)   Data 0.000 (0.087)   Loss 1.5736 (1.6298)   Prec@1 43.000 (41.070)   Prec@5 87.000 (88.960)   [2025-10-23 16:19:07]
  Epoch: [001][300/500]   Time 0.010 (0.069)   Data 0.000 (0.058)   Loss 1.4827 (1.6132)   Prec@1 46.000 (41.601)   Prec@5 91.000 (89.272)   [2025-10-23 16:19:08]
  Epoch: [001][400/500]   Time 0.013 (0.055)   Data 0.000 (0.044)   Loss 1.5378 (1.5982)   Prec@1 45.000 (42.219)   Prec@5 91.000 (89.561)   [2025-10-23 16:19:09]
  **Train** Prec@1 42.944 Prec@5 89.850 Error@1 57.056
  **Test** Prec@1 52.470 Prec@5 93.170 Error@1 47.530
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:19:29] [Epoch=002/040] [Need: 00:26:38] [LR=0.0100] [Best : Accuracy=52.47, Error=47.53]
  Epoch: [002][000/500]   Time 18.046 (18.046)   Data 18.000 (18.000)   Loss 1.5707 (1.5707)   Prec@1 51.000 (51.000)   Prec@5 88.000 (88.000)   [2025-10-23 16:19:47]
  Epoch: [002][100/500]   Time 0.008 (0.193)   Data 0.000 (0.178)   Loss 1.4565 (1.4948)   Prec@1 52.000 (45.950)   Prec@5 94.000 (91.515)   [2025-10-23 16:19:49]
  Epoch: [002][200/500]   Time 0.010 (0.102)   Data 0.000 (0.090)   Loss 1.4787 (1.4913)   Prec@1 39.000 (46.274)   Prec@5 92.000 (91.453)   [2025-10-23 16:19:50]
  Epoch: [002][300/500]   Time 0.010 (0.072)   Data 0.000 (0.060)   Loss 1.2950 (1.4824)   Prec@1 46.000 (46.787)   Prec@5 94.000 (91.585)   [2025-10-23 16:19:51]
  Epoch: [002][400/500]   Time 0.010 (0.056)   Data 0.000 (0.045)   Loss 1.2418 (1.4644)   Prec@1 56.000 (47.434)   Prec@5 97.000 (91.853)   [2025-10-23 16:19:52]
  **Train** Prec@1 47.850 Prec@5 92.014 Error@1 52.150
  **Test** Prec@1 58.220 Prec@5 95.630 Error@1 41.780
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:20:11] [Epoch=003/040] [Need: 00:25:57] [LR=0.0100] [Best : Accuracy=58.22, Error=41.78]
  Epoch: [003][000/500]   Time 17.607 (17.607)   Data 17.564 (17.564)   Loss 1.3050 (1.3050)   Prec@1 58.000 (58.000)   Prec@5 94.000 (94.000)   [2025-10-23 16:20:29]
  Epoch: [003][100/500]   Time 0.011 (0.187)   Data 0.000 (0.174)   Loss 1.3546 (1.3683)   Prec@1 51.000 (51.030)   Prec@5 91.000 (93.238)   [2025-10-23 16:20:30]
  Epoch: [003][200/500]   Time 0.009 (0.099)   Data 0.000 (0.088)   Loss 1.3897 (1.3653)   Prec@1 48.000 (51.214)   Prec@5 92.000 (93.264)   [2025-10-23 16:20:31]
  Epoch: [003][300/500]   Time 0.009 (0.070)   Data 0.000 (0.059)   Loss 1.1343 (1.3615)   Prec@1 61.000 (51.236)   Prec@5 96.000 (93.219)   [2025-10-23 16:20:33]
  Epoch: [003][400/500]   Time 0.012 (0.055)   Data 0.000 (0.044)   Loss 1.2845 (1.3546)   Prec@1 50.000 (51.541)   Prec@5 96.000 (93.249)   [2025-10-23 16:20:34]
  **Train** Prec@1 51.950 Prec@5 93.342 Error@1 48.050
  **Test** Prec@1 60.160 Prec@5 95.860 Error@1 39.840
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:20:53] [Epoch=004/040] [Need: 00:25:10] [LR=0.0100] [Best : Accuracy=60.16, Error=39.84]
  Epoch: [004][000/500]   Time 17.371 (17.371)   Data 17.326 (17.326)   Loss 1.4610 (1.4610)   Prec@1 48.000 (48.000)   Prec@5 92.000 (92.000)   [2025-10-23 16:21:10]
  Epoch: [004][100/500]   Time 0.010 (0.185)   Data 0.000 (0.172)   Loss 1.1020 (1.2930)   Prec@1 64.000 (53.465)   Prec@5 95.000 (94.079)   [2025-10-23 16:21:12]
  Epoch: [004][200/500]   Time 0.009 (0.098)   Data 0.001 (0.086)   Loss 1.2527 (1.2869)   Prec@1 61.000 (54.164)   Prec@5 94.000 (93.910)   [2025-10-23 16:21:13]
  Epoch: [004][300/500]   Time 0.012 (0.069)   Data 0.000 (0.058)   Loss 1.4205 (1.2762)   Prec@1 52.000 (54.628)   Prec@5 97.000 (94.133)   [2025-10-23 16:21:14]
  Epoch: [004][400/500]   Time 0.010 (0.054)   Data 0.001 (0.043)   Loss 1.3605 (1.2644)   Prec@1 52.000 (55.067)   Prec@5 91.000 (94.177)   [2025-10-23 16:21:15]
  **Train** Prec@1 55.262 Prec@5 94.250 Error@1 44.738
  **Test** Prec@1 65.510 Prec@5 96.890 Error@1 34.490
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:21:34] [Epoch=005/040] [Need: 00:24:24] [LR=0.0100] [Best : Accuracy=65.51, Error=34.49]
  Epoch: [005][000/500]   Time 17.362 (17.362)   Data 17.318 (17.318)   Loss 1.2902 (1.2902)   Prec@1 59.000 (59.000)   Prec@5 93.000 (93.000)   [2025-10-23 16:21:52]
  Epoch: [005][100/500]   Time 0.010 (0.185)   Data 0.000 (0.172)   Loss 1.2861 (1.2211)   Prec@1 56.000 (56.782)   Prec@5 95.000 (94.812)   [2025-10-23 16:21:53]
  Epoch: [005][200/500]   Time 0.012 (0.098)   Data 0.000 (0.086)   Loss 1.1380 (1.2093)   Prec@1 57.000 (57.080)   Prec@5 96.000 (94.821)   [2025-10-23 16:21:54]
  Epoch: [005][300/500]   Time 0.011 (0.069)   Data 0.000 (0.058)   Loss 1.2263 (1.2076)   Prec@1 54.000 (57.133)   Prec@5 97.000 (94.791)   [2025-10-23 16:21:55]
  Epoch: [005][400/500]   Time 0.012 (0.054)   Data 0.000 (0.043)   Loss 1.2649 (1.2063)   Prec@1 55.000 (57.152)   Prec@5 95.000 (94.781)   [2025-10-23 16:21:56]
  **Train** Prec@1 57.276 Prec@5 94.806 Error@1 42.724
  **Test** Prec@1 65.910 Prec@5 97.020 Error@1 34.090
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:22:16] [Epoch=006/040] [Need: 00:23:40] [LR=0.0100] [Best : Accuracy=65.91, Error=34.09]
  Epoch: [006][000/500]   Time 17.297 (17.297)   Data 17.253 (17.253)   Loss 1.3343 (1.3343)   Prec@1 55.000 (55.000)   Prec@5 93.000 (93.000)   [2025-10-23 16:22:33]
  Epoch: [006][100/500]   Time 0.010 (0.184)   Data 0.000 (0.171)   Loss 1.2265 (1.1751)   Prec@1 57.000 (58.752)   Prec@5 95.000 (95.040)   [2025-10-23 16:22:34]
  Epoch: [006][200/500]   Time 0.009 (0.098)   Data 0.000 (0.086)   Loss 0.8741 (1.1719)   Prec@1 72.000 (58.627)   Prec@5 98.000 (95.025)   [2025-10-23 16:22:35]
  Epoch: [006][300/500]   Time 0.011 (0.069)   Data 0.000 (0.057)   Loss 1.2272 (1.1703)   Prec@1 63.000 (58.575)   Prec@5 94.000 (95.176)   [2025-10-23 16:22:37]
  Epoch: [006][400/500]   Time 0.012 (0.054)   Data 0.000 (0.043)   Loss 1.2535 (1.1682)   Prec@1 54.000 (58.738)   Prec@5 96.000 (95.110)   [2025-10-23 16:22:38]
  **Train** Prec@1 58.784 Prec@5 95.102 Error@1 41.216
  **Test** Prec@1 67.320 Prec@5 97.110 Error@1 32.680
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:22:57] [Epoch=007/040] [Need: 00:22:55] [LR=0.0100] [Best : Accuracy=67.32, Error=32.68]
  Epoch: [007][000/500]   Time 17.369 (17.369)   Data 17.325 (17.325)   Loss 1.1862 (1.1862)   Prec@1 61.000 (61.000)   Prec@5 93.000 (93.000)   [2025-10-23 16:23:14]
  Epoch: [007][100/500]   Time 0.010 (0.185)   Data 0.001 (0.172)   Loss 0.9640 (1.1459)   Prec@1 71.000 (59.733)   Prec@5 96.000 (95.178)   [2025-10-23 16:23:16]
  Epoch: [007][200/500]   Time 0.010 (0.098)   Data 0.000 (0.086)   Loss 1.1909 (1.1431)   Prec@1 64.000 (60.015)   Prec@5 94.000 (95.194)   [2025-10-23 16:23:17]
  Epoch: [007][300/500]   Time 0.009 (0.069)   Data 0.000 (0.058)   Loss 0.9849 (1.1395)   Prec@1 63.000 (59.980)   Prec@5 99.000 (95.219)   [2025-10-23 16:23:18]
  Epoch: [007][400/500]   Time 0.011 (0.054)   Data 0.000 (0.043)   Loss 1.2192 (1.1381)   Prec@1 55.000 (59.975)   Prec@5 93.000 (95.299)   [2025-10-23 16:23:19]
  **Train** Prec@1 60.176 Prec@5 95.368 Error@1 39.824
  **Test** Prec@1 68.400 Prec@5 97.480 Error@1 31.600
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:23:38] [Epoch=008/040] [Need: 00:22:11] [LR=0.0100] [Best : Accuracy=68.40, Error=31.60]
  Epoch: [008][000/500]   Time 18.024 (18.024)   Data 17.978 (17.978)   Loss 1.0048 (1.0048)   Prec@1 65.000 (65.000)   Prec@5 95.000 (95.000)   [2025-10-23 16:23:56]
  Epoch: [008][100/500]   Time 0.010 (0.192)   Data 0.000 (0.178)   Loss 1.1226 (1.1181)   Prec@1 58.000 (60.337)   Prec@5 98.000 (95.347)   [2025-10-23 16:23:58]
  Epoch: [008][200/500]   Time 0.011 (0.102)   Data 0.000 (0.090)   Loss 1.1987 (1.1139)   Prec@1 56.000 (60.542)   Prec@5 95.000 (95.403)   [2025-10-23 16:23:59]
  Epoch: [008][300/500]   Time 0.011 (0.072)   Data 0.000 (0.060)   Loss 1.1073 (1.1154)   Prec@1 57.000 (60.455)   Prec@5 97.000 (95.435)   [2025-10-23 16:24:00]
  Epoch: [008][400/500]   Time 0.012 (0.057)   Data 0.000 (0.045)   Loss 1.0786 (1.1107)   Prec@1 61.000 (60.743)   Prec@5 96.000 (95.509)   [2025-10-23 16:24:01]
  **Train** Prec@1 60.940 Prec@5 95.598 Error@1 39.060
  **Test** Prec@1 69.750 Prec@5 97.640 Error@1 30.250
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:24:20] [Epoch=009/040] [Need: 00:21:32] [LR=0.0100] [Best : Accuracy=69.75, Error=30.25]
  Epoch: [009][000/500]   Time 17.463 (17.463)   Data 17.419 (17.419)   Loss 1.0201 (1.0201)   Prec@1 65.000 (65.000)   Prec@5 99.000 (99.000)   [2025-10-23 16:24:38]
  Epoch: [009][100/500]   Time 0.013 (0.185)   Data 0.001 (0.173)   Loss 1.0949 (1.0964)   Prec@1 66.000 (61.366)   Prec@5 92.000 (95.762)   [2025-10-23 16:24:39]
  Epoch: [009][200/500]   Time 0.009 (0.098)   Data 0.000 (0.087)   Loss 0.9720 (1.0790)   Prec@1 66.000 (61.990)   Prec@5 98.000 (95.930)   [2025-10-23 16:24:40]
  Epoch: [009][300/500]   Time 0.010 (0.069)   Data 0.000 (0.058)   Loss 1.1748 (1.0770)   Prec@1 60.000 (62.176)   Prec@5 96.000 (96.030)   [2025-10-23 16:24:41]
  Epoch: [009][400/500]   Time 0.012 (0.055)   Data 0.000 (0.044)   Loss 1.2928 (1.0809)   Prec@1 53.000 (62.025)   Prec@5 93.000 (95.935)   [2025-10-23 16:24:42]
  **Train** Prec@1 62.124 Prec@5 96.034 Error@1 37.876
  **Test** Prec@1 70.520 Prec@5 97.780 Error@1 29.480
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:25:02] [Epoch=010/040] [Need: 00:20:50] [LR=0.0100] [Best : Accuracy=70.52, Error=29.48]
  Epoch: [010][000/500]   Time 17.418 (17.418)   Data 17.374 (17.374)   Loss 1.0773 (1.0773)   Prec@1 58.000 (58.000)   Prec@5 95.000 (95.000)   [2025-10-23 16:25:20]
  Epoch: [010][100/500]   Time 0.009 (0.185)   Data 0.000 (0.172)   Loss 0.8549 (1.0498)   Prec@1 70.000 (62.802)   Prec@5 99.000 (96.248)   [2025-10-23 16:25:21]
  Epoch: [010][200/500]   Time 0.011 (0.098)   Data 0.000 (0.087)   Loss 0.9350 (1.0639)   Prec@1 62.000 (62.234)   Prec@5 100.000 (96.114)   [2025-10-23 16:25:22]
  Epoch: [010][300/500]   Time 0.010 (0.069)   Data 0.000 (0.058)   Loss 0.7564 (1.0624)   Prec@1 73.000 (62.615)   Prec@5 98.000 (96.027)   [2025-10-23 16:25:23]
  Epoch: [010][400/500]   Time 0.012 (0.054)   Data 0.000 (0.043)   Loss 0.9786 (1.0591)   Prec@1 68.000 (62.701)   Prec@5 95.000 (96.122)   [2025-10-23 16:25:24]
  **Train** Prec@1 62.728 Prec@5 96.044 Error@1 37.272
  **Test** Prec@1 70.680 Prec@5 97.690 Error@1 29.320
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:25:44] [Epoch=011/040] [Need: 00:20:09] [LR=0.0100] [Best : Accuracy=70.68, Error=29.32]
  Epoch: [011][000/500]   Time 17.180 (17.180)   Data 17.135 (17.135)   Loss 1.1984 (1.1984)   Prec@1 58.000 (58.000)   Prec@5 97.000 (97.000)   [2025-10-23 16:26:01]
  Epoch: [011][100/500]   Time 0.009 (0.181)   Data 0.000 (0.170)   Loss 1.2171 (1.0607)   Prec@1 58.000 (62.624)   Prec@5 93.000 (96.040)   [2025-10-23 16:26:02]
  Epoch: [011][200/500]   Time 0.008 (0.096)   Data 0.000 (0.085)   Loss 1.0239 (1.0511)   Prec@1 56.000 (63.030)   Prec@5 100.000 (96.204)   [2025-10-23 16:26:03]
  Epoch: [011][300/500]   Time 0.010 (0.068)   Data 0.000 (0.057)   Loss 1.1525 (1.0444)   Prec@1 58.000 (63.130)   Prec@5 95.000 (96.289)   [2025-10-23 16:26:04]
  Epoch: [011][400/500]   Time 0.009 (0.053)   Data 0.000 (0.043)   Loss 0.9775 (1.0443)   Prec@1 67.000 (63.162)   Prec@5 97.000 (96.259)   [2025-10-23 16:26:05]
  **Train** Prec@1 63.018 Prec@5 96.176 Error@1 36.982
  **Test** Prec@1 71.050 Prec@5 97.940 Error@1 28.950
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:26:25] [Epoch=012/040] [Need: 00:19:25] [LR=0.0100] [Best : Accuracy=71.05, Error=28.95]
  Epoch: [012][000/500]   Time 17.174 (17.174)   Data 17.130 (17.130)   Loss 1.2863 (1.2863)   Prec@1 60.000 (60.000)   Prec@5 93.000 (93.000)   [2025-10-23 16:26:42]
  Epoch: [012][100/500]   Time 0.009 (0.183)   Data 0.000 (0.170)   Loss 1.1469 (1.0240)   Prec@1 65.000 (64.673)   Prec@5 96.000 (96.475)   [2025-10-23 16:26:43]
  Epoch: [012][200/500]   Time 0.012 (0.097)   Data 0.000 (0.085)   Loss 1.0775 (1.0234)   Prec@1 61.000 (64.199)   Prec@5 95.000 (96.532)   [2025-10-23 16:26:44]
  Epoch: [012][300/500]   Time 0.013 (0.068)   Data 0.000 (0.057)   Loss 0.9451 (1.0264)   Prec@1 71.000 (63.980)   Prec@5 97.000 (96.375)   [2025-10-23 16:26:45]
  Epoch: [012][400/500]   Time 0.010 (0.054)   Data 0.000 (0.043)   Loss 1.0067 (1.0280)   Prec@1 72.000 (63.995)   Prec@5 96.000 (96.367)   [2025-10-23 16:26:46]
  **Train** Prec@1 63.962 Prec@5 96.364 Error@1 36.038
  **Test** Prec@1 71.230 Prec@5 98.110 Error@1 28.770
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:27:06] [Epoch=013/040] [Need: 00:18:42] [LR=0.0100] [Best : Accuracy=71.23, Error=28.77]
  Epoch: [013][000/500]   Time 17.224 (17.224)   Data 17.178 (17.178)   Loss 1.0764 (1.0764)   Prec@1 65.000 (65.000)   Prec@5 98.000 (98.000)   [2025-10-23 16:27:23]
  Epoch: [013][100/500]   Time 0.013 (0.183)   Data 0.000 (0.170)   Loss 1.0357 (1.0034)   Prec@1 67.000 (64.822)   Prec@5 95.000 (96.446)   [2025-10-23 16:27:24]
  Epoch: [013][200/500]   Time 0.014 (0.097)   Data 0.000 (0.086)   Loss 0.9134 (1.0207)   Prec@1 66.000 (64.055)   Prec@5 98.000 (96.274)   [2025-10-23 16:27:25]
  Epoch: [013][300/500]   Time 0.008 (0.068)   Data 0.000 (0.057)   Loss 1.1438 (1.0197)   Prec@1 62.000 (63.934)   Prec@5 95.000 (96.282)   [2025-10-23 16:27:26]
  Epoch: [013][400/500]   Time 0.010 (0.054)   Data 0.000 (0.043)   Loss 1.0596 (1.0200)   Prec@1 63.000 (63.988)   Prec@5 97.000 (96.309)   [2025-10-23 16:27:27]
  **Train** Prec@1 63.998 Prec@5 96.316 Error@1 36.002
  **Test** Prec@1 71.770 Prec@5 98.050 Error@1 28.230
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:27:47] [Epoch=014/040] [Need: 00:18:01] [LR=0.0100] [Best : Accuracy=71.77, Error=28.23]
  Epoch: [014][000/500]   Time 26.917 (26.917)   Data 26.690 (26.690)   Loss 0.7232 (0.7232)   Prec@1 78.000 (78.000)   Prec@5 100.000 (100.000)   [2025-10-23 16:28:14]
  Epoch: [014][100/500]   Time 0.012 (0.281)   Data 0.000 (0.265)   Loss 1.1224 (0.9919)   Prec@1 63.000 (64.980)   Prec@5 95.000 (96.535)   [2025-10-23 16:28:16]
  Epoch: [014][200/500]   Time 0.030 (0.151)   Data 0.019 (0.136)   Loss 1.0181 (1.0003)   Prec@1 62.000 (64.726)   Prec@5 95.000 (96.552)   [2025-10-23 16:28:18]
  Epoch: [014][300/500]   Time 0.034 (0.107)   Data 0.021 (0.093)   Loss 1.0799 (1.0021)   Prec@1 55.000 (64.638)   Prec@5 98.000 (96.585)   [2025-10-23 16:28:19]
  Epoch: [014][400/500]   Time 0.014 (0.083)   Data 0.000 (0.070)   Loss 0.9016 (1.0047)   Prec@1 68.000 (64.623)   Prec@5 98.000 (96.544)   [2025-10-23 16:28:21]
  **Train** Prec@1 64.520 Prec@5 96.566 Error@1 35.480
  **Test** Prec@1 72.710 Prec@5 97.990 Error@1 27.290
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:28:46] [Epoch=015/040] [Need: 00:17:47] [LR=0.0100] [Best : Accuracy=72.71, Error=27.29]
  Epoch: [015][000/500]   Time 17.776 (17.776)   Data 17.729 (17.729)   Loss 1.0734 (1.0734)   Prec@1 67.000 (67.000)   Prec@5 94.000 (94.000)   [2025-10-23 16:29:03]
  Epoch: [015][100/500]   Time 0.010 (0.189)   Data 0.000 (0.176)   Loss 0.8450 (1.0043)   Prec@1 63.000 (64.653)   Prec@5 98.000 (96.545)   [2025-10-23 16:29:05]
  Epoch: [015][200/500]   Time 0.009 (0.100)   Data 0.000 (0.088)   Loss 1.0326 (1.0082)   Prec@1 67.000 (64.488)   Prec@5 97.000 (96.507)   [2025-10-23 16:29:06]
  Epoch: [015][300/500]   Time 0.010 (0.070)   Data 0.001 (0.059)   Loss 0.9103 (1.0065)   Prec@1 73.000 (64.631)   Prec@5 93.000 (96.482)   [2025-10-23 16:29:07]
  Epoch: [015][400/500]   Time 0.010 (0.056)   Data 0.000 (0.044)   Loss 1.0902 (1.0020)   Prec@1 58.000 (64.766)   Prec@5 97.000 (96.529)   [2025-10-23 16:29:08]
  **Train** Prec@1 64.874 Prec@5 96.490 Error@1 35.126
  **Test** Prec@1 73.080 Prec@5 98.110 Error@1 26.920
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:29:28] [Epoch=016/040] [Need: 00:17:03] [LR=0.0100] [Best : Accuracy=73.08, Error=26.92]
  Epoch: [016][000/500]   Time 17.915 (17.915)   Data 17.866 (17.866)   Loss 0.8963 (0.8963)   Prec@1 68.000 (68.000)   Prec@5 96.000 (96.000)   [2025-10-23 16:29:45]
  Epoch: [016][100/500]   Time 0.009 (0.191)   Data 0.000 (0.177)   Loss 1.0025 (1.0011)   Prec@1 63.000 (65.050)   Prec@5 98.000 (96.505)   [2025-10-23 16:29:47]
  Epoch: [016][200/500]   Time 0.015 (0.102)   Data 0.001 (0.089)   Loss 1.0398 (0.9896)   Prec@1 67.000 (65.512)   Prec@5 95.000 (96.567)   [2025-10-23 16:29:48]
  Epoch: [016][300/500]   Time 0.011 (0.072)   Data 0.000 (0.060)   Loss 1.0776 (0.9946)   Prec@1 55.000 (65.332)   Prec@5 96.000 (96.478)   [2025-10-23 16:29:49]
  Epoch: [016][400/500]   Time 0.010 (0.056)   Data 0.000 (0.045)   Loss 0.9797 (0.9950)   Prec@1 63.000 (65.135)   Prec@5 97.000 (96.544)   [2025-10-23 16:29:50]
  **Train** Prec@1 65.170 Prec@5 96.548 Error@1 34.830
  **Test** Prec@1 73.060 Prec@5 98.000 Error@1 26.940

==>>[2025-10-23 16:30:10] [Epoch=017/040] [Need: 00:16:20] [LR=0.0100] [Best : Accuracy=73.08, Error=26.92]
  Epoch: [017][000/500]   Time 17.328 (17.328)   Data 17.283 (17.283)   Loss 0.9654 (0.9654)   Prec@1 64.000 (64.000)   Prec@5 97.000 (97.000)   [2025-10-23 16:30:27]
  Epoch: [017][100/500]   Time 0.009 (0.183)   Data 0.000 (0.171)   Loss 0.8912 (0.9764)   Prec@1 67.000 (66.099)   Prec@5 99.000 (96.673)   [2025-10-23 16:30:28]
  Epoch: [017][200/500]   Time 0.011 (0.097)   Data 0.000 (0.086)   Loss 1.1457 (0.9818)   Prec@1 63.000 (65.627)   Prec@5 96.000 (96.682)   [2025-10-23 16:30:29]
  Epoch: [017][300/500]   Time 0.010 (0.068)   Data 0.000 (0.058)   Loss 0.9409 (0.9853)   Prec@1 63.000 (65.445)   Prec@5 98.000 (96.585)   [2025-10-23 16:30:30]
  Epoch: [017][400/500]   Time 0.010 (0.054)   Data 0.000 (0.043)   Loss 0.9798 (0.9845)   Prec@1 68.000 (65.529)   Prec@5 94.000 (96.623)   [2025-10-23 16:30:31]
  **Train** Prec@1 65.580 Prec@5 96.654 Error@1 34.420
  **Test** Prec@1 73.510 Prec@5 98.270 Error@1 26.490
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:30:51] [Epoch=018/040] [Need: 00:15:36] [LR=0.0100] [Best : Accuracy=73.51, Error=26.49]
  Epoch: [018][000/500]   Time 17.714 (17.714)   Data 17.670 (17.670)   Loss 1.0551 (1.0551)   Prec@1 61.000 (61.000)   Prec@5 99.000 (99.000)   [2025-10-23 16:31:09]
  Epoch: [018][100/500]   Time 0.012 (0.188)   Data 0.000 (0.175)   Loss 1.0641 (0.9861)   Prec@1 65.000 (65.644)   Prec@5 96.000 (96.624)   [2025-10-23 16:31:10]
  Epoch: [018][200/500]   Time 0.009 (0.100)   Data 0.000 (0.088)   Loss 0.9280 (0.9767)   Prec@1 64.000 (65.692)   Prec@5 97.000 (96.662)   [2025-10-23 16:31:11]
  Epoch: [018][300/500]   Time 0.009 (0.070)   Data 0.000 (0.059)   Loss 0.8031 (0.9778)   Prec@1 71.000 (65.654)   Prec@5 99.000 (96.738)   [2025-10-23 16:31:12]
  Epoch: [018][400/500]   Time 0.010 (0.055)   Data 0.001 (0.044)   Loss 1.0199 (0.9776)   Prec@1 65.000 (65.681)   Prec@5 96.000 (96.766)   [2025-10-23 16:31:13]
  **Train** Prec@1 65.726 Prec@5 96.780 Error@1 34.274
  **Test** Prec@1 73.520 Prec@5 98.060 Error@1 26.480
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:31:34] [Epoch=019/040] [Need: 00:14:52] [LR=0.0100] [Best : Accuracy=73.52, Error=26.48]
  Epoch: [019][000/500]   Time 17.556 (17.556)   Data 17.511 (17.511)   Loss 0.9630 (0.9630)   Prec@1 67.000 (67.000)   Prec@5 93.000 (93.000)   [2025-10-23 16:31:51]
  Epoch: [019][100/500]   Time 0.012 (0.186)   Data 0.000 (0.174)   Loss 1.1495 (0.9740)   Prec@1 62.000 (66.059)   Prec@5 97.000 (96.465)   [2025-10-23 16:31:52]
  Epoch: [019][200/500]   Time 0.010 (0.099)   Data 0.000 (0.087)   Loss 0.9660 (0.9786)   Prec@1 65.000 (65.871)   Prec@5 99.000 (96.607)   [2025-10-23 16:31:53]
  Epoch: [019][300/500]   Time 0.013 (0.070)   Data 0.000 (0.058)   Loss 0.9747 (0.9731)   Prec@1 62.000 (66.143)   Prec@5 98.000 (96.674)   [2025-10-23 16:31:55]
  Epoch: [019][400/500]   Time 0.010 (0.055)   Data 0.001 (0.044)   Loss 1.0571 (0.9748)   Prec@1 59.000 (66.000)   Prec@5 96.000 (96.651)   [2025-10-23 16:31:56]
  **Train** Prec@1 65.990 Prec@5 96.670 Error@1 34.010
  **Test** Prec@1 73.680 Prec@5 98.190 Error@1 26.320
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:32:15] [Epoch=020/040] [Need: 00:14:10] [LR=0.0100] [Best : Accuracy=73.68, Error=26.32]
  Epoch: [020][000/500]   Time 17.250 (17.250)   Data 17.203 (17.203)   Loss 0.8823 (0.8823)   Prec@1 67.000 (67.000)   Prec@5 100.000 (100.000)   [2025-10-23 16:32:33]
  Epoch: [020][100/500]   Time 0.013 (0.183)   Data 0.001 (0.171)   Loss 0.9410 (0.9702)   Prec@1 69.000 (66.218)   Prec@5 98.000 (96.703)   [2025-10-23 16:32:34]
  Epoch: [020][200/500]   Time 0.011 (0.097)   Data 0.001 (0.086)   Loss 0.8673 (0.9611)   Prec@1 70.000 (66.438)   Prec@5 94.000 (96.746)   [2025-10-23 16:32:35]
  Epoch: [020][300/500]   Time 0.009 (0.068)   Data 0.000 (0.057)   Loss 1.1704 (0.9593)   Prec@1 62.000 (66.674)   Prec@5 97.000 (96.814)   [2025-10-23 16:32:36]
  Epoch: [020][400/500]   Time 0.016 (0.054)   Data 0.000 (0.043)   Loss 0.9369 (0.9574)   Prec@1 72.000 (66.741)   Prec@5 93.000 (96.798)   [2025-10-23 16:32:37]
  **Train** Prec@1 66.702 Prec@5 96.820 Error@1 33.298
  **Test** Prec@1 74.160 Prec@5 98.110 Error@1 25.840
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:32:56] [Epoch=021/040] [Need: 00:13:26] [LR=0.0100] [Best : Accuracy=74.16, Error=25.84]
  Epoch: [021][000/500]   Time 17.444 (17.444)   Data 17.398 (17.398)   Loss 0.9693 (0.9693)   Prec@1 64.000 (64.000)   Prec@5 98.000 (98.000)   [2025-10-23 16:33:14]
  Epoch: [021][100/500]   Time 0.009 (0.185)   Data 0.000 (0.172)   Loss 0.8813 (0.9464)   Prec@1 73.000 (67.030)   Prec@5 98.000 (96.921)   [2025-10-23 16:33:15]
  Epoch: [021][200/500]   Time 0.012 (0.098)   Data 0.000 (0.087)   Loss 0.8286 (0.9443)   Prec@1 70.000 (66.811)   Prec@5 99.000 (97.040)   [2025-10-23 16:33:16]
  Epoch: [021][300/500]   Time 0.013 (0.069)   Data 0.000 (0.058)   Loss 0.8699 (0.9504)   Prec@1 66.000 (66.661)   Prec@5 95.000 (96.980)   [2025-10-23 16:33:17]
  Epoch: [021][400/500]   Time 0.011 (0.054)   Data 0.000 (0.044)   Loss 1.0133 (0.9561)   Prec@1 70.000 (66.476)   Prec@5 96.000 (96.938)   [2025-10-23 16:33:18]
  **Train** Prec@1 66.418 Prec@5 96.906 Error@1 33.582
  **Test** Prec@1 73.860 Prec@5 98.260 Error@1 26.140

==>>[2025-10-23 16:33:38] [Epoch=022/040] [Need: 00:12:42] [LR=0.0100] [Best : Accuracy=74.16, Error=25.84]
  Epoch: [022][000/500]   Time 17.326 (17.326)   Data 17.279 (17.279)   Loss 0.8452 (0.8452)   Prec@1 68.000 (68.000)   Prec@5 98.000 (98.000)   [2025-10-23 16:33:55]
  Epoch: [022][100/500]   Time 0.010 (0.184)   Data 0.000 (0.171)   Loss 0.7384 (0.9424)   Prec@1 74.000 (67.257)   Prec@5 98.000 (96.743)   [2025-10-23 16:33:56]
  Epoch: [022][200/500]   Time 0.009 (0.098)   Data 0.000 (0.086)   Loss 0.9204 (0.9444)   Prec@1 64.000 (66.851)   Prec@5 99.000 (96.975)   [2025-10-23 16:33:57]
  Epoch: [022][300/500]   Time 0.010 (0.069)   Data 0.000 (0.058)   Loss 0.9369 (0.9533)   Prec@1 65.000 (66.605)   Prec@5 97.000 (96.940)   [2025-10-23 16:33:58]
  Epoch: [022][400/500]   Time 0.013 (0.054)   Data 0.000 (0.043)   Loss 1.0778 (0.9499)   Prec@1 62.000 (66.776)   Prec@5 95.000 (96.945)   [2025-10-23 16:33:59]
  **Train** Prec@1 66.794 Prec@5 96.906 Error@1 33.206
  **Test** Prec@1 73.710 Prec@5 98.200 Error@1 26.290

==>>[2025-10-23 16:34:19] [Epoch=023/040] [Need: 00:11:59] [LR=0.0100] [Best : Accuracy=74.16, Error=25.84]
  Epoch: [023][000/500]   Time 17.346 (17.346)   Data 17.300 (17.300)   Loss 0.8315 (0.8315)   Prec@1 75.000 (75.000)   Prec@5 96.000 (96.000)   [2025-10-23 16:34:36]
  Epoch: [023][100/500]   Time 0.009 (0.184)   Data 0.000 (0.171)   Loss 0.6739 (0.9722)   Prec@1 75.000 (66.079)   Prec@5 99.000 (96.683)   [2025-10-23 16:34:37]
  Epoch: [023][200/500]   Time 0.011 (0.098)   Data 0.000 (0.086)   Loss 0.8995 (0.9614)   Prec@1 63.000 (66.234)   Prec@5 100.000 (96.905)   [2025-10-23 16:34:39]
  Epoch: [023][300/500]   Time 0.010 (0.069)   Data 0.000 (0.058)   Loss 0.9981 (0.9546)   Prec@1 65.000 (66.578)   Prec@5 97.000 (96.917)   [2025-10-23 16:34:40]
  Epoch: [023][400/500]   Time 0.013 (0.054)   Data 0.001 (0.043)   Loss 0.7909 (0.9552)   Prec@1 71.000 (66.793)   Prec@5 100.000 (96.823)   [2025-10-23 16:34:41]
  **Train** Prec@1 66.922 Prec@5 96.886 Error@1 33.078
  **Test** Prec@1 74.810 Prec@5 98.360 Error@1 25.190
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:35:00] [Epoch=024/040] [Need: 00:11:16] [LR=0.0100] [Best : Accuracy=74.81, Error=25.19]
  Epoch: [024][000/500]   Time 17.377 (17.377)   Data 17.331 (17.331)   Loss 0.8242 (0.8242)   Prec@1 73.000 (73.000)   Prec@5 94.000 (94.000)   [2025-10-23 16:35:18]
  Epoch: [024][100/500]   Time 0.010 (0.185)   Data 0.001 (0.172)   Loss 1.0008 (0.9229)   Prec@1 59.000 (67.455)   Prec@5 99.000 (96.921)   [2025-10-23 16:35:19]
  Epoch: [024][200/500]   Time 0.010 (0.098)   Data 0.000 (0.086)   Loss 0.9315 (0.9334)   Prec@1 71.000 (67.159)   Prec@5 97.000 (96.915)   [2025-10-23 16:35:20]
  Epoch: [024][300/500]   Time 0.010 (0.069)   Data 0.000 (0.058)   Loss 0.9665 (0.9364)   Prec@1 67.000 (67.216)   Prec@5 98.000 (96.847)   [2025-10-23 16:35:21]
  Epoch: [024][400/500]   Time 0.009 (0.055)   Data 0.000 (0.043)   Loss 0.8950 (0.9359)   Prec@1 66.000 (67.324)   Prec@5 98.000 (96.913)   [2025-10-23 16:35:22]
  **Train** Prec@1 67.238 Prec@5 96.892 Error@1 32.762
  **Test** Prec@1 73.930 Prec@5 98.330 Error@1 26.070

==>>[2025-10-23 16:35:42] [Epoch=025/040] [Need: 00:10:33] [LR=0.0010] [Best : Accuracy=74.81, Error=25.19]
  Epoch: [025][000/500]   Time 17.396 (17.396)   Data 17.349 (17.349)   Loss 0.9453 (0.9453)   Prec@1 68.000 (68.000)   Prec@5 98.000 (98.000)   [2025-10-23 16:35:59]
  Epoch: [025][100/500]   Time 0.010 (0.185)   Data 0.000 (0.172)   Loss 0.8430 (0.9013)   Prec@1 72.000 (68.396)   Prec@5 98.000 (97.347)   [2025-10-23 16:36:00]
  Epoch: [025][200/500]   Time 0.009 (0.098)   Data 0.000 (0.087)   Loss 0.8466 (0.8908)   Prec@1 67.000 (68.806)   Prec@5 98.000 (97.328)   [2025-10-23 16:36:02]
  Epoch: [025][300/500]   Time 0.011 (0.069)   Data 0.000 (0.058)   Loss 0.7379 (0.8835)   Prec@1 76.000 (68.987)   Prec@5 98.000 (97.352)   [2025-10-23 16:36:03]
  Epoch: [025][400/500]   Time 0.010 (0.054)   Data 0.000 (0.043)   Loss 1.0096 (0.8802)   Prec@1 61.000 (69.157)   Prec@5 98.000 (97.399)   [2025-10-23 16:36:04]
  **Train** Prec@1 69.146 Prec@5 97.374 Error@1 30.854
  **Test** Prec@1 75.900 Prec@5 98.470 Error@1 24.100
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:36:23] [Epoch=026/040] [Need: 00:09:51] [LR=0.0010] [Best : Accuracy=75.90, Error=24.10]
  Epoch: [026][000/500]   Time 17.468 (17.468)   Data 17.422 (17.422)   Loss 0.9505 (0.9505)   Prec@1 69.000 (69.000)   Prec@5 96.000 (96.000)   [2025-10-23 16:36:41]
  Epoch: [026][100/500]   Time 0.013 (0.186)   Data 0.000 (0.173)   Loss 0.7955 (0.8671)   Prec@1 65.000 (69.396)   Prec@5 99.000 (97.525)   [2025-10-23 16:36:42]
  Epoch: [026][200/500]   Time 0.010 (0.099)   Data 0.000 (0.087)   Loss 1.0045 (0.8670)   Prec@1 67.000 (69.433)   Prec@5 96.000 (97.358)   [2025-10-23 16:36:43]
  Epoch: [026][300/500]   Time 0.010 (0.069)   Data 0.000 (0.058)   Loss 0.8193 (0.8594)   Prec@1 67.000 (69.724)   Prec@5 98.000 (97.405)   [2025-10-23 16:36:44]
  Epoch: [026][400/500]   Time 0.010 (0.055)   Data 0.000 (0.044)   Loss 0.8628 (0.8621)   Prec@1 68.000 (69.671)   Prec@5 98.000 (97.454)   [2025-10-23 16:36:45]
  **Train** Prec@1 69.598 Prec@5 97.472 Error@1 30.402
  **Test** Prec@1 76.220 Prec@5 98.560 Error@1 23.780
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:37:05] [Epoch=027/040] [Need: 00:09:08] [LR=0.0010] [Best : Accuracy=76.22, Error=23.78]
  Epoch: [027][000/500]   Time 17.205 (17.205)   Data 17.162 (17.162)   Loss 0.7044 (0.7044)   Prec@1 75.000 (75.000)   Prec@5 97.000 (97.000)   [2025-10-23 16:37:22]
  Epoch: [027][100/500]   Time 0.011 (0.183)   Data 0.000 (0.170)   Loss 1.0502 (0.8598)   Prec@1 69.000 (69.851)   Prec@5 95.000 (97.366)   [2025-10-23 16:37:23]
  Epoch: [027][200/500]   Time 0.011 (0.097)   Data 0.000 (0.086)   Loss 0.9036 (0.8706)   Prec@1 67.000 (69.537)   Prec@5 100.000 (97.358)   [2025-10-23 16:37:24]
  Epoch: [027][300/500]   Time 0.011 (0.068)   Data 0.001 (0.057)   Loss 0.7474 (0.8684)   Prec@1 69.000 (69.691)   Prec@5 96.000 (97.362)   [2025-10-23 16:37:25]
  Epoch: [027][400/500]   Time 0.011 (0.054)   Data 0.001 (0.043)   Loss 0.7087 (0.8618)   Prec@1 75.000 (69.868)   Prec@5 99.000 (97.464)   [2025-10-23 16:37:26]
  **Train** Prec@1 69.882 Prec@5 97.540 Error@1 30.118
  **Test** Prec@1 75.810 Prec@5 98.440 Error@1 24.190

==>>[2025-10-23 16:37:46] [Epoch=028/040] [Need: 00:08:25] [LR=0.0010] [Best : Accuracy=76.22, Error=23.78]
  Epoch: [028][000/500]   Time 17.289 (17.289)   Data 17.245 (17.245)   Loss 0.8664 (0.8664)   Prec@1 72.000 (72.000)   Prec@5 96.000 (96.000)   [2025-10-23 16:38:03]
  Epoch: [028][100/500]   Time 0.009 (0.183)   Data 0.000 (0.171)   Loss 0.9079 (0.8417)   Prec@1 58.000 (70.297)   Prec@5 99.000 (97.584)   [2025-10-23 16:38:04]
  Epoch: [028][200/500]   Time 0.008 (0.097)   Data 0.000 (0.086)   Loss 0.8115 (0.8505)   Prec@1 70.000 (70.483)   Prec@5 98.000 (97.493)   [2025-10-23 16:38:05]
  Epoch: [028][300/500]   Time 0.009 (0.068)   Data 0.000 (0.057)   Loss 0.7955 (0.8557)   Prec@1 78.000 (70.183)   Prec@5 97.000 (97.502)   [2025-10-23 16:38:06]
  Epoch: [028][400/500]   Time 0.010 (0.054)   Data 0.000 (0.043)   Loss 0.6900 (0.8503)   Prec@1 79.000 (70.449)   Prec@5 99.000 (97.556)   [2025-10-23 16:38:07]
  **Train** Prec@1 70.388 Prec@5 97.576 Error@1 29.612
  **Test** Prec@1 76.000 Prec@5 98.560 Error@1 24.000

==>>[2025-10-23 16:38:26] [Epoch=029/040] [Need: 00:07:43] [LR=0.0010] [Best : Accuracy=76.22, Error=23.78]
  Epoch: [029][000/500]   Time 21.787 (21.787)   Data 21.648 (21.648)   Loss 0.8633 (0.8633)   Prec@1 72.000 (72.000)   Prec@5 97.000 (97.000)   [2025-10-23 16:38:48]
  Epoch: [029][100/500]   Time 0.011 (0.229)   Data 0.000 (0.215)   Loss 0.6590 (0.8359)   Prec@1 77.000 (70.980)   Prec@5 99.000 (97.495)   [2025-10-23 16:38:49]
  Epoch: [029][200/500]   Time 0.011 (0.121)   Data 0.000 (0.108)   Loss 0.6953 (0.8446)   Prec@1 81.000 (70.677)   Prec@5 98.000 (97.537)   [2025-10-23 16:38:51]
  Epoch: [029][300/500]   Time 0.013 (0.085)   Data 0.001 (0.072)   Loss 1.0051 (0.8491)   Prec@1 63.000 (70.605)   Prec@5 98.000 (97.571)   [2025-10-23 16:38:52]
  Epoch: [029][400/500]   Time 0.013 (0.067)   Data 0.000 (0.054)   Loss 0.6469 (0.8472)   Prec@1 74.000 (70.606)   Prec@5 100.000 (97.646)   [2025-10-23 16:38:53]
  **Train** Prec@1 70.580 Prec@5 97.634 Error@1 29.420
  **Test** Prec@1 76.460 Prec@5 98.590 Error@1 23.540
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:39:24] [Epoch=030/040] [Need: 00:07:06] [LR=0.0010] [Best : Accuracy=76.46, Error=23.54]
  Epoch: [030][000/500]   Time 18.355 (18.355)   Data 18.211 (18.211)   Loss 0.7709 (0.7709)   Prec@1 76.000 (76.000)   Prec@5 97.000 (97.000)   [2025-10-23 16:39:43]
  Epoch: [030][100/500]   Time 0.014 (0.196)   Data 0.000 (0.180)   Loss 0.8232 (0.8338)   Prec@1 69.000 (70.812)   Prec@5 98.000 (97.911)   [2025-10-23 16:39:44]
  Epoch: [030][200/500]   Time 0.012 (0.104)   Data 0.000 (0.091)   Loss 0.8541 (0.8369)   Prec@1 71.000 (70.771)   Prec@5 98.000 (97.687)   [2025-10-23 16:39:45]
  Epoch: [030][300/500]   Time 0.009 (0.074)   Data 0.000 (0.061)   Loss 0.6809 (0.8427)   Prec@1 79.000 (70.465)   Prec@5 97.000 (97.638)   [2025-10-23 16:39:47]
  Epoch: [030][400/500]   Time 0.010 (0.058)   Data 0.000 (0.046)   Loss 0.9700 (0.8440)   Prec@1 68.000 (70.431)   Prec@5 97.000 (97.641)   [2025-10-23 16:39:48]
  **Train** Prec@1 70.458 Prec@5 97.672 Error@1 29.542
  **Test** Prec@1 76.400 Prec@5 98.550 Error@1 23.600

==>>[2025-10-23 16:40:07] [Epoch=031/040] [Need: 00:06:23] [LR=0.0010] [Best : Accuracy=76.46, Error=23.54]
  Epoch: [031][000/500]   Time 17.390 (17.390)   Data 17.344 (17.344)   Loss 0.8222 (0.8222)   Prec@1 72.000 (72.000)   Prec@5 97.000 (97.000)   [2025-10-23 16:40:25]
  Epoch: [031][100/500]   Time 0.014 (0.185)   Data 0.001 (0.172)   Loss 0.8148 (0.8380)   Prec@1 73.000 (70.802)   Prec@5 99.000 (97.485)   [2025-10-23 16:40:26]
  Epoch: [031][200/500]   Time 0.011 (0.098)   Data 0.000 (0.086)   Loss 0.7215 (0.8423)   Prec@1 74.000 (70.831)   Prec@5 98.000 (97.388)   [2025-10-23 16:40:27]
  Epoch: [031][300/500]   Time 0.011 (0.069)   Data 0.000 (0.058)   Loss 1.1591 (0.8420)   Prec@1 62.000 (70.824)   Prec@5 94.000 (97.445)   [2025-10-23 16:40:28]
  Epoch: [031][400/500]   Time 0.012 (0.054)   Data 0.001 (0.043)   Loss 0.9241 (0.8405)   Prec@1 69.000 (70.738)   Prec@5 97.000 (97.549)   [2025-10-23 16:40:29]
  **Train** Prec@1 70.534 Prec@5 97.580 Error@1 29.466
  **Test** Prec@1 76.640 Prec@5 98.580 Error@1 23.360
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:40:49] [Epoch=032/040] [Need: 00:05:40] [LR=0.0010] [Best : Accuracy=76.64, Error=23.36]
  Epoch: [032][000/500]   Time 17.483 (17.483)   Data 17.437 (17.437)   Loss 0.7605 (0.7605)   Prec@1 75.000 (75.000)   Prec@5 99.000 (99.000)   [2025-10-23 16:41:06]
  Epoch: [032][100/500]   Time 0.011 (0.186)   Data 0.000 (0.173)   Loss 0.6486 (0.8334)   Prec@1 82.000 (70.901)   Prec@5 98.000 (97.614)   [2025-10-23 16:41:08]
  Epoch: [032][200/500]   Time 0.012 (0.099)   Data 0.000 (0.087)   Loss 1.0744 (0.8414)   Prec@1 57.000 (70.517)   Prec@5 100.000 (97.657)   [2025-10-23 16:41:09]
  Epoch: [032][300/500]   Time 0.014 (0.069)   Data 0.000 (0.058)   Loss 0.8171 (0.8409)   Prec@1 74.000 (70.485)   Prec@5 95.000 (97.701)   [2025-10-23 16:41:10]
  Epoch: [032][400/500]   Time 0.012 (0.055)   Data 0.000 (0.044)   Loss 0.7493 (0.8395)   Prec@1 69.000 (70.648)   Prec@5 97.000 (97.648)   [2025-10-23 16:41:11]
  **Train** Prec@1 70.576 Prec@5 97.608 Error@1 29.424
  **Test** Prec@1 76.650 Prec@5 98.570 Error@1 23.350
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:41:30] [Epoch=033/040] [Need: 00:04:58] [LR=0.0010] [Best : Accuracy=76.65, Error=23.35]
  Epoch: [033][000/500]   Time 17.220 (17.220)   Data 17.175 (17.175)   Loss 0.8062 (0.8062)   Prec@1 72.000 (72.000)   Prec@5 97.000 (97.000)   [2025-10-23 16:41:47]
  Epoch: [033][100/500]   Time 0.011 (0.184)   Data 0.000 (0.170)   Loss 0.8954 (0.8519)   Prec@1 67.000 (70.168)   Prec@5 96.000 (97.525)   [2025-10-23 16:41:49]
  Epoch: [033][200/500]   Time 0.010 (0.097)   Data 0.000 (0.086)   Loss 0.8304 (0.8384)   Prec@1 73.000 (70.766)   Prec@5 97.000 (97.711)   [2025-10-23 16:41:50]
  Epoch: [033][300/500]   Time 0.010 (0.068)   Data 0.000 (0.057)   Loss 0.7755 (0.8391)   Prec@1 75.000 (70.691)   Prec@5 100.000 (97.701)   [2025-10-23 16:41:51]
  Epoch: [033][400/500]   Time 0.009 (0.054)   Data 0.000 (0.043)   Loss 0.6431 (0.8398)   Prec@1 77.000 (70.726)   Prec@5 97.000 (97.701)   [2025-10-23 16:41:52]
  **Train** Prec@1 70.638 Prec@5 97.696 Error@1 29.362
  **Test** Prec@1 76.600 Prec@5 98.610 Error@1 23.400

==>>[2025-10-23 16:42:11] [Epoch=034/040] [Need: 00:04:15] [LR=0.0010] [Best : Accuracy=76.65, Error=23.35]
  Epoch: [034][000/500]   Time 17.268 (17.268)   Data 17.222 (17.222)   Loss 1.0629 (1.0629)   Prec@1 68.000 (68.000)   Prec@5 97.000 (97.000)   [2025-10-23 16:42:28]
  Epoch: [034][100/500]   Time 0.016 (0.183)   Data 0.000 (0.171)   Loss 0.9444 (0.8329)   Prec@1 68.000 (71.287)   Prec@5 96.000 (97.733)   [2025-10-23 16:42:30]
  Epoch: [034][200/500]   Time 0.012 (0.097)   Data 0.000 (0.086)   Loss 0.8724 (0.8352)   Prec@1 70.000 (70.826)   Prec@5 97.000 (97.697)   [2025-10-23 16:42:31]
  Epoch: [034][300/500]   Time 0.011 (0.068)   Data 0.000 (0.057)   Loss 0.8222 (0.8255)   Prec@1 73.000 (71.110)   Prec@5 100.000 (97.784)   [2025-10-23 16:42:32]
  Epoch: [034][400/500]   Time 0.010 (0.054)   Data 0.000 (0.043)   Loss 0.9324 (0.8281)   Prec@1 66.000 (71.147)   Prec@5 99.000 (97.788)   [2025-10-23 16:42:33]
  **Train** Prec@1 70.860 Prec@5 97.748 Error@1 29.140
  **Test** Prec@1 77.060 Prec@5 98.550 Error@1 22.940
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:42:53] [Epoch=035/040] [Need: 00:03:32] [LR=0.0010] [Best : Accuracy=77.06, Error=22.94]
  Epoch: [035][000/500]   Time 17.435 (17.435)   Data 17.391 (17.391)   Loss 0.7792 (0.7792)   Prec@1 73.000 (73.000)   Prec@5 100.000 (100.000)   [2025-10-23 16:43:10]
  Epoch: [035][100/500]   Time 0.009 (0.185)   Data 0.001 (0.172)   Loss 0.9125 (0.8345)   Prec@1 66.000 (71.178)   Prec@5 95.000 (97.772)   [2025-10-23 16:43:12]
  Epoch: [035][200/500]   Time 0.013 (0.098)   Data 0.001 (0.087)   Loss 0.7955 (0.8274)   Prec@1 73.000 (71.274)   Prec@5 95.000 (97.741)   [2025-10-23 16:43:13]
  Epoch: [035][300/500]   Time 0.010 (0.069)   Data 0.000 (0.058)   Loss 0.9690 (0.8335)   Prec@1 66.000 (70.850)   Prec@5 95.000 (97.681)   [2025-10-23 16:43:14]
  Epoch: [035][400/500]   Time 0.011 (0.054)   Data 0.000 (0.044)   Loss 0.7731 (0.8354)   Prec@1 75.000 (70.736)   Prec@5 97.000 (97.693)   [2025-10-23 16:43:15]
  **Train** Prec@1 70.696 Prec@5 97.622 Error@1 29.304
  **Test** Prec@1 76.850 Prec@5 98.640 Error@1 23.150

==>>[2025-10-23 16:43:34] [Epoch=036/040] [Need: 00:02:49] [LR=0.0010] [Best : Accuracy=77.06, Error=22.94]
  Epoch: [036][000/500]   Time 17.522 (17.522)   Data 17.476 (17.476)   Loss 0.8874 (0.8874)   Prec@1 72.000 (72.000)   Prec@5 97.000 (97.000)   [2025-10-23 16:43:51]
  Epoch: [036][100/500]   Time 0.010 (0.186)   Data 0.000 (0.173)   Loss 0.7908 (0.8164)   Prec@1 75.000 (71.267)   Prec@5 99.000 (97.832)   [2025-10-23 16:43:52]
  Epoch: [036][200/500]   Time 0.010 (0.098)   Data 0.000 (0.087)   Loss 0.8095 (0.8285)   Prec@1 73.000 (70.771)   Prec@5 98.000 (97.811)   [2025-10-23 16:43:53]
  Epoch: [036][300/500]   Time 0.011 (0.069)   Data 0.000 (0.058)   Loss 0.7365 (0.8317)   Prec@1 73.000 (70.907)   Prec@5 96.000 (97.754)   [2025-10-23 16:43:54]
  Epoch: [036][400/500]   Time 0.012 (0.054)   Data 0.000 (0.044)   Loss 0.7827 (0.8275)   Prec@1 75.000 (71.095)   Prec@5 100.000 (97.781)   [2025-10-23 16:43:55]
  **Train** Prec@1 71.052 Prec@5 97.760 Error@1 28.948
  **Test** Prec@1 76.510 Prec@5 98.570 Error@1 23.490

==>>[2025-10-23 16:44:15] [Epoch=037/040] [Need: 00:02:07] [LR=0.0010] [Best : Accuracy=77.06, Error=22.94]
  Epoch: [037][000/500]   Time 17.704 (17.704)   Data 17.660 (17.660)   Loss 0.7830 (0.7830)   Prec@1 74.000 (74.000)   Prec@5 97.000 (97.000)   [2025-10-23 16:44:33]
  Epoch: [037][100/500]   Time 0.010 (0.189)   Data 0.000 (0.175)   Loss 0.7703 (0.8290)   Prec@1 72.000 (71.238)   Prec@5 100.000 (97.624)   [2025-10-23 16:44:35]
  Epoch: [037][200/500]   Time 0.009 (0.100)   Data 0.000 (0.088)   Loss 1.0683 (0.8279)   Prec@1 63.000 (71.234)   Prec@5 97.000 (97.657)   [2025-10-23 16:44:36]
  Epoch: [037][300/500]   Time 0.010 (0.071)   Data 0.000 (0.059)   Loss 0.5800 (0.8332)   Prec@1 79.000 (71.116)   Prec@5 100.000 (97.641)   [2025-10-23 16:44:37]
  Epoch: [037][400/500]   Time 0.010 (0.056)   Data 0.000 (0.044)   Loss 0.8145 (0.8327)   Prec@1 71.000 (71.017)   Prec@5 96.000 (97.646)   [2025-10-23 16:44:38]
  **Train** Prec@1 71.112 Prec@5 97.684 Error@1 28.888
  **Test** Prec@1 77.160 Prec@5 98.610 Error@1 22.840
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:44:58] [Epoch=038/040] [Need: 00:01:24] [LR=0.0010] [Best : Accuracy=77.16, Error=22.84]
  Epoch: [038][000/500]   Time 18.138 (18.138)   Data 18.095 (18.095)   Loss 0.7163 (0.7163)   Prec@1 74.000 (74.000)   Prec@5 99.000 (99.000)   [2025-10-23 16:45:16]
  Epoch: [038][100/500]   Time 0.009 (0.193)   Data 0.000 (0.179)   Loss 0.6505 (0.8518)   Prec@1 78.000 (70.228)   Prec@5 98.000 (97.644)   [2025-10-23 16:45:18]
  Epoch: [038][200/500]   Time 0.010 (0.102)   Data 0.000 (0.090)   Loss 0.9121 (0.8418)   Prec@1 67.000 (70.552)   Prec@5 96.000 (97.711)   [2025-10-23 16:45:19]
  Epoch: [038][300/500]   Time 0.010 (0.072)   Data 0.001 (0.060)   Loss 0.5790 (0.8416)   Prec@1 79.000 (70.794)   Prec@5 99.000 (97.651)   [2025-10-23 16:45:20]
  Epoch: [038][400/500]   Time 0.010 (0.056)   Data 0.000 (0.045)   Loss 0.9593 (0.8363)   Prec@1 70.000 (70.793)   Prec@5 98.000 (97.661)   [2025-10-23 16:45:21]
  **Train** Prec@1 70.826 Prec@5 97.684 Error@1 29.174
  **Test** Prec@1 76.810 Prec@5 98.590 Error@1 23.190

==>>[2025-10-23 16:45:40] [Epoch=039/040] [Need: 00:00:42] [LR=0.0010] [Best : Accuracy=77.16, Error=22.84]
  Epoch: [039][000/500]   Time 17.585 (17.585)   Data 17.541 (17.541)   Loss 0.7279 (0.7279)   Prec@1 71.000 (71.000)   Prec@5 99.000 (99.000)   [2025-10-23 16:45:58]
  Epoch: [039][100/500]   Time 0.011 (0.186)   Data 0.000 (0.174)   Loss 0.9294 (0.8220)   Prec@1 68.000 (71.198)   Prec@5 97.000 (97.921)   [2025-10-23 16:45:59]
  Epoch: [039][200/500]   Time 0.011 (0.099)   Data 0.000 (0.087)   Loss 0.8269 (0.8206)   Prec@1 70.000 (71.254)   Prec@5 96.000 (97.776)   [2025-10-23 16:46:00]
  Epoch: [039][300/500]   Time 0.012 (0.070)   Data 0.000 (0.058)   Loss 0.8826 (0.8236)   Prec@1 71.000 (71.133)   Prec@5 95.000 (97.764)   [2025-10-23 16:46:01]
  Epoch: [039][400/500]   Time 0.011 (0.055)   Data 0.001 (0.044)   Loss 0.6761 (0.8293)   Prec@1 72.000 (70.960)   Prec@5 98.000 (97.713)   [2025-10-23 16:46:02]
  **Train** Prec@1 71.054 Prec@5 97.728 Error@1 28.946
  **Test** Prec@1 76.920 Prec@5 98.580 Error@1 23.080
