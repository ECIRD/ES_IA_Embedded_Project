save path : ./save/tinyvgg_quan/clipping_0.2_0.01
{'data_path': './dataset', 'arch': 'tinyvgg_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': False, 'clipping_coeff': 0.2, 'learning_rate': 0.01, 'manualSeed': 7236, 'save_path': './save/tinyvgg_quan/clipping_0.2_0.01', 'enable_bfa': False, 'resume': '', 'quan_bitwidth': None, 'reset_weight': False, 'evaluate': False, 'n_iter': 30, 'model_only': False, 'random_bfa': False, 'use_cuda': True}
Random Seed: 7236
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'tinyvgg_quan'
=> network :
 TinyVGG(
  (features): Sequential(
    (0): quan_Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Dropout2d(p=0.25, inplace=False)
    (6): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace=True)
    (8): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace=True)
    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (11): Dropout2d(p=0.25, inplace=False)
  )
  (classifier): Sequential(
    (0): quan_Linear(in_features=2048, out_features=10, bias=True)
  )
)
=> do not use any checkpoint for tinyvgg_quan model

==>>[2025-10-23 15:48:23] [Epoch=000/040] [Need: 00:00:00] [LR=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/500]   Time 23.139 (23.139)   Data 21.875 (21.875)   Loss 2.3083 (2.3083)   Prec@1 9.000 (9.000)   Prec@5 42.000 (42.000)   [2025-10-23 15:48:46]
  Epoch: [000][100/500]   Time 0.010 (0.243)   Data 0.000 (0.217)   Loss 2.0623 (2.1646)   Prec@1 27.000 (19.297)   Prec@5 73.000 (67.653)   [2025-10-23 15:48:48]
  Epoch: [000][200/500]   Time 0.011 (0.128)   Data 0.001 (0.109)   Loss 1.9435 (2.0686)   Prec@1 32.000 (23.910)   Prec@5 79.000 (73.507)   [2025-10-23 15:48:49]
  Epoch: [000][300/500]   Time 0.010 (0.089)   Data 0.000 (0.073)   Loss 1.6188 (1.9807)   Prec@1 43.000 (27.518)   Prec@5 92.000 (77.495)   [2025-10-23 15:48:50]
  Epoch: [000][400/500]   Time 0.010 (0.069)   Data 0.000 (0.055)   Loss 1.6760 (1.9240)   Prec@1 42.000 (29.658)   Prec@5 87.000 (79.781)   [2025-10-23 15:48:51]
  **Train** Prec@1 31.406 Prec@5 81.498 Error@1 68.594
  **Test** Prec@1 47.650 Prec@5 92.280 Error@1 52.350
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 15:49:11] [Epoch=001/040] [Need: 00:31:27] [LR=0.0100] [Best : Accuracy=47.65, Error=52.35]
  Epoch: [001][000/500]   Time 18.583 (18.583)   Data 18.539 (18.539)   Loss 1.4750 (1.4750)   Prec@1 44.000 (44.000)   Prec@5 94.000 (94.000)   [2025-10-23 15:49:30]
  Epoch: [001][100/500]   Time 0.010 (0.196)   Data 0.000 (0.184)   Loss 1.6424 (1.6286)   Prec@1 40.000 (40.713)   Prec@5 87.000 (89.119)   [2025-10-23 15:49:31]
  Epoch: [001][200/500]   Time 0.011 (0.104)   Data 0.000 (0.092)   Loss 1.5157 (1.6116)   Prec@1 47.000 (41.299)   Prec@5 91.000 (89.697)   [2025-10-23 15:49:32]
  Epoch: [001][300/500]   Time 0.011 (0.073)   Data 0.001 (0.062)   Loss 1.5298 (1.6006)   Prec@1 42.000 (41.651)   Prec@5 90.000 (89.940)   [2025-10-23 15:49:33]
  Epoch: [001][400/500]   Time 0.011 (0.057)   Data 0.000 (0.046)   Loss 1.5389 (1.5828)   Prec@1 45.000 (42.282)   Prec@5 86.000 (90.232)   [2025-10-23 15:49:34]
  **Train** Prec@1 43.062 Prec@5 90.466 Error@1 56.938
  **Test** Prec@1 55.320 Prec@5 94.770 Error@1 44.680
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 15:49:55] [Epoch=002/040] [Need: 00:28:58] [LR=0.0100] [Best : Accuracy=55.32, Error=44.68]
  Epoch: [002][000/500]   Time 17.546 (17.546)   Data 17.501 (17.501)   Loss 1.4922 (1.4922)   Prec@1 50.000 (50.000)   Prec@5 93.000 (93.000)   [2025-10-23 15:50:12]
  Epoch: [002][100/500]   Time 0.011 (0.185)   Data 0.000 (0.173)   Loss 1.5871 (1.4791)   Prec@1 42.000 (46.356)   Prec@5 88.000 (91.693)   [2025-10-23 15:50:13]
  Epoch: [002][200/500]   Time 0.009 (0.098)   Data 0.000 (0.087)   Loss 1.3162 (1.4660)   Prec@1 57.000 (47.060)   Prec@5 92.000 (91.995)   [2025-10-23 15:50:14]
  Epoch: [002][300/500]   Time 0.010 (0.069)   Data 0.000 (0.058)   Loss 1.4303 (1.4539)   Prec@1 47.000 (47.482)   Prec@5 90.000 (92.186)   [2025-10-23 15:50:15]
  Epoch: [002][400/500]   Time 0.011 (0.054)   Data 0.000 (0.044)   Loss 1.5354 (1.4444)   Prec@1 48.000 (48.000)   Prec@5 91.000 (92.165)   [2025-10-23 15:50:16]
  **Train** Prec@1 48.446 Prec@5 92.304 Error@1 51.554
  **Test** Prec@1 59.030 Prec@5 95.350 Error@1 40.970
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 15:50:36] [Epoch=003/040] [Need: 00:27:18] [LR=0.0100] [Best : Accuracy=59.03, Error=40.97]
  Epoch: [003][000/500]   Time 17.671 (17.671)   Data 17.626 (17.626)   Loss 1.1994 (1.1994)   Prec@1 57.000 (57.000)   Prec@5 96.000 (96.000)   [2025-10-23 15:50:54]
  Epoch: [003][100/500]   Time 0.010 (0.187)   Data 0.000 (0.175)   Loss 1.3828 (1.3584)   Prec@1 44.000 (51.168)   Prec@5 95.000 (93.337)   [2025-10-23 15:50:55]
  Epoch: [003][200/500]   Time 0.010 (0.099)   Data 0.000 (0.088)   Loss 1.3615 (1.3590)   Prec@1 48.000 (51.323)   Prec@5 95.000 (93.348)   [2025-10-23 15:50:56]
  Epoch: [003][300/500]   Time 0.009 (0.070)   Data 0.000 (0.059)   Loss 1.2990 (1.3568)   Prec@1 51.000 (51.425)   Prec@5 93.000 (93.385)   [2025-10-23 15:50:57]
  Epoch: [003][400/500]   Time 0.011 (0.055)   Data 0.001 (0.044)   Loss 1.2999 (1.3506)   Prec@1 53.000 (51.496)   Prec@5 93.000 (93.479)   [2025-10-23 15:50:58]
  **Train** Prec@1 51.838 Prec@5 93.556 Error@1 48.162
  **Test** Prec@1 62.340 Prec@5 96.310 Error@1 37.660
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 15:51:18] [Epoch=004/040] [Need: 00:26:12] [LR=0.0100] [Best : Accuracy=62.34, Error=37.66]
  Epoch: [004][000/500]   Time 17.931 (17.931)   Data 17.886 (17.886)   Loss 1.2300 (1.2300)   Prec@1 57.000 (57.000)   Prec@5 93.000 (93.000)   [2025-10-23 15:51:36]
  Epoch: [004][100/500]   Time 0.012 (0.191)   Data 0.000 (0.177)   Loss 1.1331 (1.2932)   Prec@1 60.000 (54.188)   Prec@5 96.000 (93.941)   [2025-10-23 15:51:37]
  Epoch: [004][200/500]   Time 0.010 (0.101)   Data 0.000 (0.089)   Loss 1.2584 (1.2881)   Prec@1 58.000 (54.512)   Prec@5 92.000 (93.995)   [2025-10-23 15:51:38]
  Epoch: [004][300/500]   Time 0.011 (0.072)   Data 0.000 (0.060)   Loss 1.1960 (1.2821)   Prec@1 58.000 (54.518)   Prec@5 91.000 (94.053)   [2025-10-23 15:51:39]
  Epoch: [004][400/500]   Time 0.013 (0.057)   Data 0.000 (0.045)   Loss 1.4157 (1.2807)   Prec@1 46.000 (54.566)   Prec@5 94.000 (94.060)   [2025-10-23 15:51:40]
  **Train** Prec@1 54.672 Prec@5 94.106 Error@1 45.328
  **Test** Prec@1 61.970 Prec@5 96.420 Error@1 38.030

==>>[2025-10-23 15:52:04] [Epoch=005/040] [Need: 00:25:45] [LR=0.0100] [Best : Accuracy=62.34, Error=37.66]
  Epoch: [005][000/500]   Time 17.872 (17.872)   Data 17.827 (17.827)   Loss 1.2478 (1.2478)   Prec@1 53.000 (53.000)   Prec@5 94.000 (94.000)   [2025-10-23 15:52:22]
  Epoch: [005][100/500]   Time 0.012 (0.189)   Data 0.000 (0.177)   Loss 1.2882 (1.2626)   Prec@1 52.000 (55.020)   Prec@5 97.000 (94.653)   [2025-10-23 15:52:23]
  Epoch: [005][200/500]   Time 0.010 (0.100)   Data 0.000 (0.089)   Loss 1.5110 (1.2467)   Prec@1 52.000 (55.637)   Prec@5 87.000 (94.562)   [2025-10-23 15:52:24]
  Epoch: [005][300/500]   Time 0.011 (0.070)   Data 0.001 (0.059)   Loss 0.9796 (1.2435)   Prec@1 70.000 (56.010)   Prec@5 95.000 (94.468)   [2025-10-23 15:52:25]
  Epoch: [005][400/500]   Time 0.011 (0.055)   Data 0.001 (0.045)   Loss 1.0853 (1.2343)   Prec@1 61.000 (56.195)   Prec@5 95.000 (94.574)   [2025-10-23 15:52:26]
  **Train** Prec@1 56.420 Prec@5 94.654 Error@1 43.580
  **Test** Prec@1 65.140 Prec@5 97.050 Error@1 34.860
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 15:52:46] [Epoch=006/040] [Need: 00:24:48] [LR=0.0100] [Best : Accuracy=65.14, Error=34.86]
  Epoch: [006][000/500]   Time 17.677 (17.677)   Data 17.632 (17.632)   Loss 1.1312 (1.1312)   Prec@1 62.000 (62.000)   Prec@5 96.000 (96.000)   [2025-10-23 15:53:03]
  Epoch: [006][100/500]   Time 0.011 (0.187)   Data 0.001 (0.175)   Loss 1.1109 (1.2236)   Prec@1 64.000 (56.614)   Prec@5 94.000 (94.871)   [2025-10-23 15:53:05]
  Epoch: [006][200/500]   Time 0.011 (0.099)   Data 0.001 (0.088)   Loss 1.1601 (1.2098)   Prec@1 57.000 (57.189)   Prec@5 98.000 (94.821)   [2025-10-23 15:53:06]
  Epoch: [006][300/500]   Time 0.010 (0.069)   Data 0.001 (0.059)   Loss 1.1639 (1.2022)   Prec@1 54.000 (57.399)   Prec@5 95.000 (94.900)   [2025-10-23 15:53:07]
  Epoch: [006][400/500]   Time 0.011 (0.055)   Data 0.000 (0.044)   Loss 1.2256 (1.1951)   Prec@1 58.000 (57.698)   Prec@5 92.000 (94.850)   [2025-10-23 15:53:08]
  **Train** Prec@1 57.810 Prec@5 94.856 Error@1 42.190
  **Test** Prec@1 66.680 Prec@5 97.060 Error@1 33.320
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 15:53:27] [Epoch=007/040] [Need: 00:23:53] [LR=0.0100] [Best : Accuracy=66.68, Error=33.32]
  Epoch: [007][000/500]   Time 17.401 (17.401)   Data 17.356 (17.356)   Loss 1.0416 (1.0416)   Prec@1 59.000 (59.000)   Prec@5 95.000 (95.000)   [2025-10-23 15:53:45]
  Epoch: [007][100/500]   Time 0.010 (0.185)   Data 0.001 (0.172)   Loss 1.2971 (1.1568)   Prec@1 57.000 (58.475)   Prec@5 92.000 (95.376)   [2025-10-23 15:53:46]
  Epoch: [007][200/500]   Time 0.011 (0.098)   Data 0.000 (0.087)   Loss 1.1181 (1.1599)   Prec@1 61.000 (58.602)   Prec@5 92.000 (95.308)   [2025-10-23 15:53:47]
  Epoch: [007][300/500]   Time 0.011 (0.069)   Data 0.000 (0.058)   Loss 1.0256 (1.1652)   Prec@1 62.000 (58.648)   Prec@5 95.000 (95.256)   [2025-10-23 15:53:48]
  Epoch: [007][400/500]   Time 0.011 (0.055)   Data 0.000 (0.043)   Loss 1.1039 (1.1583)   Prec@1 58.000 (59.032)   Prec@5 98.000 (95.392)   [2025-10-23 15:53:49]
  **Train** Prec@1 59.122 Prec@5 95.368 Error@1 40.878
  **Test** Prec@1 67.770 Prec@5 97.390 Error@1 32.230
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 15:54:09] [Epoch=008/040] [Need: 00:23:03] [LR=0.0100] [Best : Accuracy=67.77, Error=32.23]
  Epoch: [008][000/500]   Time 17.966 (17.966)   Data 17.921 (17.921)   Loss 1.1021 (1.1021)   Prec@1 58.000 (58.000)   Prec@5 95.000 (95.000)   [2025-10-23 15:54:27]
  Epoch: [008][100/500]   Time 0.011 (0.189)   Data 0.000 (0.178)   Loss 1.1998 (1.1480)   Prec@1 59.000 (59.347)   Prec@5 95.000 (95.347)   [2025-10-23 15:54:28]
  Epoch: [008][200/500]   Time 0.011 (0.100)   Data 0.000 (0.089)   Loss 1.1691 (1.1316)   Prec@1 62.000 (60.164)   Prec@5 91.000 (95.453)   [2025-10-23 15:54:29]
  Epoch: [008][300/500]   Time 0.010 (0.070)   Data 0.001 (0.060)   Loss 1.3187 (1.1337)   Prec@1 52.000 (60.116)   Prec@5 95.000 (95.375)   [2025-10-23 15:54:30]
  Epoch: [008][400/500]   Time 0.012 (0.056)   Data 0.000 (0.045)   Loss 1.1216 (1.1297)   Prec@1 58.000 (60.277)   Prec@5 95.000 (95.406)   [2025-10-23 15:54:31]
  **Train** Prec@1 60.242 Prec@5 95.498 Error@1 39.758
  **Test** Prec@1 69.430 Prec@5 97.570 Error@1 30.570
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 15:55:14] [Epoch=009/040] [Need: 00:23:33] [LR=0.0100] [Best : Accuracy=69.43, Error=30.57]
  Epoch: [009][000/500]   Time 26.014 (26.014)   Data 25.776 (25.776)   Loss 1.2366 (1.2366)   Prec@1 54.000 (54.000)   Prec@5 95.000 (95.000)   [2025-10-23 15:55:40]
  Epoch: [009][100/500]   Time 0.014 (0.273)   Data 0.000 (0.256)   Loss 1.1466 (1.1122)   Prec@1 58.000 (60.901)   Prec@5 95.000 (95.594)   [2025-10-23 15:55:42]
  Epoch: [009][200/500]   Time 0.015 (0.144)   Data 0.000 (0.129)   Loss 1.0401 (1.0943)   Prec@1 68.000 (61.677)   Prec@5 97.000 (95.781)   [2025-10-23 15:55:43]
  Epoch: [009][300/500]   Time 0.011 (0.100)   Data 0.000 (0.086)   Loss 1.2647 (1.1005)   Prec@1 57.000 (61.150)   Prec@5 96.000 (95.744)   [2025-10-23 15:55:44]
  Epoch: [009][400/500]   Time 0.013 (0.079)   Data 0.001 (0.065)   Loss 1.1758 (1.1033)   Prec@1 64.000 (61.050)   Prec@5 93.000 (95.756)   [2025-10-23 15:55:45]
  **Train** Prec@1 61.100 Prec@5 95.760 Error@1 38.900
  **Test** Prec@1 69.530 Prec@5 97.290 Error@1 30.470
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 15:56:08] [Epoch=010/040] [Need: 00:23:13] [LR=0.0100] [Best : Accuracy=69.53, Error=30.47]
  Epoch: [010][000/500]   Time 18.180 (18.180)   Data 18.130 (18.130)   Loss 1.2139 (1.2139)   Prec@1 57.000 (57.000)   Prec@5 95.000 (95.000)   [2025-10-23 15:56:26]
  Epoch: [010][100/500]   Time 0.012 (0.193)   Data 0.000 (0.180)   Loss 0.9137 (1.0773)   Prec@1 69.000 (61.861)   Prec@5 99.000 (96.050)   [2025-10-23 15:56:27]
  Epoch: [010][200/500]   Time 0.013 (0.102)   Data 0.000 (0.090)   Loss 1.0781 (1.0823)   Prec@1 57.000 (61.572)   Prec@5 98.000 (96.090)   [2025-10-23 15:56:28]
  Epoch: [010][300/500]   Time 0.009 (0.072)   Data 0.000 (0.060)   Loss 1.0698 (1.0829)   Prec@1 63.000 (61.628)   Prec@5 96.000 (95.953)   [2025-10-23 15:56:29]
  Epoch: [010][400/500]   Time 0.009 (0.057)   Data 0.000 (0.045)   Loss 1.1406 (1.0840)   Prec@1 60.000 (61.628)   Prec@5 95.000 (96.002)   [2025-10-23 15:56:30]
  **Train** Prec@1 61.778 Prec@5 96.036 Error@1 38.222
  **Test** Prec@1 70.050 Prec@5 97.330 Error@1 29.950
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 15:56:51] [Epoch=011/040] [Need: 00:22:19] [LR=0.0100] [Best : Accuracy=70.05, Error=29.95]
  Epoch: [011][000/500]   Time 17.453 (17.453)   Data 17.404 (17.404)   Loss 1.1433 (1.1433)   Prec@1 57.000 (57.000)   Prec@5 96.000 (96.000)   [2025-10-23 15:57:09]
  Epoch: [011][100/500]   Time 0.009 (0.185)   Data 0.000 (0.172)   Loss 0.9492 (1.0708)   Prec@1 70.000 (62.208)   Prec@5 95.000 (96.030)   [2025-10-23 15:57:10]
  Epoch: [011][200/500]   Time 0.011 (0.098)   Data 0.000 (0.087)   Loss 0.9665 (1.0642)   Prec@1 61.000 (62.448)   Prec@5 99.000 (96.204)   [2025-10-23 15:57:11]
  Epoch: [011][300/500]   Time 0.011 (0.069)   Data 0.000 (0.058)   Loss 1.0613 (1.0602)   Prec@1 64.000 (62.787)   Prec@5 95.000 (96.199)   [2025-10-23 15:57:12]
  Epoch: [011][400/500]   Time 0.009 (0.054)   Data 0.000 (0.044)   Loss 1.0023 (1.0617)   Prec@1 67.000 (62.825)   Prec@5 96.000 (96.170)   [2025-10-23 15:57:13]
  **Train** Prec@1 62.622 Prec@5 96.154 Error@1 37.378
  **Test** Prec@1 70.640 Prec@5 97.660 Error@1 29.360
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 15:57:33] [Epoch=012/040] [Need: 00:21:22] [LR=0.0100] [Best : Accuracy=70.64, Error=29.36]
  Epoch: [012][000/500]   Time 17.233 (17.233)   Data 17.187 (17.187)   Loss 1.1598 (1.1598)   Prec@1 51.000 (51.000)   Prec@5 96.000 (96.000)   [2025-10-23 15:57:50]
  Epoch: [012][100/500]   Time 0.009 (0.183)   Data 0.000 (0.170)   Loss 1.2257 (1.0583)   Prec@1 56.000 (62.782)   Prec@5 94.000 (95.980)   [2025-10-23 15:57:51]
  Epoch: [012][200/500]   Time 0.012 (0.097)   Data 0.001 (0.086)   Loss 1.0899 (1.0490)   Prec@1 61.000 (63.179)   Prec@5 96.000 (96.164)   [2025-10-23 15:57:52]
  Epoch: [012][300/500]   Time 0.010 (0.068)   Data 0.000 (0.057)   Loss 1.2182 (1.0471)   Prec@1 56.000 (63.110)   Prec@5 95.000 (96.256)   [2025-10-23 15:57:53]
  Epoch: [012][400/500]   Time 0.010 (0.054)   Data 0.000 (0.043)   Loss 0.9260 (1.0487)   Prec@1 66.000 (63.062)   Prec@5 97.000 (96.249)   [2025-10-23 15:57:54]
  **Train** Prec@1 63.134 Prec@5 96.202 Error@1 36.866
  **Test** Prec@1 71.690 Prec@5 97.890 Error@1 28.310
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 15:58:15] [Epoch=013/040] [Need: 00:20:29] [LR=0.0100] [Best : Accuracy=71.69, Error=28.31]
  Epoch: [013][000/500]   Time 20.864 (20.864)   Data 20.817 (20.817)   Loss 0.9927 (0.9927)   Prec@1 72.000 (72.000)   Prec@5 98.000 (98.000)   [2025-10-23 15:58:36]
  Epoch: [013][100/500]   Time 0.013 (0.221)   Data 0.000 (0.206)   Loss 1.0068 (1.0361)   Prec@1 62.000 (63.307)   Prec@5 99.000 (96.426)   [2025-10-23 15:58:37]
  Epoch: [013][200/500]   Time 0.011 (0.117)   Data 0.000 (0.104)   Loss 1.0026 (1.0398)   Prec@1 60.000 (63.214)   Prec@5 96.000 (96.348)   [2025-10-23 15:58:39]
  Epoch: [013][300/500]   Time 0.010 (0.082)   Data 0.000 (0.069)   Loss 1.0384 (1.0354)   Prec@1 60.000 (63.449)   Prec@5 98.000 (96.342)   [2025-10-23 15:58:40]
  Epoch: [013][400/500]   Time 0.010 (0.065)   Data 0.000 (0.052)   Loss 1.2131 (1.0310)   Prec@1 63.000 (63.618)   Prec@5 96.000 (96.352)   [2025-10-23 15:58:41]
  **Train** Prec@1 63.544 Prec@5 96.280 Error@1 36.456
  **Test** Prec@1 71.580 Prec@5 98.040 Error@1 28.420

==>>[2025-10-23 15:59:03] [Epoch=014/040] [Need: 00:19:47] [LR=0.0100] [Best : Accuracy=71.69, Error=28.31]
  Epoch: [014][000/500]   Time 20.926 (20.926)   Data 20.877 (20.877)   Loss 0.9790 (0.9790)   Prec@1 66.000 (66.000)   Prec@5 98.000 (98.000)   [2025-10-23 15:59:24]
  Epoch: [014][100/500]   Time 0.010 (0.221)   Data 0.000 (0.207)   Loss 0.9708 (1.0296)   Prec@1 65.000 (64.228)   Prec@5 98.000 (96.386)   [2025-10-23 15:59:25]
  Epoch: [014][200/500]   Time 0.013 (0.117)   Data 0.001 (0.104)   Loss 1.2498 (1.0270)   Prec@1 59.000 (64.139)   Prec@5 96.000 (96.363)   [2025-10-23 15:59:26]
  Epoch: [014][300/500]   Time 0.018 (0.083)   Data 0.000 (0.070)   Loss 1.0114 (1.0225)   Prec@1 64.000 (64.326)   Prec@5 96.000 (96.379)   [2025-10-23 15:59:28]
  Epoch: [014][400/500]   Time 0.011 (0.065)   Data 0.000 (0.052)   Loss 1.1241 (1.0206)   Prec@1 67.000 (64.312)   Prec@5 93.000 (96.404)   [2025-10-23 15:59:29]
  **Train** Prec@1 64.248 Prec@5 96.454 Error@1 35.752
  **Test** Prec@1 71.910 Prec@5 97.810 Error@1 28.090
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 15:59:53] [Epoch=015/040] [Need: 00:19:10] [LR=0.0100] [Best : Accuracy=71.91, Error=28.09]
  Epoch: [015][000/500]   Time 19.515 (19.515)   Data 19.468 (19.468)   Loss 0.8576 (0.8576)   Prec@1 66.000 (66.000)   Prec@5 98.000 (98.000)   [2025-10-23 16:00:13]
  Epoch: [015][100/500]   Time 0.009 (0.206)   Data 0.000 (0.193)   Loss 1.0315 (1.0274)   Prec@1 67.000 (63.960)   Prec@5 97.000 (96.406)   [2025-10-23 16:00:14]
  Epoch: [015][200/500]   Time 0.010 (0.108)   Data 0.000 (0.097)   Loss 1.0721 (1.0161)   Prec@1 59.000 (64.323)   Prec@5 97.000 (96.567)   [2025-10-23 16:00:15]
  Epoch: [015][300/500]   Time 0.009 (0.076)   Data 0.000 (0.065)   Loss 0.8254 (1.0145)   Prec@1 69.000 (64.365)   Prec@5 98.000 (96.581)   [2025-10-23 16:00:16]
  Epoch: [015][400/500]   Time 0.012 (0.060)   Data 0.000 (0.049)   Loss 0.9829 (1.0183)   Prec@1 63.000 (64.177)   Prec@5 97.000 (96.549)   [2025-10-23 16:00:17]
  **Train** Prec@1 64.280 Prec@5 96.528 Error@1 35.720
  **Test** Prec@1 72.190 Prec@5 98.000 Error@1 27.810
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:00:37] [Epoch=016/040] [Need: 00:18:21] [LR=0.0100] [Best : Accuracy=72.19, Error=27.81]
  Epoch: [016][000/500]   Time 17.476 (17.476)   Data 17.426 (17.426)   Loss 0.9735 (0.9735)   Prec@1 62.000 (62.000)   Prec@5 100.000 (100.000)   [2025-10-23 16:00:55]
  Epoch: [016][100/500]   Time 0.015 (0.186)   Data 0.000 (0.173)   Loss 1.0389 (1.0123)   Prec@1 64.000 (64.356)   Prec@5 95.000 (96.446)   [2025-10-23 16:00:56]
  Epoch: [016][200/500]   Time 0.010 (0.099)   Data 0.000 (0.087)   Loss 1.1519 (1.0104)   Prec@1 63.000 (64.269)   Prec@5 93.000 (96.438)   [2025-10-23 16:00:57]
  Epoch: [016][300/500]   Time 0.014 (0.069)   Data 0.000 (0.058)   Loss 0.9628 (1.0095)   Prec@1 66.000 (64.389)   Prec@5 100.000 (96.468)   [2025-10-23 16:00:58]
  Epoch: [016][400/500]   Time 0.010 (0.055)   Data 0.000 (0.044)   Loss 1.0422 (1.0083)   Prec@1 67.000 (64.516)   Prec@5 96.000 (96.484)   [2025-10-23 16:00:59]
  **Train** Prec@1 64.508 Prec@5 96.494 Error@1 35.492
  **Test** Prec@1 72.060 Prec@5 97.970 Error@1 27.940

==>>[2025-10-23 16:01:19] [Epoch=017/040] [Need: 00:17:30] [LR=0.0100] [Best : Accuracy=72.19, Error=27.81]
  Epoch: [017][000/500]   Time 17.802 (17.802)   Data 17.759 (17.759)   Loss 1.0169 (1.0169)   Prec@1 63.000 (63.000)   Prec@5 97.000 (97.000)   [2025-10-23 16:01:37]
  Epoch: [017][100/500]   Time 0.009 (0.190)   Data 0.000 (0.176)   Loss 1.0088 (0.9954)   Prec@1 68.000 (65.109)   Prec@5 96.000 (96.772)   [2025-10-23 16:01:39]
  Epoch: [017][200/500]   Time 0.010 (0.101)   Data 0.000 (0.089)   Loss 1.0646 (0.9963)   Prec@1 67.000 (64.861)   Prec@5 95.000 (96.692)   [2025-10-23 16:01:40]
  Epoch: [017][300/500]   Time 0.009 (0.071)   Data 0.000 (0.059)   Loss 0.9912 (0.9952)   Prec@1 69.000 (65.076)   Prec@5 96.000 (96.631)   [2025-10-23 16:01:41]
  Epoch: [017][400/500]   Time 0.011 (0.056)   Data 0.000 (0.045)   Loss 1.0798 (0.9992)   Prec@1 60.000 (64.845)   Prec@5 97.000 (96.678)   [2025-10-23 16:01:42]
  **Train** Prec@1 65.010 Prec@5 96.652 Error@1 34.990
  **Test** Prec@1 72.770 Prec@5 97.900 Error@1 27.230
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:02:01] [Epoch=018/040] [Need: 00:16:39] [LR=0.0100] [Best : Accuracy=72.77, Error=27.23]
  Epoch: [018][000/500]   Time 17.566 (17.566)   Data 17.522 (17.522)   Loss 1.0348 (1.0348)   Prec@1 62.000 (62.000)   Prec@5 99.000 (99.000)   [2025-10-23 16:02:19]
  Epoch: [018][100/500]   Time 0.011 (0.186)   Data 0.000 (0.174)   Loss 1.1785 (1.0115)   Prec@1 57.000 (64.356)   Prec@5 92.000 (96.356)   [2025-10-23 16:02:20]
  Epoch: [018][200/500]   Time 0.013 (0.099)   Data 0.000 (0.087)   Loss 1.1724 (1.0013)   Prec@1 58.000 (64.871)   Prec@5 97.000 (96.711)   [2025-10-23 16:02:21]
  Epoch: [018][300/500]   Time 0.010 (0.069)   Data 0.000 (0.058)   Loss 0.9505 (0.9906)   Prec@1 65.000 (65.113)   Prec@5 98.000 (96.817)   [2025-10-23 16:02:22]
  Epoch: [018][400/500]   Time 0.012 (0.055)   Data 0.000 (0.044)   Loss 0.7961 (0.9895)   Prec@1 68.000 (65.304)   Prec@5 99.000 (96.733)   [2025-10-23 16:02:23]
  **Train** Prec@1 65.408 Prec@5 96.676 Error@1 34.592
  **Test** Prec@1 72.950 Prec@5 97.940 Error@1 27.050
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:02:43] [Epoch=019/040] [Need: 00:15:50] [LR=0.0100] [Best : Accuracy=72.95, Error=27.05]
  Epoch: [019][000/500]   Time 18.028 (18.028)   Data 17.469 (17.469)   Loss 0.9432 (0.9432)   Prec@1 72.000 (72.000)   Prec@5 96.000 (96.000)   [2025-10-23 16:03:02]
  Epoch: [019][100/500]   Time 0.010 (0.192)   Data 0.000 (0.173)   Loss 0.9613 (0.9868)   Prec@1 65.000 (65.624)   Prec@5 97.000 (96.871)   [2025-10-23 16:03:03]
  Epoch: [019][200/500]   Time 0.009 (0.102)   Data 0.000 (0.087)   Loss 0.9888 (0.9729)   Prec@1 64.000 (66.134)   Prec@5 97.000 (96.846)   [2025-10-23 16:03:04]
  Epoch: [019][300/500]   Time 0.010 (0.071)   Data 0.001 (0.058)   Loss 1.0107 (0.9777)   Prec@1 67.000 (65.807)   Prec@5 96.000 (96.821)   [2025-10-23 16:03:05]
  Epoch: [019][400/500]   Time 0.010 (0.056)   Data 0.000 (0.044)   Loss 1.0704 (0.9825)   Prec@1 58.000 (65.581)   Prec@5 97.000 (96.743)   [2025-10-23 16:03:06]
  **Train** Prec@1 65.652 Prec@5 96.706 Error@1 34.348
  **Test** Prec@1 73.470 Prec@5 98.000 Error@1 26.530
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:03:26] [Epoch=020/040] [Need: 00:15:02] [LR=0.0100] [Best : Accuracy=73.47, Error=26.53]
  Epoch: [020][000/500]   Time 18.587 (18.587)   Data 18.542 (18.542)   Loss 1.0557 (1.0557)   Prec@1 63.000 (63.000)   Prec@5 97.000 (97.000)   [2025-10-23 16:03:44]
  Epoch: [020][100/500]   Time 0.010 (0.196)   Data 0.000 (0.184)   Loss 0.9589 (0.9756)   Prec@1 70.000 (65.960)   Prec@5 96.000 (96.881)   [2025-10-23 16:03:45]
  Epoch: [020][200/500]   Time 0.010 (0.103)   Data 0.000 (0.092)   Loss 0.9134 (0.9806)   Prec@1 62.000 (65.776)   Prec@5 97.000 (96.741)   [2025-10-23 16:03:46]
  Epoch: [020][300/500]   Time 0.010 (0.073)   Data 0.001 (0.062)   Loss 1.1213 (0.9781)   Prec@1 65.000 (65.837)   Prec@5 94.000 (96.691)   [2025-10-23 16:03:47]
  Epoch: [020][400/500]   Time 0.015 (0.057)   Data 0.000 (0.046)   Loss 0.9384 (0.9757)   Prec@1 65.000 (65.843)   Prec@5 100.000 (96.701)   [2025-10-23 16:03:48]
  **Train** Prec@1 66.026 Prec@5 96.728 Error@1 33.974
  **Test** Prec@1 73.940 Prec@5 98.020 Error@1 26.060
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:04:08] [Epoch=021/040] [Need: 00:14:15] [LR=0.0100] [Best : Accuracy=73.94, Error=26.06]
  Epoch: [021][000/500]   Time 17.815 (17.815)   Data 17.769 (17.769)   Loss 0.8827 (0.8827)   Prec@1 67.000 (67.000)   Prec@5 98.000 (98.000)   [2025-10-23 16:04:26]
  Epoch: [021][100/500]   Time 0.011 (0.190)   Data 0.000 (0.176)   Loss 0.8510 (0.9768)   Prec@1 69.000 (65.475)   Prec@5 98.000 (96.990)   [2025-10-23 16:04:28]
  Epoch: [021][200/500]   Time 0.009 (0.101)   Data 0.000 (0.089)   Loss 0.9205 (0.9715)   Prec@1 68.000 (65.483)   Prec@5 96.000 (96.935)   [2025-10-23 16:04:29]
  Epoch: [021][300/500]   Time 0.009 (0.071)   Data 0.000 (0.059)   Loss 0.8058 (0.9671)   Prec@1 72.000 (65.864)   Prec@5 99.000 (96.934)   [2025-10-23 16:04:30]
  Epoch: [021][400/500]   Time 0.010 (0.056)   Data 0.000 (0.044)   Loss 0.9460 (0.9720)   Prec@1 66.000 (65.656)   Prec@5 98.000 (96.898)   [2025-10-23 16:04:31]
  **Train** Prec@1 65.674 Prec@5 96.816 Error@1 34.326
  **Test** Prec@1 74.140 Prec@5 98.240 Error@1 25.860
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:04:51] [Epoch=022/040] [Need: 00:13:28] [LR=0.0100] [Best : Accuracy=74.14, Error=25.86]
  Epoch: [022][000/500]   Time 17.809 (17.809)   Data 17.762 (17.762)   Loss 0.9751 (0.9751)   Prec@1 68.000 (68.000)   Prec@5 97.000 (97.000)   [2025-10-23 16:05:08]
  Epoch: [022][100/500]   Time 0.013 (0.188)   Data 0.000 (0.176)   Loss 0.9506 (0.9505)   Prec@1 67.000 (66.238)   Prec@5 96.000 (96.931)   [2025-10-23 16:05:10]
  Epoch: [022][200/500]   Time 0.012 (0.100)   Data 0.000 (0.089)   Loss 0.8927 (0.9601)   Prec@1 73.000 (66.234)   Prec@5 98.000 (96.896)   [2025-10-23 16:05:11]
  Epoch: [022][300/500]   Time 0.011 (0.070)   Data 0.000 (0.059)   Loss 1.1965 (0.9601)   Prec@1 59.000 (66.435)   Prec@5 96.000 (96.831)   [2025-10-23 16:05:12]
  Epoch: [022][400/500]   Time 0.012 (0.055)   Data 0.000 (0.044)   Loss 1.0577 (0.9618)   Prec@1 64.000 (66.389)   Prec@5 95.000 (96.813)   [2025-10-23 16:05:13]
  **Train** Prec@1 66.262 Prec@5 96.838 Error@1 33.738
  **Test** Prec@1 74.030 Prec@5 98.220 Error@1 25.970

==>>[2025-10-23 16:05:33] [Epoch=023/040] [Need: 00:12:40] [LR=0.0100] [Best : Accuracy=74.14, Error=25.86]
  Epoch: [023][000/500]   Time 18.395 (18.395)   Data 18.348 (18.348)   Loss 0.9169 (0.9169)   Prec@1 70.000 (70.000)   Prec@5 97.000 (97.000)   [2025-10-23 16:05:51]
  Epoch: [023][100/500]   Time 0.016 (0.198)   Data 0.001 (0.182)   Loss 1.1265 (0.9488)   Prec@1 62.000 (67.089)   Prec@5 95.000 (97.050)   [2025-10-23 16:05:53]
  Epoch: [023][200/500]   Time 0.014 (0.106)   Data 0.001 (0.092)   Loss 0.8970 (0.9457)   Prec@1 67.000 (66.970)   Prec@5 98.000 (97.124)   [2025-10-23 16:05:54]
  Epoch: [023][300/500]   Time 0.013 (0.075)   Data 0.000 (0.061)   Loss 0.7941 (0.9472)   Prec@1 69.000 (66.890)   Prec@5 100.000 (97.096)   [2025-10-23 16:05:55]
  Epoch: [023][400/500]   Time 0.014 (0.060)   Data 0.000 (0.046)   Loss 1.1727 (0.9514)   Prec@1 57.000 (66.820)   Prec@5 97.000 (97.027)   [2025-10-23 16:05:56]
  **Train** Prec@1 66.718 Prec@5 96.978 Error@1 33.282
  **Test** Prec@1 74.310 Prec@5 98.120 Error@1 25.690
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:06:24] [Epoch=024/040] [Need: 00:12:00] [LR=0.0100] [Best : Accuracy=74.31, Error=25.69]
  Epoch: [024][000/500]   Time 23.466 (23.466)   Data 23.381 (23.381)   Loss 0.9707 (0.9707)   Prec@1 66.000 (66.000)   Prec@5 95.000 (95.000)   [2025-10-23 16:06:47]
  Epoch: [024][100/500]   Time 0.010 (0.243)   Data 0.000 (0.232)   Loss 0.8564 (0.9420)   Prec@1 71.000 (67.079)   Prec@5 99.000 (97.000)   [2025-10-23 16:06:48]
  Epoch: [024][200/500]   Time 0.011 (0.127)   Data 0.000 (0.117)   Loss 0.8189 (0.9486)   Prec@1 71.000 (66.896)   Prec@5 98.000 (96.900)   [2025-10-23 16:06:49]
  Epoch: [024][300/500]   Time 0.015 (0.089)   Data 0.000 (0.078)   Loss 1.1628 (0.9514)   Prec@1 58.000 (66.801)   Prec@5 97.000 (96.920)   [2025-10-23 16:06:50]
  Epoch: [024][400/500]   Time 0.010 (0.069)   Data 0.000 (0.058)   Loss 0.8877 (0.9525)   Prec@1 70.000 (66.771)   Prec@5 98.000 (96.885)   [2025-10-23 16:06:52]
  **Train** Prec@1 66.730 Prec@5 96.880 Error@1 33.270
  **Test** Prec@1 74.540 Prec@5 98.050 Error@1 25.460
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:07:12] [Epoch=025/040] [Need: 00:11:17] [LR=0.0010] [Best : Accuracy=74.54, Error=25.46]
  Epoch: [025][000/500]   Time 18.244 (18.244)   Data 18.195 (18.195)   Loss 0.9901 (0.9901)   Prec@1 68.000 (68.000)   Prec@5 95.000 (95.000)   [2025-10-23 16:07:30]
  Epoch: [025][100/500]   Time 0.013 (0.194)   Data 0.001 (0.180)   Loss 0.6726 (0.9241)   Prec@1 73.000 (67.881)   Prec@5 100.000 (97.188)   [2025-10-23 16:07:31]
  Epoch: [025][200/500]   Time 0.013 (0.104)   Data 0.000 (0.091)   Loss 1.0023 (0.9060)   Prec@1 65.000 (68.129)   Prec@5 96.000 (97.333)   [2025-10-23 16:07:33]
  Epoch: [025][300/500]   Time 0.012 (0.073)   Data 0.000 (0.061)   Loss 0.8245 (0.9003)   Prec@1 70.000 (68.342)   Prec@5 96.000 (97.352)   [2025-10-23 16:07:34]
  Epoch: [025][400/500]   Time 0.009 (0.057)   Data 0.000 (0.046)   Loss 0.9955 (0.8944)   Prec@1 67.000 (68.579)   Prec@5 97.000 (97.372)   [2025-10-23 16:07:35]
  **Train** Prec@1 68.694 Prec@5 97.404 Error@1 31.306
  **Test** Prec@1 75.950 Prec@5 98.180 Error@1 24.050
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:07:54] [Epoch=026/040] [Need: 00:10:30] [LR=0.0010] [Best : Accuracy=75.95, Error=24.05]
  Epoch: [026][000/500]   Time 17.560 (17.560)   Data 17.513 (17.513)   Loss 0.8516 (0.8516)   Prec@1 68.000 (68.000)   Prec@5 97.000 (97.000)   [2025-10-23 16:08:12]
  Epoch: [026][100/500]   Time 0.009 (0.187)   Data 0.000 (0.174)   Loss 0.7552 (0.8744)   Prec@1 73.000 (69.723)   Prec@5 99.000 (97.257)   [2025-10-23 16:08:13]
  Epoch: [026][200/500]   Time 0.009 (0.099)   Data 0.000 (0.087)   Loss 0.9022 (0.8697)   Prec@1 67.000 (69.493)   Prec@5 98.000 (97.522)   [2025-10-23 16:08:14]
  Epoch: [026][300/500]   Time 0.012 (0.069)   Data 0.000 (0.058)   Loss 1.1144 (0.8755)   Prec@1 65.000 (69.365)   Prec@5 96.000 (97.452)   [2025-10-23 16:08:15]
  Epoch: [026][400/500]   Time 0.011 (0.055)   Data 0.000 (0.044)   Loss 0.7323 (0.8710)   Prec@1 77.000 (69.516)   Prec@5 100.000 (97.464)   [2025-10-23 16:08:16]
  **Train** Prec@1 69.440 Prec@5 97.470 Error@1 30.560
  **Test** Prec@1 76.250 Prec@5 98.520 Error@1 23.750
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:08:36] [Epoch=027/040] [Need: 00:09:44] [LR=0.0010] [Best : Accuracy=76.25, Error=23.75]
  Epoch: [027][000/500]   Time 17.514 (17.514)   Data 17.465 (17.465)   Loss 0.6678 (0.6678)   Prec@1 74.000 (74.000)   Prec@5 100.000 (100.000)   [2025-10-23 16:08:54]
  Epoch: [027][100/500]   Time 0.008 (0.186)   Data 0.000 (0.173)   Loss 0.8216 (0.8430)   Prec@1 69.000 (70.059)   Prec@5 99.000 (97.931)   [2025-10-23 16:08:55]
  Epoch: [027][200/500]   Time 0.011 (0.099)   Data 0.000 (0.087)   Loss 0.9379 (0.8584)   Prec@1 72.000 (69.721)   Prec@5 98.000 (97.632)   [2025-10-23 16:08:56]
  Epoch: [027][300/500]   Time 0.013 (0.070)   Data 0.000 (0.058)   Loss 0.8664 (0.8575)   Prec@1 66.000 (69.904)   Prec@5 100.000 (97.561)   [2025-10-23 16:08:57]
  Epoch: [027][400/500]   Time 0.008 (0.055)   Data 0.000 (0.044)   Loss 0.8562 (0.8579)   Prec@1 72.000 (69.960)   Prec@5 97.000 (97.541)   [2025-10-23 16:08:58]
  **Train** Prec@1 69.850 Prec@5 97.580 Error@1 30.150
  **Test** Prec@1 76.410 Prec@5 98.420 Error@1 23.590
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:09:18] [Epoch=028/040] [Need: 00:08:57] [LR=0.0010] [Best : Accuracy=76.41, Error=23.59]
  Epoch: [028][000/500]   Time 17.408 (17.408)   Data 17.361 (17.361)   Loss 0.8650 (0.8650)   Prec@1 69.000 (69.000)   Prec@5 97.000 (97.000)   [2025-10-23 16:09:35]
  Epoch: [028][100/500]   Time 0.010 (0.185)   Data 0.000 (0.172)   Loss 0.9791 (0.8741)   Prec@1 66.000 (69.762)   Prec@5 98.000 (97.416)   [2025-10-23 16:09:37]
  Epoch: [028][200/500]   Time 0.009 (0.098)   Data 0.000 (0.087)   Loss 0.7261 (0.8669)   Prec@1 73.000 (69.642)   Prec@5 99.000 (97.493)   [2025-10-23 16:09:38]
  Epoch: [028][300/500]   Time 0.009 (0.069)   Data 0.000 (0.058)   Loss 0.9841 (0.8667)   Prec@1 61.000 (69.681)   Prec@5 97.000 (97.518)   [2025-10-23 16:09:39]
  Epoch: [028][400/500]   Time 0.012 (0.055)   Data 0.001 (0.043)   Loss 0.8690 (0.8588)   Prec@1 69.000 (70.015)   Prec@5 97.000 (97.549)   [2025-10-23 16:09:40]
  **Train** Prec@1 69.798 Prec@5 97.530 Error@1 30.202
  **Test** Prec@1 76.360 Prec@5 98.380 Error@1 23.640

==>>[2025-10-23 16:10:00] [Epoch=029/040] [Need: 00:08:11] [LR=0.0010] [Best : Accuracy=76.41, Error=23.59]
  Epoch: [029][000/500]   Time 17.320 (17.320)   Data 17.275 (17.275)   Loss 0.9537 (0.9537)   Prec@1 66.000 (66.000)   Prec@5 97.000 (97.000)   [2025-10-23 16:10:17]
  Epoch: [029][100/500]   Time 0.012 (0.184)   Data 0.000 (0.171)   Loss 0.7911 (0.8695)   Prec@1 74.000 (69.832)   Prec@5 98.000 (97.564)   [2025-10-23 16:10:18]
  Epoch: [029][200/500]   Time 0.009 (0.098)   Data 0.000 (0.086)   Loss 0.8413 (0.8671)   Prec@1 74.000 (69.856)   Prec@5 98.000 (97.617)   [2025-10-23 16:10:19]
  Epoch: [029][300/500]   Time 0.009 (0.069)   Data 0.000 (0.058)   Loss 0.8375 (0.8645)   Prec@1 68.000 (69.774)   Prec@5 97.000 (97.571)   [2025-10-23 16:10:20]
  Epoch: [029][400/500]   Time 0.012 (0.054)   Data 0.000 (0.043)   Loss 0.7892 (0.8634)   Prec@1 70.000 (69.955)   Prec@5 97.000 (97.541)   [2025-10-23 16:10:21]
  **Train** Prec@1 70.040 Prec@5 97.592 Error@1 29.960
  **Test** Prec@1 76.610 Prec@5 98.430 Error@1 23.390
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:10:41] [Epoch=030/040] [Need: 00:07:26] [LR=0.0010] [Best : Accuracy=76.61, Error=23.39]
  Epoch: [030][000/500]   Time 17.465 (17.465)   Data 17.416 (17.416)   Loss 0.8383 (0.8383)   Prec@1 69.000 (69.000)   Prec@5 97.000 (97.000)   [2025-10-23 16:10:59]
  Epoch: [030][100/500]   Time 0.010 (0.185)   Data 0.000 (0.173)   Loss 0.8754 (0.8497)   Prec@1 66.000 (70.050)   Prec@5 98.000 (97.574)   [2025-10-23 16:11:00]
  Epoch: [030][200/500]   Time 0.011 (0.098)   Data 0.000 (0.087)   Loss 1.0877 (0.8591)   Prec@1 63.000 (69.726)   Prec@5 97.000 (97.537)   [2025-10-23 16:11:01]
  Epoch: [030][300/500]   Time 0.013 (0.069)   Data 0.000 (0.058)   Loss 1.0241 (0.8597)   Prec@1 67.000 (69.754)   Prec@5 94.000 (97.601)   [2025-10-23 16:11:02]
  Epoch: [030][400/500]   Time 0.010 (0.055)   Data 0.000 (0.044)   Loss 0.7592 (0.8565)   Prec@1 69.000 (69.960)   Prec@5 97.000 (97.569)   [2025-10-23 16:11:03]
  **Train** Prec@1 69.900 Prec@5 97.588 Error@1 30.100
  **Test** Prec@1 76.660 Prec@5 98.400 Error@1 23.340
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:11:23] [Epoch=031/040] [Need: 00:06:40] [LR=0.0010] [Best : Accuracy=76.66, Error=23.34]
  Epoch: [031][000/500]   Time 17.368 (17.368)   Data 17.320 (17.320)   Loss 0.9019 (0.9019)   Prec@1 80.000 (80.000)   Prec@5 95.000 (95.000)   [2025-10-23 16:11:40]
  Epoch: [031][100/500]   Time 0.011 (0.185)   Data 0.001 (0.172)   Loss 0.7963 (0.8532)   Prec@1 71.000 (70.089)   Prec@5 97.000 (97.911)   [2025-10-23 16:11:42]
  Epoch: [031][200/500]   Time 0.011 (0.098)   Data 0.001 (0.086)   Loss 0.9565 (0.8454)   Prec@1 68.000 (70.786)   Prec@5 98.000 (97.706)   [2025-10-23 16:11:43]
  Epoch: [031][300/500]   Time 0.013 (0.069)   Data 0.000 (0.058)   Loss 1.0071 (0.8548)   Prec@1 63.000 (70.379)   Prec@5 97.000 (97.641)   [2025-10-23 16:11:44]
  Epoch: [031][400/500]   Time 0.010 (0.054)   Data 0.000 (0.043)   Loss 0.6442 (0.8536)   Prec@1 75.000 (70.352)   Prec@5 99.000 (97.661)   [2025-10-23 16:11:45]
  **Train** Prec@1 70.464 Prec@5 97.676 Error@1 29.536
  **Test** Prec@1 76.910 Prec@5 98.540 Error@1 23.090
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:12:04] [Epoch=032/040] [Need: 00:05:55] [LR=0.0010] [Best : Accuracy=76.91, Error=23.09]
  Epoch: [032][000/500]   Time 17.424 (17.424)   Data 17.379 (17.379)   Loss 0.7571 (0.7571)   Prec@1 70.000 (70.000)   Prec@5 99.000 (99.000)   [2025-10-23 16:12:21]
  Epoch: [032][100/500]   Time 0.010 (0.186)   Data 0.000 (0.172)   Loss 0.7446 (0.8433)   Prec@1 69.000 (70.386)   Prec@5 99.000 (97.644)   [2025-10-23 16:12:23]
  Epoch: [032][200/500]   Time 0.012 (0.099)   Data 0.000 (0.087)   Loss 0.8218 (0.8526)   Prec@1 66.000 (70.050)   Prec@5 97.000 (97.572)   [2025-10-23 16:12:24]
  Epoch: [032][300/500]   Time 0.011 (0.070)   Data 0.001 (0.058)   Loss 0.7247 (0.8480)   Prec@1 75.000 (70.316)   Prec@5 100.000 (97.571)   [2025-10-23 16:12:25]
  Epoch: [032][400/500]   Time 0.011 (0.055)   Data 0.001 (0.044)   Loss 0.7853 (0.8448)   Prec@1 72.000 (70.564)   Prec@5 99.000 (97.579)   [2025-10-23 16:12:26]
  **Train** Prec@1 70.490 Prec@5 97.604 Error@1 29.510
  **Test** Prec@1 76.780 Prec@5 98.410 Error@1 23.220

==>>[2025-10-23 16:12:45] [Epoch=033/040] [Need: 00:05:10] [LR=0.0010] [Best : Accuracy=76.91, Error=23.09]
  Epoch: [033][000/500]   Time 17.301 (17.301)   Data 17.255 (17.255)   Loss 0.7361 (0.7361)   Prec@1 68.000 (68.000)   Prec@5 100.000 (100.000)   [2025-10-23 16:13:03]
  Epoch: [033][100/500]   Time 0.011 (0.183)   Data 0.000 (0.171)   Loss 0.8744 (0.8379)   Prec@1 71.000 (70.802)   Prec@5 96.000 (97.634)   [2025-10-23 16:13:04]
  Epoch: [033][200/500]   Time 0.009 (0.097)   Data 0.000 (0.086)   Loss 0.8581 (0.8384)   Prec@1 71.000 (70.667)   Prec@5 99.000 (97.692)   [2025-10-23 16:13:05]
  Epoch: [033][300/500]   Time 0.011 (0.068)   Data 0.001 (0.058)   Loss 0.6693 (0.8380)   Prec@1 76.000 (70.821)   Prec@5 98.000 (97.648)   [2025-10-23 16:13:06]
  Epoch: [033][400/500]   Time 0.010 (0.054)   Data 0.001 (0.043)   Loss 0.9817 (0.8433)   Prec@1 65.000 (70.579)   Prec@5 97.000 (97.616)   [2025-10-23 16:13:07]
  **Train** Prec@1 70.736 Prec@5 97.586 Error@1 29.264
  **Test** Prec@1 76.730 Prec@5 98.470 Error@1 23.270

==>>[2025-10-23 16:13:27] [Epoch=034/040] [Need: 00:04:25] [LR=0.0010] [Best : Accuracy=76.91, Error=23.09]
  Epoch: [034][000/500]   Time 17.311 (17.311)   Data 17.267 (17.267)   Loss 0.6763 (0.6763)   Prec@1 72.000 (72.000)   Prec@5 100.000 (100.000)   [2025-10-23 16:13:44]
  Epoch: [034][100/500]   Time 0.013 (0.184)   Data 0.001 (0.171)   Loss 0.8898 (0.8584)   Prec@1 61.000 (69.673)   Prec@5 97.000 (97.564)   [2025-10-23 16:13:45]
  Epoch: [034][200/500]   Time 0.011 (0.098)   Data 0.000 (0.086)   Loss 0.8675 (0.8507)   Prec@1 73.000 (69.920)   Prec@5 97.000 (97.697)   [2025-10-23 16:13:47]
  Epoch: [034][300/500]   Time 0.009 (0.069)   Data 0.000 (0.058)   Loss 0.6404 (0.8516)   Prec@1 80.000 (70.140)   Prec@5 99.000 (97.611)   [2025-10-23 16:13:48]
  Epoch: [034][400/500]   Time 0.011 (0.054)   Data 0.001 (0.043)   Loss 0.5927 (0.8487)   Prec@1 79.000 (70.277)   Prec@5 98.000 (97.596)   [2025-10-23 16:13:49]
  **Train** Prec@1 70.426 Prec@5 97.634 Error@1 29.574
  **Test** Prec@1 76.920 Prec@5 98.510 Error@1 23.080
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:14:08] [Epoch=035/040] [Need: 00:03:40] [LR=0.0010] [Best : Accuracy=76.92, Error=23.08]
  Epoch: [035][000/500]   Time 17.954 (17.954)   Data 17.910 (17.910)   Loss 0.9983 (0.9983)   Prec@1 68.000 (68.000)   Prec@5 92.000 (92.000)   [2025-10-23 16:14:26]
  Epoch: [035][100/500]   Time 0.012 (0.192)   Data 0.000 (0.178)   Loss 0.8510 (0.8387)   Prec@1 67.000 (70.762)   Prec@5 99.000 (97.614)   [2025-10-23 16:14:28]
  Epoch: [035][200/500]   Time 0.009 (0.102)   Data 0.000 (0.089)   Loss 0.7244 (0.8333)   Prec@1 74.000 (71.095)   Prec@5 100.000 (97.721)   [2025-10-23 16:14:29]
  Epoch: [035][300/500]   Time 0.009 (0.071)   Data 0.000 (0.060)   Loss 0.6812 (0.8372)   Prec@1 76.000 (70.980)   Prec@5 100.000 (97.581)   [2025-10-23 16:14:30]
  Epoch: [035][400/500]   Time 0.009 (0.056)   Data 0.000 (0.045)   Loss 0.8105 (0.8410)   Prec@1 70.000 (70.860)   Prec@5 99.000 (97.581)   [2025-10-23 16:14:31]
  **Train** Prec@1 70.794 Prec@5 97.594 Error@1 29.206
  **Test** Prec@1 76.990 Prec@5 98.490 Error@1 23.010
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:14:51] [Epoch=036/040] [Need: 00:02:56] [LR=0.0010] [Best : Accuracy=76.99, Error=23.01]
  Epoch: [036][000/500]   Time 17.365 (17.365)   Data 17.320 (17.320)   Loss 0.8712 (0.8712)   Prec@1 65.000 (65.000)   Prec@5 98.000 (98.000)   [2025-10-23 16:15:08]
  Epoch: [036][100/500]   Time 0.011 (0.185)   Data 0.000 (0.172)   Loss 0.8162 (0.8383)   Prec@1 75.000 (70.634)   Prec@5 96.000 (97.842)   [2025-10-23 16:15:09]
  Epoch: [036][200/500]   Time 0.009 (0.098)   Data 0.000 (0.086)   Loss 0.7849 (0.8335)   Prec@1 77.000 (70.761)   Prec@5 99.000 (97.811)   [2025-10-23 16:15:11]
  Epoch: [036][300/500]   Time 0.009 (0.069)   Data 0.000 (0.058)   Loss 0.9935 (0.8401)   Prec@1 61.000 (70.588)   Prec@5 99.000 (97.688)   [2025-10-23 16:15:12]
  Epoch: [036][400/500]   Time 0.010 (0.055)   Data 0.000 (0.043)   Loss 0.8226 (0.8383)   Prec@1 69.000 (70.828)   Prec@5 98.000 (97.673)   [2025-10-23 16:15:13]
  **Train** Prec@1 70.874 Prec@5 97.690 Error@1 29.126
  **Test** Prec@1 76.800 Prec@5 98.440 Error@1 23.200

==>>[2025-10-23 16:15:32] [Epoch=037/040] [Need: 00:02:12] [LR=0.0010] [Best : Accuracy=76.99, Error=23.01]
  Epoch: [037][000/500]   Time 18.132 (18.132)   Data 18.084 (18.084)   Loss 0.7058 (0.7058)   Prec@1 77.000 (77.000)   Prec@5 97.000 (97.000)   [2025-10-23 16:15:50]
  Epoch: [037][100/500]   Time 0.008 (0.193)   Data 0.000 (0.179)   Loss 0.7456 (0.8368)   Prec@1 68.000 (70.713)   Prec@5 100.000 (97.634)   [2025-10-23 16:15:52]
  Epoch: [037][200/500]   Time 0.009 (0.102)   Data 0.000 (0.090)   Loss 0.8626 (0.8335)   Prec@1 66.000 (70.776)   Prec@5 99.000 (97.716)   [2025-10-23 16:15:53]
  Epoch: [037][300/500]   Time 0.010 (0.072)   Data 0.000 (0.060)   Loss 0.8577 (0.8333)   Prec@1 73.000 (70.877)   Prec@5 100.000 (97.721)   [2025-10-23 16:15:54]
  Epoch: [037][400/500]   Time 0.012 (0.056)   Data 0.000 (0.045)   Loss 0.7138 (0.8358)   Prec@1 78.000 (70.860)   Prec@5 98.000 (97.726)   [2025-10-23 16:15:55]
  **Train** Prec@1 70.714 Prec@5 97.714 Error@1 29.286
  **Test** Prec@1 77.020 Prec@5 98.520 Error@1 22.980
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 16:16:14] [Epoch=038/040] [Need: 00:01:27] [LR=0.0010] [Best : Accuracy=77.02, Error=22.98]
  Epoch: [038][000/500]   Time 17.321 (17.321)   Data 17.275 (17.275)   Loss 0.8680 (0.8680)   Prec@1 67.000 (67.000)   Prec@5 97.000 (97.000)   [2025-10-23 16:16:32]
  Epoch: [038][100/500]   Time 0.011 (0.185)   Data 0.000 (0.171)   Loss 0.8394 (0.8318)   Prec@1 71.000 (70.921)   Prec@5 97.000 (97.663)   [2025-10-23 16:16:33]
  Epoch: [038][200/500]   Time 0.010 (0.098)   Data 0.000 (0.086)   Loss 0.9266 (0.8403)   Prec@1 71.000 (70.483)   Prec@5 95.000 (97.746)   [2025-10-23 16:16:34]
  Epoch: [038][300/500]   Time 0.010 (0.069)   Data 0.000 (0.058)   Loss 0.7555 (0.8383)   Prec@1 73.000 (70.508)   Prec@5 97.000 (97.691)   [2025-10-23 16:16:35]
  Epoch: [038][400/500]   Time 0.010 (0.054)   Data 0.000 (0.043)   Loss 0.6745 (0.8424)   Prec@1 76.000 (70.526)   Prec@5 98.000 (97.688)   [2025-10-23 16:16:36]
  **Train** Prec@1 70.750 Prec@5 97.672 Error@1 29.250
  **Test** Prec@1 76.940 Prec@5 98.520 Error@1 23.060

==>>[2025-10-23 16:17:00] [Epoch=039/040] [Need: 00:00:44] [LR=0.0010] [Best : Accuracy=77.02, Error=22.98]
  Epoch: [039][000/500]   Time 25.782 (25.782)   Data 25.649 (25.649)   Loss 0.8322 (0.8322)   Prec@1 68.000 (68.000)   Prec@5 97.000 (97.000)   [2025-10-23 16:17:26]
  Epoch: [039][100/500]   Time 0.035 (0.280)   Data 0.027 (0.263)   Loss 0.7080 (0.8488)   Prec@1 74.000 (70.228)   Prec@5 98.000 (97.594)   [2025-10-23 16:17:28]
  Epoch: [039][200/500]   Time 0.014 (0.148)   Data 0.000 (0.134)   Loss 0.5914 (0.8395)   Prec@1 77.000 (70.572)   Prec@5 99.000 (97.687)   [2025-10-23 16:17:30]
  Epoch: [039][300/500]   Time 0.011 (0.103)   Data 0.000 (0.090)   Loss 0.8803 (0.8389)   Prec@1 74.000 (70.811)   Prec@5 96.000 (97.628)   [2025-10-23 16:17:31]
  Epoch: [039][400/500]   Time 0.013 (0.080)   Data 0.000 (0.068)   Loss 0.8090 (0.8419)   Prec@1 68.000 (70.733)   Prec@5 99.000 (97.658)   [2025-10-23 16:17:32]
  **Train** Prec@1 70.638 Prec@5 97.638 Error@1 29.362
  **Test** Prec@1 77.240 Prec@5 98.460 Error@1 22.760
=> Obtain best accuracy, and update the best model
