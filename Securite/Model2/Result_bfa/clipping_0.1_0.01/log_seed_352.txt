save path : ./save/tinyvgg_quan/clipping_0.1_0.01
{'data_path': './dataset', 'arch': 'tinyvgg_quan', 'dataset': 'cifar10', 'epochs': 40, 'start_epoch': 0, 'attack_sample_size': 100, 'test_batch_size': 100, 'optimizer': 'SGD', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 1, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 100, 'randbet': False, 'clipping_coeff': 0.1, 'learning_rate': 0.01, 'manualSeed': 352, 'save_path': './save/tinyvgg_quan/clipping_0.1_0.01', 'enable_bfa': False, 'resume': '', 'quan_bitwidth': None, 'reset_weight': False, 'evaluate': False, 'n_iter': 30, 'model_only': False, 'random_bfa': False, 'use_cuda': True}
Random Seed: 352
python version : 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 14 2025, 16:10:16) [MSC v.1929 64 bit (AMD64)]
torch  version : 2.6.0+cu124
cudnn  version : 90100
=> creating model 'tinyvgg_quan'
=> network :
 TinyVGG(
  (features): Sequential(
    (0): quan_Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Dropout2d(p=0.25, inplace=False)
    (6): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace=True)
    (8): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace=True)
    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (11): Dropout2d(p=0.25, inplace=False)
  )
  (classifier): Sequential(
    (0): quan_Linear(in_features=2048, out_features=10, bias=True)
  )
)
=> do not use any checkpoint for tinyvgg_quan model

==>>[2025-10-23 00:28:44] [Epoch=000/040] [Need: 00:00:00] [LR=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/500]   Time 17.986 (17.986)   Data 17.641 (17.641)   Loss 2.2977 (2.2977)   Prec@1 12.000 (12.000)   Prec@5 52.000 (52.000)   [2025-10-23 00:29:02]
  Epoch: [000][100/500]   Time 0.010 (0.190)   Data 0.000 (0.175)   Loss 2.2211 (2.2245)   Prec@1 24.000 (17.000)   Prec@5 71.000 (62.257)   [2025-10-23 00:29:03]
  Epoch: [000][200/500]   Time 0.010 (0.101)   Data 0.001 (0.088)   Loss 1.9557 (2.1258)   Prec@1 28.000 (21.607)   Prec@5 86.000 (69.861)   [2025-10-23 00:29:04]
  Epoch: [000][300/500]   Time 0.013 (0.071)   Data 0.000 (0.059)   Loss 1.8626 (2.0527)   Prec@1 33.000 (24.857)   Prec@5 85.000 (74.106)   [2025-10-23 00:29:05]
  Epoch: [000][400/500]   Time 0.008 (0.056)   Data 0.000 (0.044)   Loss 1.7274 (1.9919)   Prec@1 36.000 (27.279)   Prec@5 90.000 (76.820)   [2025-10-23 00:29:06]
  **Train** Prec@1 29.266 Prec@5 78.914 Error@1 70.734
  **Test** Prec@1 44.490 Prec@5 91.970 Error@1 55.510
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:29:26] [Epoch=001/040] [Need: 00:27:20] [LR=0.0100] [Best : Accuracy=44.49, Error=55.51]
  Epoch: [001][000/500]   Time 18.046 (18.046)   Data 18.000 (18.000)   Loss 1.6541 (1.6541)   Prec@1 35.000 (35.000)   Prec@5 90.000 (90.000)   [2025-10-23 00:29:44]
  Epoch: [001][100/500]   Time 0.012 (0.193)   Data 0.000 (0.178)   Loss 1.6390 (1.6918)   Prec@1 47.000 (38.040)   Prec@5 86.000 (87.554)   [2025-10-23 00:29:46]
  Epoch: [001][200/500]   Time 0.011 (0.102)   Data 0.000 (0.090)   Loss 1.5416 (1.6654)   Prec@1 45.000 (39.428)   Prec@5 93.000 (87.975)   [2025-10-23 00:29:47]
  Epoch: [001][300/500]   Time 0.011 (0.072)   Data 0.000 (0.060)   Loss 1.6899 (1.6489)   Prec@1 38.000 (39.904)   Prec@5 91.000 (88.635)   [2025-10-23 00:29:48]
  Epoch: [001][400/500]   Time 0.012 (0.057)   Data 0.001 (0.045)   Loss 1.6600 (1.6298)   Prec@1 36.000 (40.559)   Prec@5 89.000 (89.087)   [2025-10-23 00:29:49]
  **Train** Prec@1 41.200 Prec@5 89.450 Error@1 58.800
  **Test** Prec@1 50.790 Prec@5 93.800 Error@1 49.210
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:30:09] [Epoch=002/040] [Need: 00:26:51] [LR=0.0100] [Best : Accuracy=50.79, Error=49.21]
  Epoch: [002][000/500]   Time 17.943 (17.943)   Data 17.897 (17.897)   Loss 1.5693 (1.5693)   Prec@1 40.000 (40.000)   Prec@5 88.000 (88.000)   [2025-10-23 00:30:27]
  Epoch: [002][100/500]   Time 0.012 (0.190)   Data 0.000 (0.177)   Loss 1.5678 (1.5387)   Prec@1 40.000 (44.059)   Prec@5 94.000 (91.218)   [2025-10-23 00:30:28]
  Epoch: [002][200/500]   Time 0.010 (0.101)   Data 0.000 (0.089)   Loss 1.4218 (1.5174)   Prec@1 49.000 (44.801)   Prec@5 92.000 (91.428)   [2025-10-23 00:30:29]
  Epoch: [002][300/500]   Time 0.010 (0.071)   Data 0.000 (0.060)   Loss 1.4550 (1.5034)   Prec@1 44.000 (45.375)   Prec@5 98.000 (91.502)   [2025-10-23 00:30:30]
  Epoch: [002][400/500]   Time 0.013 (0.056)   Data 0.000 (0.045)   Loss 1.4318 (1.4986)   Prec@1 45.000 (45.574)   Prec@5 93.000 (91.499)   [2025-10-23 00:30:31]
  **Train** Prec@1 45.948 Prec@5 91.576 Error@1 54.052
  **Test** Prec@1 55.190 Prec@5 94.610 Error@1 44.810
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:30:52] [Epoch=003/040] [Need: 00:26:16] [LR=0.0100] [Best : Accuracy=55.19, Error=44.81]
  Epoch: [003][000/500]   Time 17.674 (17.674)   Data 17.626 (17.626)   Loss 1.3738 (1.3738)   Prec@1 49.000 (49.000)   Prec@5 92.000 (92.000)   [2025-10-23 00:31:10]
  Epoch: [003][100/500]   Time 0.008 (0.187)   Data 0.001 (0.175)   Loss 1.4596 (1.4355)   Prec@1 48.000 (48.000)   Prec@5 94.000 (92.267)   [2025-10-23 00:31:11]
  Epoch: [003][200/500]   Time 0.011 (0.099)   Data 0.000 (0.088)   Loss 1.3987 (1.4214)   Prec@1 47.000 (48.667)   Prec@5 93.000 (92.522)   [2025-10-23 00:31:12]
  Epoch: [003][300/500]   Time 0.012 (0.070)   Data 0.000 (0.059)   Loss 1.4975 (1.4184)   Prec@1 52.000 (48.638)   Prec@5 89.000 (92.658)   [2025-10-23 00:31:13]
  Epoch: [003][400/500]   Time 0.009 (0.055)   Data 0.000 (0.044)   Loss 1.3651 (1.4157)   Prec@1 46.000 (48.845)   Prec@5 89.000 (92.589)   [2025-10-23 00:31:14]
  **Train** Prec@1 49.036 Prec@5 92.596 Error@1 50.964
  **Test** Prec@1 58.760 Prec@5 95.790 Error@1 41.240
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:31:35] [Epoch=004/040] [Need: 00:25:33] [LR=0.0100] [Best : Accuracy=58.76, Error=41.24]
  Epoch: [004][000/500]   Time 17.990 (17.990)   Data 17.946 (17.946)   Loss 1.4014 (1.4014)   Prec@1 52.000 (52.000)   Prec@5 92.000 (92.000)   [2025-10-23 00:31:52]
  Epoch: [004][100/500]   Time 0.009 (0.191)   Data 0.000 (0.178)   Loss 1.5268 (1.3707)   Prec@1 46.000 (50.743)   Prec@5 92.000 (93.000)   [2025-10-23 00:31:54]
  Epoch: [004][200/500]   Time 0.009 (0.101)   Data 0.000 (0.089)   Loss 1.2870 (1.3594)   Prec@1 52.000 (51.229)   Prec@5 94.000 (93.214)   [2025-10-23 00:31:55]
  Epoch: [004][300/500]   Time 0.010 (0.071)   Data 0.000 (0.060)   Loss 1.2922 (1.3471)   Prec@1 46.000 (51.701)   Prec@5 93.000 (93.292)   [2025-10-23 00:31:56]
  Epoch: [004][400/500]   Time 0.011 (0.056)   Data 0.001 (0.045)   Loss 1.2725 (1.3435)   Prec@1 56.000 (51.638)   Prec@5 95.000 (93.329)   [2025-10-23 00:31:57]
  **Train** Prec@1 51.898 Prec@5 93.418 Error@1 48.102
  **Test** Prec@1 61.960 Prec@5 96.440 Error@1 38.040
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:32:17] [Epoch=005/040] [Need: 00:24:48] [LR=0.0100] [Best : Accuracy=61.96, Error=38.04]
  Epoch: [005][000/500]   Time 17.808 (17.808)   Data 17.759 (17.759)   Loss 1.2247 (1.2247)   Prec@1 56.000 (56.000)   Prec@5 93.000 (93.000)   [2025-10-23 00:32:35]
  Epoch: [005][100/500]   Time 0.009 (0.189)   Data 0.000 (0.176)   Loss 1.1914 (1.3199)   Prec@1 55.000 (52.713)   Prec@5 96.000 (93.515)   [2025-10-23 00:32:36]
  Epoch: [005][200/500]   Time 0.009 (0.100)   Data 0.000 (0.089)   Loss 1.2737 (1.3111)   Prec@1 59.000 (53.139)   Prec@5 95.000 (93.706)   [2025-10-23 00:32:37]
  Epoch: [005][300/500]   Time 0.011 (0.070)   Data 0.000 (0.059)   Loss 1.2649 (1.2980)   Prec@1 56.000 (53.551)   Prec@5 97.000 (93.917)   [2025-10-23 00:32:38]
  Epoch: [005][400/500]   Time 0.009 (0.055)   Data 0.000 (0.044)   Loss 1.4037 (1.2919)   Prec@1 47.000 (53.683)   Prec@5 92.000 (94.082)   [2025-10-23 00:32:39]
  **Train** Prec@1 54.114 Prec@5 94.118 Error@1 45.886
  **Test** Prec@1 63.370 Prec@5 96.350 Error@1 36.630
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:32:59] [Epoch=006/040] [Need: 00:24:03] [LR=0.0100] [Best : Accuracy=63.37, Error=36.63]
  Epoch: [006][000/500]   Time 17.965 (17.965)   Data 17.920 (17.920)   Loss 1.2428 (1.2428)   Prec@1 55.000 (55.000)   Prec@5 98.000 (98.000)   [2025-10-23 00:33:17]
  Epoch: [006][100/500]   Time 0.009 (0.191)   Data 0.001 (0.178)   Loss 1.1821 (1.2515)   Prec@1 61.000 (55.990)   Prec@5 95.000 (94.485)   [2025-10-23 00:33:18]
  Epoch: [006][200/500]   Time 0.012 (0.101)   Data 0.000 (0.089)   Loss 1.2290 (1.2505)   Prec@1 57.000 (55.726)   Prec@5 95.000 (94.388)   [2025-10-23 00:33:19]
  Epoch: [006][300/500]   Time 0.012 (0.071)   Data 0.000 (0.060)   Loss 1.2999 (1.2461)   Prec@1 52.000 (55.794)   Prec@5 94.000 (94.302)   [2025-10-23 00:33:20]
  Epoch: [006][400/500]   Time 0.010 (0.056)   Data 0.000 (0.045)   Loss 1.1488 (1.2399)   Prec@1 58.000 (56.110)   Prec@5 97.000 (94.429)   [2025-10-23 00:33:21]
  **Train** Prec@1 56.216 Prec@5 94.418 Error@1 43.784
  **Test** Prec@1 65.370 Prec@5 96.880 Error@1 34.630
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:33:41] [Epoch=007/040] [Need: 00:23:19] [LR=0.0100] [Best : Accuracy=65.37, Error=34.63]
  Epoch: [007][000/500]   Time 17.696 (17.696)   Data 17.654 (17.654)   Loss 1.2450 (1.2450)   Prec@1 52.000 (52.000)   Prec@5 92.000 (92.000)   [2025-10-23 00:33:59]
  Epoch: [007][100/500]   Time 0.009 (0.188)   Data 0.000 (0.175)   Loss 1.1712 (1.2107)   Prec@1 58.000 (57.089)   Prec@5 94.000 (94.663)   [2025-10-23 00:34:00]
  Epoch: [007][200/500]   Time 0.009 (0.100)   Data 0.000 (0.088)   Loss 1.1685 (1.2154)   Prec@1 54.000 (57.055)   Prec@5 92.000 (94.781)   [2025-10-23 00:34:01]
  Epoch: [007][300/500]   Time 0.010 (0.070)   Data 0.000 (0.059)   Loss 1.3520 (1.2103)   Prec@1 55.000 (57.146)   Prec@5 91.000 (94.980)   [2025-10-23 00:34:02]
  Epoch: [007][400/500]   Time 0.009 (0.055)   Data 0.000 (0.044)   Loss 1.1707 (1.2045)   Prec@1 59.000 (57.352)   Prec@5 95.000 (94.913)   [2025-10-23 00:34:03]
  **Train** Prec@1 57.620 Prec@5 94.944 Error@1 42.380
  **Test** Prec@1 65.360 Prec@5 96.920 Error@1 34.640

==>>[2025-10-23 00:34:23] [Epoch=008/040] [Need: 00:22:35] [LR=0.0100] [Best : Accuracy=65.37, Error=34.63]
  Epoch: [008][000/500]   Time 17.854 (17.854)   Data 17.810 (17.810)   Loss 1.2188 (1.2188)   Prec@1 60.000 (60.000)   Prec@5 96.000 (96.000)   [2025-10-23 00:34:41]
  Epoch: [008][100/500]   Time 0.014 (0.190)   Data 0.000 (0.177)   Loss 1.1862 (1.1790)   Prec@1 58.000 (57.891)   Prec@5 95.000 (95.525)   [2025-10-23 00:34:42]
  Epoch: [008][200/500]   Time 0.009 (0.101)   Data 0.000 (0.089)   Loss 1.2980 (1.1727)   Prec@1 54.000 (58.493)   Prec@5 89.000 (95.383)   [2025-10-23 00:34:43]
  Epoch: [008][300/500]   Time 0.010 (0.071)   Data 0.000 (0.059)   Loss 1.2237 (1.1740)   Prec@1 48.000 (58.415)   Prec@5 98.000 (95.405)   [2025-10-23 00:34:44]
  Epoch: [008][400/500]   Time 0.011 (0.056)   Data 0.001 (0.045)   Loss 1.1830 (1.1763)   Prec@1 52.000 (58.434)   Prec@5 95.000 (95.247)   [2025-10-23 00:34:46]
  **Train** Prec@1 58.534 Prec@5 95.260 Error@1 41.466
  **Test** Prec@1 67.080 Prec@5 97.180 Error@1 32.920
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:35:06] [Epoch=009/040] [Need: 00:21:54] [LR=0.0100] [Best : Accuracy=67.08, Error=32.92]
  Epoch: [009][000/500]   Time 17.830 (17.830)   Data 17.785 (17.785)   Loss 1.1079 (1.1079)   Prec@1 65.000 (65.000)   Prec@5 93.000 (93.000)   [2025-10-23 00:35:24]
  Epoch: [009][100/500]   Time 0.010 (0.188)   Data 0.000 (0.176)   Loss 1.0508 (1.1547)   Prec@1 68.000 (59.050)   Prec@5 96.000 (95.109)   [2025-10-23 00:35:25]
  Epoch: [009][200/500]   Time 0.009 (0.100)   Data 0.000 (0.089)   Loss 1.0441 (1.1554)   Prec@1 64.000 (59.104)   Prec@5 98.000 (95.129)   [2025-10-23 00:35:26]
  Epoch: [009][300/500]   Time 0.010 (0.070)   Data 0.000 (0.059)   Loss 1.1489 (1.1510)   Prec@1 61.000 (59.498)   Prec@5 96.000 (95.229)   [2025-10-23 00:35:27]
  Epoch: [009][400/500]   Time 0.011 (0.055)   Data 0.000 (0.045)   Loss 1.2049 (1.1448)   Prec@1 52.000 (59.608)   Prec@5 96.000 (95.342)   [2025-10-23 00:35:28]
  **Train** Prec@1 59.518 Prec@5 95.358 Error@1 40.482
  **Test** Prec@1 67.410 Prec@5 97.160 Error@1 32.590
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:35:49] [Epoch=010/040] [Need: 00:21:13] [LR=0.0100] [Best : Accuracy=67.41, Error=32.59]
  Epoch: [010][000/500]   Time 17.763 (17.763)   Data 17.716 (17.716)   Loss 1.0870 (1.0870)   Prec@1 64.000 (64.000)   Prec@5 96.000 (96.000)   [2025-10-23 00:36:06]
  Epoch: [010][100/500]   Time 0.011 (0.189)   Data 0.000 (0.176)   Loss 1.0820 (1.1282)   Prec@1 62.000 (60.950)   Prec@5 95.000 (95.327)   [2025-10-23 00:36:08]
  Epoch: [010][200/500]   Time 0.010 (0.100)   Data 0.000 (0.088)   Loss 1.1802 (1.1332)   Prec@1 63.000 (60.677)   Prec@5 94.000 (95.259)   [2025-10-23 00:36:09]
  Epoch: [010][300/500]   Time 0.009 (0.070)   Data 0.000 (0.059)   Loss 1.2142 (1.1329)   Prec@1 57.000 (60.236)   Prec@5 95.000 (95.369)   [2025-10-23 00:36:10]
  Epoch: [010][400/500]   Time 0.011 (0.055)   Data 0.000 (0.044)   Loss 1.0593 (1.1327)   Prec@1 63.000 (60.175)   Prec@5 98.000 (95.444)   [2025-10-23 00:36:11]
  **Train** Prec@1 60.234 Prec@5 95.480 Error@1 39.766
  **Test** Prec@1 68.620 Prec@5 97.230 Error@1 31.380
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:36:31] [Epoch=011/040] [Need: 00:20:31] [LR=0.0100] [Best : Accuracy=68.62, Error=31.38]
  Epoch: [011][000/500]   Time 17.728 (17.728)   Data 17.682 (17.682)   Loss 1.0598 (1.0598)   Prec@1 62.000 (62.000)   Prec@5 92.000 (92.000)   [2025-10-23 00:36:49]
  Epoch: [011][100/500]   Time 0.013 (0.189)   Data 0.000 (0.175)   Loss 1.2470 (1.1149)   Prec@1 54.000 (60.673)   Prec@5 95.000 (95.693)   [2025-10-23 00:36:50]
  Epoch: [011][200/500]   Time 0.010 (0.100)   Data 0.000 (0.088)   Loss 1.3212 (1.1220)   Prec@1 51.000 (60.542)   Prec@5 93.000 (95.612)   [2025-10-23 00:36:51]
  Epoch: [011][300/500]   Time 0.011 (0.070)   Data 0.000 (0.059)   Loss 1.1144 (1.1143)   Prec@1 55.000 (60.844)   Prec@5 96.000 (95.664)   [2025-10-23 00:36:52]
  Epoch: [011][400/500]   Time 0.012 (0.055)   Data 0.000 (0.044)   Loss 1.3291 (1.1120)   Prec@1 48.000 (60.823)   Prec@5 93.000 (95.648)   [2025-10-23 00:36:53]
  **Train** Prec@1 60.948 Prec@5 95.688 Error@1 39.052
  **Test** Prec@1 67.970 Prec@5 97.190 Error@1 32.030

==>>[2025-10-23 00:37:13] [Epoch=012/040] [Need: 00:19:47] [LR=0.0100] [Best : Accuracy=68.62, Error=31.38]
  Epoch: [012][000/500]   Time 17.479 (17.479)   Data 17.434 (17.434)   Loss 1.1554 (1.1554)   Prec@1 58.000 (58.000)   Prec@5 96.000 (96.000)   [2025-10-23 00:37:31]
  Epoch: [012][100/500]   Time 0.010 (0.185)   Data 0.000 (0.173)   Loss 1.1332 (1.0850)   Prec@1 61.000 (61.782)   Prec@5 92.000 (95.762)   [2025-10-23 00:37:32]
  Epoch: [012][200/500]   Time 0.010 (0.098)   Data 0.000 (0.087)   Loss 0.9670 (1.0884)   Prec@1 63.000 (61.537)   Prec@5 98.000 (95.796)   [2025-10-23 00:37:33]
  Epoch: [012][300/500]   Time 0.009 (0.069)   Data 0.000 (0.058)   Loss 0.9423 (1.0924)   Prec@1 67.000 (61.472)   Prec@5 95.000 (95.771)   [2025-10-23 00:37:34]
  Epoch: [012][400/500]   Time 0.008 (0.054)   Data 0.000 (0.044)   Loss 1.1449 (1.0969)   Prec@1 57.000 (61.349)   Prec@5 97.000 (95.771)   [2025-10-23 00:37:35]
  **Train** Prec@1 61.490 Prec@5 95.806 Error@1 38.510
  **Test** Prec@1 68.620 Prec@5 97.330 Error@1 31.380
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:37:55] [Epoch=013/040] [Need: 00:19:04] [LR=0.0100] [Best : Accuracy=68.62, Error=31.38]
  Epoch: [013][000/500]   Time 17.758 (17.758)   Data 17.714 (17.714)   Loss 1.1175 (1.1175)   Prec@1 63.000 (63.000)   Prec@5 94.000 (94.000)   [2025-10-23 00:38:13]
  Epoch: [013][100/500]   Time 0.012 (0.189)   Data 0.000 (0.176)   Loss 1.2299 (1.0755)   Prec@1 53.000 (61.802)   Prec@5 94.000 (96.059)   [2025-10-23 00:38:14]
  Epoch: [013][200/500]   Time 0.011 (0.100)   Data 0.000 (0.088)   Loss 1.0082 (1.0805)   Prec@1 61.000 (61.438)   Prec@5 98.000 (95.910)   [2025-10-23 00:38:15]
  Epoch: [013][300/500]   Time 0.010 (0.070)   Data 0.001 (0.059)   Loss 1.0944 (1.0867)   Prec@1 59.000 (61.508)   Prec@5 98.000 (95.827)   [2025-10-23 00:38:17]
  Epoch: [013][400/500]   Time 0.010 (0.055)   Data 0.001 (0.044)   Loss 1.1827 (1.0822)   Prec@1 61.000 (61.688)   Prec@5 95.000 (95.830)   [2025-10-23 00:38:18]
  **Train** Prec@1 61.854 Prec@5 95.822 Error@1 38.146
  **Test** Prec@1 69.700 Prec@5 97.520 Error@1 30.300
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:38:37] [Epoch=014/040] [Need: 00:18:21] [LR=0.0100] [Best : Accuracy=69.70, Error=30.30]
  Epoch: [014][000/500]   Time 17.535 (17.535)   Data 17.489 (17.489)   Loss 1.0486 (1.0486)   Prec@1 63.000 (63.000)   Prec@5 96.000 (96.000)   [2025-10-23 00:38:55]
  Epoch: [014][100/500]   Time 0.010 (0.187)   Data 0.000 (0.173)   Loss 1.0477 (1.0622)   Prec@1 63.000 (63.000)   Prec@5 99.000 (96.416)   [2025-10-23 00:38:56]
  Epoch: [014][200/500]   Time 0.009 (0.099)   Data 0.000 (0.087)   Loss 0.9407 (1.0749)   Prec@1 68.000 (62.274)   Prec@5 97.000 (96.154)   [2025-10-23 00:38:57]
  Epoch: [014][300/500]   Time 0.011 (0.070)   Data 0.000 (0.058)   Loss 1.0008 (1.0713)   Prec@1 64.000 (62.259)   Prec@5 96.000 (96.090)   [2025-10-23 00:38:58]
  Epoch: [014][400/500]   Time 0.012 (0.055)   Data 0.000 (0.044)   Loss 0.9127 (1.0739)   Prec@1 67.000 (62.155)   Prec@5 98.000 (95.960)   [2025-10-23 00:38:59]
  **Train** Prec@1 62.186 Prec@5 95.994 Error@1 37.814
  **Test** Prec@1 70.230 Prec@5 97.620 Error@1 29.770
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:39:19] [Epoch=015/040] [Need: 00:17:38] [LR=0.0100] [Best : Accuracy=70.23, Error=29.77]
  Epoch: [015][000/500]   Time 17.468 (17.468)   Data 17.422 (17.422)   Loss 1.0597 (1.0597)   Prec@1 61.000 (61.000)   Prec@5 100.000 (100.000)   [2025-10-23 00:39:37]
  Epoch: [015][100/500]   Time 0.013 (0.185)   Data 0.000 (0.173)   Loss 1.1724 (1.0680)   Prec@1 60.000 (62.356)   Prec@5 93.000 (96.178)   [2025-10-23 00:39:38]
  Epoch: [015][200/500]   Time 0.011 (0.098)   Data 0.000 (0.087)   Loss 1.2106 (1.0684)   Prec@1 61.000 (62.234)   Prec@5 92.000 (96.174)   [2025-10-23 00:39:39]
  Epoch: [015][300/500]   Time 0.009 (0.069)   Data 0.000 (0.058)   Loss 1.0724 (1.0648)   Prec@1 67.000 (62.409)   Prec@5 91.000 (96.120)   [2025-10-23 00:39:40]
  Epoch: [015][400/500]   Time 0.012 (0.055)   Data 0.000 (0.044)   Loss 1.0440 (1.0631)   Prec@1 66.000 (62.571)   Prec@5 97.000 (96.145)   [2025-10-23 00:39:41]
  **Train** Prec@1 62.522 Prec@5 96.178 Error@1 37.478
  **Test** Prec@1 71.170 Prec@5 97.520 Error@1 28.830
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:40:02] [Epoch=016/040] [Need: 00:16:56] [LR=0.0100] [Best : Accuracy=71.17, Error=28.83]
  Epoch: [016][000/500]   Time 17.924 (17.924)   Data 17.880 (17.880)   Loss 1.0168 (1.0168)   Prec@1 53.000 (53.000)   Prec@5 97.000 (97.000)   [2025-10-23 00:40:20]
  Epoch: [016][100/500]   Time 0.008 (0.191)   Data 0.000 (0.177)   Loss 1.3517 (1.0646)   Prec@1 57.000 (62.584)   Prec@5 92.000 (95.871)   [2025-10-23 00:40:21]
  Epoch: [016][200/500]   Time 0.011 (0.101)   Data 0.000 (0.089)   Loss 0.9774 (1.0524)   Prec@1 68.000 (62.881)   Prec@5 97.000 (96.109)   [2025-10-23 00:40:22]
  Epoch: [016][300/500]   Time 0.009 (0.071)   Data 0.000 (0.060)   Loss 0.9923 (1.0504)   Prec@1 66.000 (62.860)   Prec@5 96.000 (96.193)   [2025-10-23 00:40:23]
  Epoch: [016][400/500]   Time 0.010 (0.056)   Data 0.000 (0.045)   Loss 1.0283 (1.0505)   Prec@1 66.000 (63.045)   Prec@5 98.000 (96.157)   [2025-10-23 00:40:24]
  **Train** Prec@1 63.110 Prec@5 96.210 Error@1 36.890
  **Test** Prec@1 71.700 Prec@5 97.770 Error@1 28.300
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:40:46] [Epoch=017/040] [Need: 00:16:16] [LR=0.0100] [Best : Accuracy=71.70, Error=28.30]
  Epoch: [017][000/500]   Time 17.866 (17.866)   Data 17.821 (17.821)   Loss 1.1280 (1.1280)   Prec@1 63.000 (63.000)   Prec@5 95.000 (95.000)   [2025-10-23 00:41:04]
  Epoch: [017][100/500]   Time 0.009 (0.189)   Data 0.000 (0.177)   Loss 1.0640 (1.0500)   Prec@1 65.000 (63.238)   Prec@5 95.000 (96.040)   [2025-10-23 00:41:05]
  Epoch: [017][200/500]   Time 0.014 (0.100)   Data 0.000 (0.089)   Loss 1.0234 (1.0490)   Prec@1 63.000 (63.139)   Prec@5 96.000 (96.080)   [2025-10-23 00:41:06]
  Epoch: [017][300/500]   Time 0.012 (0.070)   Data 0.001 (0.059)   Loss 1.1298 (1.0457)   Prec@1 60.000 (63.282)   Prec@5 96.000 (96.140)   [2025-10-23 00:41:07]
  Epoch: [017][400/500]   Time 0.009 (0.055)   Data 0.000 (0.045)   Loss 0.8667 (1.0417)   Prec@1 70.000 (63.389)   Prec@5 96.000 (96.237)   [2025-10-23 00:41:08]
  **Train** Prec@1 63.440 Prec@5 96.274 Error@1 36.560
  **Test** Prec@1 71.880 Prec@5 97.750 Error@1 28.120
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:41:29] [Epoch=018/040] [Need: 00:15:34] [LR=0.0100] [Best : Accuracy=71.88, Error=28.12]
  Epoch: [018][000/500]   Time 17.643 (17.643)   Data 17.598 (17.598)   Loss 1.0287 (1.0287)   Prec@1 64.000 (64.000)   Prec@5 98.000 (98.000)   [2025-10-23 00:41:46]
  Epoch: [018][100/500]   Time 0.010 (0.187)   Data 0.000 (0.174)   Loss 0.9662 (1.0402)   Prec@1 64.000 (63.257)   Prec@5 97.000 (96.020)   [2025-10-23 00:41:47]
  Epoch: [018][200/500]   Time 0.010 (0.099)   Data 0.000 (0.088)   Loss 1.0139 (1.0394)   Prec@1 68.000 (63.224)   Prec@5 95.000 (96.045)   [2025-10-23 00:41:48]
  Epoch: [018][300/500]   Time 0.012 (0.070)   Data 0.001 (0.059)   Loss 1.0750 (1.0355)   Prec@1 69.000 (63.542)   Prec@5 95.000 (96.133)   [2025-10-23 00:41:50]
  Epoch: [018][400/500]   Time 0.014 (0.055)   Data 0.000 (0.044)   Loss 1.0219 (1.0374)   Prec@1 62.000 (63.591)   Prec@5 98.000 (96.137)   [2025-10-23 00:41:51]
  **Train** Prec@1 63.602 Prec@5 96.152 Error@1 36.398
  **Test** Prec@1 71.800 Prec@5 97.980 Error@1 28.200

==>>[2025-10-23 00:42:37] [Epoch=019/040] [Need: 00:15:20] [LR=0.0100] [Best : Accuracy=71.88, Error=28.12]
  Epoch: [019][000/500]   Time 19.965 (19.965)   Data 19.920 (19.920)   Loss 0.9093 (0.9093)   Prec@1 74.000 (74.000)   Prec@5 99.000 (99.000)   [2025-10-23 00:42:57]
  Epoch: [019][100/500]   Time 0.010 (0.211)   Data 0.000 (0.197)   Loss 1.0653 (1.0240)   Prec@1 58.000 (64.277)   Prec@5 98.000 (96.446)   [2025-10-23 00:42:58]
  Epoch: [019][200/500]   Time 0.011 (0.112)   Data 0.000 (0.099)   Loss 1.0149 (1.0294)   Prec@1 63.000 (63.955)   Prec@5 98.000 (96.284)   [2025-10-23 00:42:59]
  Epoch: [019][300/500]   Time 0.011 (0.078)   Data 0.000 (0.066)   Loss 1.1619 (1.0309)   Prec@1 56.000 (64.017)   Prec@5 97.000 (96.322)   [2025-10-23 00:43:01]
  Epoch: [019][400/500]   Time 0.011 (0.062)   Data 0.000 (0.050)   Loss 0.9911 (1.0308)   Prec@1 65.000 (63.995)   Prec@5 98.000 (96.349)   [2025-10-23 00:43:02]
  **Train** Prec@1 64.028 Prec@5 96.286 Error@1 35.972
  **Test** Prec@1 72.390 Prec@5 97.890 Error@1 27.610
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:43:22] [Epoch=020/040] [Need: 00:14:37] [LR=0.0100] [Best : Accuracy=72.39, Error=27.61]
  Epoch: [020][000/500]   Time 17.613 (17.613)   Data 17.568 (17.568)   Loss 0.9499 (0.9499)   Prec@1 74.000 (74.000)   Prec@5 97.000 (97.000)   [2025-10-23 00:43:40]
  Epoch: [020][100/500]   Time 0.010 (0.187)   Data 0.000 (0.174)   Loss 1.1889 (1.0083)   Prec@1 60.000 (64.703)   Prec@5 94.000 (96.307)   [2025-10-23 00:43:41]
  Epoch: [020][200/500]   Time 0.012 (0.099)   Data 0.000 (0.088)   Loss 1.1938 (1.0291)   Prec@1 52.000 (63.811)   Prec@5 97.000 (96.323)   [2025-10-23 00:43:42]
  Epoch: [020][300/500]   Time 0.010 (0.070)   Data 0.000 (0.059)   Loss 0.9293 (1.0253)   Prec@1 66.000 (64.037)   Prec@5 98.000 (96.329)   [2025-10-23 00:43:43]
  Epoch: [020][400/500]   Time 0.012 (0.055)   Data 0.000 (0.044)   Loss 1.0576 (1.0265)   Prec@1 63.000 (64.005)   Prec@5 96.000 (96.334)   [2025-10-23 00:43:44]
  **Train** Prec@1 64.072 Prec@5 96.364 Error@1 35.928
  **Test** Prec@1 71.810 Prec@5 97.530 Error@1 28.190

==>>[2025-10-23 00:44:04] [Epoch=021/040] [Need: 00:13:51] [LR=0.0100] [Best : Accuracy=72.39, Error=27.61]
  Epoch: [021][000/500]   Time 17.556 (17.556)   Data 17.510 (17.510)   Loss 1.2055 (1.2055)   Prec@1 61.000 (61.000)   Prec@5 96.000 (96.000)   [2025-10-23 00:44:21]
  Epoch: [021][100/500]   Time 0.008 (0.187)   Data 0.000 (0.174)   Loss 0.8874 (1.0092)   Prec@1 74.000 (64.178)   Prec@5 98.000 (96.822)   [2025-10-23 00:44:23]
  Epoch: [021][200/500]   Time 0.010 (0.099)   Data 0.000 (0.087)   Loss 1.0012 (1.0141)   Prec@1 64.000 (64.303)   Prec@5 96.000 (96.657)   [2025-10-23 00:44:24]
  Epoch: [021][300/500]   Time 0.012 (0.070)   Data 0.000 (0.058)   Loss 0.9736 (1.0137)   Prec@1 67.000 (64.449)   Prec@5 98.000 (96.714)   [2025-10-23 00:44:25]
  Epoch: [021][400/500]   Time 0.010 (0.055)   Data 0.000 (0.044)   Loss 1.1347 (1.0149)   Prec@1 56.000 (64.531)   Prec@5 94.000 (96.581)   [2025-10-23 00:44:26]
  **Train** Prec@1 64.490 Prec@5 96.482 Error@1 35.510
  **Test** Prec@1 71.990 Prec@5 97.850 Error@1 28.010

==>>[2025-10-23 00:44:46] [Epoch=022/040] [Need: 00:13:06] [LR=0.0100] [Best : Accuracy=72.39, Error=27.61]
  Epoch: [022][000/500]   Time 17.979 (17.979)   Data 17.933 (17.933)   Loss 0.8558 (0.8558)   Prec@1 69.000 (69.000)   Prec@5 97.000 (97.000)   [2025-10-23 00:45:04]
  Epoch: [022][100/500]   Time 0.008 (0.190)   Data 0.000 (0.178)   Loss 0.9826 (1.0159)   Prec@1 64.000 (64.535)   Prec@5 97.000 (96.535)   [2025-10-23 00:45:05]
  Epoch: [022][200/500]   Time 0.011 (0.101)   Data 0.001 (0.089)   Loss 1.0648 (1.0260)   Prec@1 59.000 (64.144)   Prec@5 98.000 (96.299)   [2025-10-23 00:45:06]
  Epoch: [022][300/500]   Time 0.012 (0.071)   Data 0.000 (0.060)   Loss 0.9685 (1.0218)   Prec@1 67.000 (64.336)   Prec@5 98.000 (96.329)   [2025-10-23 00:45:07]
  Epoch: [022][400/500]   Time 0.009 (0.056)   Data 0.000 (0.045)   Loss 1.0100 (1.0186)   Prec@1 64.000 (64.254)   Prec@5 97.000 (96.444)   [2025-10-23 00:45:08]
  **Train** Prec@1 64.206 Prec@5 96.474 Error@1 35.794
  **Test** Prec@1 71.790 Prec@5 97.730 Error@1 28.210

==>>[2025-10-23 00:45:27] [Epoch=023/040] [Need: 00:12:21] [LR=0.0100] [Best : Accuracy=72.39, Error=27.61]
  Epoch: [023][000/500]   Time 18.268 (18.268)   Data 18.223 (18.223)   Loss 0.8329 (0.8329)   Prec@1 70.000 (70.000)   Prec@5 96.000 (96.000)   [2025-10-23 00:45:46]
  Epoch: [023][100/500]   Time 0.010 (0.193)   Data 0.000 (0.181)   Loss 1.1585 (1.0082)   Prec@1 57.000 (64.089)   Prec@5 98.000 (96.525)   [2025-10-23 00:45:47]
  Epoch: [023][200/500]   Time 0.010 (0.102)   Data 0.001 (0.091)   Loss 0.9467 (1.0110)   Prec@1 68.000 (64.383)   Prec@5 99.000 (96.428)   [2025-10-23 00:45:48]
  Epoch: [023][300/500]   Time 0.010 (0.072)   Data 0.001 (0.061)   Loss 0.9824 (1.0061)   Prec@1 67.000 (64.827)   Prec@5 95.000 (96.429)   [2025-10-23 00:45:49]
  Epoch: [023][400/500]   Time 0.009 (0.057)   Data 0.000 (0.046)   Loss 0.9589 (1.0065)   Prec@1 68.000 (64.843)   Prec@5 95.000 (96.504)   [2025-10-23 00:45:50]
  **Train** Prec@1 64.748 Prec@5 96.466 Error@1 35.252
  **Test** Prec@1 73.290 Prec@5 98.010 Error@1 26.710
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:46:10] [Epoch=024/040] [Need: 00:11:37] [LR=0.0100] [Best : Accuracy=73.29, Error=26.71]
  Epoch: [024][000/500]   Time 18.144 (18.144)   Data 18.098 (18.098)   Loss 1.1555 (1.1555)   Prec@1 62.000 (62.000)   Prec@5 93.000 (93.000)   [2025-10-23 00:46:28]
  Epoch: [024][100/500]   Time 0.013 (0.194)   Data 0.000 (0.179)   Loss 1.1560 (1.0015)   Prec@1 60.000 (64.782)   Prec@5 94.000 (96.545)   [2025-10-23 00:46:29]
  Epoch: [024][200/500]   Time 0.010 (0.103)   Data 0.000 (0.090)   Loss 1.0560 (1.0107)   Prec@1 64.000 (64.542)   Prec@5 96.000 (96.328)   [2025-10-23 00:46:30]
  Epoch: [024][300/500]   Time 0.014 (0.072)   Data 0.000 (0.060)   Loss 1.0872 (1.0049)   Prec@1 60.000 (64.744)   Prec@5 95.000 (96.468)   [2025-10-23 00:46:32]
  Epoch: [024][400/500]   Time 0.010 (0.057)   Data 0.000 (0.045)   Loss 1.0184 (1.0048)   Prec@1 64.000 (64.648)   Prec@5 95.000 (96.491)   [2025-10-23 00:46:33]
  **Train** Prec@1 64.730 Prec@5 96.436 Error@1 35.270
  **Test** Prec@1 72.530 Prec@5 97.960 Error@1 27.470

==>>[2025-10-23 00:46:53] [Epoch=025/040] [Need: 00:10:53] [LR=0.0010] [Best : Accuracy=73.29, Error=26.71]
  Epoch: [025][000/500]   Time 17.566 (17.566)   Data 17.521 (17.521)   Loss 0.9670 (0.9670)   Prec@1 64.000 (64.000)   Prec@5 97.000 (97.000)   [2025-10-23 00:47:10]
  Epoch: [025][100/500]   Time 0.010 (0.187)   Data 0.000 (0.174)   Loss 0.9197 (0.9609)   Prec@1 65.000 (66.356)   Prec@5 98.000 (96.931)   [2025-10-23 00:47:12]
  Epoch: [025][200/500]   Time 0.012 (0.099)   Data 0.000 (0.087)   Loss 0.8131 (0.9479)   Prec@1 71.000 (66.796)   Prec@5 99.000 (96.970)   [2025-10-23 00:47:13]
  Epoch: [025][300/500]   Time 0.010 (0.070)   Data 0.000 (0.058)   Loss 0.8103 (0.9438)   Prec@1 71.000 (67.070)   Prec@5 97.000 (96.953)   [2025-10-23 00:47:14]
  Epoch: [025][400/500]   Time 0.010 (0.055)   Data 0.000 (0.044)   Loss 0.8297 (0.9389)   Prec@1 68.000 (67.165)   Prec@5 99.000 (97.005)   [2025-10-23 00:47:15]
  **Train** Prec@1 67.164 Prec@5 97.064 Error@1 32.836
  **Test** Prec@1 74.180 Prec@5 98.110 Error@1 25.820
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:47:34] [Epoch=026/040] [Need: 00:10:08] [LR=0.0010] [Best : Accuracy=74.18, Error=25.82]
  Epoch: [026][000/500]   Time 17.859 (17.859)   Data 17.815 (17.815)   Loss 0.8998 (0.8998)   Prec@1 64.000 (64.000)   Prec@5 99.000 (99.000)   [2025-10-23 00:47:52]
  Epoch: [026][100/500]   Time 0.011 (0.189)   Data 0.000 (0.177)   Loss 0.8371 (0.9217)   Prec@1 72.000 (67.455)   Prec@5 98.000 (97.396)   [2025-10-23 00:47:53]
  Epoch: [026][200/500]   Time 0.010 (0.100)   Data 0.000 (0.089)   Loss 0.8469 (0.9192)   Prec@1 68.000 (67.677)   Prec@5 99.000 (97.259)   [2025-10-23 00:47:55]
  Epoch: [026][300/500]   Time 0.009 (0.070)   Data 0.000 (0.059)   Loss 1.0169 (0.9212)   Prec@1 66.000 (67.578)   Prec@5 98.000 (97.252)   [2025-10-23 00:47:56]
  Epoch: [026][400/500]   Time 0.011 (0.055)   Data 0.001 (0.045)   Loss 0.8569 (0.9210)   Prec@1 71.000 (67.633)   Prec@5 96.000 (97.244)   [2025-10-23 00:47:57]
  **Train** Prec@1 67.728 Prec@5 97.162 Error@1 32.272
  **Test** Prec@1 74.630 Prec@5 98.180 Error@1 25.370
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:48:16] [Epoch=027/040] [Need: 00:09:24] [LR=0.0010] [Best : Accuracy=74.63, Error=25.37]
  Epoch: [027][000/500]   Time 17.673 (17.673)   Data 17.630 (17.630)   Loss 0.8112 (0.8112)   Prec@1 69.000 (69.000)   Prec@5 99.000 (99.000)   [2025-10-23 00:48:34]
  Epoch: [027][100/500]   Time 0.010 (0.188)   Data 0.000 (0.175)   Loss 0.9211 (0.9055)   Prec@1 70.000 (68.059)   Prec@5 98.000 (97.208)   [2025-10-23 00:48:35]
  Epoch: [027][200/500]   Time 0.009 (0.099)   Data 0.000 (0.088)   Loss 0.7588 (0.9100)   Prec@1 75.000 (68.294)   Prec@5 100.000 (97.164)   [2025-10-23 00:48:36]
  Epoch: [027][300/500]   Time 0.010 (0.070)   Data 0.001 (0.059)   Loss 0.8890 (0.9117)   Prec@1 69.000 (68.296)   Prec@5 97.000 (97.093)   [2025-10-23 00:48:38]
  Epoch: [027][400/500]   Time 0.010 (0.055)   Data 0.000 (0.044)   Loss 0.9917 (0.9109)   Prec@1 65.000 (68.257)   Prec@5 96.000 (97.157)   [2025-10-23 00:48:39]
  **Train** Prec@1 68.188 Prec@5 97.138 Error@1 31.812
  **Test** Prec@1 74.680 Prec@5 98.210 Error@1 25.320
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:48:58] [Epoch=028/040] [Need: 00:08:40] [LR=0.0010] [Best : Accuracy=74.68, Error=25.32]
  Epoch: [028][000/500]   Time 17.660 (17.660)   Data 17.613 (17.613)   Loss 0.9714 (0.9714)   Prec@1 66.000 (66.000)   Prec@5 95.000 (95.000)   [2025-10-23 00:49:16]
  Epoch: [028][100/500]   Time 0.009 (0.187)   Data 0.000 (0.175)   Loss 0.9210 (0.9127)   Prec@1 70.000 (68.505)   Prec@5 96.000 (96.782)   [2025-10-23 00:49:17]
  Epoch: [028][200/500]   Time 0.010 (0.099)   Data 0.000 (0.088)   Loss 1.1081 (0.9109)   Prec@1 64.000 (68.522)   Prec@5 97.000 (96.896)   [2025-10-23 00:49:18]
  Epoch: [028][300/500]   Time 0.010 (0.070)   Data 0.000 (0.059)   Loss 0.9550 (0.9125)   Prec@1 66.000 (68.233)   Prec@5 98.000 (97.050)   [2025-10-23 00:49:19]
  Epoch: [028][400/500]   Time 0.009 (0.055)   Data 0.000 (0.044)   Loss 0.6411 (0.9140)   Prec@1 77.000 (68.155)   Prec@5 99.000 (97.095)   [2025-10-23 00:49:20]
  **Train** Prec@1 68.120 Prec@5 97.080 Error@1 31.880
  **Test** Prec@1 74.550 Prec@5 98.220 Error@1 25.450

==>>[2025-10-23 00:49:40] [Epoch=029/040] [Need: 00:07:56] [LR=0.0010] [Best : Accuracy=74.68, Error=25.32]
  Epoch: [029][000/500]   Time 18.419 (18.419)   Data 18.374 (18.374)   Loss 0.7478 (0.7478)   Prec@1 73.000 (73.000)   Prec@5 100.000 (100.000)   [2025-10-23 00:49:59]
  Epoch: [029][100/500]   Time 0.011 (0.195)   Data 0.001 (0.182)   Loss 0.8920 (0.9066)   Prec@1 70.000 (68.406)   Prec@5 97.000 (97.198)   [2025-10-23 00:50:00]
  Epoch: [029][200/500]   Time 0.012 (0.103)   Data 0.000 (0.092)   Loss 0.9235 (0.9102)   Prec@1 69.000 (68.448)   Prec@5 96.000 (97.234)   [2025-10-23 00:50:01]
  Epoch: [029][300/500]   Time 0.014 (0.073)   Data 0.000 (0.061)   Loss 0.9072 (0.9127)   Prec@1 69.000 (68.346)   Prec@5 97.000 (97.120)   [2025-10-23 00:50:02]
  Epoch: [029][400/500]   Time 0.012 (0.057)   Data 0.000 (0.046)   Loss 0.8362 (0.9118)   Prec@1 70.000 (68.364)   Prec@5 98.000 (97.187)   [2025-10-23 00:50:03]
  **Train** Prec@1 68.360 Prec@5 97.202 Error@1 31.640
  **Test** Prec@1 74.730 Prec@5 98.150 Error@1 25.270
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:50:23] [Epoch=030/040] [Need: 00:07:13] [LR=0.0010] [Best : Accuracy=74.73, Error=25.27]
  Epoch: [030][000/500]   Time 18.178 (18.178)   Data 18.133 (18.133)   Loss 1.0626 (1.0626)   Prec@1 66.000 (66.000)   Prec@5 94.000 (94.000)   [2025-10-23 00:50:42]
  Epoch: [030][100/500]   Time 0.013 (0.194)   Data 0.001 (0.180)   Loss 0.8335 (0.9017)   Prec@1 71.000 (68.921)   Prec@5 98.000 (97.248)   [2025-10-23 00:50:43]
  Epoch: [030][200/500]   Time 0.009 (0.103)   Data 0.000 (0.090)   Loss 0.9174 (0.8980)   Prec@1 73.000 (69.114)   Prec@5 96.000 (97.279)   [2025-10-23 00:50:44]
  Epoch: [030][300/500]   Time 0.009 (0.072)   Data 0.001 (0.060)   Loss 0.8319 (0.8998)   Prec@1 73.000 (68.874)   Prec@5 96.000 (97.332)   [2025-10-23 00:50:45]
  Epoch: [030][400/500]   Time 0.013 (0.057)   Data 0.000 (0.045)   Loss 0.9045 (0.9044)   Prec@1 70.000 (68.586)   Prec@5 96.000 (97.294)   [2025-10-23 00:50:46]
  **Train** Prec@1 68.596 Prec@5 97.290 Error@1 31.404
  **Test** Prec@1 74.690 Prec@5 98.080 Error@1 25.310

==>>[2025-10-23 00:51:06] [Epoch=031/040] [Need: 00:06:29] [LR=0.0010] [Best : Accuracy=74.73, Error=25.27]
  Epoch: [031][000/500]   Time 18.425 (18.425)   Data 18.381 (18.381)   Loss 0.8113 (0.8113)   Prec@1 73.000 (73.000)   Prec@5 99.000 (99.000)   [2025-10-23 00:51:24]
  Epoch: [031][100/500]   Time 0.011 (0.196)   Data 0.000 (0.182)   Loss 0.9200 (0.8974)   Prec@1 68.000 (68.545)   Prec@5 98.000 (97.356)   [2025-10-23 00:51:26]
  Epoch: [031][200/500]   Time 0.009 (0.104)   Data 0.000 (0.092)   Loss 0.9871 (0.9009)   Prec@1 59.000 (68.294)   Prec@5 97.000 (97.433)   [2025-10-23 00:51:27]
  Epoch: [031][300/500]   Time 0.010 (0.073)   Data 0.000 (0.061)   Loss 0.8193 (0.8924)   Prec@1 72.000 (68.754)   Prec@5 99.000 (97.445)   [2025-10-23 00:51:28]
  Epoch: [031][400/500]   Time 0.013 (0.057)   Data 0.001 (0.046)   Loss 1.0818 (0.8911)   Prec@1 59.000 (68.878)   Prec@5 95.000 (97.446)   [2025-10-23 00:51:29]
  **Train** Prec@1 68.658 Prec@5 97.318 Error@1 31.342
  **Test** Prec@1 74.950 Prec@5 98.280 Error@1 25.050
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:51:49] [Epoch=032/040] [Need: 00:05:46] [LR=0.0010] [Best : Accuracy=74.95, Error=25.05]
  Epoch: [032][000/500]   Time 17.808 (17.808)   Data 17.761 (17.761)   Loss 0.8464 (0.8464)   Prec@1 68.000 (68.000)   Prec@5 98.000 (98.000)   [2025-10-23 00:52:06]
  Epoch: [032][100/500]   Time 0.012 (0.189)   Data 0.000 (0.176)   Loss 0.7861 (0.9186)   Prec@1 72.000 (67.713)   Prec@5 100.000 (97.149)   [2025-10-23 00:52:08]
  Epoch: [032][200/500]   Time 0.009 (0.100)   Data 0.000 (0.089)   Loss 0.9832 (0.9098)   Prec@1 65.000 (68.204)   Prec@5 93.000 (97.184)   [2025-10-23 00:52:09]
  Epoch: [032][300/500]   Time 0.011 (0.070)   Data 0.000 (0.059)   Loss 0.8157 (0.9017)   Prec@1 67.000 (68.412)   Prec@5 99.000 (97.243)   [2025-10-23 00:52:10]
  Epoch: [032][400/500]   Time 0.010 (0.055)   Data 0.001 (0.044)   Loss 0.8422 (0.9013)   Prec@1 73.000 (68.486)   Prec@5 95.000 (97.204)   [2025-10-23 00:52:11]
  **Train** Prec@1 68.522 Prec@5 97.242 Error@1 31.478
  **Test** Prec@1 74.890 Prec@5 98.300 Error@1 25.110

==>>[2025-10-23 00:52:30] [Epoch=033/040] [Need: 00:05:02] [LR=0.0010] [Best : Accuracy=74.95, Error=25.05]
  Epoch: [033][000/500]   Time 17.545 (17.545)   Data 17.498 (17.498)   Loss 0.7933 (0.7933)   Prec@1 72.000 (72.000)   Prec@5 99.000 (99.000)   [2025-10-23 00:52:48]
  Epoch: [033][100/500]   Time 0.010 (0.187)   Data 0.000 (0.173)   Loss 0.9317 (0.9103)   Prec@1 72.000 (67.970)   Prec@5 96.000 (97.257)   [2025-10-23 00:52:49]
  Epoch: [033][200/500]   Time 0.009 (0.099)   Data 0.000 (0.087)   Loss 0.7569 (0.9011)   Prec@1 72.000 (68.433)   Prec@5 99.000 (97.234)   [2025-10-23 00:52:50]
  Epoch: [033][300/500]   Time 0.011 (0.070)   Data 0.000 (0.058)   Loss 0.8994 (0.9063)   Prec@1 69.000 (68.213)   Prec@5 97.000 (97.236)   [2025-10-23 00:52:51]
  Epoch: [033][400/500]   Time 0.009 (0.055)   Data 0.000 (0.044)   Loss 1.0406 (0.9039)   Prec@1 58.000 (68.262)   Prec@5 95.000 (97.262)   [2025-10-23 00:52:52]
  **Train** Prec@1 68.466 Prec@5 97.334 Error@1 31.534
  **Test** Prec@1 74.930 Prec@5 98.230 Error@1 25.070

==>>[2025-10-23 00:53:12] [Epoch=034/040] [Need: 00:04:19] [LR=0.0010] [Best : Accuracy=74.95, Error=25.05]
  Epoch: [034][000/500]   Time 17.433 (17.433)   Data 17.389 (17.389)   Loss 0.8193 (0.8193)   Prec@1 69.000 (69.000)   Prec@5 98.000 (98.000)   [2025-10-23 00:53:30]
  Epoch: [034][100/500]   Time 0.010 (0.184)   Data 0.000 (0.172)   Loss 0.8456 (0.8827)   Prec@1 68.000 (69.218)   Prec@5 99.000 (97.396)   [2025-10-23 00:53:31]
  Epoch: [034][200/500]   Time 0.009 (0.098)   Data 0.000 (0.087)   Loss 0.8511 (0.8903)   Prec@1 70.000 (68.766)   Prec@5 99.000 (97.378)   [2025-10-23 00:53:32]
  Epoch: [034][300/500]   Time 0.009 (0.069)   Data 0.000 (0.058)   Loss 0.9893 (0.8914)   Prec@1 61.000 (69.013)   Prec@5 98.000 (97.322)   [2025-10-23 00:53:33]
  Epoch: [034][400/500]   Time 0.012 (0.054)   Data 0.000 (0.044)   Loss 0.8702 (0.8963)   Prec@1 77.000 (68.875)   Prec@5 98.000 (97.304)   [2025-10-23 00:53:34]
  **Train** Prec@1 68.808 Prec@5 97.290 Error@1 31.192
  **Test** Prec@1 75.060 Prec@5 98.360 Error@1 24.940
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:53:54] [Epoch=035/040] [Need: 00:03:35] [LR=0.0010] [Best : Accuracy=75.06, Error=24.94]
  Epoch: [035][000/500]   Time 17.655 (17.655)   Data 17.610 (17.610)   Loss 0.7469 (0.7469)   Prec@1 77.000 (77.000)   Prec@5 99.000 (99.000)   [2025-10-23 00:54:11]
  Epoch: [035][100/500]   Time 0.010 (0.187)   Data 0.000 (0.175)   Loss 0.7362 (0.8971)   Prec@1 72.000 (68.762)   Prec@5 99.000 (97.109)   [2025-10-23 00:54:13]
  Epoch: [035][200/500]   Time 0.012 (0.099)   Data 0.000 (0.088)   Loss 0.7964 (0.8937)   Prec@1 74.000 (68.975)   Prec@5 98.000 (97.199)   [2025-10-23 00:54:14]
  Epoch: [035][300/500]   Time 0.010 (0.070)   Data 0.000 (0.059)   Loss 0.9656 (0.8932)   Prec@1 68.000 (68.864)   Prec@5 95.000 (97.219)   [2025-10-23 00:54:15]
  Epoch: [035][400/500]   Time 0.015 (0.055)   Data 0.000 (0.044)   Loss 1.0603 (0.8957)   Prec@1 65.000 (68.848)   Prec@5 96.000 (97.202)   [2025-10-23 00:54:16]
  **Train** Prec@1 68.816 Prec@5 97.212 Error@1 31.184
  **Test** Prec@1 75.450 Prec@5 98.430 Error@1 24.550
=> Obtain best accuracy, and update the best model

==>>[2025-10-23 00:54:35] [Epoch=036/040] [Need: 00:02:52] [LR=0.0010] [Best : Accuracy=75.45, Error=24.55]
  Epoch: [036][000/500]   Time 18.508 (18.508)   Data 18.463 (18.463)   Loss 0.8242 (0.8242)   Prec@1 70.000 (70.000)   Prec@5 98.000 (98.000)   [2025-10-23 00:54:54]
  Epoch: [036][100/500]   Time 0.010 (0.195)   Data 0.000 (0.183)   Loss 0.8120 (0.8870)   Prec@1 69.000 (68.881)   Prec@5 99.000 (97.426)   [2025-10-23 00:54:55]
  Epoch: [036][200/500]   Time 0.013 (0.103)   Data 0.001 (0.092)   Loss 1.0272 (0.8904)   Prec@1 57.000 (68.612)   Prec@5 98.000 (97.522)   [2025-10-23 00:54:56]
  Epoch: [036][300/500]   Time 0.011 (0.072)   Data 0.000 (0.061)   Loss 0.8917 (0.8981)   Prec@1 70.000 (68.478)   Prec@5 96.000 (97.382)   [2025-10-23 00:54:57]
  Epoch: [036][400/500]   Time 0.010 (0.057)   Data 0.000 (0.046)   Loss 0.7971 (0.8935)   Prec@1 72.000 (68.873)   Prec@5 97.000 (97.374)   [2025-10-23 00:54:58]
  **Train** Prec@1 68.834 Prec@5 97.370 Error@1 31.166
  **Test** Prec@1 75.220 Prec@5 98.270 Error@1 24.780

==>>[2025-10-23 00:55:18] [Epoch=037/040] [Need: 00:02:09] [LR=0.0010] [Best : Accuracy=75.45, Error=24.55]
  Epoch: [037][000/500]   Time 17.757 (17.757)   Data 17.712 (17.712)   Loss 0.8545 (0.8545)   Prec@1 68.000 (68.000)   Prec@5 98.000 (98.000)   [2025-10-23 00:55:36]
  Epoch: [037][100/500]   Time 0.009 (0.190)   Data 0.000 (0.176)   Loss 0.8582 (0.8990)   Prec@1 71.000 (68.624)   Prec@5 96.000 (97.317)   [2025-10-23 00:55:37]
  Epoch: [037][200/500]   Time 0.014 (0.101)   Data 0.000 (0.088)   Loss 0.9759 (0.8938)   Prec@1 61.000 (68.886)   Prec@5 99.000 (97.418)   [2025-10-23 00:55:38]
  Epoch: [037][300/500]   Time 0.011 (0.071)   Data 0.000 (0.059)   Loss 0.8330 (0.8848)   Prec@1 72.000 (69.216)   Prec@5 97.000 (97.495)   [2025-10-23 00:55:39]
  Epoch: [037][400/500]   Time 0.012 (0.056)   Data 0.000 (0.044)   Loss 0.9171 (0.8920)   Prec@1 66.000 (68.873)   Prec@5 99.000 (97.389)   [2025-10-23 00:55:40]
  **Train** Prec@1 68.858 Prec@5 97.376 Error@1 31.142
  **Test** Prec@1 74.940 Prec@5 98.230 Error@1 25.060

==>>[2025-10-23 00:56:00] [Epoch=038/040] [Need: 00:01:26] [LR=0.0010] [Best : Accuracy=75.45, Error=24.55]
  Epoch: [038][000/500]   Time 17.522 (17.522)   Data 17.477 (17.477)   Loss 0.8891 (0.8891)   Prec@1 66.000 (66.000)   Prec@5 97.000 (97.000)   [2025-10-23 00:56:18]
  Epoch: [038][100/500]   Time 0.013 (0.186)   Data 0.000 (0.173)   Loss 0.8664 (0.9002)   Prec@1 71.000 (69.099)   Prec@5 98.000 (97.228)   [2025-10-23 00:56:19]
  Epoch: [038][200/500]   Time 0.009 (0.099)   Data 0.000 (0.087)   Loss 0.8687 (0.8949)   Prec@1 72.000 (68.945)   Prec@5 97.000 (97.229)   [2025-10-23 00:56:20]
  Epoch: [038][300/500]   Time 0.011 (0.070)   Data 0.000 (0.058)   Loss 0.7649 (0.8888)   Prec@1 70.000 (69.033)   Prec@5 100.000 (97.382)   [2025-10-23 00:56:21]
  Epoch: [038][400/500]   Time 0.012 (0.055)   Data 0.000 (0.044)   Loss 0.7457 (0.8893)   Prec@1 72.000 (69.090)   Prec@5 99.000 (97.319)   [2025-10-23 00:56:22]
  **Train** Prec@1 68.994 Prec@5 97.332 Error@1 31.006
  **Test** Prec@1 75.110 Prec@5 98.330 Error@1 24.890

==>>[2025-10-23 00:56:42] [Epoch=039/040] [Need: 00:00:43] [LR=0.0010] [Best : Accuracy=75.45, Error=24.55]
  Epoch: [039][000/500]   Time 17.581 (17.581)   Data 17.537 (17.537)   Loss 1.0862 (1.0862)   Prec@1 57.000 (57.000)   Prec@5 95.000 (95.000)   [2025-10-23 00:57:00]
  Epoch: [039][100/500]   Time 0.010 (0.187)   Data 0.000 (0.174)   Loss 1.0456 (0.9041)   Prec@1 65.000 (68.505)   Prec@5 97.000 (97.050)   [2025-10-23 00:57:01]
  Epoch: [039][200/500]   Time 0.010 (0.099)   Data 0.001 (0.087)   Loss 0.7655 (0.8855)   Prec@1 73.000 (69.239)   Prec@5 100.000 (97.423)   [2025-10-23 00:57:02]
  Epoch: [039][300/500]   Time 0.010 (0.070)   Data 0.000 (0.058)   Loss 0.9162 (0.8846)   Prec@1 66.000 (69.289)   Prec@5 99.000 (97.395)   [2025-10-23 00:57:03]
  Epoch: [039][400/500]   Time 0.011 (0.055)   Data 0.000 (0.044)   Loss 0.8905 (0.8848)   Prec@1 69.000 (69.312)   Prec@5 100.000 (97.344)   [2025-10-23 00:57:04]
  **Train** Prec@1 69.282 Prec@5 97.318 Error@1 30.718
  **Test** Prec@1 75.510 Prec@5 98.340 Error@1 24.490
=> Obtain best accuracy, and update the best model
